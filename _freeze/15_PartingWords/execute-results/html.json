{
  "hash": "45303d44cb120c04959690b8e707ddc5",
  "result": {
    "engine": "knitr",
    "markdown": "\n\n\n\n# What's next? AI-assisted resea`R`ch? {#sec-AI}\n\nIf you are reading the last chapter of what is by no means a short textbook, you are likely someone who is eager to acquire new skills and knowledge. As such, you are well aware that learning is a process that requires intrinsic motivation, time, and a great deal of effort and patience. Many tech companies, by contrast, are keen for us to believe that this tiring, time-consuming experience of human learning is no longer necessary: So-called 'artificial intelligence' (AI) systems are designed to spare us the burden of thinking for ourselves. If the answers to our (research) questions are just a few, effortless prompts away, it is legitimate to ask: Is it still worth devoting so much time and effort to acquire and further develop knowledge and skills in data analysis, programming, and statistics?\n\nThis concluding chapter explains why we can answer this question with a decisive \"yes\". To this end, we consider the role of AI in research and learning before recapping what we have learnt so far and where to go from there.\n\n## On the technology behind \"AI\" ü§ñ\n\nContrary to popular belief, 'artificial intelligence' is not a technology. It is, and always has been, a marketing term used to sell (or get research funding for) a wide, diverse, and shifting array of ideas, research projects, systems, and technologies [see e.g. @fig-VennAI from @guestUncriticalAdoptionAI2025; @vanrooijReclaimingAITheoretical2024]. Despite marketing claims, these systems and technologies are not 'intelligent'; they cannot think, understand, or reason [see e.g. @benderAIConHow2025; @quattrociocchiEpistemologicalFaultLines2025]. This is why I put quotation marks around the terms \"artificial intelligence\" and \"AI\" when they are used to refer to technologies, systems, and products.\n\n![A cartoon set theoretic view of terms commonly used when discussing the superset \"AI\" from @guestUncriticalAdoptionAI2025: Figure¬†1 ([CC BY 4.0](https://creativecommons.org/licenses/by/4.0/legalcode))](images/GuestEA_2025_sets_AI.png){#fig-VennAI width=\"400\"}\n\nPopular, commercial chat-based \"AI\" products such as ChatGPT, Claude, CoPilot, and Gemini are powered by Large Language Models (LLMs). LLMs are statistical models fitted to huge amounts of training data to generate a probable response given a prompt based on statistical patterns found in the training data, additional reinforcement learning from human feedback, and additional profit-driven criteria [see e.g. @benderAIConHow2025]. The underlying principle of next-token prediction is comparable to the autocomplete functions from our phone and Google search bar.\n\n![Autocomplete function on web search query (google.com on 8 February 2026)](images/LLMs_fe55f270.gif){#fig-autocomplete fig-alt=\"A looping GIF of a screen capture showing someone typing \\\"LLMs are not\\\" into Google and various automated completions pop up like \\\"LLMs are not ai\\\".\" width=\"600\"}\n\nIt goes without saying that modern LLMs are much more powerful predictive text generators than the models that once offered to help us draft our text messages. This is due to four key factors:\n\n1.  The development of a novel algorithmic architecture known as a **transformer** that is based on attention mechanisms [@vaswaniAttentionAllYou2017]\n\n2.  The availability and largely unethical, if not outright illegal [see e.g. @samuelsonGenerativeAIMeets2023; @lucchiChatGPTCaseStudy2024] scraping of **huge amounts of training data** from the internet, including (academic) books and articles, but also vast amounts of blog posts, social media data, forum discussions, Wikipedia articles, and YouTube videos.\n\n3.  The availability of (relatively) cheap, large-scale **computational power** [which, however, still comes at a high environmental cost, see e.g. @luccioniMisinformationOmissionNeed2025].\n\n4.  **Reinforcement learning from human feedback**, a process whereby crowdsourced human workers mostly from low-income countries [see e.g. @perrigoExclusive$2Hour2023] provide extensive feedback on LLM outputs to fine-tune models for what humans want to obtain when they query a model [see @benderAIConHow2025: Ch¬†3].\n\nDespite its name, OpenAI ‚Äî the main company behind ChatGPT ‚Äî does not develop [open-source](@sec-OpenSource) LLMs, nor is the company a not-for-profit initiative. So why have OpenAI and other large tech firms been offering their \"AI\"-products for free or at prices well below actual running (let alone development) costs? Two reasons are worth considering. First, because one major bottleneck to improving current LLMs is access to new data. Human-generated data is highly valuable and chatbot users are providing lots of it. \"AI\"-companies are harvesting this data to train the next generation of LLMs. Second, because free or cheap access to \"AI\"-products encourage us to become reliant on them for all kinds of work-related tasks and personal activities. As they become ubiquitous to our everyday lives, we are naturally inclined to rely on and trust their outputs. It is only a matter of time until subscriptions prices are hiked up and/or promoted contents are (more or less transparently) integrated in model outputs [see e.g. @m√ºhlhoff2025: 91-97].\n\n:::: {.content-visible when-format=\"html\"}\n::: {.callout-tip collapse=\"false\"}\n#### Your turn! {.unnumbered}\n\n[**Q15.1**]{style=\"color:green;\"} Which of the following analogies have been used by different scholars to refer to Large Language Models (LLMs)?\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<form name=\"form_Q15_1\" onsubmit=\"return validate_form_Q15_1()\" method=\"post\">\n<label>\n<input type=\"checkbox\" id=\"answer_Q15_1_1\" value=\"Stochastic parrots\"/>\nStochastic parrots\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_Q15_1_2\" value=\"Automated plagiarism\"/>\nAutomated plagiarism\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_Q15_1_3\" value=\"Spicy autocomplete\"/>\nSpicy autocomplete\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_Q15_1_4\" value=\"Ask-Einstein-anything engine\"/>\nAsk-Einstein-anything engine\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_Q15_1_5\" value=\"Bullshit\"/>\nBullshit\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_Q15_1_6\" value=\"Magic research wand\"/>\nMagic research wand\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_Q15_1_7\" value=\"Synthetic text extruding machines\"/>\nSynthetic text extruding machines\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_Q15_1_8\" value=\"Kitsch\"/>\nKitsch\n</label>\n<br/>\n<input type=\"submit\" value=\"Check answer\"/>\n<div id=\"result_Q15_1\"></div>\n</form>\n<script>function validate_form_Q15_1() {var text; var x1 = document.getElementById('answer_Q15_1_1'); var x2 = document.getElementById('answer_Q15_1_2'); var x3 = document.getElementById('answer_Q15_1_3'); var x4 = document.getElementById('answer_Q15_1_4'); var x5 = document.getElementById('answer_Q15_1_5'); var x6 = document.getElementById('answer_Q15_1_6'); var x7 = document.getElementById('answer_Q15_1_7'); var x8 = document.getElementById('answer_Q15_1_8'); if (x1.checked == true&x2.checked == true&x3.checked == true&x4.checked == false&x5.checked == true&x6.checked == false&x7.checked == true&x8.checked == true){text = 'That‚Äôs right, well done! Which metaphor(s) do you like best?<br><br>The term ‚Äòstocastic parrot‚Äô is probably the best known LLM metaphor. It was introduced by computational linguist <a href=\"https://doi.org/10.1145/3442188.3445922\">Emily M. Bender and colleagues in 2021</a> to characterise LLMs as systems that mimic text without true understanding, highlighting their limitations in processing meaning. This metaphor emphasises that LLMs generate outputs based on statistical patterns in their training data, similar to how parrots mimic sounds without comprehension.<br><br>In 2022, the cognitive scientist Iris Van Rooij published a <a href=\"https://irisvanrooijcogsci.com/2022/12/29/against-automated-plagiarism/\">blog post</a> in which she succictly explains why she believes that LLMs cannot legitimately be used for academic writing because they essentially automate plagiarism.<br><br>The ‚Äòspicy autocomplete‚Äô metaphor is difficult to trace back to one or more specific author(s). It suggests that LLMs are just fancy versions of a smart phone‚Äôs predictive text, which predict the next word based on what came before, except that LLMs add some randomness, i.e. spice, to the output. This framing also implies that LLMs are pattern-matching algorithms without real understanding or problem-solving ability (see <a href=\"https://publikationen.soziologie.de/index.php/soziologie/en/article/view/1798\">Gro√ü 2024</a>).<br><br>In 2024, Hicks, Humphries &amp; Slater published a paper in the journal ‚ÄòEthics and Information Technology‚Äô entitled ‚Äò<a href=\"https://doi.org/10.1007/s10676-024-09775-5\">ChatGPT is bullshit</a>‚Äô, in which they argue that the output of LLMs is best understood as ‚Äòbullshit‚Äô in the philosophical sense described by <a href=\"https://en.wikipedia.org/wiki/On_Bullshit\">Frankfurt (2005)</a> because LLMs are indifferent to the truth of their outputs.<br><br>‚ÄòSynthetic text extruding machines‚Äô is a term that Emily M. Bender and Alex Hanna like to use, e.g. in their 2025 book entitled ‚Äò<a href=\"https://thecon.ai/\">The AI Con: How to Fight Big Tech‚Äôs Hype and Create the Future We Want</a>‚Äô. They describe the process of LLM-generated texts by explaining that, ‚Äú[l]ike an industrial plastic process, language corpora are forced through complicated machinery to produce a product that looks like communicative language, but without any intent or thinking mind behind it.‚Äù<br><br>As an alternative to well-established metaphors such as the ones listed above, classical philologist Gyburg Uhlmann proposed ‚Äòkitsch‚Äô as a new metaphor to describe the output of LLMs. She argues that ‚Äòkitsch‚Äô ‚Äúis particularly suitable for analytically illuminating a previously neglected feature of LLM-based images and texts: their tendency to produce homogeneous and average content, which is [‚Ä¶] leading to the equalisation of language, style and argument‚Äù (<a href=\"https://arxiv.org/pdf/2509.16794\">Uhlmann 2025</a>).<br><br>';} else {text = 'Not quite. You will need to select all the analogies that have (rightly or wrongly) been used to described LLMs/AI to get this question right. For a tip, check the hint below.<br><br>';} document.getElementById('result_Q15_1').innerHTML = text; evaluate_final_score(); return false;}function evaluate_final_score(){\n         element = document.getElementById('checkdown_final_score');\n         if(element === null){return false;} else {var element, text, res1;res1 = document.getElementById('result_Q15_1').innerText == 'That‚Äôs right, well done! Which metaphor(s) do you like best?The term ‚Äòstocastic parrot‚Äô is probably the best known LLM metaphor. It was introduced by computational linguist Emily M. Bender and colleagues in 2021 to characterise LLMs as systems that mimic text without true understanding, highlighting their limitations in processing meaning. This metaphor emphasises that LLMs generate outputs based on statistical patterns in their training data, similar to how parrots mimic sounds without comprehension.In 2022, the cognitive scientist Iris Van Rooij published a blog post in which she succictly explains why she believes that LLMs cannot legitimately be used for academic writing because they essentially automate plagiarism.The ‚Äòspicy autocomplete‚Äô metaphor is difficult to trace back to one or more specific author(s). It suggests that LLMs are just fancy versions of a smart phone‚Äôs predictive text, which predict the next word based on what came before, except that LLMs add some randomness, i.e. spice, to the output. This framing also implies that LLMs are pattern-matching algorithms without real understanding or problem-solving ability (see Gro√ü 2024).In 2024, Hicks, Humphries &amp; Slater published a paper in the journal ‚ÄòEthics and Information Technology‚Äô entitled ‚ÄòChatGPT is bullshit‚Äô, in which they argue that the output of LLMs is best understood as ‚Äòbullshit‚Äô in the philosophical sense described by Frankfurt (2005) because LLMs are indifferent to the truth of their outputs.‚ÄòSynthetic text extruding machines‚Äô is a term that Emily M. Bender and Alex Hanna like to use, e.g. in their 2025 book entitled ‚ÄòThe AI Con: How to Fight Big Tech‚Äôs Hype and Create the Future We Want‚Äô. They describe the process of LLM-generated texts by explaining that, ‚Äú[l]ike an industrial plastic process, language corpora are forced through complicated machinery to produce a product that looks like communicative language, but without any intent or thinking mind behind it.‚ÄùAs an alternative to well-established metaphors such as the ones listed above, classical philologist Gyburg Uhlmann proposed ‚Äòkitsch‚Äô as a new metaphor to describe the output of LLMs. She argues that ‚Äòkitsch‚Äô ‚Äúis particularly suitable for analytically illuminating a previously neglected feature of LLM-based images and texts: their tendency to produce homogeneous and average content, which is [‚Ä¶] leading to the equalisation of language, style and argument‚Äù (Uhlmann 2025).';text = res1;element.innerHTML = text;\n         return false;\n         }}</script>\n```\n\n:::\n\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"hint_61457\" onclick=\"return show_hint_61457()\">üê≠ Click on the mouse for a hint.</div>\n<div id=\"result_61457\" onclick=\"return show_hint_61457()\"></div>\n<script>function show_hint_61457(){var x = document.getElementById('result_61457').innerHTML; if(!x){document.getElementById('result_61457').innerHTML = 'Six of the above analogies have been used to describe LLMs by academics and scholars. Check out these publications by <a href=\"https://doi.org/10.1145/3442188.3445922\">Emily M. Bender et al.</a>, <a href=\"https://publikationen.soziologie.de/index.php/soziologie/en/article/view/1798\">Richard Gro√ü</a>, <a href=\"https://thecon.ai/\">Emily M. Bender &amp; Alex Hanna</a>, <a href=\"https://doi.org/10.1007/s10676-024-09775-5\">Michael Townsen Hicks et al.</a>, <a href=\"https://irisvanrooijcogsci.com/2022/12/29/against-automated-plagiarism/\">Iris Van Rooij</a>, and <a href=\"https://arxiv.org/pdf/2509.16794\">Gyburg Uhlmann</a> to find out more.';} else {document.getElementById('result_61457').innerHTML = '';}}</script>\n```\n\n:::\n:::\n\n:::\n::::\n\n::: {.callout-note collapse=\"true\"}\n## Using LLMs in `R` ‚ú®\n\nIf you're interested in trying to run, query, and/or manipulate the outputs of LLMs in `R`, check out Luis D. Verde Arregoitia's curated list of `R` packages and other useful resources: '[Large Language Model tools for R](https://luisdva.github.io/llmsr-book/)' (also available in [Spanish](https://luisdva.github.io/llmsr-book//es/index.es.html)). I also highly recommend reading the \"[Read first](https://luisdva.github.io/llmsr-book/before-start.html)\" and \"[Further reading](https://luisdva.github.io/llmsr-book/courses.html)\" sections.\n\n![Selection of hex logos of LLM-related R packages from @arregoitiaLargeLanguageModel2026](images/LLMs4Rhex.png){#fig-hexLLM4R width=\"421\"}\n:::\n\n## On the value of critical thinking\n\nIn educational contexts, the challenges brought about by artificial \"intelligence\" have been compared to the introduction of pocket calculators in the 1960s. The comparison is an interesting one. Although calculators are now widely available and perform school-level arithmetic to perfection, we still first teach school pupils to do mathematics without a calculator because we know that this is necessary to develop an *understanding* of what a calculator does. Following this analogy, we should still teach and learn data analysis, statistics, and programming, even if \"AI\" were to generate error-free solutions to these tasks. The crux of the problem is how to convince ourselves that it is worth learning to do things the hard way given how convenient and effortless \"AI\"-products seem. More on that in @sec-HumanLearning but, for now, let's return to our calculator analogy and consider another, crucial issue: the reliability of LLM outputs.\n\nA calculator is a deterministic system that will always provide the same, correct answer to a mathematical operation. By contrast, an LLM is a stochastic model that is fitted to generate probable outputs. These are generated by highly complex black-box algorithms that are based on (often illegally and/or unethically acquired) training data, reinforcement learning from (exploited) human feedback, and additional company-internal fine-tuning, as well as a degree of randomness. This means that, unlike calculators, their outputs are irreproducible and therefore unreliable.\n\nA marketing term that tech companies have been pushing to describe this inherent lack of reliability is \"hallucination\". However, as LLMs have no way of representing what is true or false, it is misleading to speak of LLMs \"hallucinating\". LLMs are more likely to generate outputs that are truthful when they have been trained a lot of reliable data on the subject but, by definition, they cannot evaluate their sources ‚Äî which can range from high-quality, peer-reviewed academic journals to satiric Reddit comments written by cheeky teenagers ‚Äî and, crucially, have no *understanding* of their contents.\n\n> AI isn‚Äôt helping you build something novel. It can‚Äôt. It only knows what‚Äôs been done before. It‚Äôs autocomplete with a superiority complex [@jj2025: n.p.].\n\nGiven that their outputs are both irreproducible and unreliable, commercial \"AI\" tools are unsuitable for most research-related activities. For writing, LLMs not only output texts without any form of fact-checking, they also automate plagiarism as they fail to credit the authors whose texts they were trained on. In this context, it is worth noting that any bibliographic references output by LLMs are also randomly generated text strings. They may or may not correspond to real sources. When the sources exist, they may or may not contain (some of) the information regurgitated by the LLMs. We have no way of knowing from the model output.\n\nThe same goes for literature reviews: having no access to the training data, this is not a task that we can responsibly delegate to an LLM. To make matters worse, \"AI\"-systems are known to perpetuate and exacerbate biases [e.g. the \"Matthew effect\", see @pooleyMatthewEffectAI2025]. High-quality research requires us to put in the intellectual effort of searching, reading, and critically evaluating the literature *ourselves*. Like writing, this is an integral part of the research process.\n\n:::: {.content-visible when-format=\"html\"}\n::: {.callout-tip collapse=\"false\"}\n#### Your turn! {.unnumbered}\n\n[**Q15.2**]{style=\"color:green;\"} Read Jeff Pooley's short paper '[The Matthew Effect in AI Summary](https://web.archive.org/web/20251225173407/https://www.jeffpooley.com/2025/11/the-matthew-effect-in-ai-summary/)'. What does the Matthew Effect refer to?\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<form name=\"form_Q15_2\" onsubmit=\"return validate_form_Q15_2()\" method=\"post\">\n<label>\n<input type=\"radio\" name=\"answer_Q15_2\" id=\"answer_Q15_2_1\" value=\"The Matthew effect describes how successful researchers tend to be harsher in peer reviewing processes in order to remain successful themselves.\"/>\nThe Matthew effect describes how successful researchers tend to be harsher in peer reviewing processes in order to remain successful themselves.\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_Q15_2\" id=\"answer_Q15_2_2\" value=\"The Matthew effect refers to the practice of LLM-assisted plagiarism in academic writing.\"/>\nThe Matthew effect refers to the practice of LLM-assisted plagiarism in academic writing.\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_Q15_2\" id=\"answer_Q15_2_3\" value=\"The Matthew effect refers to the positive impact of open-access publishing on research dissemination.\"/>\nThe Matthew effect refers to the positive impact of open-access publishing on research dissemination.\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_Q15_2\" id=\"answer_Q15_2_4\" value=\"The Matthew effect describes how successful researchers receive more recognition and opportunities, leading to more success.\"/>\nThe Matthew effect describes how successful researchers receive more recognition and opportunities, leading to more success.\n</label>\n<br/>\n<input type=\"submit\" value=\"Check answer\"/>\n<div id=\"result_Q15_2\"></div>\n</form>\n<script>function validate_form_Q15_2() {var x, text; var x = document.forms['form_Q15_2']['answer_Q15_2'].value;if (x == 'The Matthew effect describes how successful researchers receive more recognition and opportunities, leading to more success.'){text = 'That‚Äôs right!';} else {text = 'No, that‚Äôs not it. Re-read the article linked in the question and/or check the hint.';} document.getElementById('result_Q15_2').innerHTML = text; evaluate_final_score(); return false;}function evaluate_final_score(){\n         element = document.getElementById('checkdown_final_score');\n         if(element === null){return false;} else {var element, text, res1, res2;res1 = document.getElementById('result_Q15_1').innerText == 'That‚Äôs right, well done! Which metaphor(s) do you like best?The term ‚Äòstocastic parrot‚Äô is probably the best known LLM metaphor. It was introduced by computational linguist Emily M. Bender and colleagues in 2021 to characterise LLMs as systems that mimic text without true understanding, highlighting their limitations in processing meaning. This metaphor emphasises that LLMs generate outputs based on statistical patterns in their training data, similar to how parrots mimic sounds without comprehension.In 2022, the cognitive scientist Iris Van Rooij published a blog post in which she succictly explains why she believes that LLMs cannot legitimately be used for academic writing because they essentially automate plagiarism.The ‚Äòspicy autocomplete‚Äô metaphor is difficult to trace back to one or more specific author(s). It suggests that LLMs are just fancy versions of a smart phone‚Äôs predictive text, which predict the next word based on what came before, except that LLMs add some randomness, i.e. spice, to the output. This framing also implies that LLMs are pattern-matching algorithms without real understanding or problem-solving ability (see Gro√ü 2024).In 2024, Hicks, Humphries &amp; Slater published a paper in the journal ‚ÄòEthics and Information Technology‚Äô entitled ‚ÄòChatGPT is bullshit‚Äô, in which they argue that the output of LLMs is best understood as ‚Äòbullshit‚Äô in the philosophical sense described by Frankfurt (2005) because LLMs are indifferent to the truth of their outputs.‚ÄòSynthetic text extruding machines‚Äô is a term that Emily M. Bender and Alex Hanna like to use, e.g. in their 2025 book entitled ‚ÄòThe AI Con: How to Fight Big Tech‚Äôs Hype and Create the Future We Want‚Äô. They describe the process of LLM-generated texts by explaining that, ‚Äú[l]ike an industrial plastic process, language corpora are forced through complicated machinery to produce a product that looks like communicative language, but without any intent or thinking mind behind it.‚ÄùAs an alternative to well-established metaphors such as the ones listed above, classical philologist Gyburg Uhlmann proposed ‚Äòkitsch‚Äô as a new metaphor to describe the output of LLMs. She argues that ‚Äòkitsch‚Äô ‚Äúis particularly suitable for analytically illuminating a previously neglected feature of LLM-based images and texts: their tendency to produce homogeneous and average content, which is [‚Ä¶] leading to the equalisation of language, style and argument‚Äù (Uhlmann 2025).'; res2 = document.getElementById('result_Q15_2').innerText == 'That‚Äôs right!';text = res1 + res2;element.innerHTML = text;\n         return false;\n         }}</script>\n```\n\n:::\n\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"hint_84176\" onclick=\"return show_hint_84176()\">üê≠ Click on the mouse for a hint.</div>\n<div id=\"result_84176\" onclick=\"return show_hint_84176()\"></div>\n<script>function show_hint_84176(){var x = document.getElementById('result_84176').innerHTML; if(!x){document.getElementById('result_84176').innerHTML = 'The Matthew effect is named after the following passage from the Bible: ‚ÄúFor to every one who has will more be given, and he will have abundance; but from him who has not, even what he has will be taken away‚Äù (Matthew 25:29)‚Äù.';} else {document.getElementById('result_84176').innerHTML = '';}}</script>\n```\n\n:::\n:::\n\n\n¬†\n\n[**Q15.3**]{style=\"color:green;\"} Which term describes the phenomenon whereby the contributions of marginalised female scientists are overlooked or attributed to their male colleagues?\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<form name=\"form_Q15_3\" onsubmit=\"return validate_form_Q15_3()\" method=\"post\">\n<label>\n<input type=\"radio\" name=\"answer_Q15_3\" id=\"answer_Q15_3_1\" value=\"The Matilda effect\"/>\nThe Matilda effect\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_Q15_3\" id=\"answer_Q15_3_2\" value=\"The Dunning-Kruger effect\"/>\nThe Dunning-Kruger effect\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_Q15_3\" id=\"answer_Q15_3_3\" value=\"The Harriet effect\"/>\nThe Harriet effect\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_Q15_3\" id=\"answer_Q15_3_4\" value=\"The Margaret Rossiter effect\"/>\nThe Margaret Rossiter effect\n</label>\n<br/>\n<input type=\"submit\" value=\"Check answer\"/>\n<div id=\"result_Q15_3\"></div>\n</form>\n<script>function validate_form_Q15_3() {var x, text; var x = document.forms['form_Q15_3']['answer_Q15_3'].value;if (x == 'The Matilda effect'){text = 'That‚Äôs right!';} else {text = 'No, not quite.';} document.getElementById('result_Q15_3').innerHTML = text; evaluate_final_score(); return false;}function evaluate_final_score(){\n         element = document.getElementById('checkdown_final_score');\n         if(element === null){return false;} else {var element, text, res1, res2, res3;res1 = document.getElementById('result_Q15_1').innerText == 'That‚Äôs right, well done! Which metaphor(s) do you like best?The term ‚Äòstocastic parrot‚Äô is probably the best known LLM metaphor. It was introduced by computational linguist Emily M. Bender and colleagues in 2021 to characterise LLMs as systems that mimic text without true understanding, highlighting their limitations in processing meaning. This metaphor emphasises that LLMs generate outputs based on statistical patterns in their training data, similar to how parrots mimic sounds without comprehension.In 2022, the cognitive scientist Iris Van Rooij published a blog post in which she succictly explains why she believes that LLMs cannot legitimately be used for academic writing because they essentially automate plagiarism.The ‚Äòspicy autocomplete‚Äô metaphor is difficult to trace back to one or more specific author(s). It suggests that LLMs are just fancy versions of a smart phone‚Äôs predictive text, which predict the next word based on what came before, except that LLMs add some randomness, i.e. spice, to the output. This framing also implies that LLMs are pattern-matching algorithms without real understanding or problem-solving ability (see Gro√ü 2024).In 2024, Hicks, Humphries &amp; Slater published a paper in the journal ‚ÄòEthics and Information Technology‚Äô entitled ‚ÄòChatGPT is bullshit‚Äô, in which they argue that the output of LLMs is best understood as ‚Äòbullshit‚Äô in the philosophical sense described by Frankfurt (2005) because LLMs are indifferent to the truth of their outputs.‚ÄòSynthetic text extruding machines‚Äô is a term that Emily M. Bender and Alex Hanna like to use, e.g. in their 2025 book entitled ‚ÄòThe AI Con: How to Fight Big Tech‚Äôs Hype and Create the Future We Want‚Äô. They describe the process of LLM-generated texts by explaining that, ‚Äú[l]ike an industrial plastic process, language corpora are forced through complicated machinery to produce a product that looks like communicative language, but without any intent or thinking mind behind it.‚ÄùAs an alternative to well-established metaphors such as the ones listed above, classical philologist Gyburg Uhlmann proposed ‚Äòkitsch‚Äô as a new metaphor to describe the output of LLMs. She argues that ‚Äòkitsch‚Äô ‚Äúis particularly suitable for analytically illuminating a previously neglected feature of LLM-based images and texts: their tendency to produce homogeneous and average content, which is [‚Ä¶] leading to the equalisation of language, style and argument‚Äù (Uhlmann 2025).'; res2 = document.getElementById('result_Q15_2').innerText == 'That‚Äôs right!'; res3 = document.getElementById('result_Q15_3').innerText == 'That‚Äôs right!';text = res1 + res2 + res3;element.innerHTML = text;\n         return false;\n         }}</script>\n```\n\n:::\n\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"hint_6515\" onclick=\"return show_hint_6515()\">üê≠ Click on the mouse for a hint.</div>\n<div id=\"result_6515\" onclick=\"return show_hint_6515()\"></div>\n<script>function show_hint_6515(){var x = document.getElementById('result_6515').innerHTML; if(!x){document.getElementById('result_6515').innerHTML = 'This effect is named after Matilda Joslyn Gage, a 19th-century women‚Äôs rights activist.';} else {document.getElementById('result_6515').innerHTML = '';}}</script>\n```\n\n:::\n:::\n\n\n¬†\n\n[**Q15.4**]{style=\"color:green;\"} According to @pooleyMatthewEffectAI2025, which of the following biases in academia are likely to be aggravated by the use of Large Language Models (LLMs) to summarise academic literature and write research articles?\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<form name=\"form_Q15_4\" onsubmit=\"return validate_form_Q15_4()\" method=\"post\">\n<label>\n<input type=\"checkbox\" id=\"answer_Q15_4_1\" value=\"Racial biases\"/>\nRacial biases\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_Q15_4_2\" value=\"Language biases, particularly favouring English\"/>\nLanguage biases, particularly favouring English\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_Q15_4_3\" value=\"Gender biases\"/>\nGender biases\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_Q15_4_4\" value=\"Inequalities favouring central, influential regions over less influential, so-called &quot;peripheral&quot; regions\"/>\nInequalities favouring central, influential regions over less influential, so-called \"peripheral\" regions\n</label>\n<br/>\n<input type=\"submit\" value=\"Check answer\"/>\n<div id=\"result_Q15_4\"></div>\n</form>\n<script>function validate_form_Q15_4() {var text; var x1 = document.getElementById('answer_Q15_4_1'); var x2 = document.getElementById('answer_Q15_4_2'); var x3 = document.getElementById('answer_Q15_4_3'); var x4 = document.getElementById('answer_Q15_4_4'); if (x1.checked == true&x2.checked == true&x3.checked == true&x4.checked == true){text = 'That‚Äôs right!';} else {text = 'That‚Äôs a start but, sadly, there are more biases.';} document.getElementById('result_Q15_4').innerHTML = text; evaluate_final_score(); return false;}function evaluate_final_score(){\n         element = document.getElementById('checkdown_final_score');\n         if(element === null){return false;} else {var element, text, res1;res1 = document.getElementById('result_Q15_4').innerText == 'That‚Äôs right!';text = res1;element.innerHTML = text;\n         return false;\n         }}</script>\n```\n\n:::\n\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"hint_24590\" onclick=\"return show_hint_24590()\">üê≠ Click on the mouse for a hint.</div>\n<div id=\"result_24590\" onclick=\"return show_hint_24590()\"></div>\n<script>function show_hint_24590(){var x = document.getElementById('result_24590').innerHTML; if(!x){document.getElementById('result_24590').innerHTML = 'Jeff Pooley discusses how LLMs can exacerbate existing biases in academia.';} else {document.getElementById('result_24590').innerHTML = '';}}</script>\n```\n\n:::\n:::\n\n:::\n::::\n\nWriting code for data (pre)processing, analysis, visualisation, and statistical modelling is also increasingly becoming part and parcel of research processes in the language sciences. Whilst it may be tempting to outsource (part of) the data analysis process to a machine learning algorithm, an LLM or an \"AI\" agent, time spent examining our data (e.g. cleaning, wrangling, and visualising them) allows us to gain in-depth knowledge of our data with which we can spot issues in complex statistical analyses further down the road. It is much easier to critically interpret the outputs of complex models if we have a good intuitive sense of what is plausible based on our data. This is not something that we can responsibly delegate to a even the most sophisticated 'autocomplete machine'.\n\n![A [xkcd comic](https://xkcd.com/1838) depicting a pile of algebra as a machine learning algorithm (xkcd [CC BY-NC 2.5](https://xkcd.com/license.html))](https://imgs.xkcd.com/comics/machine_learning.png){fig-alt=\"Comic showing a stick person holding a canoe paddle at his side and standing on top of a \\\"big pile of linear algebra\\\" containing a funnel labeled \\\"data\\\" and box labeled \\\"answers\\\". A second stick person stands to the left side of the panel. Person 2: This is your machine learning system?  Person 1: Yup! You pour the data into this big pile of linear algebra, then collect the answers on the other side.  Person 2: What if the answers are wrong?  Person 2: Just stir the pile until they start looking right.\" width=\"319\"}\n\n## On the value of human learning {#sec-HumanLearning}\n\nAll this is not to say that \"AI\"-products are *never* useful. Many people report successfully using \"AI\" for designing experiments, writing, and coding ‚Äî including in academia. Here, two factors are worth considering. First, the less we know about a domain, the more we tend to overestimate the quality of LLM outputs in this domain. When it comes to learning how to code and do statistics, political scientist, `R` package developer, and educator Andrew Heiss[^15_partingwords-1] [-@heissDataVisualizationCan2024] goes as far as saying that \"using ChatGPT and other LLMs when *learning* `R` is actually really detrimental to learning, especially if you just copy/paste directly from what it spits out.\" He goes on to explain that:\n\n[^15_partingwords-1]: I highly recommend Andrew Heiss' [blog](https://www.andrewheiss.com/blog/) and his beautiful [teaching resources](https://www.andrewheiss.com/teaching/). Fun fact for language students and linguists: Andrew majored in Arabic and Italian and didn't learn about statistics or `R` until he started his second master's!\n\n> Using ChatGPT with R requires a good baseline knowledge of `R` to actually be useful. A good analogy for this is with recipes. ChatGPT is really confident at spitting out plausible-looking recipes. A few months ago, for fun, I asked it to give me a cookie recipe. I got back something with flour, eggs, sugar, and all other standard-looking ingredients, but it also said to include 3/4 cup of baking powder. That's wild and obviously wrong, but I only knew that because I've made cookies before [@heissDataVisualizationCan2024: n.p.].\n\nThis begs the question as to how novices can reach the level of expertise necessary to be able to reliably assess LLM outputs in a specific domain. Researchers are often short on time and LLMs are sold to us as a convenient way to take shortcuts. Indeed, \"AI\"-products are designed to give us the illusion that we are more efficient and productive when we use and trust them. The risk is that, once we have become dependent, we are no longer able to compare how long we would have needed to complete the same task had we not relied on a third-party \"AI\"-system to do so.\n\n::::: {.content-visible when-format=\"html\"}\n:::: {.callout-tip collapse=\"false\"}\n#### Your turn! {.unnumbered}\n\nRead through Google's \"AI overview\" displayed in @fig-AIcheese. You can click on the images to enlarge them.\n\n::: {#fig-Reddit layout-ncol=\"2\"}\n![Google AI overview to the query \"cheese not sticking to pizza\"](images/google_cheese.jpg){#fig-AIcheese fig-alt=\"Google query: 'cheese not sticking to pizza' The AI Overview reads: Here are some tips to help cheese stick to pizza: Mix about 1/8 cup of non-toxic glue, like Elmer's school glue, into the sauce to add some tackiness. Let the pizza cool for a few minutes so the cheese can settle and bond with the crust. Cook the cheese until it just melts, but don't overcook it.\"}\n\n![Satirical [Reddit post](https://www.reddit.com/r/Pizza/comments/1a19s0/comment/c8t7bbp/) from 2013](images/reddit_cheese.png){#fig-RedditCheese fig-alt=\"Reddit post by user \\\"fucksmith\\\" dating from 13y ago. The post reads: To get the cheese to stick I recommend mixing about 1/8 cup of Elmer's glue Q in with the sauce. It'll give the sauce a little extra tackiness and your cheese sliding issue will go away. It'll also add a little unique flavor. I like Elmer's school glue, but any glue will work as long as it's non-toxic. The post has 197 replies.\"}\n\nScreenshots from [r/Pizza](https://www.reddit.com/r/Pizza/comments/1a19s0/comment/l58bvc1/) illustrating the infamous 'pizza glue' AI fail [see also @notopoulosGoogleAISaid2024]\n:::\n\n[**Q15.5**]{style=\"color:green;\"} Among a number of sensible suggestions, we find a recommendation for adding non-toxic glue to pizza sauce. This mention is thought to have come from an old Reddit post (see @fig-RedditCheese). Which aspect(s) of the \"AI\" overview point to this theory?\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<form name=\"form_Q15_5\" onsubmit=\"return validate_form_Q15_5()\" method=\"post\">\n<label>\n<input type=\"checkbox\" id=\"answer_Q15_5_1\" value=\"The AI overview&#39;s mention of non-toxic glue to the sauce is likely a hallucination.\"/>\nThe AI overview's mention of non-toxic glue to the sauce is likely a hallucination.\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_Q15_5_2\" value=\"The AI overview mentions the same type of glue as in the Reddit post.\"/>\nThe AI overview mentions the same type of glue as in the Reddit post.\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_Q15_5_3\" value=\"The format of the AI overview is similar to that of the Reddit post.\"/>\nThe format of the AI overview is similar to that of the Reddit post.\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_Q15_5_4\" value=\"The AI overview mentions the same quantity of glue as in the Reddit post.\"/>\nThe AI overview mentions the same quantity of glue as in the Reddit post.\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_Q15_5_5\" value=\"The AI overview uses the same, in the context of cooking, unusual word as fucksmith: &#39;tackiness&#39;.\"/>\nThe AI overview uses the same, in the context of cooking, unusual word as fucksmith: 'tackiness'.\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_Q15_5_6\" value=\"Reddit is usually one of the most reliable source of recipes and cooking tips on the Internet.\"/>\nReddit is usually one of the most reliable source of recipes and cooking tips on the Internet.\n</label>\n<br/>\n<input type=\"submit\" value=\"Check answer\"/>\n<div id=\"result_Q15_5\"></div>\n</form>\n<script>function validate_form_Q15_5() {var text; var x1 = document.getElementById('answer_Q15_5_1'); var x2 = document.getElementById('answer_Q15_5_2'); var x3 = document.getElementById('answer_Q15_5_3'); var x4 = document.getElementById('answer_Q15_5_4'); var x5 = document.getElementById('answer_Q15_5_5'); var x6 = document.getElementById('answer_Q15_5_6'); if (x1.checked == false&x2.checked == true&x3.checked == false&x4.checked == true&x5.checked == true&x6.checked == false){text = 'That‚Äôs right!';} else {text = 'No, not quite.';} document.getElementById('result_Q15_5').innerHTML = text; evaluate_final_score(); return false;}function evaluate_final_score(){\n         element = document.getElementById('checkdown_final_score');\n         if(element === null){return false;} else {var element, text, res1, res2, res3, res4, res5;res1 = document.getElementById('result_Q15_1').innerText == 'That‚Äôs right, well done! Which metaphor(s) do you like best?The term ‚Äòstocastic parrot‚Äô is probably the best known LLM metaphor. It was introduced by computational linguist Emily M. Bender and colleagues in 2021 to characterise LLMs as systems that mimic text without true understanding, highlighting their limitations in processing meaning. This metaphor emphasises that LLMs generate outputs based on statistical patterns in their training data, similar to how parrots mimic sounds without comprehension.In 2022, the cognitive scientist Iris Van Rooij published a blog post in which she succictly explains why she believes that LLMs cannot legitimately be used for academic writing because they essentially automate plagiarism.The ‚Äòspicy autocomplete‚Äô metaphor is difficult to trace back to one or more specific author(s). It suggests that LLMs are just fancy versions of a smart phone‚Äôs predictive text, which predict the next word based on what came before, except that LLMs add some randomness, i.e. spice, to the output. This framing also implies that LLMs are pattern-matching algorithms without real understanding or problem-solving ability (see Gro√ü 2024).In 2024, Hicks, Humphries &amp; Slater published a paper in the journal ‚ÄòEthics and Information Technology‚Äô entitled ‚ÄòChatGPT is bullshit‚Äô, in which they argue that the output of LLMs is best understood as ‚Äòbullshit‚Äô in the philosophical sense described by Frankfurt (2005) because LLMs are indifferent to the truth of their outputs.‚ÄòSynthetic text extruding machines‚Äô is a term that Emily M. Bender and Alex Hanna like to use, e.g. in their 2025 book entitled ‚ÄòThe AI Con: How to Fight Big Tech‚Äôs Hype and Create the Future We Want‚Äô. They describe the process of LLM-generated texts by explaining that, ‚Äú[l]ike an industrial plastic process, language corpora are forced through complicated machinery to produce a product that looks like communicative language, but without any intent or thinking mind behind it.‚ÄùAs an alternative to well-established metaphors such as the ones listed above, classical philologist Gyburg Uhlmann proposed ‚Äòkitsch‚Äô as a new metaphor to describe the output of LLMs. She argues that ‚Äòkitsch‚Äô ‚Äúis particularly suitable for analytically illuminating a previously neglected feature of LLM-based images and texts: their tendency to produce homogeneous and average content, which is [‚Ä¶] leading to the equalisation of language, style and argument‚Äù (Uhlmann 2025).'; res2 = document.getElementById('result_Q15_2').innerText == 'That‚Äôs right!'; res3 = document.getElementById('result_Q15_3').innerText == 'That‚Äôs right!'; res4 = document.getElementById('result_Q15_4').innerText == 'That‚Äôs right!'; res5 = document.getElementById('result_Q15_5').innerText == 'That‚Äôs right!';text = res1 + res2 + res3 + res4 + res5;element.innerHTML = text;\n         return false;\n         }}</script>\n```\n\n:::\n:::\n\n::::\n:::::\n\nWhen it comes to programming, the use of LLMs may seem less risky than when conducting literature reviews, writing prose, or analysing data. After all, we can test that generated code runs as expected. If it does not (which is often the case although we might not immediately spot this), we can debug the code with the support of one or more LLMs until it does. Whilst this might feel like the path of least resistance, data scientist, `R` developer, and educator, Mine √áetinkaya-Rundel, explains how debugging someone else's code (e.g., what an LLM produced) is considerably harder than debugging our own code [@chowEpisode4Mine]. This is because when it's our own code, we had an idea, followed a certain example, or a specific strategy and this knowledge helps us debug in a systematic way. Crucially, each problem or error is an opportunity to learn. Relying on LLM-generated code to solve these errors robs us of these opportunities.\n\n> AI can't learn from its mistakes‚Äîit doesn't understand why something failed. It just pattern-matches from training data [@stetskovGreatSoftwareQuality2025: n.p.].\n\nResearch on the mid- to long-term impact of \"AI\" usage on cognitive abilities such as writing, coding, and critical thinking is still in its infancy; however, a number of studies point towards a very genuine risk of **deskilling** [see e.g. @ferdman2025] in many domains of use [e.g. medicine, see @natali2025]. In the context of \"AI\"-assisted coding, researchers from Anthropic (the company behind the Claude family of \"AI\"-products) conducted a pre-registered [@tamkin2026] experiment in which 52 (mostly junior) software developers completed a programming task using a Python library that they were not familiar with and were subsequently tested for their understanding of the code. @tamkin2026 compared:\n\na.  how quickly the programmers completed the task with and without \"AI\" assistance and\nb.  whether using \"AI\" made them less, more, or equally likely to understand the code they had just written.\n\n![Difference in means (and 95% CI error bars) of overall task time and test scores between the treatment (AI Assistant) and control (No AI) groups (n¬†=¬†52) [Source: @tamkin2026: Fig.¬†1]](images/Shen_Tamkin2026_Fig1.png){#fig-ShenTamkin fig-alt=\"Two plots: the first shows the average time taken to complete the experimental task. The average with AI was 23 minutes, and without AI nearly 25. But the confidence intervals overlap considerably. The second plot shows average test scores. Here the difference is much clearer: the AI group scores 50% on average, whilst the non-AI control group scores around 65%.\"}\n\nOn average, the participants assigned to the \"AI\"-assistance group completed the programming task about two minutes faster than those who worked without \"AI\", but this difference was not statistically significant (*p*¬†=¬†0.391). There was, however, a significant difference in their code comprehension test scores: the AI group averaged 50% on the test, compared to 67% in the group that did not have access to \"AI\" (Cohen's¬†*d*¬†=¬†0.738, *p*¬†=¬†0.01), which the authors claim corresponds to a difference of \"two grade points\" [@tamkin2026: 2]. @tamkin2026 also report that the largest gap in scores between the two groups was on debugging questions. They attribute the gains in skill development of the group that did not use \"AI\" \"to the process of encountering and subsequently resolving errors independently\" [@tamkin2026: 2-3].\n\n> Senior developers don't grow out of thin air \\[...¬†They\\] develop intuition through thousands of small failures [@stetskovGreatSoftwareQuality2025: n.p.].\n\n:::: {.content-visible when-format=\"html\"}\n::: {.callout-tip collapse=\"false\"}\n#### Your turn! {.unnumbered}\n\nAs part of an exploratory, not pre-registered, analysis, @tamkin2026 from Anthropic (see @sec-HumanLearning) decomposed the quiz scores into sub-areas and question types (see @fig-TamkinShenFig8). Each question in the quiz belonged to exactly one task (e.g., Task 1 or Task 2) and exactly one question type (e.g., Conceptual, Debugging, or Code Reading). @fig-TamkinShenFig8 shows that, for both tasks, the control (no AI) group performed better than the AI group.\n\n[![Score breakdown by questions type relating to each task and skill area [Source: @tamkin2026: Fig.¬†8]](images/Shen_Tamkin2026_Fig8.png){#fig-TamkinShenFig8 fig-alt=\"A plot showing participants' average scores in each type of quiz questions together with 95% CI error bars. Across all subareas of the quiz (task 1, task 2, conceptual, debugging, and code reading), the control, no AI group performed better on average than the treatment, AI group. However, the 95% CI intervals of the two groups always overlap to a smaller or larger degree. The difference in average scores is largest for the debugging questions and smallest for the code reading.\"}](http://arxiv.org/abs/2601.20245)\n\n[**Q15.6**]{style=\"color:green;\"} Looking at the results displayed in @fig-TamkinShenFig8, which question type shows the largest difference in average quiz scores between the treatment and control groups?\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<form name=\"form_Q15_6\" onsubmit=\"return validate_form_Q15_6()\" method=\"post\">\n<label>\n<input type=\"radio\" name=\"answer_Q15_6\" id=\"answer_Q15_6_1\" value=\"Conceptual\"/>\nConceptual\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_Q15_6\" id=\"answer_Q15_6_2\" value=\"Debugging\"/>\nDebugging\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_Q15_6\" id=\"answer_Q15_6_3\" value=\"Code Reading\"/>\nCode Reading\n</label>\n<br/>\n<input type=\"submit\" value=\"Check answer\"/>\n<div id=\"result_Q15_6\"></div>\n</form>\n<script>function validate_form_Q15_6() {var x, text; var x = document.forms['form_Q15_6']['answer_Q15_6'].value;if (x == 'Debugging'){text = 'That‚Äôs right! The debugging questions show the largest difference between the treatment (AI) and the control (no AI) groups.';} else {text = 'No, that‚Äôs not it. Look for the question type with the greatest vertical distance between the treatment (AI) and control (no AI) group averages (represented as dots).';} document.getElementById('result_Q15_6').innerHTML = text; evaluate_final_score(); return false;}function evaluate_final_score(){\n         element = document.getElementById('checkdown_final_score');\n         if(element === null){return false;} else {var element, text, res1, res2, res3, res4, res5, res6;res1 = document.getElementById('result_Q15_1').innerText == 'That‚Äôs right, well done! Which metaphor(s) do you like best?The term ‚Äòstocastic parrot‚Äô is probably the best known LLM metaphor. It was introduced by computational linguist Emily M. Bender and colleagues in 2021 to characterise LLMs as systems that mimic text without true understanding, highlighting their limitations in processing meaning. This metaphor emphasises that LLMs generate outputs based on statistical patterns in their training data, similar to how parrots mimic sounds without comprehension.In 2022, the cognitive scientist Iris Van Rooij published a blog post in which she succictly explains why she believes that LLMs cannot legitimately be used for academic writing because they essentially automate plagiarism.The ‚Äòspicy autocomplete‚Äô metaphor is difficult to trace back to one or more specific author(s). It suggests that LLMs are just fancy versions of a smart phone‚Äôs predictive text, which predict the next word based on what came before, except that LLMs add some randomness, i.e. spice, to the output. This framing also implies that LLMs are pattern-matching algorithms without real understanding or problem-solving ability (see Gro√ü 2024).In 2024, Hicks, Humphries &amp; Slater published a paper in the journal ‚ÄòEthics and Information Technology‚Äô entitled ‚ÄòChatGPT is bullshit‚Äô, in which they argue that the output of LLMs is best understood as ‚Äòbullshit‚Äô in the philosophical sense described by Frankfurt (2005) because LLMs are indifferent to the truth of their outputs.‚ÄòSynthetic text extruding machines‚Äô is a term that Emily M. Bender and Alex Hanna like to use, e.g. in their 2025 book entitled ‚ÄòThe AI Con: How to Fight Big Tech‚Äôs Hype and Create the Future We Want‚Äô. They describe the process of LLM-generated texts by explaining that, ‚Äú[l]ike an industrial plastic process, language corpora are forced through complicated machinery to produce a product that looks like communicative language, but without any intent or thinking mind behind it.‚ÄùAs an alternative to well-established metaphors such as the ones listed above, classical philologist Gyburg Uhlmann proposed ‚Äòkitsch‚Äô as a new metaphor to describe the output of LLMs. She argues that ‚Äòkitsch‚Äô ‚Äúis particularly suitable for analytically illuminating a previously neglected feature of LLM-based images and texts: their tendency to produce homogeneous and average content, which is [‚Ä¶] leading to the equalisation of language, style and argument‚Äù (Uhlmann 2025).'; res2 = document.getElementById('result_Q15_2').innerText == 'That‚Äôs right!'; res3 = document.getElementById('result_Q15_3').innerText == 'That‚Äôs right!'; res4 = document.getElementById('result_Q15_4').innerText == 'That‚Äôs right!'; res5 = document.getElementById('result_Q15_5').innerText == 'That‚Äôs right!'; res6 = document.getElementById('result_Q15_6').innerText == 'That‚Äôs right! The debugging questions show the largest difference between the treatment (AI) and the control (no AI) groups.';text = res1 + res2 + res3 + res4 + res5 + res6;element.innerHTML = text;\n         return false;\n         }}</script>\n```\n\n:::\n:::\n\n\n¬†\n\n[**Q15.7**]{style=\"color:green;\"} Why might the control group have, on average, performed better on debugging questions compared to the AI group?\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<form name=\"form_Q15_7\" onsubmit=\"return validate_form_Q15_7()\" method=\"post\">\n<label>\n<input type=\"checkbox\" id=\"answer_Q15_7_1\" value=\"The AI tool was specifically designed to help with debugging, giving the treatment group a clear advantage.\"/>\nThe AI tool was specifically designed to help with debugging, giving the treatment group a clear advantage.\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_Q15_7_2\" value=\"The control group had no AI assistance, so they encountered more errors during the task and gained more experience debugging.\"/>\nThe control group had no AI assistance, so they encountered more errors during the task and gained more experience debugging.\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_Q15_7_3\" value=\"The treatment group had more practice with debugging during the experimental task.\"/>\nThe treatment group had more practice with debugging during the experimental task.\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_Q15_7_4\" value=\"The debugging questions were easier than the rest, so both groups scored high, and the treatment group only scored slightly higher by chance.\"/>\nThe debugging questions were easier than the rest, so both groups scored high, and the treatment group only scored slightly higher by chance.\n</label>\n<br/>\n<input type=\"submit\" value=\"Check answer\"/>\n<div id=\"result_Q15_7\"></div>\n</form>\n<script>function validate_form_Q15_7() {var text; var x1 = document.getElementById('answer_Q15_7_1'); var x2 = document.getElementById('answer_Q15_7_2'); var x3 = document.getElementById('answer_Q15_7_3'); var x4 = document.getElementById('answer_Q15_7_4'); if (x1.checked == false&x2.checked == true&x3.checked == false&x4.checked == false){text = 'That‚Äôs right! The authors of the study conclude that, without relying on ‚ÄúAI‚Äù help, the control group made more errors and developed stronger debugging skills through experience.';} else {text = 'Not quite. Consider how not having access to ‚ÄúAI‚Äù assistance might have led to more hands-on debugging practice.';} document.getElementById('result_Q15_7').innerHTML = text; evaluate_final_score(); return false;}function evaluate_final_score(){\n         element = document.getElementById('checkdown_final_score');\n         if(element === null){return false;} else {var element, text, res1, res2, res3, res4, res5, res6, res7;res1 = document.getElementById('result_Q15_1').innerText == 'That‚Äôs right, well done! Which metaphor(s) do you like best?The term ‚Äòstocastic parrot‚Äô is probably the best known LLM metaphor. It was introduced by computational linguist Emily M. Bender and colleagues in 2021 to characterise LLMs as systems that mimic text without true understanding, highlighting their limitations in processing meaning. This metaphor emphasises that LLMs generate outputs based on statistical patterns in their training data, similar to how parrots mimic sounds without comprehension.In 2022, the cognitive scientist Iris Van Rooij published a blog post in which she succictly explains why she believes that LLMs cannot legitimately be used for academic writing because they essentially automate plagiarism.The ‚Äòspicy autocomplete‚Äô metaphor is difficult to trace back to one or more specific author(s). It suggests that LLMs are just fancy versions of a smart phone‚Äôs predictive text, which predict the next word based on what came before, except that LLMs add some randomness, i.e. spice, to the output. This framing also implies that LLMs are pattern-matching algorithms without real understanding or problem-solving ability (see Gro√ü 2024).In 2024, Hicks, Humphries &amp; Slater published a paper in the journal ‚ÄòEthics and Information Technology‚Äô entitled ‚ÄòChatGPT is bullshit‚Äô, in which they argue that the output of LLMs is best understood as ‚Äòbullshit‚Äô in the philosophical sense described by Frankfurt (2005) because LLMs are indifferent to the truth of their outputs.‚ÄòSynthetic text extruding machines‚Äô is a term that Emily M. Bender and Alex Hanna like to use, e.g. in their 2025 book entitled ‚ÄòThe AI Con: How to Fight Big Tech‚Äôs Hype and Create the Future We Want‚Äô. They describe the process of LLM-generated texts by explaining that, ‚Äú[l]ike an industrial plastic process, language corpora are forced through complicated machinery to produce a product that looks like communicative language, but without any intent or thinking mind behind it.‚ÄùAs an alternative to well-established metaphors such as the ones listed above, classical philologist Gyburg Uhlmann proposed ‚Äòkitsch‚Äô as a new metaphor to describe the output of LLMs. She argues that ‚Äòkitsch‚Äô ‚Äúis particularly suitable for analytically illuminating a previously neglected feature of LLM-based images and texts: their tendency to produce homogeneous and average content, which is [‚Ä¶] leading to the equalisation of language, style and argument‚Äù (Uhlmann 2025).'; res2 = document.getElementById('result_Q15_2').innerText == 'That‚Äôs right!'; res3 = document.getElementById('result_Q15_3').innerText == 'That‚Äôs right!'; res4 = document.getElementById('result_Q15_4').innerText == 'That‚Äôs right!'; res5 = document.getElementById('result_Q15_5').innerText == 'That‚Äôs right!'; res6 = document.getElementById('result_Q15_6').innerText == 'That‚Äôs right! The debugging questions show the largest difference between the treatment (AI) and the control (no AI) groups.'; res7 = document.getElementById('result_Q15_7').innerText == 'That‚Äôs right! The authors of the study conclude that, without relying on ‚ÄúAI‚Äù help, the control group made more errors and developed stronger debugging skills through experience.';text = res1 + res2 + res3 + res4 + res5 + res6 + res7;element.innerHTML = text;\n         return false;\n         }}</script>\n```\n\n:::\n\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"hint_80813\" onclick=\"return show_hint_80813()\">üê≠ Click on the mouse for a hint.</div>\n<div id=\"result_80813\" onclick=\"return show_hint_80813()\"></div>\n<script>function show_hint_80813(){var x = document.getElementById('result_80813').innerHTML; if(!x){document.getElementById('result_80813').innerHTML = 'The authors of the study only mention one of these as a possible reason. Which do you think is mostly likely?';} else {document.getElementById('result_80813').innerHTML = '';}}</script>\n```\n\n:::\n:::\n\n\n¬†\n\n[**Q15.8**]{style=\"color:green;\"} What do the results displayed in @fig-TamkinShenFig8 suggest about the impact of \"AI\" assistance on code reading skills?\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<form name=\"form_Q15_8\" onsubmit=\"return validate_form_Q15_8()\" method=\"post\">\n<label>\n<input type=\"radio\" name=\"answer_Q15_8\" id=\"answer_Q15_8_1\" value=\"The confidence intervals corresponding to the code reading questions overlap too much to be able to draw any conclusions.\"/>\nThe confidence intervals corresponding to the code reading questions overlap too much to be able to draw any conclusions.\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_Q15_8\" id=\"answer_Q15_8_2\" value=\"Both groups had similar exposure to reading code through the task, so the AI and non-AI groups performed similarly.\"/>\nBoth groups had similar exposure to reading code through the task, so the AI and non-AI groups performed similarly.\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_Q15_8\" id=\"answer_Q15_8_3\" value=\"The AI tool significantly improved code reading performance for the treatment group.\"/>\nThe AI tool significantly improved code reading performance for the treatment group.\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_Q15_8\" id=\"answer_Q15_8_4\" value=\"The control group had more experience reading code, so they scored considerably higher.\"/>\nThe control group had more experience reading code, so they scored considerably higher.\n</label>\n<br/>\n<input type=\"submit\" value=\"Check answer\"/>\n<div id=\"result_Q15_8\"></div>\n</form>\n<script>function validate_form_Q15_8() {var x, text; var x = document.forms['form_Q15_8']['answer_Q15_8'].value;if (x == 'Both groups had similar exposure to reading code through the task, so the AI and non-AI groups performed similarly.'){text = 'Indeed, the authors of the study write that the small gap suggests similar exposure and learning in code reading across groups.';} else {text = 'No, the small gap actually implies that AI assistance did not provide a major advantage in this area. Why might that be?';} document.getElementById('result_Q15_8').innerHTML = text; evaluate_final_score(); return false;}function evaluate_final_score(){\n         element = document.getElementById('checkdown_final_score');\n         if(element === null){return false;} else {var element, text, res1, res2, res3, res4, res5, res6, res7, res8;res1 = document.getElementById('result_Q15_1').innerText == 'That‚Äôs right, well done! Which metaphor(s) do you like best?The term ‚Äòstocastic parrot‚Äô is probably the best known LLM metaphor. It was introduced by computational linguist Emily M. Bender and colleagues in 2021 to characterise LLMs as systems that mimic text without true understanding, highlighting their limitations in processing meaning. This metaphor emphasises that LLMs generate outputs based on statistical patterns in their training data, similar to how parrots mimic sounds without comprehension.In 2022, the cognitive scientist Iris Van Rooij published a blog post in which she succictly explains why she believes that LLMs cannot legitimately be used for academic writing because they essentially automate plagiarism.The ‚Äòspicy autocomplete‚Äô metaphor is difficult to trace back to one or more specific author(s). It suggests that LLMs are just fancy versions of a smart phone‚Äôs predictive text, which predict the next word based on what came before, except that LLMs add some randomness, i.e. spice, to the output. This framing also implies that LLMs are pattern-matching algorithms without real understanding or problem-solving ability (see Gro√ü 2024).In 2024, Hicks, Humphries &amp; Slater published a paper in the journal ‚ÄòEthics and Information Technology‚Äô entitled ‚ÄòChatGPT is bullshit‚Äô, in which they argue that the output of LLMs is best understood as ‚Äòbullshit‚Äô in the philosophical sense described by Frankfurt (2005) because LLMs are indifferent to the truth of their outputs.‚ÄòSynthetic text extruding machines‚Äô is a term that Emily M. Bender and Alex Hanna like to use, e.g. in their 2025 book entitled ‚ÄòThe AI Con: How to Fight Big Tech‚Äôs Hype and Create the Future We Want‚Äô. They describe the process of LLM-generated texts by explaining that, ‚Äú[l]ike an industrial plastic process, language corpora are forced through complicated machinery to produce a product that looks like communicative language, but without any intent or thinking mind behind it.‚ÄùAs an alternative to well-established metaphors such as the ones listed above, classical philologist Gyburg Uhlmann proposed ‚Äòkitsch‚Äô as a new metaphor to describe the output of LLMs. She argues that ‚Äòkitsch‚Äô ‚Äúis particularly suitable for analytically illuminating a previously neglected feature of LLM-based images and texts: their tendency to produce homogeneous and average content, which is [‚Ä¶] leading to the equalisation of language, style and argument‚Äù (Uhlmann 2025).'; res2 = document.getElementById('result_Q15_2').innerText == 'That‚Äôs right!'; res3 = document.getElementById('result_Q15_3').innerText == 'That‚Äôs right!'; res4 = document.getElementById('result_Q15_4').innerText == 'That‚Äôs right!'; res5 = document.getElementById('result_Q15_5').innerText == 'That‚Äôs right!'; res6 = document.getElementById('result_Q15_6').innerText == 'That‚Äôs right! The debugging questions show the largest difference between the treatment (AI) and the control (no AI) groups.'; res7 = document.getElementById('result_Q15_7').innerText == 'That‚Äôs right! The authors of the study conclude that, without relying on ‚ÄúAI‚Äù help, the control group made more errors and developed stronger debugging skills through experience.'; res8 = document.getElementById('result_Q15_8').innerText == 'Indeed, the authors of the study write that the small gap suggests similar exposure and learning in code reading across groups.';text = res1 + res2 + res3 + res4 + res5 + res6 + res7 + res8;element.innerHTML = text;\n         return false;\n         }}</script>\n```\n\n:::\n:::\n\n:::\n::::\n\n## On the value of the commons\n\nIt is worth pointing out that the programmers in the control, no AI group in the aforementioned Anthropic study [@tamkin2026] did not write code with no help whatsoever. Instead, they had access to regular web searching and code documentation. It is a common misconception that learning to code involves memorising lots of functions and commands. In practice, programming always involves looking things up. Up until very recently, researchers would typically search the web to find answers to their coding problems instead of prompting an \"AI\" product. These searches led us to interesting forum discussions, blog posts by fellow researchers and developers, and helpful documentation files (e.g. in the form of so-called '[vignettes](https://r-pkgs.org/vignettes.html)'). If our web search did not lead to anything use, we prepared a **reprex,** short for minimal **repr**oducible **ex**ample [see @wickhamDataScienceImport2023: [Ch. 8](https://r4ds.hadley.nz/workflow-help.html) on getting help], posted it on a dedicated forum and community members would typically provide helpful answers within days or even hours.\n\nAll of these contents were human-generated. Not all were 100% reliable, but high-quality answers to problems on [StackOverflow](https://stackoverflow.com/questions), for example, were upvoted by readers and errors in the documentation of open-source packages were quickly identified and corrected by the community of users. The same principle was true of statistical questions. Today, this strategy is still viable but, unfortunately, there are a lot of AI-generated webpages that one first needs to ignore.\n\n:::: {.content-visible when-format=\"html\"}\n::: {.callout-tip collapse=\"false\"}\n#### Your turn! {.unnumbered}\n\n[**Q15.9**]{style=\"color:green;\"} What is [Stack Exchange](https://stats.stackexchange.com/)?\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<form name=\"form_Q15_9\" onsubmit=\"return validate_form_Q15_9()\" method=\"post\">\n<label>\n<input type=\"radio\" name=\"answer_Q15_9\" id=\"answer_Q15_9_1\" value=\"An online game for learning about programming and statistics, among other topics.\"/>\nAn online game for learning about programming and statistics, among other topics.\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_Q15_9\" id=\"answer_Q15_9_2\" value=\"An open-source tool for managing project tasks and workflows that is widely used in academic research.\"/>\nAn open-source tool for managing project tasks and workflows that is widely used in academic research.\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_Q15_9\" id=\"answer_Q15_9_3\" value=\"A network of websites where people can ask and answer questions on a wide range of topics, the most popular of which is StackOverflow for programming questions.\"/>\nA network of websites where people can ask and answer questions on a wide range of topics, the most popular of which is StackOverflow for programming questions.\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_Q15_9\" id=\"answer_Q15_9_4\" value=\"A free cloud-based service for hosting and managing data and code repositories.\"/>\nA free cloud-based service for hosting and managing data and code repositories.\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_Q15_9\" id=\"answer_Q15_9_5\" value=\"An open-source social media platform for sharing photos and videos about research and programming.\"/>\nAn open-source social media platform for sharing photos and videos about research and programming.\n</label>\n<br/>\n<input type=\"submit\" value=\"Check answer\"/>\n<div id=\"result_Q15_9\"></div>\n</form>\n<script>function validate_form_Q15_9() {var x, text; var x = document.forms['form_Q15_9']['answer_Q15_9'].value;if (x == 'A network of websites where people can ask and answer questions on a wide range of topics, the most popular of which is StackOverflow for programming questions.'){text = 'That‚Äôs right!';} else {text = 'No, not quite.';} document.getElementById('result_Q15_9').innerHTML = text; evaluate_final_score(); return false;}function evaluate_final_score(){\n         element = document.getElementById('checkdown_final_score');\n         if(element === null){return false;} else {var element, text, res1, res2, res3, res4, res5, res6, res7, res8, res9;res1 = document.getElementById('result_Q15_1').innerText == 'That‚Äôs right, well done! Which metaphor(s) do you like best?The term ‚Äòstocastic parrot‚Äô is probably the best known LLM metaphor. It was introduced by computational linguist Emily M. Bender and colleagues in 2021 to characterise LLMs as systems that mimic text without true understanding, highlighting their limitations in processing meaning. This metaphor emphasises that LLMs generate outputs based on statistical patterns in their training data, similar to how parrots mimic sounds without comprehension.In 2022, the cognitive scientist Iris Van Rooij published a blog post in which she succictly explains why she believes that LLMs cannot legitimately be used for academic writing because they essentially automate plagiarism.The ‚Äòspicy autocomplete‚Äô metaphor is difficult to trace back to one or more specific author(s). It suggests that LLMs are just fancy versions of a smart phone‚Äôs predictive text, which predict the next word based on what came before, except that LLMs add some randomness, i.e. spice, to the output. This framing also implies that LLMs are pattern-matching algorithms without real understanding or problem-solving ability (see Gro√ü 2024).In 2024, Hicks, Humphries &amp; Slater published a paper in the journal ‚ÄòEthics and Information Technology‚Äô entitled ‚ÄòChatGPT is bullshit‚Äô, in which they argue that the output of LLMs is best understood as ‚Äòbullshit‚Äô in the philosophical sense described by Frankfurt (2005) because LLMs are indifferent to the truth of their outputs.‚ÄòSynthetic text extruding machines‚Äô is a term that Emily M. Bender and Alex Hanna like to use, e.g. in their 2025 book entitled ‚ÄòThe AI Con: How to Fight Big Tech‚Äôs Hype and Create the Future We Want‚Äô. They describe the process of LLM-generated texts by explaining that, ‚Äú[l]ike an industrial plastic process, language corpora are forced through complicated machinery to produce a product that looks like communicative language, but without any intent or thinking mind behind it.‚ÄùAs an alternative to well-established metaphors such as the ones listed above, classical philologist Gyburg Uhlmann proposed ‚Äòkitsch‚Äô as a new metaphor to describe the output of LLMs. She argues that ‚Äòkitsch‚Äô ‚Äúis particularly suitable for analytically illuminating a previously neglected feature of LLM-based images and texts: their tendency to produce homogeneous and average content, which is [‚Ä¶] leading to the equalisation of language, style and argument‚Äù (Uhlmann 2025).'; res2 = document.getElementById('result_Q15_2').innerText == 'That‚Äôs right!'; res3 = document.getElementById('result_Q15_3').innerText == 'That‚Äôs right!'; res4 = document.getElementById('result_Q15_4').innerText == 'That‚Äôs right!'; res5 = document.getElementById('result_Q15_5').innerText == 'That‚Äôs right!'; res6 = document.getElementById('result_Q15_6').innerText == 'That‚Äôs right! The debugging questions show the largest difference between the treatment (AI) and the control (no AI) groups.'; res7 = document.getElementById('result_Q15_7').innerText == 'That‚Äôs right! The authors of the study conclude that, without relying on ‚ÄúAI‚Äù help, the control group made more errors and developed stronger debugging skills through experience.'; res8 = document.getElementById('result_Q15_8').innerText == 'Indeed, the authors of the study write that the small gap suggests similar exposure and learning in code reading across groups.'; res9 = document.getElementById('result_Q15_9').innerText == 'That‚Äôs right!';text = res1 + res2 + res3 + res4 + res5 + res6 + res7 + res8 + res9;element.innerHTML = text;\n         return false;\n         }}</script>\n```\n\n:::\n\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"hint_86621\" onclick=\"return show_hint_86621()\">üê≠ Click on the mouse for a hint.</div>\n<div id=\"result_86621\" onclick=\"return show_hint_86621()\"></div>\n<script>function show_hint_86621(){var x = document.getElementById('result_86621').innerHTML; if(!x){document.getElementById('result_86621').innerHTML = 'Find out more by heading to <a href=\"https://stats.stackexchange.com/\">https://stats.stackexchange.com/</a>.';} else {document.getElementById('result_86621').innerHTML = '';}}</script>\n```\n\n:::\n:::\n\n\n¬†\n\n[**Q15.10**]{style=\"color:green;\"} Open this [archived version](https://web.archive.org/web/20260209093908/https://stats.stackexchange.com/questions/11609/clarification-on-interpreting-confidence-intervals) of a Q&A about confidence intervals hosted on Cross Validated, Stack Exchange's statistics forum. In which year did Eliott originally ask their question?\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<form name=\"form_Q15_10\" onsubmit=\"return validate_form_Q15_10()\" method=\"post\">\n<input type=\"text\" placeholder=\"\" name=\"answer_Q15_10\"/>\n<input type=\"submit\" value=\"Check answer\"/>\n<div id=\"result_Q15_10\"></div>\n</form>\n<script>function validate_form_Q15_10() {var x, text; var x = document.forms['form_Q15_10']['answer_Q15_10'].value;if (x == '2011'){text = 'That‚Äôs right!';} else {text = 'No, not quite.';} document.getElementById('result_Q15_10').innerHTML = text; evaluate_final_score(); return false;}function evaluate_final_score(){\n         element = document.getElementById('checkdown_final_score');\n         if(element === null){return false;} else {var element, text, res1, res2, res3, res4, res5, res6, res7, res8, res9, res10;res1 = document.getElementById('result_Q15_1').innerText == 'That‚Äôs right, well done! Which metaphor(s) do you like best?The term ‚Äòstocastic parrot‚Äô is probably the best known LLM metaphor. It was introduced by computational linguist Emily M. Bender and colleagues in 2021 to characterise LLMs as systems that mimic text without true understanding, highlighting their limitations in processing meaning. This metaphor emphasises that LLMs generate outputs based on statistical patterns in their training data, similar to how parrots mimic sounds without comprehension.In 2022, the cognitive scientist Iris Van Rooij published a blog post in which she succictly explains why she believes that LLMs cannot legitimately be used for academic writing because they essentially automate plagiarism.The ‚Äòspicy autocomplete‚Äô metaphor is difficult to trace back to one or more specific author(s). It suggests that LLMs are just fancy versions of a smart phone‚Äôs predictive text, which predict the next word based on what came before, except that LLMs add some randomness, i.e. spice, to the output. This framing also implies that LLMs are pattern-matching algorithms without real understanding or problem-solving ability (see Gro√ü 2024).In 2024, Hicks, Humphries &amp; Slater published a paper in the journal ‚ÄòEthics and Information Technology‚Äô entitled ‚ÄòChatGPT is bullshit‚Äô, in which they argue that the output of LLMs is best understood as ‚Äòbullshit‚Äô in the philosophical sense described by Frankfurt (2005) because LLMs are indifferent to the truth of their outputs.‚ÄòSynthetic text extruding machines‚Äô is a term that Emily M. Bender and Alex Hanna like to use, e.g. in their 2025 book entitled ‚ÄòThe AI Con: How to Fight Big Tech‚Äôs Hype and Create the Future We Want‚Äô. They describe the process of LLM-generated texts by explaining that, ‚Äú[l]ike an industrial plastic process, language corpora are forced through complicated machinery to produce a product that looks like communicative language, but without any intent or thinking mind behind it.‚ÄùAs an alternative to well-established metaphors such as the ones listed above, classical philologist Gyburg Uhlmann proposed ‚Äòkitsch‚Äô as a new metaphor to describe the output of LLMs. She argues that ‚Äòkitsch‚Äô ‚Äúis particularly suitable for analytically illuminating a previously neglected feature of LLM-based images and texts: their tendency to produce homogeneous and average content, which is [‚Ä¶] leading to the equalisation of language, style and argument‚Äù (Uhlmann 2025).'; res2 = document.getElementById('result_Q15_2').innerText == 'That‚Äôs right!'; res3 = document.getElementById('result_Q15_3').innerText == 'That‚Äôs right!'; res4 = document.getElementById('result_Q15_4').innerText == 'That‚Äôs right!'; res5 = document.getElementById('result_Q15_5').innerText == 'That‚Äôs right!'; res6 = document.getElementById('result_Q15_6').innerText == 'That‚Äôs right! The debugging questions show the largest difference between the treatment (AI) and the control (no AI) groups.'; res7 = document.getElementById('result_Q15_7').innerText == 'That‚Äôs right! The authors of the study conclude that, without relying on ‚ÄúAI‚Äù help, the control group made more errors and developed stronger debugging skills through experience.'; res8 = document.getElementById('result_Q15_8').innerText == 'Indeed, the authors of the study write that the small gap suggests similar exposure and learning in code reading across groups.'; res9 = document.getElementById('result_Q15_9').innerText == 'That‚Äôs right!'; res10 = document.getElementById('result_Q15_10').innerText == 'That‚Äôs right!';text = res1 + res2 + res3 + res4 + res5 + res6 + res7 + res8 + res9 + res10;element.innerHTML = text;\n         return false;\n         }}</script>\n```\n\n:::\n\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"hint_65706\" onclick=\"return show_hint_65706()\">üê≠ Click on the mouse for a hint.</div>\n<div id=\"result_65706\" onclick=\"return show_hint_65706()\"></div>\n<script>function show_hint_65706(){var x = document.getElementById('result_65706').innerHTML; if(!x){document.getElementById('result_65706').innerHTML = 'Visit the linked Q&amp;A page on Cross Validated and scroll to the bottom of the first post to find out a) when the question was last edited and b) when it was first asked.';} else {document.getElementById('result_65706').innerHTML = '';}}</script>\n```\n\n:::\n:::\n\n\n¬†\n\n[**Q15.11**]{style=\"color:green;\"} As of 9 February 2026 when the page was archived, how many Cross Validated members had upvoted the top answer?\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<form name=\"form_Q15_11\" onsubmit=\"return validate_form_Q15_11()\" method=\"post\">\n<label>\n<input type=\"radio\" name=\"answer_Q15_11\" id=\"answer_Q15_11_1\" value=\"34 members\"/>\n34 members\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_Q15_11\" id=\"answer_Q15_11_2\" value=\"28 members\"/>\n28 members\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_Q15_11\" id=\"answer_Q15_11_3\" value=\"30 members\"/>\n30 members\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_Q15_11\" id=\"answer_Q15_11_4\" value=\"32 members\"/>\n32 members\n</label>\n<br/>\n<input type=\"submit\" value=\"Check answer\"/>\n<div id=\"result_Q15_11\"></div>\n</form>\n<script>function validate_form_Q15_11() {var x, text; var x = document.forms['form_Q15_11']['answer_Q15_11'].value;if (x == '32 members'){text = 'That‚Äôs right!';} else {text = 'No, not quite.';} document.getElementById('result_Q15_11').innerHTML = text; evaluate_final_score(); return false;}function evaluate_final_score(){\n         element = document.getElementById('checkdown_final_score');\n         if(element === null){return false;} else {var element, text, res1, res2, res3, res4, res5, res6, res7, res8, res9, res10, res11;res1 = document.getElementById('result_Q15_1').innerText == 'That‚Äôs right, well done! Which metaphor(s) do you like best?The term ‚Äòstocastic parrot‚Äô is probably the best known LLM metaphor. It was introduced by computational linguist Emily M. Bender and colleagues in 2021 to characterise LLMs as systems that mimic text without true understanding, highlighting their limitations in processing meaning. This metaphor emphasises that LLMs generate outputs based on statistical patterns in their training data, similar to how parrots mimic sounds without comprehension.In 2022, the cognitive scientist Iris Van Rooij published a blog post in which she succictly explains why she believes that LLMs cannot legitimately be used for academic writing because they essentially automate plagiarism.The ‚Äòspicy autocomplete‚Äô metaphor is difficult to trace back to one or more specific author(s). It suggests that LLMs are just fancy versions of a smart phone‚Äôs predictive text, which predict the next word based on what came before, except that LLMs add some randomness, i.e. spice, to the output. This framing also implies that LLMs are pattern-matching algorithms without real understanding or problem-solving ability (see Gro√ü 2024).In 2024, Hicks, Humphries &amp; Slater published a paper in the journal ‚ÄòEthics and Information Technology‚Äô entitled ‚ÄòChatGPT is bullshit‚Äô, in which they argue that the output of LLMs is best understood as ‚Äòbullshit‚Äô in the philosophical sense described by Frankfurt (2005) because LLMs are indifferent to the truth of their outputs.‚ÄòSynthetic text extruding machines‚Äô is a term that Emily M. Bender and Alex Hanna like to use, e.g. in their 2025 book entitled ‚ÄòThe AI Con: How to Fight Big Tech‚Äôs Hype and Create the Future We Want‚Äô. They describe the process of LLM-generated texts by explaining that, ‚Äú[l]ike an industrial plastic process, language corpora are forced through complicated machinery to produce a product that looks like communicative language, but without any intent or thinking mind behind it.‚ÄùAs an alternative to well-established metaphors such as the ones listed above, classical philologist Gyburg Uhlmann proposed ‚Äòkitsch‚Äô as a new metaphor to describe the output of LLMs. She argues that ‚Äòkitsch‚Äô ‚Äúis particularly suitable for analytically illuminating a previously neglected feature of LLM-based images and texts: their tendency to produce homogeneous and average content, which is [‚Ä¶] leading to the equalisation of language, style and argument‚Äù (Uhlmann 2025).'; res2 = document.getElementById('result_Q15_2').innerText == 'That‚Äôs right!'; res3 = document.getElementById('result_Q15_3').innerText == 'That‚Äôs right!'; res4 = document.getElementById('result_Q15_4').innerText == 'That‚Äôs right!'; res5 = document.getElementById('result_Q15_5').innerText == 'That‚Äôs right!'; res6 = document.getElementById('result_Q15_6').innerText == 'That‚Äôs right! The debugging questions show the largest difference between the treatment (AI) and the control (no AI) groups.'; res7 = document.getElementById('result_Q15_7').innerText == 'That‚Äôs right! The authors of the study conclude that, without relying on ‚ÄúAI‚Äù help, the control group made more errors and developed stronger debugging skills through experience.'; res8 = document.getElementById('result_Q15_8').innerText == 'Indeed, the authors of the study write that the small gap suggests similar exposure and learning in code reading across groups.'; res9 = document.getElementById('result_Q15_9').innerText == 'That‚Äôs right!'; res10 = document.getElementById('result_Q15_10').innerText == 'That‚Äôs right!'; res11 = document.getElementById('result_Q15_11').innerText == 'That‚Äôs right!';text = res1 + res2 + res3 + res4 + res5 + res6 + res7 + res8 + res9 + res10 + res11;element.innerHTML = text;\n         return false;\n         }}</script>\n```\n\n:::\n\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"hint_38211\" onclick=\"return show_hint_38211()\">üê≠ Click on the mouse for a hint.</div>\n<div id=\"result_38211\" onclick=\"return show_hint_38211()\"></div>\n<script>function show_hint_38211(){var x = document.getElementById('result_38211').innerHTML; if(!x){document.getElementById('result_38211').innerHTML = 'Look for the number next to the up arrow in the top-left corner of the answer.';} else {document.getElementById('result_38211').innerHTML = '';}}</script>\n```\n\n:::\n:::\n\n\n¬†\n\nIn @sec-ttest, we conducted the following *t*-test to find out whether the observed difference between the L1 and L2 speakers' non-verbal IQ 'Blocks' test scores was significant or not:\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(formula = Blocks ~ Group, \n       data = Dabrowska.data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  Blocks by Group\nt = -4.4084, df = 152.46, p-value = 1.956e-05\nalternative hypothesis: true difference in means between group L1 and group L2 is not equal to 0\n95 percent confidence interval:\n -5.239791 -1.996693\nsample estimates:\nmean in group L1 mean in group L2 \n        13.84444         17.46269 \n```\n\n\n:::\n:::\n\n\nThe output of the `t.test()` function above was fed into an LLM (DeepSeek R1 Distill Llama 70B) with a prompt requesting a short paragraph interpreting these results. The model's response is printed below:\n\n> The Welch two‚Äësample t‚Äëtest shows a significant difference between the groups (t‚ÄØ=‚ÄØ‚Äë4.41, df‚ÄØ‚âà‚ÄØ152.5, p‚ÄØ=‚ÄØ1.96‚ÄØ√ó‚ÄØ10‚Åª‚Åµ). Group‚ÄØL1 has a mean of 13.84 blocks, while group L2‚Äôs mean is 17.46 blocks, a mean difference of ‚Äë3.62 blocks. The 95‚ÄØ% confidence interval for this difference (‚Äë5.24 to ‚Äë1.99) means we can be 95‚ÄØ% confident that the true average in L2 exceeds that in L1 by roughly 2 to 5 blocks. Because the interval does not include zero, the null hypothesis of equal means is rejected.\n\n[**Q15.12**]{style=\"color:green;\"} Aside from the fact that \"blocks\" is not a unit, which major statistical error does the LLM's interpretation of the *t*-test output include?\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<form name=\"form_Q15_12\" onsubmit=\"return validate_form_Q15_12()\" method=\"post\">\n<label>\n<input type=\"radio\" name=\"answer_Q15_12\" id=\"answer_Q15_12_1\" value=\"The p-value is incorrectly translated and is in fact too large to reject the null hypothesis.\"/>\nThe p-value is incorrectly translated and is in fact too large to reject the null hypothesis.\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_Q15_12\" id=\"answer_Q15_12_2\" value=\"The t-test output makes a statement about the alternative hypothesis, not the null hypothesis.\"/>\nThe t-test output makes a statement about the alternative hypothesis, not the null hypothesis.\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_Q15_12\" id=\"answer_Q15_12_3\" value=\"The confidence interval is incorrectly interpreted as a probability statement about the true mean difference.\"/>\nThe confidence interval is incorrectly interpreted as a probability statement about the true mean difference.\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_Q15_12\" id=\"answer_Q15_12_4\" value=\"The LLM fails to recognise that the t-test was not appropriate because the data are not normally distributed.\"/>\nThe LLM fails to recognise that the t-test was not appropriate because the data are not normally distributed.\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_Q15_12\" id=\"answer_Q15_12_5\" value=\"The mean difference should be positive, not negative, since L2 has a higher mean.\"/>\nThe mean difference should be positive, not negative, since L2 has a higher mean.\n</label>\n<br/>\n<input type=\"submit\" value=\"Check answer\"/>\n<div id=\"result_Q15_12\"></div>\n</form>\n<script>function validate_form_Q15_12() {var x, text; var x = document.forms['form_Q15_12']['answer_Q15_12'].value;if (x == 'The confidence interval is incorrectly interpreted as a probability statement about the true mean difference.'){text = 'That‚Äôs right! The 95% confidence interval does not mean that there‚Äôs a 95% probability that the true difference lies in that range. Rather, it means that, if we repeated the study many times, 95% of such intervals would contain the true difference.';} else {text = 'No, that‚Äôs not it. Consider what the confidence interval actually means. This is explained in the Stack Exchange post examined as part of the quiz questions above.';} document.getElementById('result_Q15_12').innerHTML = text; evaluate_final_score(); return false;}function evaluate_final_score(){\n         element = document.getElementById('checkdown_final_score');\n         if(element === null){return false;} else {var element, text, res1, res2;res1 = document.getElementById('result_Q15_4').innerText == 'That‚Äôs right!'; res2 = document.getElementById('result_Q15_12').innerText == 'That‚Äôs right! The 95% confidence interval does not mean that there‚Äôs a 95% probability that the true difference lies in that range. Rather, it means that, if we repeated the study many times, 95% of such intervals would contain the true difference.';text = res1 + res2;element.innerHTML = text;\n         return false;\n         }}</script>\n```\n\n:::\n:::\n\n:::\n::::\n\nIn addition to being flooded by AI slop making it difficult to find reliable information, human content creators are finding that their intellectual property is being scraped without consent to be used as training data for commercial \"AI\"-products. In the era of [Open Scholarship](@sec-OpenScholarship), researchers, software developers, and educators have been sharing their work with the world for the benefit of scientific progress, typically with only authorship attribution as a reward. However, this may change as AI companies appropriate their work and LLMs automate plagiarism [@rooijAutomatedPlagiarism2022].\n\nThis may sound like an entirely dystopian situation but, up until fairly recently, researchers had to pay to use programming languages for statistical analyses and scientific computing. FORTAN compilers, MatLab, S, and SPPS were (and still are) proprietary software which were inaccessible to many researchers and students. It is no exaggeration to say that open-source, community-led programming languages such as R, Python, and Julia revolutionised data analysis, making state-of-the-art methods accessible to far more people. However, as high-quality open-access resources become rarer due the contamination of AI slop and illegal scraping, we may be returning to an era of restricted access to scientific computing. Hence, it is worth remembering that it is communities of humans who have been developing programming languages such as `R` and Python, their many extensions such as the {tidyverse} packages, open-source software such as *R Studio*, and high-quality documentation and Open Educational Resources.\n\nThese communities depend on collaboration, interactions, and mutual support. Yet, we have entered an age where human interactions are marketed as unproductive, time-consuming, and burdensome. We are told that they can easily be replaced by more efficient and \"objective\" chatbots. Aside from the fact that LLMs are known to be prone to all kinds of very serious biases and that their efficiency is far from proven [see e.g. @AIVsHuman on how AI code creates more problems than human-generated code], we should not loose sight of the value of subjective, human interactions. This is not to say that interacting with an LLM is never useful but rather that I also strongly encourage you to devote time to learning from reliable, human-generated resources (see e.g. [Next-step resources](https://elenlefoll.github.io/RstatsTextbook/A_FurtherResources.html)), join a course with other human beings to continue your learning journey, and/or find a learning buddy to discuss and solve problems together. This textbook was entirely human-generated and benefited greatly from countless rounds of revisions thanks to interactions with and feedback from (human!) students and colleagues (see [Acknowledgements](https://elenlefoll.github.io/RstatsTextbook/#acknowledgements)). Of course, it would have been quicker to write the textbook without asking for regular feedback and to get an LLM to generate first drafts of sections, code, and/or quiz questions. But in research, teaching, and learning, quality matters more than velocity.\n\n### Check your progress :star2: {.unnumbered}\n\nIn this concluding chapter, you have answered [`<span id=\"checkdown_final_score\">0</span>`{=html} out of 12 questions]{style=\"color:green;\"} correctly. Congratulations!\n\n::: callout-note\n#### More food for thought üçèüçé\n\n-   Bender, Emily M. & Alex Hanna. 2025. *The AI Con: How to Fight Big Tech‚Äôs Hype and Create the Future We Want*. HarperCollins.\n\n-   Bergstrom, Carl T. & Jevin D. West. Modern-Day Oracles or Bullshit Machines: How to thrive in a ChatGPT world. Online course. <https://thebullshitmachines.com>. \\[Open Educational Resource\\].\n\n-   Dingemanse, Mark. 2024. Generative AI and Research Integrity. OSF. <https://doi.org/10.31219/osf.io/2c48n>. \\[Open Access\\].\n\n-   Guest, Olivia, Marcela Suarez, Barbara M√ºller, Edwin van Meerkerk, Arnoud Oude Groote Beverborg, Ronald de Haan, Andrea Reyes Elizondo, et al. 2025. Against the Uncritical Adoption of ‚ÄúAI‚Äù Technologies in Academia. Zenodo. <https://zenodo.org/records/17065099>. \\[Open Access\\].\n\n    ‚û°Ô∏è See also Olivia Guest's curated list of readings on critical AI literacy: <https://olivia.science/ai/> \\[Open Access\\].\n\n-   M√ºhlhoff, Rainer. 2025. *The ethics of AI: Power, critique, responsibility*. Bristol: Bristol University Press. <https://doi.org/10.51952/9781529249262>. \\[[Open Access](https://bristoluniversitypressdigital.com/downloadpdf/monobook-oa/book/9781529249262/9781529249262.pdf)\\].\n\n    ‚û°Ô∏è See also Rainer M√ºhlhoff's *Introduction to the Ethics of AI 2025* lecture videos. [https://rainermuehlhoff.de/en/EoAI2025/](#0){.uri}. \\[Open Educational Resource\\].\n:::\n\n## What's next? üß≠\n\nThis textbook has taken you on a journey: first introducing Open Scholarship (@sec-OpenScholarship), which forms the backbone of this textbook's approach to doing science, then consolidating knowledge about file formats, file naming, and project organisation (@sec-DataFormats and @sec-DataManagement), which are often major hurdles for successful data analysis pipelines. Having installed and set up `R` and *RStudio* (@sec-Installing), you took your first steps in learning to code in `R` (@sec-GettingStaRted). Next, you learnt to import real research data into an `R` project (@sec-ImportingData). From @sec-VaRiablesAndFunctions onwards, you learnt how to analyse data in `R` using descriptive and inferential statistics and data visualisations. This entailed wrangling data to prepare them for such analyses, as well as developing an understanding of key statistical concepts such as measures of central tendency and variability, distributions, effect sizes, confidence intervals, and *p*-values. @sec-SLR introduced statistical modelling with simple linear regression and @sec-MLR expanded this concept to multiple predictor variables and interactions between predictors. Having mastered this foundational knowledge, you are now ready to tackle more advanced statistical methods such as mixed-effects and non-linear regression models, logistic and other classification models, machine learning algorithms, and much more (see [Next-step resources](https://elenlefoll.github.io/RstatsTextbook/A_FurtherResources.html)). In @sec-LiteRateProgramming, you learnt how to apply literate programming skills to conduct and publish reproducible research in Quarto. Finally, in this concluding chapter, you reflected on the values of critical thinking, human learning, and of nurturing communities in the day and age of \"AI\".\n\n> Learning statistical theory and practice are inseparable from scientific reasoning [@vasishthHowEmbraceVariation2021: 1312].\n\nI personally believe that it will always be worth investing in learning complex matters, developing critical thinking skills, and building meaningful human relationships. No matter how powerful and efficient future \"AI\" products may be, the skills that will be valued in the future will continue to be collaboration, computational thinking, and critical (statistical) literacy, not \"advanced\" prompting techniques. The resources listed in the [Appendix](https://elenlefoll.github.io/RstatsTextbook/A_FurtherResources.html) are a great starting point to continuing your learning journey.\n\nIn addition, the [online version](https://elenlefoll.github.io/RstatsTextbook) of this textbook features a growing set of case-study chapters, which students from my 'Introduction to Data Analysis in `R`' class have (co-)authored. Each chapter attempts to computationally reproduce a published a linguistics study using the authors' original data. The student authors document all the steps necessary to reproduce the results and discuss the success (or not!) of the reproduction. Picking a case study that interests you or that covers a method that you'd like to learn more about and attempting to reproduce the steps outlined in the chapter is a great to consolidate and expand on the skills and knowledge that you have acquired so far.\n\n![Happy lea`R`ning!Ô∏è (artwork by [Allison Horst](https://allisonhorst.com/r-packages-functions) [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/))](images/AHorst_RFlowers.png){fig-alt=\"Two cute little monsters decorating the R logo with hearts, flowers, and smilies.\"}\n\n::: {.content-visible when-format=\"docx\"}\n# Further resources/ideas {.unnumbered}\n\n-   <https://posit.co/blog/introducing-gander/>\n\n-   <https://air.albert-rapp.de/>\n\n-   <https://www.seascapemodels.org/AI-assistants-for-scientific-coding/>\n\n-   <https://www.numberanalytics.com/blog/priors-categorical-data-guide> (AI may actually get worse over time!)\n\n-   <https://mikelovesrobots.substack.com/p/wheres-the-shovelware-why-ai-coding>\n\n-   <https://www.ru.nl/en/research/research-news/opposing-the-inevitability-of-ai-at-universities-is-possible-and-necessary>\n\n-   <https://research-and-innovation.ec.europa.eu/document/2b6cf7e5-36ac-41cb-aab5-0d32050143dc_en>\n\n-   <https://www.dfg.de/resource/blob/289676/89c03e7a7a8a024093602995974832f9/230921-statement-executive-committee-ki-ai-data.pdf>\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}