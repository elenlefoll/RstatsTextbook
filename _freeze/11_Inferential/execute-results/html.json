{
  "hash": "b318cdf3e14730d754fc748dcf24ad5f",
  "result": {
    "engine": "knitr",
    "markdown": "---\nengine: knitr\nbibliography: references.bib\nhtml:\n  code-link: true\n---\n\n# Infe`R`ential statistics {#sec-Inferential}\n\n::: callout-warning\n## Warning\n\nAs with the rest of this textbook (see [Preface](https://elenlefoll.github.io/RstatsTextbook/)), this chapter is very much **work in progress**. All feedback is very welcome.\n:::\n\n\n\n### Chapter overview {.unnumbered}\n\nThis chapter provides an introduction to:\n\n-   sampling procedures\n-   null hypothesis significance testing (NHST)\n-   *t*-tests\n-   correlations and correlation tests\n-   *p-*values\n-   effect sizes\n-   confidence intervals\n-   assumptions of statistical significance tests\n\nDescribing and visualising sample data belongs to the realm of descriptive statistics. Attempting to generalise trends from our observed data to a larger population takes us to inferential statistics. Whereas descriptive statistics is about summarising and describing a specific dataset (our sample), with inferential statistics, we draw on our sample data to make educated guesses about a larger population that we have not directly studied. This helps us to determine whether the patterns that we observed thanks to descriptive statistics and data visualisations are likely to reflect broader trends that apply to the larger population or, instead, can more likely be attributed to random variation in the sample data.\n\n::: {.callout-warning collapse=\"false\"}\n### Prerequisites\n\nAs with previous chapters, all the examples, tasks, and quiz questions from this chapter are based on data from:\n\n> Dąbrowska, Ewa. 2019. Experience, Aptitude, and Individual Differences in Linguistic Attainment: A Comparison of Native and Nonnative Speakers. Language Learning 69(S1). 72–100. <https://doi.org/10.1111/lang.12323>.\n\nOur starting point for this chapter is the wrangled combined dataset that we created and saved in @sec-DataWrangling. Follow the instructions in @sec-filter to create this `R` object.\n\nAlternatively, you can download `Dabrowska2019.zip` from [the textbook's GitHub repository](https://github.com/elenlefoll/RstatsTextbook/raw/69d1e31be7394f2b612825f031ebffeb75886390/Dabrowska2019.zip){.uri}. To launch the project correctly, first unzip the file and then double-click on the `Dabrowska2019.Rproj` file.\n\nTo begin, load the `combined_L1_L2_data.rds` file that we created in @sec-DataWrangling. This file contains the full data of all the L1 and L2 participants of @DabrowskaExperienceAptitudeIndividual2019. The categorical variables are stored as factors and obvious data entry inconsistencies and typos have been corrected (see @sec-DataWrangling).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(here)\n\nDabrowska.data <- readRDS(file = here(\"data\", \"processed\", \"combined_L1_L2_data.rds\"))\n```\n:::\n\n\nBefore you get started, check that you have correctly imported the data by examining the output of `View(Dabrowska.data)` and `str(Dabrowska.data)`. In addition, run the following lines of code to load the {tidyverse} and create \"clean\" versions of both the L1 and L2 datasets as separate `R` objects.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\nL1.data <- Dabrowska.data |> \n  filter(Group == \"L1\")\n\nL2.data <- Dabrowska.data |> \n  filter(Group == \"L2\")\n```\n:::\n\n\nOnce you are satisfied that your data are sound, read on to learn about frequentist statistical inference and significance testing!\n:::\n\n## From the sample to the population {#sec-Sampling}\n\nSo far, we have described the data collected by @DabrowskaExperienceAptitudeIndividual2019. In @sec-DescRiptiveStats, we described these data using **descriptive statistics** thanks to measures of [central tendencies](#sec-CentralTendency) (e.g., [means](#sec-means)) and [measures of variability](#sec-Variability) around the central tendency (e.g., [standard deviations](#sec-SD)). In @sec-DataViz, we described the data **visually** using different kinds of statistical plots. These descriptive analyses enabled us to spot some interesting patterns (and there are many more for you to explore!). For instance, we noticed that:\n\n-   On average, L2 participants scored lower than the L1 participants on the English grammar, vocabulary and collocation comprehension tests. This was to be expected, but our visualisations also revealed that many L2 participants scored at least as well and sometimes even better than average L1 participants.\n\n-   On average, L2 participants obtained higher non-verbal IQ scores (as measured by the Blocks test) than L1 participants but, here, too, there was a lot of overlap between the two distributions.\n\n-   For both L1 and L2 participants, there was a positive correlation between the number of years they were in formal education and their English grammar comprehension test scores: the longer they were in education, the better they performed on the test.\n\nIn this chapter, we ask whether these observations are **likely to be generalisable** beyond @DabrowskaExperienceAptitudeIndividual2019's **sample** of 90 English native speakers and 67 non-native English speakers to a broader **population**. In the context of this study, we will define the full population as all adult English native and non-native speakers living in the UK.\n\nIn the language and education sciences we rarely have access to the entire population for which we would ideally like to generalise our findings. For example, we can hardly go and test *all* English speakers living in the UK. Instead, we have to make due with a **sample** of L1 and L2 English speakers from the UK. Since our studies attempt to *infer* information about entire populations based only sample data, the quality of our samples is crucial: no sophisticated statistical procedure can produce any meaningful inferential statistics from a biased or otherwise flawed sample!\n\n::: callout-note\n# Sampling methods\n\nThere are different ways to draw samples from a population. The most common methods are summarised below.\n\n+----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Method                           | What it means                                                                                                                                 | Typical use in linguistics                                                                                                                                                                                 | Main advantages                                                                                 | Main limitations                                                                                                                                                             |\n+==================================+===============================================================================================================================================+============================================================================================================================================================================================================+=================================================================================================+==============================================================================================================================================================================+\n| **Random sampling**              | Every member of the target population has an equal chance of being selected.                                                                  | Rarely feasible for human populations, but can sometimes be approximated with census lists. Feasible in other contexts, e.g., when the target population are all words featured in a specific dictionary.  | Minimises systematic bias. The results can be generalised using inferential statistics methods. | Requires a complete sampling frame (e.g., a list of all English speakers living in the UK) and a perfect world in which everyone sampled consents to participating in study. |\n+----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| **Stratified sampling\\           | The population is divided into strata (e.g., by dialectal region, age group, education level) and a random sample is drawn from each stratum. | Used when researchers need balanced data from distinct sub‑populations (e.g., speakers from several dialectal areas) and their intersections (e.g., a good balance of genders across each dialectal area). | Guarantees coverage of all relevant sub‑populations; reduces sampling error within the strata.  | As above, requires reliable, exhaustive lists for each stratum and consent for all selected; more complex to organise.                                                       |\n| **(a subtype of random sampling) |                                                                                                                                               |                                                                                                                                                                                                            |                                                                                                 |                                                                                                                                                                              |\n+----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| **Cluster sampling\\              | Whole clusters (e.g., villages, schools, neighbourhoods) are randomly selected, then all members of the chosen clusters are studied.          | Data collection in schools and fieldwork in remote regions where a full speaker list is impractical.                                                                                                       | Efficient when clusters are naturally defined; reduces costs and organisational burden.         | Increases sampling error if clusters are internally homogeneous; may miss variation outside selected clusters.                                                               |\n| **(a subtype of random sampling) |                                                                                                                                               |                                                                                                                                                                                                            |                                                                                                 |                                                                                                                                                                              |\n+----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| **Representative\\                | The sample is designed so as to match the population on key characteristics (e.g., age, gender, region, education).                           | Researchers recruit speakers until the sample mirrors known demographics from census data or other sources.                                                                                                | Often considered to provide a “good enough” picture when true random sampling is impossible.    | Often difficult to implement because we rarely known enough about the characteristics of the full population; vulnerable to self-selection bias.                             |\n| (quota) sampling**               |                                                                                                                                               |                                                                                                                                                                                                            |                                                                                                 |                                                                                                                                                                              |\n+----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| **Convenience sampling**         | Participants are chosen because they are reachable and willing to participate.                                                                | Most common in experimental linguistics, online, and classroom‑based surveys.                                                                                                                              | Quicker, cheaper, and simpler.                                                                  | Over‑represents certain groups; suffers from self-selection bias.                                                                                                            |\n+----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\nIn practice, it is quite common for combination of these methods to be used. For example, when researchers run linguistics experiments via commercial platforms such as *Qualtrics* or *Amazon* *MechanicalTurk,* they can select their participants based on some demographic data to obtain a more **representative** **sample**. But the sample nonetheless remains a **convenience** **sample** as only people who sign up to earn money on these platforms can, by definition, be recruited on these platforms. As you can imagine, these click-workers are hardly representative of the full population, even if they are carefully sampled for age, gender, or socio-economic status.\n:::\n\nProvided we have a sufficiently **representative sample**, inferential statistics can help us to answer questions such as: Across all adult English speakers in the UK...\n\n-   do L1 speakers, on average, achieve higher scores than L2 speakers on English grammar and vocabulary comprehension tests?\n\n-   do L2 English speakers, on average, perform better on the non‑verbal IQ (Blocks) test than L1 speakers?\n\n-   is there a positive linear relationship between the number of years speakers were in formal education and their performance in an English grammar comprehension test?\n\n-   does the strength of this education‑grammar comprehension relationship differ between L1 and L2 speakers, or across different age cohorts?\n\nThough by no means the only framework available to us, the most common approach to answering such questions is **null hypothesis significance testing (NHST)** within the framework of **frequentist** statistical philosophy. What's *philosophy* got to with statistics, you may ask? It turns out that, contrary to popular belief, statistics is anything but an exact science. By definition, statistical inference involves making *inferences* about the unknown. Therefore, there are different ways to approach these questions.\n\nA promising alternative framework that is gaining traction in many disciplines — including in the language sciences — is **Bayesian statistics** (see Appendix of [next-step resources](https://elenlefoll.github.io/RstatsTextbook/A_FurtherResources.html) for recommended readings). In this textbook, however, we will focus on the basic principles of statistical inference within the **frequentist framework** — not because it is easier than Bayesian statistics, but rather because it remains the most widely used framework to date. Hence, even if you decide not to use frequentist statistics for your own research, you will certainly need to understand its principles in order to correctly interpret the results of published studies.\n\n::: {.callout-tip collapse=\"false\"}\n#### Your turn! {.unnumbered}\n\n[**Q11.1**]{style=\"color:green;\"} Which of the following statements best describes random sampling?\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<form name=\"form_12592\" onsubmit=\"return validate_form_12592()\" method=\"post\">\n<label>\n<input type=\"radio\" name=\"answer_12592\" id=\"answer_12592_1\" value=\"Each individual in the population has the same known probability of being chosen (e.g., 0.001%).\"/>\nEach individual in the population has the same known probability of being chosen (e.g., 0.001%).\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_12592\" id=\"answer_12592_2\" value=\"Participants are selected because they are easy to reach (e.g., university students).\"/>\nParticipants are selected because they are easy to reach (e.g., university students).\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_12592\" id=\"answer_12592_3\" value=\"The sample is built so that its composition (age, gender, socioeconomic status, etc.) matches known census figures.\"/>\nThe sample is built so that its composition (age, gender, socioeconomic status, etc.) matches known census figures.\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_12592\" id=\"answer_12592_4\" value=\"Every member of the target population is guaranteed to take part in the study.\"/>\nEvery member of the target population is guaranteed to take part in the study.\n</label>\n<br/>\n<input type=\"submit\" value=\"Check answer\"/>\n<div id=\"result_12592\"></div>\n</form>\n<script>function validate_form_12592() {var x, text; var x = document.forms['form_12592']['answer_12592'].value;if (x == 'Each individual in the population has the same known probability of being chosen (e.g., 0.001%).'){text = 'Correct! Random sampling (also called probability sampling) gives every individual an equal, known chance of being selected.';} else {text = 'Incorrect. Review the definition of random sampling in the table above. It is not about convenience or quota‑matching.';} document.getElementById('result_12592').innerHTML = text; evaluate_final_score(); return false;}function evaluate_final_score(){\n         element = document.getElementById('checkdown_final_score');\n         if(element === null){return false;} else {var element, text, res1;res1 = document.getElementById('result_12592').innerText == 'Correct! Random sampling (also called probability sampling) gives every individual an equal, known chance of being selected.';text = res1;element.innerHTML = text;\n         return false;\n         }}</script>\n```\n\n:::\n:::\n\n\n[**Q11.2**]{style=\"color:green;\"} True or false: In a convenience sample, the researcher can ensure that the sample is representative of the entire population if they are careful to match the sample to census demographics (age, gender, region, etc.).\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<form name=\"form_5683\" onsubmit=\"return validate_form_5683()\" method=\"post\">\n<label>\n<input type=\"radio\" name=\"answer_5683\" id=\"answer_5683_1\" value=\"True\"/>\nTrue\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_5683\" id=\"answer_5683_2\" value=\"False\"/>\nFalse\n</label>\n<br/>\n<input type=\"submit\" value=\"Check answer\"/>\n<div id=\"result_5683\"></div>\n</form>\n<script>function validate_form_5683() {var x, text; var x = document.forms['form_5683']['answer_5683'].value;if (x == 'False'){text = 'Correct. Even when a convenience sample is “quota‑matched” on observable demographics, it remains a convenience sample because participants first volunteered to take part (e.g., they signed up on a platform like MTurk). This means that unobserved factors, such as motivation, socioeconomic status, and language background, can still render the sample unrepresentative, thus limiting the external validity of inferential data analyses made on the basis of these data.';} else {text = 'Incorrect.';} document.getElementById('result_5683').innerHTML = text; evaluate_final_score(); return false;}function evaluate_final_score(){\n         element = document.getElementById('checkdown_final_score');\n         if(element === null){return false;} else {var element, text, res1, res2;res1 = document.getElementById('result_12592').innerText == 'Correct! Random sampling (also called probability sampling) gives every individual an equal, known chance of being selected.'; res2 = document.getElementById('result_5683').innerText == 'Correct. Even when a convenience sample is “quota‑matched” on observable demographics, it remains a convenience sample because participants first volunteered to take part (e.g., they signed up on a platform like MTurk). This means that unobserved factors, such as motivation, socioeconomic status, and language background, can still render the sample unrepresentative, thus limiting the external validity of inferential data analyses made on the basis of these data.';text = res1 + res2;element.innerHTML = text;\n         return false;\n         }}</script>\n```\n\n:::\n:::\n\n:::\n\n### Null hypothesis significance testing (NHST) {#sec-NHST}\n\nLet's begin by considering the following, intriguing research question:\n\n-   Across all adult English speakers in the UK, do L2 English speakers, on average, perform better on the non‑verbal IQ (Blocks) test than L1 speakers?\n\nIn the sample collected by @DabrowskaExperienceAptitudeIndividual2019, we can see that L2 speakers, on average, performed better than L1 speakers on the Blocks test. For the two groups, the mean `Blocks` test scores were:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nDabrowska.data |> \n  group_by(Group) |>\n  summarise(mean = mean(Blocks),\n            SD = sd(Blocks))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  Group  mean    SD\n  <fct> <dbl> <dbl>\n1 L1     13.8  5.56\n2 L2     17.5  4.70\n```\n\n\n:::\n:::\n\n\nHowever, we are also aware that there is a lot of variation around these average values. The standard deviations (SD) around these mean values inform us that there is more variability in the L1 group than in the L2 group. @fig-IQTestPlot visualises this variability (see @sec-IQR on how to interpret boxplots). On this boxplot, diamonds represent the mean values.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"See code to generate plot.\"}\ngroup_means <- Dabrowska.data |> \n  group_by(Group) |> \n  summarise(mean_blocks = mean(Blocks))\n\nggplot(data = Dabrowska.data, \n       aes(x = Group, \n           y = Blocks)) +\n  geom_boxplot(width = 0.5) +\n  stat_summary(\n    fun = mean,                     # what to plot\n    geom = \"point\",                 # as a point\n    shape = 18,                     # in a diamond shape\n    size = 4,                       # a little larger than the default\n    colour = \"purple\") +\n  geom_line(\n    data = group_means,                   \n    aes(x = as.numeric(Group), \n        y = mean_blocks),\n    colour = \"purple\",\n    linewidth = 1,\n    linetype = \"dashed\") +\n  geom_text(data = group_means,\n    aes(x = as.numeric(Group), \n        y = mean_blocks,\n        label = sprintf(\"%.2f\", mean_blocks)), # print the means to two decimal points\n    vjust = -1.4,\n    colour = \"purple\") +\n  labs(x = NULL,\n    y = \"Non-verbal IQ (Blocks) test\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![Comparison of non-verbal IQ (blocks) test scores between groups L1 and L2](11_Inferential_files/figure-html/fig-IQTestPlot-1.png){#fig-IQTestPlot fig-alt='A Boxplot comparing Non-verbal IQ (Blocks) test scores between two groups, L1 and L2. L1 has a mean of 13.84, while L2 has a mean of 17.46. A dashed purple line connects the mean values of both groups, indicating an increase from L1 to L2.' width=576}\n:::\n:::\n\n\nFollowing the NHST framework, we can quantify how *likely* it is that this observed difference in means — which is visualised as the dotted line in @fig-IQTestPlot — could have occurred due to chance, i.e. due to naturally occurring variation, only.\n\nWe start with the assumption that the patterns observed in our sample occurred by chance alone. This initial assumption is known as the **null hypothesis (H~0~)**. It suggests that any patterns observed in our data arose by mere coincidence rather than due to any real effect or underlying relationship. To answer our research question, we formulate the following null hypothesis:\n\n-   **H~0~**: On average, adult English L1 and L2 speakers in the UK perform *equally well* on the non-verbal IQ (Blocks) test.\n\nWe can also formulate an **alternative hypothesis**. This is the hypothesis that we will adopt if we have enough evidence to reject the null hypothesis:\n\n-   **H~1~**: On average, adult English L2 speakers in the UK perform *differently* on the non-verbal IQ (Blocks) test than L1 speakers.\n\nWe can conduct a **statistical significance test** to test the likelihood of the null hypothesis given our observed data. Such tests allow us to estimate the probability of observing the patterns that we have noticed (or more extreme ones) in a sample size similar to ours, assuming that there is *no* real pattern or relationship in the full population. In other words, these tests help us evaluate whether our findings are likely to be a random occurrence within our sample or indicative of an actual trend that is likely to be found in the entire population.\n\nIt is important to understand that these tests do not *prove* anything. They rely on probabilities and, as such, can only inform us as to how *likely* our observations (or more extreme ones) are, assuming that the null hypothesis is true. Thus, statistical significance tests can only provide information about whether we can reasonably **reject** or **fail to reject a null hypothesis** based on our sample data.\n\n::: callout-warning\n## Common misconception\n\nContrary to what is sometimes claimed, significance tests do *not* provide any information about the likelihood of either the null or the alternative hypothesis being true or false. In fact, what they provide can be conceptualised as the opposite: they help us to estimate the likelihood of our data given the null hypothesis.\n\n> Always remember that the null hypothesis is an assumption — its truth cannot be known. [@WinterStatisticsLinguistsIntroduction2019: 171] \n\nBy definition, inferential statistics is about attempting to *infer* unknown information about a population from a - often very small - sample of that population. As such, we cannot use statistics to prove or disprove a hypothesis that makes a claim about a population to which we do not have full access. Statistics may be a powerful science, but it's not magic! 🧙‍♀️\n:::\n\n## Using *t*-tests to compare two groups {#sec-ttest}\n\nThe null hypothesis that we formulated concerns the average non-verbal IQ test scores of two groups of individuals (English native speakers and non-native English speakers):\n\n-   **H~0~**: On [average]{.underline}, adult English L1 and L2 speakers in the UK perform *equally well* on the non-verbal IQ (Blocks) test.\n\nProvided that certain assumptions are met (see @sec-Assumptions), we can test null hypotheses involving the comparison of two **mean values** using a ***t*****-test**. The *t*-test takes three things into account:\n\n1.  The magnitude of the difference between the two mean values (i.e. how big is the difference?)\n2.  The amount of variability around the two means (i.e. how much variation is there around the means?)\n3.  The sample size (i.e. how many data points — in this case, participants — are there?)\n\nOur descriptive analysis showed that, on average, the L2 participants scored 3.62 points higher than the L1 participants in the sample. But we know that mean values alone are not sufficient to describe data and, as we saw in @fig-IQTestPlot, there was a lot of variability around these mean values. Moreover, we know that our sample is relatively small: it only has 90 L1 speakers and 67 L2 speakers (though many experimental linguistics study have fewer data points). The less data we have, the more likely we are to observe extreme values and this aspect is also taken into account by the significance tests such as the *t*-test.[^11_inferential-1]\n\n[^11_inferential-1]: If you are not immediately convinced by this statement, imagine that a friend flips a coin three times and gets heads all three times. On the basis of these observed data, he claims that the coin is biased towards heads. How likely are you to believe his claim that the coin is unfair? What about if he flips the coin 10 times and it lands on head all 10 times? This seems far more unlikely. In fact, the probability of getting three heads in a row with a fair coin is 0.125%, which means that it will happen about 1 in 8 times, whereas the probability of getting 10 heads in a row is just 0.001%, which is a one-in-a-thousand occurrence!\n\nThe `R` function `t.test()` takes a **formula** as its first argument and the **data** at its second argument. In `R`, formulas rely on the tilde symbol (`~`) to indicate that the variable to the left of the tilde is *dependent* on the variables to the right of the tilde. By specifying the formula as `Blocks ~ Group`, we are therefore testing whether the mean results of the `Blocks` test are *dependent* on whether the participants are L1 or L2 speakers of English (`Group`). In other words, we apply the *t*-test to test our null hypothesis, which can be reformulated as:\n\n-   **H~0~**: On average, the results of the non-verbal IQ (Blocks) test are *not* dependent on whether the test-takers are L1 or L2 speakers of English.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(formula = Blocks ~ Group, \n       data = Dabrowska.data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  Blocks by Group\nt = -4.4084, df = 152.46, p-value = 1.956e-05\nalternative hypothesis: true difference in means between group L1 and group L2 is not equal to 0\n95 percent confidence interval:\n -5.239791 -1.996693\nsample estimates:\nmean in group L1 mean in group L2 \n        13.84444         17.46269 \n```\n\n\n:::\n:::\n\n\nThe output of the `t.test()` function is a bit overwhelming at first, so let's focus on the most relevant aspects:\n\n-   The first line of the output informs us that we ran a `Welch Two Sample t-test`. We ran a **two-sample test** because we are comparing the means of two *independent* groups: L1 vs. L2 speakers. And it is a **Welch *t*-test** as opposed to a **Student's *t*-test** because we are not assuming that the standard deviation of the two groups' test scores in the full population is equal.[^11_inferential-2]\n\n-   Next, the ***t*****-statistic** is reported as `t = -4.4084`. In this case, it is a negative value which means that the mean score of the first group (here L1) is *lower* than that of the second group (here L2). Note that, by default, the groups are ordered alphabetically. The larger the absolute value of the *t*-statistic, the greater the difference between the group means. At the same time, however, the more variability there is in the data, the lower the absolute value of the *t*-statistic.\n\n-   `df = 152.46` corresponds to the **degrees of freedom**. These are automatically calculated by the `t.test()` function based on the number of data points in our sample and the number of constraints in our test.\n\n-   The ***p*****-value** is reported as `1.956e-05`. This is scientific notation for: 1.956 multiplied by 10 to the power of minus 5 (`1.956 * 10^-5`), which equals `0.00001956`.[^11_inferential-3] This means that our test estimates that there is a very, very small probability — namely 0.001956% — of observing a difference in mean scores on the Blocks test as large as we observed (3.62 points) or an even larger one [under the null hypothesis]{.underline}, i.e., under the assumption that there is *no* real-world difference between L1 and L2 speakers' performance on this test.\n\n-   The output also includes a **95% confidence interval (CI)** of the difference between the means. It ranges from `-5.239791` to `-1.996693`. In our sample, we observed a mean difference between L1 participants' and L2 participants' Blocks test scores of -3.62 points. If we were to repeat this experiment a 100 times with a 100 different samples of L1 and L2 speakers, we can be confident that, in 95 out of 100 repetitions, the confidence interval that we compute would include the true average difference across the entire population. In other words, the average difference between L1 and L2 speakers could be quite a bit larger than in Dabrowska's sample or quite a bit lower, but is very unlikely to be zero (which would correspond to the null hypothesis of no difference). Given the same observed difference, the larger our sample, the smaller our confidence interval.\n\n-   At the very bottom of the output, we can read the **sample estimates** for the L1 and the L2 groups. These are the mean Blocks test scores that we had already calculated using descriptive statistics (see @sec-NHST). They simply serve as a reminder that we are testing the statistical significance of the difference between these two means under the null hypothesis of no difference.\n\n[^11_inferential-2]: Which, based on the results of our descriptive statistics, is a very reasonable assumption to make about the full population. If, however, you wanted to conduct a Student's *t*-test that treats the variance of both groups as equal, then you would need to change the default value of the `var.equal` argument of the `t.test()` function to `TRUE` (for details see `?t.test`).\n\n[^11_inferential-3]: In scientific notation, “E” stands for “exponent”, which refers to the number of times a number needs to be multiplied by 10 or, if it is followed by a minus sign, multiplied by minus 10. This notation is used as a shorthand way of writing very large or very small numbers. One way to convert values from scientific notation to standard notation in `R` is to use the `format()` function like this:\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    format(1.956e-05, scientific = FALSE)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] \"0.00001956\"\n    ```\n    \n    \n    :::\n    :::\n\n\n::: callout-note\n# How to report\n\nTo summarise these results, we can write that we conducted a Welch two-sample *t*-test to compare the mean Blocks score of L1 and L2 English speakers. On average, L2 speakers performed significantly better (*M* = 17.46, *SD* = 4.70) than L1 speakers (*M* = 13.84, *SD* = 5.56), *t*~(152.46)~ = -4.4084, *p* \\< 0.001.\n:::\n\n::: callout-warning\n## Common misconception\n\nUnfortunately, **confidence intervals (CI)** are a bit of a misnomer, which frequently leads to misunderstandings. Contrary to what so-called \"AI\" tools (see @sec-PartingWords) and even some statistics textbooks may claim, confidence intervals do *not* tell us that we can be 95% confident that the true difference across the entire population lies within the 95% confidence interval. Bodo Winter [-@WinterStatisticsLinguistsIntroduction2019: 165] clarifies this common misconception as follows:\n\n> \\[T\\]he actual population parameter of interest \\[i.e. in the case of a *t*-test, the difference in mean values\\] may or may not be inside the confidence interval -- you will actually never know for sure. However, if you imagine an infinite series of experiments and compute a confidence interval each time, 95% of the time this interval would contain the true population parameter.\n\nIn other words, a 95% confidence interval does not tell us how confident we can be about any specific value, but rather that, in the long-run, if the study were to be repeated many times, 95% of the time, the 95% confidence interval would contain the true value.\n:::\n\n::: {.callout-tip collapse=\"false\"}\n#### Your turn! {.unnumbered}\n\nConsider the following research question:\n\n-   Across all adult English speakers in the UK, do L1 speakers, on average, achieve higher scores than L2 speakers on the English receptive vocabulary test?\n\n[**Q11.3**]{style=\"color:green;\"} Which null hypothesis could you formulate for this research question?\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<form name=\"form_15824\" onsubmit=\"return validate_form_15824()\" method=\"post\">\n<label>\n<input type=\"radio\" name=\"answer_15824\" id=\"answer_15824_1\" value=\"Across the entire adult UK population, there is no statistically significant difference between the mean scores of L1 speakers and those of L2 speakers on the English receptive vocabulary test.\"/>\nAcross the entire adult UK population, there is no statistically significant difference between the mean scores of L1 speakers and those of L2 speakers on the English receptive vocabulary test.\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_15824\" id=\"answer_15824_2\" value=\"Across the entire adult UK population, there is no difference in the mean scores of L1 vs. L2 speakers on the English receptive vocabulary test.\"/>\nAcross the entire adult UK population, there is no difference in the mean scores of L1 vs. L2 speakers on the English receptive vocabulary test.\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_15824\" id=\"answer_15824_3\" value=\"Across the entire adult UK population, the mean scores of both L1 and L2 speakers on the English receptive vocabulary test are null.\"/>\nAcross the entire adult UK population, the mean scores of both L1 and L2 speakers on the English receptive vocabulary test are null.\n</label>\n<br/>\n<input type=\"submit\" value=\"Check answer\"/>\n<div id=\"result_15824\"></div>\n</form>\n<script>function validate_form_15824() {var x, text; var x = document.forms['form_15824']['answer_15824'].value;if (x == 'Across the entire adult UK population, there is no difference in the mean scores of L1 vs. L2 speakers on the English receptive vocabulary test.'){text = 'That’s right, well done!';} else {text = 'No, that’s incorrect.';} document.getElementById('result_15824').innerHTML = text; evaluate_final_score(); return false;}function evaluate_final_score(){\n         element = document.getElementById('checkdown_final_score');\n         if(element === null){return false;} else {var element, text, res1, res2, res3;res1 = document.getElementById('result_12592').innerText == 'Correct! Random sampling (also called probability sampling) gives every individual an equal, known chance of being selected.'; res2 = document.getElementById('result_5683').innerText == 'Correct. Even when a convenience sample is “quota‑matched” on observable demographics, it remains a convenience sample because participants first volunteered to take part (e.g., they signed up on a platform like MTurk). This means that unobserved factors, such as motivation, socioeconomic status, and language background, can still render the sample unrepresentative, thus limiting the external validity of inferential data analyses made on the basis of these data.'; res3 = document.getElementById('result_15824').innerText == 'That’s right, well done!';text = res1 + res2 + res3;element.innerHTML = text;\n         return false;\n         }}</script>\n```\n\n:::\n:::\n\n\n[**Q11.4**]{style=\"color:green;\"} Run a *t*-test on the @DabrowskaExperienceAptitudeIndividual2019 data to test the null hypothesis that you selected in Q11.3 above. What is the value of the *t*-statistic?\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<form name=\"form_95146\" onsubmit=\"return validate_form_95146()\" method=\"post\">\n<label>\n<input type=\"radio\" name=\"answer_95146\" id=\"answer_95146_1\" value=\"0.000007032\"/>\n0.000007032\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_95146\" id=\"answer_95146_2\" value=\"4.68\"/>\n4.68\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_95146\" id=\"answer_95146_3\" value=\"133.83\"/>\n133.83\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_95146\" id=\"answer_95146_4\" value=\"9.425801\"/>\n9.425801\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_95146\" id=\"answer_95146_5\" value=\"23.240497\"/>\n23.240497\n</label>\n<br/>\n<input type=\"submit\" value=\"Check answer\"/>\n<div id=\"result_95146\"></div>\n</form>\n<script>function validate_form_95146() {var x, text; var x = document.forms['form_95146']['answer_95146'].value;if (x == '4.68'){text = 'Correct!';} else {text = 'Incorrect.';} document.getElementById('result_95146').innerHTML = text; evaluate_final_score(); return false;}function evaluate_final_score(){\n         element = document.getElementById('checkdown_final_score');\n         if(element === null){return false;} else {var element, text, res1, res2, res3, res4;res1 = document.getElementById('result_12592').innerText == 'Correct! Random sampling (also called probability sampling) gives every individual an equal, known chance of being selected.'; res2 = document.getElementById('result_5683').innerText == 'Correct. Even when a convenience sample is “quota‑matched” on observable demographics, it remains a convenience sample because participants first volunteered to take part (e.g., they signed up on a platform like MTurk). This means that unobserved factors, such as motivation, socioeconomic status, and language background, can still render the sample unrepresentative, thus limiting the external validity of inferential data analyses made on the basis of these data.'; res3 = document.getElementById('result_15824').innerText == 'That’s right, well done!'; res4 = document.getElementById('result_95146').innerText == 'Correct!';text = res1 + res2 + res3 + res4;element.innerHTML = text;\n         return false;\n         }}</script>\n```\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show sample code to answer Q11.4.\"}\n# It is always a good idea to visualise the data before running a statistical test:\nDabrowska.data |> \n  ggplot(mapping = aes(y = Vocab,\n                       x = Group)) +\n  geom_boxplot(alpha = 0.7) +\n  theme_bw()\n\n# Running the t-test:\nt.test(formula = Vocab ~ Group, \n       data = Dabrowska.data)\n```\n:::\n\n\n[**Q11.5**]{style=\"color:green;\"} What is the *p*-value associated with the *t*-test you ran above?\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<form name=\"form_23300\" onsubmit=\"return validate_form_23300()\" method=\"post\">\n<label>\n<input type=\"radio\" name=\"answer_23300\" id=\"answer_23300_1\" value=\"0.0000007032\"/>\n0.0000007032\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_23300\" id=\"answer_23300_2\" value=\"7.032 multiplied by minus six\"/>\n7.032 multiplied by minus six\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_23300\" id=\"answer_23300_3\" value=\"7.03206\"/>\n7.03206\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_23300\" id=\"answer_23300_4\" value=\"0.000007032\"/>\n0.000007032\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_23300\" id=\"answer_23300_5\" value=\"7.032 to the power of minus six\"/>\n7.032 to the power of minus six\n</label>\n<br/>\n<input type=\"submit\" value=\"Check answer\"/>\n<div id=\"result_23300\"></div>\n</form>\n<script>function validate_form_23300() {var x, text; var x = document.forms['form_23300']['answer_23300'].value;if (x == '0.000007032'){text = 'Correct!';} else {text = 'Incorrect.';} document.getElementById('result_23300').innerHTML = text; evaluate_final_score(); return false;}function evaluate_final_score(){\n         element = document.getElementById('checkdown_final_score');\n         if(element === null){return false;} else {var element, text, res1, res2, res3, res4, res5;res1 = document.getElementById('result_12592').innerText == 'Correct! Random sampling (also called probability sampling) gives every individual an equal, known chance of being selected.'; res2 = document.getElementById('result_5683').innerText == 'Correct. Even when a convenience sample is “quota‑matched” on observable demographics, it remains a convenience sample because participants first volunteered to take part (e.g., they signed up on a platform like MTurk). This means that unobserved factors, such as motivation, socioeconomic status, and language background, can still render the sample unrepresentative, thus limiting the external validity of inferential data analyses made on the basis of these data.'; res3 = document.getElementById('result_15824').innerText == 'That’s right, well done!'; res4 = document.getElementById('result_95146').innerText == 'Correct!'; res5 = document.getElementById('result_23300').innerText == 'Correct!';text = res1 + res2 + res3 + res4 + res5;element.innerHTML = text;\n         return false;\n         }}</script>\n```\n\n:::\n:::\n\n\n[**Q11.6**]{style=\"color:green;\"} The *p*-value is very, very small. What does this mean? Select all that apply.\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<form name=\"form_73587\" onsubmit=\"return validate_form_73587()\" method=\"post\">\n<label>\n<input type=\"checkbox\" id=\"answer_73587_1\" value=\"If there were no association between English native-speaker status and receptive English vocabulary scores, it is very unlikely that we would observe this difference in mean values by chance alone.\"/>\nIf there were no association between English native-speaker status and receptive English vocabulary scores, it is very unlikely that we would observe this difference in mean values by chance alone.\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_73587_2\" value=\"The difference in the mean receptive English vocabulary scores of English native and non-native speakers is highly relevant.\"/>\nThe difference in the mean receptive English vocabulary scores of English native and non-native speakers is highly relevant.\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_73587_3\" value=\"The true difference in the mean receptive English vocabulary scores of English native and non-native speakers is not equal to zero.\"/>\nThe true difference in the mean receptive English vocabulary scores of English native and non-native speakers is not equal to zero.\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_73587_4\" value=\"The true difference (across the full population) in the mean receptive English vocabulary scores of English native speakers is only very slightly higher than that of non-native speakers.\"/>\nThe true difference (across the full population) in the mean receptive English vocabulary scores of English native speakers is only very slightly higher than that of non-native speakers.\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_73587_5\" value=\"Given the sample size, the results of this t-test are highly precise.\"/>\nGiven the sample size, the results of this t-test are highly precise.\n</label>\n<br/>\n<input type=\"submit\" value=\"Check answer\"/>\n<div id=\"result_73587\"></div>\n</form>\n<script>function validate_form_73587() {var text; var x1 = document.getElementById('answer_73587_1'); var x2 = document.getElementById('answer_73587_2'); var x3 = document.getElementById('answer_73587_3'); var x4 = document.getElementById('answer_73587_4'); var x5 = document.getElementById('answer_73587_5'); if (x1.checked == true&x2.checked == false&x3.checked == false&x4.checked == false&x5.checked == false){text = 'Correct!';} else {text = 'Incorrect.';} document.getElementById('result_73587').innerHTML = text; evaluate_final_score(); return false;}function evaluate_final_score(){\n         element = document.getElementById('checkdown_final_score');\n         if(element === null){return false;} else {var element, text, res1, res2, res3, res4, res5, res6;res1 = document.getElementById('result_12592').innerText == 'Correct! Random sampling (also called probability sampling) gives every individual an equal, known chance of being selected.'; res2 = document.getElementById('result_5683').innerText == 'Correct. Even when a convenience sample is “quota‑matched” on observable demographics, it remains a convenience sample because participants first volunteered to take part (e.g., they signed up on a platform like MTurk). This means that unobserved factors, such as motivation, socioeconomic status, and language background, can still render the sample unrepresentative, thus limiting the external validity of inferential data analyses made on the basis of these data.'; res3 = document.getElementById('result_15824').innerText == 'That’s right, well done!'; res4 = document.getElementById('result_95146').innerText == 'Correct!'; res5 = document.getElementById('result_23300').innerText == 'Correct!'; res6 = document.getElementById('result_73587').innerText == 'Correct!';text = res1 + res2 + res3 + res4 + res5 + res6;element.innerHTML = text;\n         return false;\n         }}</script>\n```\n\n:::\n\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"hint_31946\" onclick=\"return show_hint_31946()\">🐭 Click on the mouse for a hint.</div>\n<div id=\"result_31946\" onclick=\"return show_hint_31946()\"></div>\n<script>function show_hint_31946(){var x = document.getElementById('result_31946').innerHTML; if(!x){document.getElementById('result_31946').innerHTML = 'Only one of these statements is correct.';} else {document.getElementById('result_31946').innerHTML = '';}}</script>\n```\n\n:::\n:::\n\n:::\n\n## Statistical significance and *p*-values {#sec-PValues}\n\nWhen used correctly, the *p*‑value is a very useful metric that can help us to determine whether an observed statistic, such as a difference in means in a *t*-test, is likely to be due to chance variation in the sample rather than indicative of a true effect in the population. A common way[^11_inferential-4] to use *p*‑values is to define — prior to conducting the analysis — a **significance level** threshold (also called **alpha** or **α-level**), which corresponds to the risk that we are willing to accept of mistakenly concluding that there is an effect when, in fact, there is none (this is called a false positive result). In the language and social sciences, the significance level is typically set to 0.05. This means that, if the null hypothesis is true, we accept a 5% risk of obtaining a *p*-value that suggests that we should reject the null hypothesis when, in fact, we shouldn't.[^11_inferential-5] The significance level should be chosen *before* looking at the data and be clearly mentioned in the methods section of every study that uses statistical significance testing.\n\n[^11_inferential-4]: Again, it is important to note that this is by no means the *only* way to interpret *p*-values. As with many things in statistics, how you interpret *p*-values depends on the statistical philosophy that you subscribe to. The approach described in this textbook corresponds to the statistical hypothesis testing approach developed by Neyman & Pearson ([1933](https://lakens.github.io/statistical_inferences/references.html#ref-neyman_problem_1933)). \"In a Neyman-Pearson framework, the goal of statistical tests is to guide the behavior of researchers with respect to these two hypotheses. Based on the results of a statistical test, and without ever knowing whether the hypothesis is true or not, researchers choose to tentatively act as if the null hypothesis or the alternative hypothesis is true.\" [@lakensImprovingYourStatistical2022: Section 1.1]\n\n[^11_inferential-5]: It is worth noting that, just because 0.05 is (currently) the most widely used threshold, doesn't mean that you have to use 0.05, too. If you are not comfortable accepting a 5% risk (which, statistically speaking, will happen one in 20 times, after all), you can define a lower threshold, e.g., 0.01, corresponding to a 1% risk. However, depending on how large your observed effect is and how much data you have, you may find that, with a lower significance‑level, you fail to reject the null hypothesis even when there is a true effect in the population so it's a difficult balance to strike. To find out more, I recommend reading about **statistical power**, e.g. in Lakens [-@lakensImprovingYourStatistical2022: Chapter 2].\n\nIn the NHST framework, when the calculated *p*‑value is *smaller* than our chosen significance level (α), we reject the null hypothesis in favour of the alternative hypothesis and say that the result is **statistically significant**. When the *p*‑value is larger than α, we fail to reject the null hypothesis and say that the result is not statistically significant. However, the latter does not *prove* that the null hypothesis is true. It only tells us that the data that we have do not provide enough evidence against the null hypothesis. Note that, following this school of statistics, the actual *p*-value is irrelevant: it is either *below* or *above* the α-level threshold. We do not compare *p*-values and it does not make sense to claim that one result is more or less statistically significant than the other.\n\n> Once *p* \\< α, a result is claimed to be 'statistically significant', which is just the same as saying that the data are sufficiently incompatible with the null hypothesis. If the researcher obtained a significant result for a *t*-test, the researcher may act as if there actually was a group difference in the population. [@WinterStatisticsLinguistsIntroduction2019: 168]\n\nContrary to what some researchers seem to believe, in and of themselves, *p*-values are not the holy grail! They can only meaningfully be interpreted together with other important contextual information such as the **context** in which the data were collected, the magnitude of the observed effect (the **effect size**), and the **variability** around the estimated effect (e.g., as shown in data visualisation) (see @fig-pvalue).\n\n![Don't let your *p*-values sing solo! (artwork by [Allison Horst](https://twitter.com/allison_horst) CC BY 4.0)](images/AHorst_p-value.jpg){#fig-pvalue fig-alt=\"Cartoon of a p-value taking over the microphone on a stage with the rest of the band (effect sizes, context, awesome dataviz and correlations) standing at the back and effect sizes saying \\\"I can't do it anymore. I can't explain \\\"ensemble\\\" again.\\\"\" width=\"454\"}\n\nThe problem with *p*-values is that they are a composite metric that is dependent on three aspects:\n\n1.  The **size of the observed effect** (the larger the effect, the smaller the *p*-value)\n2.  The **variability within the data** (the less variability, the smaller the *p*-value)\n3.  The **sample size** (the larger the sample size, the smaller the *p*-value)\n\nNote that the size (or magnitude) of the observed effect is only *one* of three factors that influence the *p*-value! It is therefore incorrect to claim that an effect (e.g., a difference in means) is particularly large based on a particularly small *p*-value. It is equally incorrect to claim that a *p*-value that falls below the chosen significance level points to a (statistically) relevant result. To evaluate the *relevance* of a result, we need contextual information that goes far beyond the results of a single statistical test. In statistics, \"significance\" and \"significant\" are terms that have nothing to do with either the relevance or importance of results.\n\n## Effect sizes and confidence intervals {#sec-EffectSize}\n\nIn @sec-ttest, we saw that the larger the absolute value of the *t*-statistic, the greater the difference between the group means. At the same time, the more variability there is in the data, the lower the absolute value of the *t*-statistic. This makes the ***t-*****statistic** a measure of effect size. However, it is an **unstandardised measure**, which means that t-statistic values cannot be compared across different studies.\n\n::: column-margin\n![Hex sticker of the [{effectsize}](https://easystats.github.io/effectsize/) package](images/hex_effectsize.png){#fig-hexeffectsize width=\"100\" fig-alt=\"The hexagonal logo of the effectsize package showing a three-eyed monster eating a pizza (I honestly have no idea why...).\"}\n:::\n\nBy contrast, **Cohen's *d*** is a **standardised effect size measure**. As such, it can be used to compare the magnitude of the difference in mean values across different variables, samples, and studies. Cohen's *d* (the *d* stands for difference) can be calculated by dividing the difference between two means (the raw strength of an effect) by the standard deviation of both groups together (the overall variability of the data). But fear not: we don't need to do the maths ourselves as the formula is implemented in several `R` packages. In the following, we will use `cohens_d()` from the {[effectsize](https://easystats.github.io/effectsize/)} package [@ben-shachar2020] which, like the `t.test()` function, also takes a formula as its first argument.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"effectsize\")\nlibrary(effectsize)\n```\n:::\n\n\nRecall the difference that we observed between L1 and L2 English speakers' non-verbal IQ (Blocks) test results in @fig-IQTestPlot. With the `cohens_d()` function, we can now answer the question: How large is this effect?\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncohens_d(Blocks ~ Group, \n         data = Dabrowska.data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCohen's d |         95% CI\n--------------------------\n-0.69     | [-1.02, -0.37]\n\n- Estimated using pooled SD.\n```\n\n\n:::\n:::\n\n\nThe output shows that Cohen's *d* is `-0.69`. As with the *t*-statistic, the minus sign tells us that the L1 group performed worse than the L2 participants. The absolute value, here `0.69`, corresponds to the strength of the effect. According to Cohen's [-@cohenStatisticalPowerAnalysis1988] own rule of thumb, the absolute values can be interpreted as follows:\n\n-   Cohen's *d* = 0.2 - 0.5 → Small effect size\n-   Cohen's *d* = 0.5 - 0.8 → Medium effect size\n-   Cohen's *d* \\> 0.8 → Large effect size\n\nHowever, Cohen [-@cohenStatisticalPowerAnalysis1988: 25] himself cautioned that:\n\n> The terms \"small,\" \"medium,\" and \"large\" are relative, not only to each other, but to the area of behavioral science or even more particularly to the specific content and research method being employed in any given investigation \\[...\\].\n\nHence, it is important that linguists and education researchers base their interpretation of standardised effect sizes on prior research relevant to their field of research (see, e.g., @plonskyHowBigBig2014 for L2 research).\n\nThe output of the `cohens_d()` function above also includes a 95% confidence interval (CI) around Cohen's *d*. It turns out that there is a direct relationship between the confidence interval around an effect size and the statistical significance of a null hypothesis significance test: if an effect is statistically significant in a two-sided[^11_inferential-6] independent *t*-test with a significance (α) level of 0.05, the 95% confidence interval (CI) for the mean difference between the two groups will *not* include zero. The *t*-test that we conducted on the results of the Blocks test across the L1 and L2 groups produced a *p*-value of 0.00001956 which is less than 0.05 and was therefore statistically significant at the α-level of 0.05. But we didn't really need to check the *p*-value because we can see that the effect is statistically significant at the α-level of 0.05 by looking at the 95% CI around Cohen's *d*: the lower bound is `-1.02` and the upper bound `-0.37`. In other words, the CI does not straddle zero.\n\n[^11_inferential-6]: All of the statistical tests performed in this chapter are two-sided. For a discussion of one-sided vs. two-sided tests, see @lakensImprovingYourStatistical2022: Section 5.10).\n\nNow, let's consider a new research question and a new null hypothesis:\n\n-   **H~0~**: On average, the results of the non-verbal IQ (Blocks) test are not dependent on the gender of the test-takers.\n\nRecall that, in this dataset, `Gender` is a binary variable. The descriptive statistics suggest that male participants perform slightly better than female participants on the non-verbal IQ test, but that there is quite a bit of variability in the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nDabrowska.data |> \n  group_by(Gender) |> \n  summarise(mean = mean(Blocks),\n            SD = sd(Blocks))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  Gender  mean    SD\n  <fct>  <dbl> <dbl>\n1 F       15.2  5.30\n2 M       15.7  5.81\n```\n\n\n:::\n:::\n\n\nWe now compute a standardised effect size for this gender gap: Cohen's *d*.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncohens_d(Blocks ~ Gender, \n       data = Dabrowska.data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCohen's d |        95% CI\n-------------------------\n-0.10     | [-0.42, 0.22]\n\n- Estimated using pooled SD.\n```\n\n\n:::\n:::\n\n\nHere, the `cohens_d()` function compares the scores of the female participants with those of the male participants because 'female' comes first alphabetically. Hence, the negative Cohen's *d* value means that, on average, males perform better than females. However, we can see that the effect size (`-0.10`) is very small.\n\nNow turning to the 95% confidence interval (CI) also output by the function, we can see that, while the lower confidence bound corresponds to a negative effect size, the upper bound is positive, which means that the confidence interval contains the possibility of an effect size of zero, corresponding to no effect at all. Hence, we must conclude that this difference in scores between female and male participants is not statistically significant at an α-level of 0.05. We can confirm this by performing a *t*-test. It returns a *p*-value that is greater than 0.05:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(formula = Blocks ~ Gender, \n       data = Dabrowska.data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  Blocks by Gender\nt = -0.59558, df = 124.58, p-value = 0.5525\nalternative hypothesis: true difference in means between group F and group M is not equal to 0\n95 percent confidence interval:\n -2.352092  1.263946\nsample estimates:\nmean in group F mean in group M \n       15.17021        15.71429 \n```\n\n\n:::\n:::\n\n\n::: callout-note\n# How to report\n\nTo summarise these results, we can write that, while male participants performed marginally better (*M* = 15.71, *SD* = 5.81) than female participants (*M* = 15.17, *SD* = 5.30), this difference is very small (Cohen's *d* = -0.10; 95% CI \\[-042, 0.22\\]) and is not statistically significant at an α-level of 0.05: *t*~(124.58)~ = 0.5956, *p* = 0.5525.\n:::\n\nBy default, the `cohens_d()` function computes a 95% confidence interval, but, if we had chosen a lower α-level of, say, 0.01, we can change this default:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncohens_d(Blocks ~ Gender, \n       data = Dabrowska.data,\n       ci = 0.99)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCohen's d |        99% CI\n-------------------------\n-0.10     | [-0.52, 0.32]\n\n- Estimated using pooled SD.\n```\n\n\n:::\n:::\n\n\nAs you can see, this increases the size of the interval and hence makes it harder to obtain a statistically significant result. This is because lowering the α-level means that we are less willing to risk reporting a false positive result, i.e. reporting a difference based on our data where no real difference exists.\n\n::: {.callout-tip collapse=\"false\"}\n#### Your turn! {.unnumbered}\n\nCompute three Cohen's *d* values to capture the magnitude of the difference between L1 and L2 speakers:\n\n1.  English grammar (`Grammar`) test scores\n2.  English receptive vocabulary (`Vocab`) test scores, and\n3.  English collocation (`Colloc`) test scores.\n\nAs these are three standardised effect sizes, we can compare them.\n\n[**Q11.7**]{style=\"color:green;\"} Comparing the three Cohen's *d* that you computed, which statement(s) is/are true?\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<form name=\"form_4666\" onsubmit=\"return validate_form_4666()\" method=\"post\">\n<label>\n<input type=\"checkbox\" id=\"answer_4666_1\" value=\"By far the largest difference between L1 and L2 speakers is in the English collocation test.\"/>\nBy far the largest difference between L1 and L2 speakers is in the English collocation test.\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_4666_2\" value=\"The effect sizes across the three English tests are practically equal.\"/>\nThe effect sizes across the three English tests are practically equal.\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_4666_3\" value=\"The largest difference between L1 and L2 speakers is in the receptive English vocabulary test.\"/>\nThe largest difference between L1 and L2 speakers is in the receptive English vocabulary test.\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_4666_4\" value=\"The smallest effect size is observed in the collocation test.\"/>\nThe smallest effect size is observed in the collocation test.\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_4666_5\" value=\"The difference in all three effect sizes is statistically significant.\"/>\nThe difference in all three effect sizes is statistically significant.\n</label>\n<br/>\n<input type=\"submit\" value=\"Check answer\"/>\n<div id=\"result_4666\"></div>\n</form>\n<script>function validate_form_4666() {var text; var x1 = document.getElementById('answer_4666_1'); var x2 = document.getElementById('answer_4666_2'); var x3 = document.getElementById('answer_4666_3'); var x4 = document.getElementById('answer_4666_4'); var x5 = document.getElementById('answer_4666_5'); if (x1.checked == true&x2.checked == false&x3.checked == false&x4.checked == false&x5.checked == false){text = 'That’s right, well done!';} else {text = 'No, that’s incorrect.';} document.getElementById('result_4666').innerHTML = text; evaluate_final_score(); return false;}function evaluate_final_score(){\n         element = document.getElementById('checkdown_final_score');\n         if(element === null){return false;} else {var element, text, res1, res2, res3, res4, res5, res6, res7;res1 = document.getElementById('result_12592').innerText == 'Correct! Random sampling (also called probability sampling) gives every individual an equal, known chance of being selected.'; res2 = document.getElementById('result_5683').innerText == 'Correct. Even when a convenience sample is “quota‑matched” on observable demographics, it remains a convenience sample because participants first volunteered to take part (e.g., they signed up on a platform like MTurk). This means that unobserved factors, such as motivation, socioeconomic status, and language background, can still render the sample unrepresentative, thus limiting the external validity of inferential data analyses made on the basis of these data.'; res3 = document.getElementById('result_15824').innerText == 'That’s right, well done!'; res4 = document.getElementById('result_95146').innerText == 'Correct!'; res5 = document.getElementById('result_23300').innerText == 'Correct!'; res6 = document.getElementById('result_73587').innerText == 'Correct!'; res7 = document.getElementById('result_4666').innerText == 'That’s right, well done!';text = res1 + res2 + res3 + res4 + res5 + res6 + res7;element.innerHTML = text;\n         return false;\n         }}</script>\n```\n\n:::\n\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"hint_89204\" onclick=\"return show_hint_89204()\">🐭 Click on the mouse for a hint.</div>\n<div id=\"result_89204\" onclick=\"return show_hint_89204()\"></div>\n<script>function show_hint_89204(){var x = document.getElementById('result_89204').innerHTML; if(!x){document.getElementById('result_89204').innerHTML = 'Only one of these statements is correct.';} else {document.getElementById('result_89204').innerHTML = '';}}</script>\n```\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show sample code to answer Q11.7.\"}\ncohens_d(Grammar ~ Group, \n       data = Dabrowska.data)\n\ncohens_d(Vocab ~ Group, \n       data = Dabrowska.data)\n\ncohens_d(Colloc ~ Group, \n       data = Dabrowska.data)\n```\n:::\n\n\n[**Q11.8**]{style=\"color:green;\"} Now compute 99% confidence intervals (CI) around the same three Cohen's *d* values. Which statement(s) is/are true?\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<form name=\"form_27514\" onsubmit=\"return validate_form_27514()\" method=\"post\">\n<label>\n<input type=\"checkbox\" id=\"answer_27514_1\" value=\"We can be 99% confident that all three differences in means are real.\"/>\nWe can be 99% confident that all three differences in means are real.\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_27514_2\" value=\"For all three effect sizes, the 99% CIs are wider than the 95% CI.\"/>\nFor all three effect sizes, the 99% CIs are wider than the 95% CI.\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_27514_3\" value=\"All three differences in means are statistically confident at the α-level of 0.01.\"/>\nAll three differences in means are statistically confident at the α-level of 0.01.\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_27514_4\" value=\"All three differences in means are statistically significant at the α-level of 0.01.\"/>\nAll three differences in means are statistically significant at the α-level of 0.01.\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_27514_5\" value=\"In 99% of cases, L1 speakers perform better than L2 speakers on all three tests.\"/>\nIn 99% of cases, L1 speakers perform better than L2 speakers on all three tests.\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_27514_6\" value=\"The most relevant difference can be found in the collocation test data.\"/>\nThe most relevant difference can be found in the collocation test data.\n</label>\n<br/>\n<input type=\"submit\" value=\"Check answer\"/>\n<div id=\"result_27514\"></div>\n</form>\n<script>function validate_form_27514() {var text; var x1 = document.getElementById('answer_27514_1'); var x2 = document.getElementById('answer_27514_2'); var x3 = document.getElementById('answer_27514_3'); var x4 = document.getElementById('answer_27514_4'); var x5 = document.getElementById('answer_27514_5'); var x6 = document.getElementById('answer_27514_6'); if (x1.checked == false&x2.checked == true&x3.checked == false&x4.checked == true&x5.checked == false&x6.checked == false){text = 'Correct!';} else {text = 'No, take another minute to carefully re-read all the statements.';} document.getElementById('result_27514').innerHTML = text; evaluate_final_score(); return false;}function evaluate_final_score(){\n         element = document.getElementById('checkdown_final_score');\n         if(element === null){return false;} else {var element, text, res1, res2, res3, res4, res5, res6, res7, res8;res1 = document.getElementById('result_12592').innerText == 'Correct! Random sampling (also called probability sampling) gives every individual an equal, known chance of being selected.'; res2 = document.getElementById('result_5683').innerText == 'Correct. Even when a convenience sample is “quota‑matched” on observable demographics, it remains a convenience sample because participants first volunteered to take part (e.g., they signed up on a platform like MTurk). This means that unobserved factors, such as motivation, socioeconomic status, and language background, can still render the sample unrepresentative, thus limiting the external validity of inferential data analyses made on the basis of these data.'; res3 = document.getElementById('result_15824').innerText == 'That’s right, well done!'; res4 = document.getElementById('result_95146').innerText == 'Correct!'; res5 = document.getElementById('result_23300').innerText == 'Correct!'; res6 = document.getElementById('result_73587').innerText == 'Correct!'; res7 = document.getElementById('result_4666').innerText == 'That’s right, well done!'; res8 = document.getElementById('result_27514').innerText == 'Correct!';text = res1 + res2 + res3 + res4 + res5 + res6 + res7 + res8;element.innerHTML = text;\n         return false;\n         }}</script>\n```\n\n:::\n\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"hint_34019\" onclick=\"return show_hint_34019()\">🐭 Click on the mouse for a hint.</div>\n<div id=\"result_34019\" onclick=\"return show_hint_34019()\"></div>\n<script>function show_hint_34019(){var x = document.getElementById('result_34019').innerHTML; if(!x){document.getElementById('result_34019').innerHTML = 'Two of these statements are correct. Click on the link below for a reminder as to how to compute 99% confidence intervals around Cohen’s d.';} else {document.getElementById('result_34019').innerHTML = '';}}</script>\n```\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show code to answer Q11.8.\"}\ncohens_d(Grammar ~ Group, \n       data = Dabrowska.data,\n       ci = 0.99)\n\ncohens_d(Vocab ~ Group, \n       data = Dabrowska.data,\n       ci = 0.99)\n\ncohens_d(Colloc ~ Group, \n       data = Dabrowska.data,\n       ci = 0.99)\n```\n:::\n\n\nConsider the code and its outputs below. We are now comparing the English grammar comprehension test scores of male and female native speakers of Romance languages only. Hence, we are now looking at a very small sample size comprising just five female and one male participants.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nDabrowska.Romance <- Dabrowska.data |> \n  filter(NativeLgFamily == \"Romance\")\n\ntable(Dabrowska.Romance$Gender)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nF M \n5 1 \n```\n\n\n:::\n\n```{.r .cell-code}\ncohens_d(Grammar ~ Gender,\n         data = Dabrowska.Romance)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCohen's d |        95% CI\n-------------------------\n-1.04     | [-3.24, 1.27]\n\n- Estimated using pooled SD.\n```\n\n\n:::\n:::\n\n\n[**Q11.9**]{style=\"color:green;\"} Which statement(s) is/are true about the standardised effect size computed for the difference in the English comprehension grammar scores of male and female Romance L1 speakers in this very small dataset (n = 6)?\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<form name=\"form_66174\" onsubmit=\"return validate_form_66174()\" method=\"post\">\n<label>\n<input type=\"checkbox\" id=\"answer_66174_1\" value=\"The effect size is negative, which means that the single male participant performed better on the grammar test than the five female participants.\"/>\nThe effect size is negative, which means that the single male participant performed better on the grammar test than the five female participants.\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_66174_2\" value=\"We cannot reject the null hypothesis that there is no difference between male and female performance on the grammar test.\"/>\nWe cannot reject the null hypothesis that there is no difference between male and female performance on the grammar test.\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_66174_3\" value=\"The 95% CI around Cohen&#39;s d includes zero, which means that there is no practical difference in the performance of male and female participants on the grammar test.\"/>\nThe 95% CI around Cohen's d includes zero, which means that there is no practical difference in the performance of male and female participants on the grammar test.\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_66174_4\" value=\"All six participants did equally well at the α-level of 0.05.\"/>\nAll six participants did equally well at the α-level of 0.05.\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_66174_5\" value=\"The difference in mean grammar test scores across the two genders is not statistically significant at the α-level of 0.05.\"/>\nThe difference in mean grammar test scores across the two genders is not statistically significant at the α-level of 0.05.\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_66174_6\" value=\"According to Cohen&#39;s rule of thumb, the effect size is very small.\"/>\nAccording to Cohen's rule of thumb, the effect size is very small.\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_66174_7\" value=\"According to Cohen&#39;s rule of thumb, the effect size is very large.\"/>\nAccording to Cohen's rule of thumb, the effect size is very large.\n</label>\n<br/>\n<input type=\"submit\" value=\"Check answer\"/>\n<div id=\"result_66174\"></div>\n</form>\n<script>function validate_form_66174() {var text; var x1 = document.getElementById('answer_66174_1'); var x2 = document.getElementById('answer_66174_2'); var x3 = document.getElementById('answer_66174_3'); var x4 = document.getElementById('answer_66174_4'); var x5 = document.getElementById('answer_66174_5'); var x6 = document.getElementById('answer_66174_6'); var x7 = document.getElementById('answer_66174_7'); if (x1.checked == true&x2.checked == true&x3.checked == false&x4.checked == false&x5.checked == true&x6.checked == false&x7.checked == true){text = 'That’s right, very well done! The results of this test show the value of statistical significance testing: Here, the difference between the male and female participants is very large, but because we have such a small sample size, the test warns us that this large difference could very well be due to chance and not actually correspond to a real difference in the population.';} else {text = 'No, not quite.';} document.getElementById('result_66174').innerHTML = text; evaluate_final_score(); return false;}function evaluate_final_score(){\n         element = document.getElementById('checkdown_final_score');\n         if(element === null){return false;} else {var element, text, res1, res2, res3, res4, res5, res6, res7, res8, res9;res1 = document.getElementById('result_12592').innerText == 'Correct! Random sampling (also called probability sampling) gives every individual an equal, known chance of being selected.'; res2 = document.getElementById('result_5683').innerText == 'Correct. Even when a convenience sample is “quota‑matched” on observable demographics, it remains a convenience sample because participants first volunteered to take part (e.g., they signed up on a platform like MTurk). This means that unobserved factors, such as motivation, socioeconomic status, and language background, can still render the sample unrepresentative, thus limiting the external validity of inferential data analyses made on the basis of these data.'; res3 = document.getElementById('result_15824').innerText == 'That’s right, well done!'; res4 = document.getElementById('result_95146').innerText == 'Correct!'; res5 = document.getElementById('result_23300').innerText == 'Correct!'; res6 = document.getElementById('result_73587').innerText == 'Correct!'; res7 = document.getElementById('result_4666').innerText == 'That’s right, well done!'; res8 = document.getElementById('result_27514').innerText == 'Correct!'; res9 = document.getElementById('result_66174').innerText == 'That’s right, very well done! The results of this test show the value of statistical significance testing: Here, the difference between the male and female participants is very large, but because we have such a small sample size, the test warns us that this large difference could very well be due to chance and not actually correspond to a real difference in the population.';text = res1 + res2 + res3 + res4 + res5 + res6 + res7 + res8 + res9;element.innerHTML = text;\n         return false;\n         }}</script>\n```\n\n:::\n\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"hint_57552\" onmouseover=\"return show_hint_57552()\">🦉 Hover over the owl for a first hint.</div>\n<div id=\"result_57552\" onmouseover=\"return show_hint_57552()\"></div>\n<script>function show_hint_57552(){var x = document.getElementById('result_57552').innerHTML; if(!x){document.getElementById('result_57552').innerHTML = 'Here we have an interesting case of a large effect size, but with a very wide confidence interval that includes zero. What does this mean?';} else {document.getElementById('result_57552').innerHTML = '';}}</script>\n```\n\n:::\n\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"hint_2398\" onclick=\"return show_hint_2398()\">🐭 Click on the mouse for a second hint.</div>\n<div id=\"result_2398\" onclick=\"return show_hint_2398()\"></div>\n<script>function show_hint_2398(){var x = document.getElementById('result_2398').innerHTML; if(!x){document.getElementById('result_2398').innerHTML = 'Four of these statements are correct.';} else {document.getElementById('result_2398').innerHTML = '';}}</script>\n```\n\n:::\n:::\n\n:::\n\n## Correlation tests {#sec-Correlations}\n\nSo far, we have looked at just one type of statistical significance test, the *t*-test, which we used to compare two mean values. This kind of *t*-test is used to test the association between a numeric variable (e.g., test scores ranging from 0 to 100) and a binary categorical variable (e.g., L1 vs. L2 status).\n\nIn this section, we return to correlations — a concept that we encountered in @sec-Scatterplots when we generated and interpreted scatter plots. Recall that correlations capture the strength of the association between two numeric variables (e.g., age and grammar test scores).\n\nAt the beginning of the chapter, we summarised the following observations based on our descriptive analyses of the @DabrowskaExperienceAptitudeIndividual2019 data:\n\n-   For both L1 and L2 participants, there is a positive correlation between the number of years they were in formal education and their English grammar comprehension test scores: the longer they were in formal education, the better they did on the test (see @fig-YearEducationPlot).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nDabrowska.data |> \n  ggplot(mapping = aes(x = EduTotal, \n                       y = Grammar)) +\n  facet_wrap(~ Group) +\n  geom_point() +\n  geom_smooth(method = \"lm\",\n              se = FALSE) +\n  labs(x = \"Number of years in formal education\",\n       y = \"English grammar comprehension test scores\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![Relationship between years in formal education and English grammar comprehension test scores for groups L1 and L2](11_Inferential_files/figure-html/fig-YearEducationPlot-1.png){#fig-YearEducationPlot fig-alt='Two scatter plot showing the relationship between the number of years in formal education and English grammar comprehension test scores for two groups, L1 and L2. Each black dot represents an individual’s score. A blue regression line in each plot shows a positive trend: more years in education are associated with higher test scores in both groups.' width=576}\n:::\n:::\n\n\nHow strong are the correlations visualised by the blue regression lines on @fig-YearEducationPlot? And how likely is it that we might observe such correlations or stronger ones by chance alone? The first question is about the size of the effect, whilst the second is about its statistical significance.\n\nWe can answer both questions using the `cor.test()` function. This function also takes a formula as its first argument: the two numeric variables whose correlation we want to estimate come after the tilde (`~`) and the two variables are combined using the plus (`+`) operator:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor.test(formula = ~ EduTotal + Grammar,\n         data = L1.data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's product-moment correlation\n\ndata:  EduTotal and Grammar\nt = 3.5821, df = 88, p-value = 0.0005581\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.1615747 0.5250334\nsample estimates:\n      cor \n0.3567294 \n```\n\n\n:::\n:::\n\n\nAs with the `t.test()` (see @sec-ttest), the output of the `cor.test()` function includes a *t*-statistic (`t`), the number of degrees of freedom (`df`; which here corresponds to the number of data points minus two), and a *p*-value. Immediately, we can see that the *p*-value is \\< 0.05, which means that, for the L1 population, we can reject the null hypothesis that there is no correlation between the total number of years that participants spent in education and their English grammar comprehension test scores. But how *strong* is the correlation? To find out, we turn to the sample estimate, **Pearson's *r***: `cor = 0.36`. Like Cohen's *d*, Pearson's *r* is also a standardised effect size. It can range between `-1` and `+1`.\n\n-   A correlation coefficient of **1** means that there is a **perfect positive linear relationship** between the two variables.\n\n    -   [Example]{.underline}: The relationship between the number of questions that a person correctly answered in a test and the percentage of questions that they got right.\n\n\n        ::: {.cell}\n        \n        ```{.r .cell-code  code-fold=\"true\" code-summary=\"Show `R` code to simulate data with a perfect positive correlation.\"}\n        # First, let's imagine that 100 learners took a test with 10 questions. We simulate the number of questions that they answered correctly using the `sample()` function that generates random numbers, specifying that learners can get between zero and 10 questions right:\n        correct.answers <- sample(0:10, 100, replace = TRUE)\n        \n        # Next, we convert the number of correctly answered questions into the percentage of questions that they answered correctly:\n        accuracy <- correct.answers / 10 * 100\n        \n        # Finally, we compute the correlation coefficient between the number of  and the percentage of correctly answered questions:\n        cor(correct.answers, accuracy)\n        \n        # The correlation coefficient (Pearson's r) equals 1 because the two variables are perfectly correlated with each other. If we know one variable, there is a simple mathematical formula that allows us to obtain the exact value of the other!\n        ```\n        :::\n\n\n-   A correlation coefficient of **0** means that there is **no linear relationship** between the two variables. Note that, in the real world, we will never find correlations of exactly zero, but rather very close to zero.\n\n    -   Example: The relationship between two completely randomly generated strings of numbers.\n\n\n        ::: {.cell}\n        \n        ```{.r .cell-code  code-fold=\"true\" code-summary=\"Show `R` code to simulate random data with a near-zero correlation\"}\n        # First, we set a seed to ensure that the outcome of our randomly generated number series are always exactly the same:\n        set.seed(42)\n        \n        # Next, we generate two series of a hundred randomly generated numbers ranging between 0 and 10:\n        x <- runif(100, 0, 10)\n        y <- runif(100, 0, 10)\n        \n        # We can now compute the correlation coefficient. As expected, we find that it is very close to zero but not exactly zero:\n        cor(x, y)\n        \n        # However, if we run a correlation test on our two randomly generated variables x and y, we find that a) the p-value is > 0.05 and b) our 95% confidence interval includes 0. \n        cor.test(x, y)\n        \n        # We can therefore conclude there is not enough evidence in our sample data to reject the null hypothesis of no correlation in the full population. This makes sense because we are testing the correlation of two independently, randomly generated strings of numbers that shouldn't have anything to do with each other!\n        ```\n        :::\n\n\n-   A correlation coefficient of **-1** means that there is a **perfect negative linear relationship** between the two variables.\n\n    -   [Example]{.underline}: The relationship between the number of errors a person makes in a test and the percentage of questions that they got right.\n\n\n        ::: {.cell}\n        \n        ```{.r .cell-code  code-fold=\"true\" code-summary=\"Show `R` code to simulate data with a perfectly negative correlation\"}\n        # First, let's imagine that 100 learners took a test with 20 questions. We simulate the number of errors that they each made:\n        errors <- sample(0:20, 100, replace = TRUE)\n        \n        # Next, we convert the number of errors into the percentage of questions that they correctly answered:\n        accuracy <- ((20 - errors)/20) * 100\n        \n        # Finally, we compute the correlation coefficient between the number of errors and the percentage of correctly answered questions:\n        cor(errors, accuracy)\n        ```\n        :::\n\n\nGoing back to the output of the `cor.test()` function above, we have a **correlation coefficient** of `0.36`, which is positive, meaning that we are looking at a positive correlation. We already knew this from the direction of the regression line in the L1 panel of @fig-YearEducationPlot. The question is now: is `0.36` a *weak*, *medium* or *strong* positive correlation? Again, it depends... For some research questions in the language sciences, `0.36` may be considered a medium-sized correlation because humans are strange and complex animals but, for others, it may be considered a small correlation... As always, numbers alone do not suffice to draw conclusions: we need contextual information about our domain of research.\n\nThe output of the `cor.test()` function also returned a 95% confidence interval around the correlation coefficient: `[0.16, 0.53]`. It does not straddle zero which is why our *p*-value was \\< 0.05. That said, the lower bound of the interval corresponds to a very small correlation, suggesting that the correlation in the full L1 population may be considerably smaller than what we observed in our data (or quite a bit larger as demonstrated by the upper bound). In other words, there is quite a bit of uncertainty around the strength of this correlation coefficient because there is a lot variability in the data and we do not have a particularly large sample size.\n\n::: callout-note\n# How to report\n\nTo summarise these results, we can write that, in this dataset, there is a positive and statistically significant correlation between the number of years that L1 participants reported spending in formal education and their receptive English vocabulary test scores, *r* = 0.36, 95% CI [0.16, 0.53], df = 88, *p* = 0.0005581.\n:::\n\nHow about L2 speakers? From the L2 panel of @fig-YearEducationPlot, we can see that the `Grammar` scores of L2 participants are, on average, much further away from the regression line than in the L1 panel, suggesting that it summarises the data far less well. Indeed, when we run the correlation test on the L2 data, we not only find that the correlation coefficient is much smaller (`0.13`), the 95% confidence interval around this coefficient \\[`-0.11, 0.36`\\] includes zero:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor.test(formula = ~ EduTotal + Grammar,\n         data = L2.data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's product-moment correlation\n\ndata:  EduTotal and Grammar\nt = 1.0936, df = 65, p-value = 0.2782\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.1093254  0.3629045\nsample estimates:\n      cor \n0.1344131 \n```\n\n\n:::\n:::\n\n\nAlong with a *p*-value of \\> 0.05, this suggests that we do not have enough evidence to reject the null hypothesis of no correlation between years in formal education and English grammar comprehension in the L2 population.\n\n::: callout-note\n# How to report\n\nTo summarise these results, we can write that, in this dataset, there is a small positive, but  non-significant correlation between the number of years that L2 participants reported spending in formal education and their receptive English vocabulary test scores, *r* = 0.13, 95% CI [-0.11, 0.36] df = 65, *p* = 0.2782.\n:::\n\nConfidence intervals around correlation coefficients can be difficult to interpret as numbers. The good news is that they can easily be visualised using the {ggplot2} library. In @sec-Scatterplots, we used the argument `se = FALSE` inside the `geom_smooth()` function. If, instead, we set it to `TRUE`, 95% confidence intervals will be displayed as grey bands around the regression lines. To change the α-level, you will need to change the default value of the `level` argument.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nDabrowska.data |> \n  ggplot(mapping = aes(x = EduTotal, \n                       y = Grammar)) +\n  facet_wrap(~ Group) +\n  geom_point() +\n  geom_smooth(method = \"lm\",\n              se = TRUE,\n              level = 0.95) +\n  labs(x = \"Number of years in formal education\",\n       y = \"English grammar comprehension test scores\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![Relationship between years in formal education and English grammar comprehension test scores for groups L1 and L2 with 95% confidence bands](11_Inferential_files/figure-html/fig-YearEducationPLotGrayBand-1.png){#fig-YearEducationPLotGrayBand fig-alt='This is the same plot as above. The only difference is that two shaded gray bands sloping upwards are drawn around the blue lines of two panels. The band around the correlation in the L1 panel is quite narrow and does not encompass the possibility of a flat horizontal line. By constrast, the band around the correlation in the L2 panel is much wider and it is easy to imagine a horizontal line going through it.' width=576}\n:::\n:::\n\n\nThe interpretation of the grey bands is as follows: if it's possible to draw a horizontal (flat) line that stays within the 95 % confidence band, it is very likely that there is *no* statistically significant correlation between the two numeric variables displayed on the plot at the α-level of 0.05.\n\nAs illustrated in @fig-YearEducationPLotGrayBandNullEffectLines, it is impossible to draw such a line in the L1 panel, which is why we reject the null hypothesis of no correlation between years spent in formal education and grammar comprehension test scores for L1 speakers. We conclude that this correlation is significantly different from zero. By contrast, it is perfectly possible to draw such a horizontal line in the L2 panel, which is why we must conclude that, at the α-level of 0.05, we do not have enough evidence to reject the null hypothesis of no correlation in the L2 population. The observed correlation is not statistically significantly different from zero.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![The dotted lines are compatible with the null hypothesis of no correlation.](11_Inferential_files/figure-html/fig-YearEducationPLotGrayBandNullEffectLines-1.png){#fig-YearEducationPLotGrayBandNullEffectLines fig-alt='This is the same plot as above. The only difference is that dotted horizontal horizontal lines have been added to both panels. Only in the L2 panel can the horizontal dotted line fit within the confidence band.' width=576}\n:::\n:::\n\n\n::: {.callout-tip collapse=\"false\"}\n#### Your turn! {.unnumbered}\n\nGenerate a facetted scatter plot (similar to @fig-YearEducationPLotGrayBand) visualising the correlation between age (`Age`) and English receptive vocabulary test scores (`Vocab`) for L1 and L2 participants in two separate panels. Add a grey band visualising 95% confidence intervals around the regression lines.\n\n[**Q11.10**]{style=\"color:green;\"} Looking at your plot only, which of these statements can you confidently say is/are true?\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<form name=\"form_35485\" onsubmit=\"return validate_form_35485()\" method=\"post\">\n<label>\n<input type=\"checkbox\" id=\"answer_35485_1\" value=\"There is more variability around the correlation estimate of the L2 than the L1 group.\"/>\nThere is more variability around the correlation estimate of the L2 than the L1 group.\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_35485_2\" value=\"In the L1 group, the correlation is positive, whereas in the L2 group it is negative.\"/>\nIn the L1 group, the correlation is positive, whereas in the L2 group it is negative.\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_35485_3\" value=\"For both the L1 and L2 groups, the correlations are positive.\"/>\nFor both the L1 and L2 groups, the correlations are positive.\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_35485_4\" value=\"There is more variability around the correlation estimate of the L1 than the L2 group.\"/>\nThere is more variability around the correlation estimate of the L1 than the L2 group.\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_35485_5\" value=\"Even though one correlation is positive and the other negative, in absolute values, the correlations are equally strong.\"/>\nEven though one correlation is positive and the other negative, in absolute values, the correlations are equally strong.\n</label>\n<br/>\n<input type=\"submit\" value=\"Check answer\"/>\n<div id=\"result_35485\"></div>\n</form>\n<script>function validate_form_35485() {var text; var x1 = document.getElementById('answer_35485_1'); var x2 = document.getElementById('answer_35485_2'); var x3 = document.getElementById('answer_35485_3'); var x4 = document.getElementById('answer_35485_4'); var x5 = document.getElementById('answer_35485_5'); if (x1.checked == true&x2.checked == true&x3.checked == false&x4.checked == false&x5.checked == false){text = 'That’s right, well done!';} else {text = 'No, that’s incorrect.';} document.getElementById('result_35485').innerHTML = text; evaluate_final_score(); return false;}function evaluate_final_score(){\n         element = document.getElementById('checkdown_final_score');\n         if(element === null){return false;} else {var element, text, res1, res2, res3, res4, res5, res6, res7, res8, res9, res10;res1 = document.getElementById('result_12592').innerText == 'Correct! Random sampling (also called probability sampling) gives every individual an equal, known chance of being selected.'; res2 = document.getElementById('result_5683').innerText == 'Correct. Even when a convenience sample is “quota‑matched” on observable demographics, it remains a convenience sample because participants first volunteered to take part (e.g., they signed up on a platform like MTurk). This means that unobserved factors, such as motivation, socioeconomic status, and language background, can still render the sample unrepresentative, thus limiting the external validity of inferential data analyses made on the basis of these data.'; res3 = document.getElementById('result_15824').innerText == 'That’s right, well done!'; res4 = document.getElementById('result_95146').innerText == 'Correct!'; res5 = document.getElementById('result_23300').innerText == 'Correct!'; res6 = document.getElementById('result_73587').innerText == 'Correct!'; res7 = document.getElementById('result_4666').innerText == 'That’s right, well done!'; res8 = document.getElementById('result_27514').innerText == 'Correct!'; res9 = document.getElementById('result_66174').innerText == 'That’s right, very well done! The results of this test show the value of statistical significance testing: Here, the difference between the male and female participants is very large, but because we have such a small sample size, the test warns us that this large difference could very well be due to chance and not actually correspond to a real difference in the population.'; res10 = document.getElementById('result_35485').innerText == 'That’s right, well done!';text = res1 + res2 + res3 + res4 + res5 + res6 + res7 + res8 + res9 + res10;element.innerHTML = text;\n         return false;\n         }}</script>\n```\n\n:::\n\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"hint_40933\" onclick=\"return show_hint_40933()\">🐭 Click on the mouse for a first hint.</div>\n<div id=\"result_40933\" onclick=\"return show_hint_40933()\"></div>\n<script>function show_hint_40933(){var x = document.getElementById('result_40933').innerHTML; if(!x){document.getElementById('result_40933').innerHTML = 'Consider the direction of the regression lines and the width of the confidence bands around the lines.';} else {document.getElementById('result_40933').innerHTML = '';}}</script>\n```\n\n:::\n\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"hint_99126\" onmouseover=\"return show_hint_99126()\">🦉 Hover over the owl for a second hint.</div>\n<div id=\"result_99126\" onmouseover=\"return show_hint_99126()\"></div>\n<script>function show_hint_99126(){var x = document.getElementById('result_99126').innerHTML; if(!x){document.getElementById('result_99126').innerHTML = 'Two of these statements are correct.';} else {document.getElementById('result_99126').innerHTML = '';}}</script>\n```\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show code to generate the plot needed to answer Q11.10.\"}\nDabrowska.data |> \n   ggplot(mapping = aes(x = Age, \n                        y = Vocab)) +\n     geom_point() +\n     facet_wrap(~ Group) +\n     geom_smooth(method = \"lm\", \n                 se = TRUE) +\n  theme_bw()\n```\n:::\n\n\nNow use the `cor.test()` function to find out how strong the correlations are in both the L1 and the L2 groups. Use the default α-level of 0.05.\n\n[**Q11.11**]{style=\"color:green;\"} Based on the outputs of the `cor.test()` function, which of these statements can you confidently say is/are true?\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<form name=\"form_22435\" onsubmit=\"return validate_form_22435()\" method=\"post\">\n<label>\n<input type=\"checkbox\" id=\"answer_22435_1\" value=\"In absolute values, the correlation between Age and Vocab scores is stronger in the L1 than in the L2 sample.\"/>\nIn absolute values, the correlation between Age and Vocab scores is stronger in the L1 than in the L2 sample.\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_22435_2\" value=\"The correlation of Age and Vocab scores in the L2 population is -0.20.\"/>\nThe correlation of Age and Vocab scores in the L2 population is -0.20.\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_22435_3\" value=\"Based on the data available to us and an α-level of 0.05, we cannot reject the null hypothesis of no correlation between Age and Vocab scores for the L2 population.\"/>\nBased on the data available to us and an α-level of 0.05, we cannot reject the null hypothesis of no correlation between Age and Vocab scores for the L2 population.\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_22435_4\" value=\"The correlation of Age and Vocab scores in the L1 sample is 0.37.\"/>\nThe correlation of Age and Vocab scores in the L1 sample is 0.37.\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_22435_5\" value=\"Assuming that the null hypothesis of no correlation between Age and Vocab scores in the L1 population is true, there is only a 0.03% chance of observing a correlation of 0.37 or larger in a sample of this size.\"/>\nAssuming that the null hypothesis of no correlation between Age and Vocab scores in the L1 population is true, there is only a 0.03% chance of observing a correlation of 0.37 or larger in a sample of this size.\n</label>\n<br/>\n<input type=\"submit\" value=\"Check answer\"/>\n<div id=\"result_22435\"></div>\n</form>\n<script>function validate_form_22435() {var text; var x1 = document.getElementById('answer_22435_1'); var x2 = document.getElementById('answer_22435_2'); var x3 = document.getElementById('answer_22435_3'); var x4 = document.getElementById('answer_22435_4'); var x5 = document.getElementById('answer_22435_5'); if (x1.checked == true&x2.checked == false&x3.checked == true&x4.checked == true&x5.checked == true){text = 'That’s right, well done!';} else {text = 'No, take another minute to carefully re-read all the statements.';} document.getElementById('result_22435').innerHTML = text; evaluate_final_score(); return false;}function evaluate_final_score(){\n         element = document.getElementById('checkdown_final_score');\n         if(element === null){return false;} else {var element, text, res1, res2, res3, res4, res5, res6, res7, res8, res9, res10, res11;res1 = document.getElementById('result_12592').innerText == 'Correct! Random sampling (also called probability sampling) gives every individual an equal, known chance of being selected.'; res2 = document.getElementById('result_5683').innerText == 'Correct. Even when a convenience sample is “quota‑matched” on observable demographics, it remains a convenience sample because participants first volunteered to take part (e.g., they signed up on a platform like MTurk). This means that unobserved factors, such as motivation, socioeconomic status, and language background, can still render the sample unrepresentative, thus limiting the external validity of inferential data analyses made on the basis of these data.'; res3 = document.getElementById('result_15824').innerText == 'That’s right, well done!'; res4 = document.getElementById('result_95146').innerText == 'Correct!'; res5 = document.getElementById('result_23300').innerText == 'Correct!'; res6 = document.getElementById('result_73587').innerText == 'Correct!'; res7 = document.getElementById('result_4666').innerText == 'That’s right, well done!'; res8 = document.getElementById('result_27514').innerText == 'Correct!'; res9 = document.getElementById('result_66174').innerText == 'That’s right, very well done! The results of this test show the value of statistical significance testing: Here, the difference between the male and female participants is very large, but because we have such a small sample size, the test warns us that this large difference could very well be due to chance and not actually correspond to a real difference in the population.'; res10 = document.getElementById('result_35485').innerText == 'That’s right, well done!'; res11 = document.getElementById('result_22435').innerText == 'That’s right, well done!';text = res1 + res2 + res3 + res4 + res5 + res6 + res7 + res8 + res9 + res10 + res11;element.innerHTML = text;\n         return false;\n         }}</script>\n```\n\n:::\n\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"hint_73664\" onclick=\"return show_hint_73664()\">🐭 Click on the mouse for a hint.</div>\n<div id=\"result_73664\" onclick=\"return show_hint_73664()\"></div>\n<script>function show_hint_73664(){var x = document.getElementById('result_73664').innerHTML; if(!x){document.getElementById('result_73664').innerHTML = 'Four of these statements are correct.';} else {document.getElementById('result_73664').innerHTML = '';}}</script>\n```\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show code to answer Q11.11.\"}\ncor.test(formula = ~ Age + Vocab,\n         data = L1.data)\n\ncor.test(formula = ~ Age + Vocab,\n         data = L2.data)\n```\n:::\n\n\nConsider the code and its output below. @fig-VocabArrivalPLot visualises the correlation between L2 speakers' English receptive vocabulary test scores (`Vocab`) and their age of arrival in the UK (`Arrival`).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nL2.data |> \n   ggplot(mapping = aes(x = Arrival, \n                        y = Vocab)) +\n     geom_point() +\n     geom_smooth(method = \"lm\", \n                 se = TRUE) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![Correlation between L2 speakers’ English vocabulary test scores and their age of arrival](11_Inferential_files/figure-html/fig-VocabArrivalPLot-1.png){#fig-VocabArrivalPLot fig-alt='A scatter plot showing the relationship between the L2 speakers\\' vocabulary test scores and their age of arrival in the UK. Each black dot represents an individual’s score. A blue regression line in the plot shows a negative trend: higher test scores are associated with younger ages of arrival. A shaded gray band sloping downwards is drawn around the blue line.' width=576}\n:::\n:::\n\n\n[**Q11.12**]{style=\"color:green;\"} Run a correlation test on the correlation visualised in @fig-VocabArrivalPLot. How strong is the correlation (to two decimal places)?\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<form name=\"form_6341\" onsubmit=\"return validate_form_6341()\" method=\"post\">\n<input type=\"text\" placeholder=\"\" name=\"answer_6341\"/>\n<input type=\"submit\" value=\"Check answer\"/>\n<div id=\"result_6341\"></div>\n</form>\n<script>function validate_form_6341() {var x, text; var x = document.forms['form_6341']['answer_6341'].value;if (x == '-0.31'|x == '- 0.31'|x == '-0.30'|x == '- 0.30'){text = 'That’s right, very well done!';} else {text = 'No. Are you perhaps looking at the wrong dataset (we are interested in L2 speakers only here) or the wrong variables (we are now looking at the association between Vocab and Arrival)?';} document.getElementById('result_6341').innerHTML = text; evaluate_final_score(); return false;}function evaluate_final_score(){\n         element = document.getElementById('checkdown_final_score');\n         if(element === null){return false;} else {var element, text, res1, res2, res3, res4, res5, res6, res7, res8, res9, res10, res11, res12;res1 = document.getElementById('result_12592').innerText == 'Correct! Random sampling (also called probability sampling) gives every individual an equal, known chance of being selected.'; res2 = document.getElementById('result_5683').innerText == 'Correct. Even when a convenience sample is “quota‑matched” on observable demographics, it remains a convenience sample because participants first volunteered to take part (e.g., they signed up on a platform like MTurk). This means that unobserved factors, such as motivation, socioeconomic status, and language background, can still render the sample unrepresentative, thus limiting the external validity of inferential data analyses made on the basis of these data.'; res3 = document.getElementById('result_15824').innerText == 'That’s right, well done!'; res4 = document.getElementById('result_95146').innerText == 'Correct!'; res5 = document.getElementById('result_23300').innerText == 'Correct!'; res6 = document.getElementById('result_73587').innerText == 'Correct!'; res7 = document.getElementById('result_4666').innerText == 'That’s right, well done!'; res8 = document.getElementById('result_27514').innerText == 'Correct!'; res9 = document.getElementById('result_66174').innerText == 'That’s right, very well done! The results of this test show the value of statistical significance testing: Here, the difference between the male and female participants is very large, but because we have such a small sample size, the test warns us that this large difference could very well be due to chance and not actually correspond to a real difference in the population.'; res10 = document.getElementById('result_35485').innerText == 'That’s right, well done!'; res11 = document.getElementById('result_22435').innerText == 'That’s right, well done!'; res12 = document.getElementById('result_6341').innerText == 'That’s right, very well done!';text = res1 + res2 + res3 + res4 + res5 + res6 + res7 + res8 + res9 + res10 + res11 + res12;element.innerHTML = text;\n         return false;\n         }}</script>\n```\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show code to answer Q11.12.\"}\ncor.test(formula = ~ Vocab + Arrival,\n         data = L2.data)\n```\n:::\n\n\n[**Q11.13**]{style=\"color:green;\"} How likely are we to observe such a strong correlation or an even stronger one in a sample of this size, if there is actually no correlation in the full L2 population?\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<form name=\"form_29482\" onsubmit=\"return validate_form_29482()\" method=\"post\">\n<label>\n<input type=\"radio\" name=\"answer_29482\" id=\"answer_29482_1\" value=\"about 1%\"/>\nabout 1%\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_29482\" id=\"answer_29482_2\" value=\"about 0.01%\"/>\nabout 0.01%\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_29482\" id=\"answer_29482_3\" value=\"about 10%\"/>\nabout 10%\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_29482\" id=\"answer_29482_4\" value=\"It&#39;s impossible to tell because the correlation is not statistically significant.\"/>\nIt's impossible to tell because the correlation is not statistically significant.\n</label>\n<br/>\n<input type=\"submit\" value=\"Check answer\"/>\n<div id=\"result_29482\"></div>\n</form>\n<script>function validate_form_29482() {var x, text; var x = document.forms['form_29482']['answer_29482'].value;if (x == 'about 1%'){text = 'That’s right, very well done!';} else {text = 'No. You will need to interpret the p-value to answer this question.';} document.getElementById('result_29482').innerHTML = text; evaluate_final_score(); return false;}function evaluate_final_score(){\n         element = document.getElementById('checkdown_final_score');\n         if(element === null){return false;} else {var element, text, res1, res2, res3, res4, res5, res6, res7, res8, res9, res10, res11, res12, res13;res1 = document.getElementById('result_12592').innerText == 'Correct! Random sampling (also called probability sampling) gives every individual an equal, known chance of being selected.'; res2 = document.getElementById('result_5683').innerText == 'Correct. Even when a convenience sample is “quota‑matched” on observable demographics, it remains a convenience sample because participants first volunteered to take part (e.g., they signed up on a platform like MTurk). This means that unobserved factors, such as motivation, socioeconomic status, and language background, can still render the sample unrepresentative, thus limiting the external validity of inferential data analyses made on the basis of these data.'; res3 = document.getElementById('result_15824').innerText == 'That’s right, well done!'; res4 = document.getElementById('result_95146').innerText == 'Correct!'; res5 = document.getElementById('result_23300').innerText == 'Correct!'; res6 = document.getElementById('result_73587').innerText == 'Correct!'; res7 = document.getElementById('result_4666').innerText == 'That’s right, well done!'; res8 = document.getElementById('result_27514').innerText == 'Correct!'; res9 = document.getElementById('result_66174').innerText == 'That’s right, very well done! The results of this test show the value of statistical significance testing: Here, the difference between the male and female participants is very large, but because we have such a small sample size, the test warns us that this large difference could very well be due to chance and not actually correspond to a real difference in the population.'; res10 = document.getElementById('result_35485').innerText == 'That’s right, well done!'; res11 = document.getElementById('result_22435').innerText == 'That’s right, well done!'; res12 = document.getElementById('result_6341').innerText == 'That’s right, very well done!'; res13 = document.getElementById('result_29482').innerText == 'That’s right, very well done!';text = res1 + res2 + res3 + res4 + res5 + res6 + res7 + res8 + res9 + res10 + res11 + res12 + res13;element.innerHTML = text;\n         return false;\n         }}</script>\n```\n\n:::\n\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"hint_49747\" onclick=\"return show_hint_49747()\">🐭 Click on the mouse for a hint.</div>\n<div id=\"result_49747\" onclick=\"return show_hint_49747()\"></div>\n<script>function show_hint_49747(){var x = document.getElementById('result_49747').innerHTML; if(!x){document.getElementById('result_49747').innerHTML = 'A p-value of 1 would correspond to 100% likelihood, whereas a p-value of 0 would correspond to 0% probability of observing a correlation of this strength or larger in a sample of this size under the assumption of no correlation in the population.';} else {document.getElementById('result_49747').innerHTML = '';}}</script>\n```\n\n:::\n:::\n\n:::\n\n## Assumptions of statistical tests {#sec-Assumptions}\n\nA crucial aspect that we have glossed over so far in this chapter is that the results of statistical tests, such as *t*-tests and correlation tests, can only be considered accurate if the test's underlying assumptions are met.\n\n### Randomisation {#sec-Randomisation}\n\nThe only assumption that we have mentioned so far is that of **random sampling** (@sec-Sampling). In practice, this assumption is rarely met in the language sciences. For instance, the L1 and L2 participants recruited for @DabrowskaExperienceAptitudeIndividual2019 were not randomly drawn from the entire adult UK population. Dąbrowska [-@DabrowskaExperienceAptitudeIndividual2019: 5-6] reports that the participants \"were recruited through personal contacts, church and social clubs, and advertisements in local newspapers\". Such transparency over the sampling procedure is crucial to interpreting the results of a study.\n\n> When the assumption of random sampling is not met, inferences to the population become difficult. In this case, researchers should describe the sample and population in sufficient detail to justify that the sample was at least representative of the intended population ([Wilkinson and APA Task Force on Statistical Inference, 1999](https://doi.org/10.1037/0003-066X.54.8.594)). If such a justification is not made, readers are left to their own interpretation as to the generalizability of the results. [@nimonStatisticalAssumptionsSubstantive2012: 1]\n\n### Independence {#sec-Independence}\n\nAnother crucial assumption for both tests covered in this chapter is that the data points be independent of each other. We assumed that this is the case with the @DabrowskaExperienceAptitudeIndividual2019 data because it only has one observation per participant (although interdependencies may occur at other levels, e.g. when several participants come from the same school, work place, or neighbourhood). If, however, we had multiple observations per participant because they completed the tests twice, say once in the morning and once in the evening, and then entered both the morning and the evening data into a single statistical test, our data would violate the assumption of independence and the results of the test would be inaccurate. When our data violate the assumption of independence, we cannot use statistical tests like *t*-tests. Instead, we must turn to statistical methods that allow us to model these interdependencies in the data. In the language sciences, this is most commonly achieved using **mixed-effects models** [see e.g. @WinterStatisticsLinguistsIntroduction2019: Ch. 14-15].\n\n### Normality {#sec-Normality}\n\nFor many inferential statistical tests commonly reported in the language sciences, it is also assumed that the **population data** are normally distributed (see @sec-Normal). This assumption is often quite reasonable, because many real-world quantities are normally distributed. This is why we typically say that this assumption can be relaxed if we have more than 30 observations per group.\n\nHowever, there are some things in the world that are inherently non-normally distributed. For instance, word frequencies in a text or a collection of texts (i.e., a corpus) are never normally distributed: a handful of words occur extremely frequently (in written English typically: *the*, *of*, *a*, *in*, *to*, etc.), some words are fairly frequent, but the vast majority are very infrequent. Reaction times is another example of a kind of variable that hardly ever meets the criterion of normality. One way to deal with highly skewed distributions like word frequencies and reaction times is to apply **transformations** to these variables before attempting to do any inferential statistics [see @WinterStatisticsLinguistsIntroduction2019: Chapter 5].\n\nOf course, we cannot check if the population data meet the assumption of normality because we do not have the entire population data (and if we did, we wouldn't *need* inferential statistics!) so the best we can do is check if our *data* are normally distributed. This is best achieved visually. For instance, before conducting a *t*-test comparing the mean difference in `Grammar` scores in the L1 and L2 groups using the `t.test()` function, we should first visualise the two distributions of `Grammar` scores to check that they are approximately normally distributed. @fig-GrammarDensityPlot shows that this is clearly *not* the case!\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show code to generate plot.\"}\nDabrowska.data |> \n  ggplot(mapping = aes(x = Grammar,\n                       fill = Group)) +\n  geom_density(alpha = 0.7) +\n  scale_fill_viridis_d() +\n  labs(x = \"English grammar comprehension test scores\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![Density plot comparing English grammar comprehension test scores between groups L1 and L2](11_Inferential_files/figure-html/fig-GrammarDensityPlot-1.png){#fig-GrammarDensityPlot fig-alt='A density plot comparing English grammar comprehension test scores for two groups, L1 (purple) and L2 (yellow). The x-axis represents the test scores, while the y-axis indicates the density. The L1 group shows a higher concentration of scores near the top end of the scale. The L2 group has a broader distribution, indicating more variability.' width=576}\n:::\n:::\n\n\nWe are dealing here with **non-normal** or **non-parametric data**, hence we need a non-parametric version of the *t*-test: the **Wilcoxon test** (also known as the Mann-Whitney test or the *U*-test):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwilcox.test(Grammar ~ Group, \n            data = Dabrowska.data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWilcoxon rank sum test with continuity correction\n\ndata:  Grammar by Group\nW = 3640, p-value = 0.026\nalternative hypothesis: true location shift is not equal to 0\n```\n\n\n:::\n:::\n\n\nThe output of the `wilcox.test()` function tells us that there is a 2.6% chance of obtaining our observed difference in **median** `Grammar` scores in an L1 and L2 sample of this size or an even larger difference under the assumption of the null hypothesis (of no difference across the two groups). Hence, if we had previously defined our α-level as 0.05, we reject the null hypothesis. If, however, we had chosen a stricter α-level of 0.01, we fail to reject the null hypothesis.\n\nThe Wilcoxon test does not make any assumptions about the distributions of the population data, which means that it can be used for all kinds of continuous variables including word frequencies. However, there is a drawback: it is usually less powerful than the standard *t*-test. Using it, we are more likely to report a false negative (i.e., to fail to report a real difference in two means), which is why a transformation may be wiser [for more about transformations, see @WinterStatisticsLinguistsIntroduction2019: Chapter 5].\n\n### Linearity and outliers {#sec-Linearity}\n\nAll statistical significance tests are sensitive to outliers. Hence, it is important to identify any outliers (e.g., by plotting your data as part of preliminary data exploration) and to carefully consider the extent to which they may influence the results of your analysis.\n\nAnother important assumption of Pearson's correlation coefficient (*r*) is that the continuous variables are linearly related. Again, this is best perceived visually: if you plot the two variables against one another in a scatter plot (see @sec-Scatterplots), the points should fall roughly along a single, straight line. When the true association is non-linear (e.g., curvilinear), Pearson's *r* will underestimate the strength of that association as it only captures the linear component of an association [@TabachnickUsingMultivariateStatistics2014: 117].\n\nIn 1973, the statistician Frank Anscombe put together a small dataset with four pairs of variables (*x* and *y*) with the aim of illustrating the necessity to visualise data and not rely solely on statistics. The dataset is included in base `R` so we can access it by calling its name without downloading or installing anything:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanscombe\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   x1 x2 x3 x4    y1   y2    y3    y4\n1  10 10 10  8  8.04 9.14  7.46  6.58\n2   8  8  8  8  6.95 8.14  6.77  5.76\n3  13 13 13  8  7.58 8.74 12.74  7.71\n4   9  9  9  8  8.81 8.77  7.11  8.84\n5  11 11 11  8  8.33 9.26  7.81  8.47\n6  14 14 14  8  9.96 8.10  8.84  7.04\n7   6  6  6  8  7.24 6.13  6.08  5.25\n8   4  4  4 19  4.26 3.10  5.39 12.50\n9  12 12 12  8 10.84 9.13  8.15  5.56\n10  7  7  7  8  4.82 7.26  6.42  7.91\n11  5  5  5  8  5.68 4.74  5.73  6.89\n```\n\n\n:::\n:::\n\n\nKnown as the Anscombe Quartet, the dataset's four pairs of variables have exactly the same correlation coefficient (when rounded to two decimal places):\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(anscombe$x1, anscombe$y1) |> \n  round(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.82\n```\n\n\n:::\n\n```{.r .cell-code}\ncor(anscombe$x2, anscombe$y2) |> \n  round(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.82\n```\n\n\n:::\n\n```{.r .cell-code}\ncor(anscombe$x3, anscombe$y3) |> \n  round(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.82\n```\n\n\n:::\n\n```{.r .cell-code}\ncor(anscombe$x4, anscombe$y4) |> \n  round(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.82\n```\n\n\n:::\n:::\n\n\nHowever, when we visualise the data in scatter plots, it turns out that the relationships between each pair are completely different!\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"See {ggplot2} code to generate plots.\"}\nPair1 <- ggplot(data = anscombe, \n       mapping = aes(x = x1, y = y1)) +\n  geom_point(colour = \"blue\",\n             size = 2,\n             alpha = 0.8) +\n  labs(title=\"Pair 1\") +\n  geom_smooth(method = \"lm\", \n              colour = \"blue\",\n              se = FALSE) +\n  theme_bw()\n\nPair2 <- ggplot(data = anscombe, \n       mapping = aes(x = x2, y = y2)) +\n  geom_point(colour = \"orange\",\n             size = 2,\n             alpha = 0.8) +\n  labs(title=\"Pair 2\") +\n  geom_smooth(method = \"lm\",\n              colour = \"orange\",\n              se = FALSE) +\n  theme_bw()\n\nPair3 <- ggplot(data = anscombe, \n       mapping = aes(x = x3, y = y3)) +\n  geom_point(colour = \"darkgreen\",\n             size = 2,\n             alpha = 0.8) +\n  labs(title=\"Pair 3\") +\n  geom_smooth(method = \"lm\",\n              colour = \"darkgreen\",\n              se = FALSE) +\n  theme_bw()\n\nPair4 <- ggplot(data = anscombe, \n       mapping = aes(x = x4, y = y4)) +\n  geom_point(colour = \"darkred\",\n             size = 2,\n             alpha = 0.8) +\n  labs(title=\"Pair 4\") +\n  geom_smooth(method = \"lm\", \n              colour = \"darkred\",\n              se = FALSE) +\n  theme_bw()\n\nlibrary(patchwork)\nPair1 + Pair2 + Pair3 + Pair4\n```\n\n::: {.cell-output-display}\n![Four plots showing the relationship between pairs of variables](11_Inferential_files/figure-html/fig-4PairPlot-1.png){#fig-4PairPlot fig-alt='A set of four scatter plots showing the relationship between four pairs of variables (x1, y1), (x2, y2), (x3, y3), and (x4, y4). The Pair 2 plot shows a non-linear relationship between variables.' width=576}\n:::\n:::\n\n\nFrom @fig-4PairPlot, we can immediately see that the relationship between the two variables in Pair 2 is **non-linear**, which makes the yellow linear regression line nonsensical. The data visualisations for Pairs 3 and 4 illustrate how a single **outlier** can either create an illusion of a correlation that does not exist (Pair 4) or overestimate one that does exist but is probably considerably weaker than Pearson's r would lead us to believe (Pair 3).\n\nThe assumption of linearity is relevant to many widely used statistical methods -- notably linear regression models (see @sec-SLR and @sec-MLR) -- which rely on Pearson's correlation coefficients. In practice, researchers usually assume linearity unless there is a strong theoretical reason to expect a non‑linear pattern (Cohen et al. 2003). However, this assumption should always be checked. The most straightforward way to do this is to visualise the data.\n\n::: callout-note\n# Non-parametric correlations\n\nFor an in-depth explanation of non-parametric correlation coefficients (Spearman's ρ and Kendall's τ) and statistical significance tests with examples from the language sciences, I recommend reading Levshina [-@LevshinaHowlinguisticsData2015: Chapter 6].\n:::\n\n### Homogeneity of variance and homoscedasticity {#sec-Homoscedasticity}\n\nWhen conducting a Student's *t*-test, the variances of the samples should be constant, or homogeneous. This is referred to as the assumption of homogeneity of variance. It means that the variances of the groups entered in a test should be roughly the same.\n\nAgain, this assumption is best examined visually. For example, we can generate a boxplot to check that the variance of the two groups that we want to compare with a *t*-test are roughly equal (see @fig-GrammarBoxplot).\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"See code to generate plot.\"}\nggplot(Dabrowska.data,\n       aes(x = Group, y = Grammar)) +\n  geom_boxplot() +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![Comparison of grammar scores between L1 and L2 groups](11_Inferential_files/figure-html/fig-GrammarBoxplot-1.png){#fig-GrammarBoxplot fig-alt='A box plot comparing grammar scores between two groups, L1 and L2. The L1 group shows a narrow range of high scores clustered near 100, with one outlier around 50. The L2 group displays a wider range of scores.' width=576}\n:::\n:::\n\n\nAs we already saw in @sec-Normality, the `Grammar` scores of the L2 group are not normally distributed; hence, we can also see from @fig-GrammarBoxplot that there is much more variance in the L2 data than there is in the L1. In other words, we have heterogeneity of variance. One way to deal with this is to conduct a non-parametric test instead: Wilcox's test does not assume homogeneity of variance. However, if your data meet the criterion of normality and only fails to meet that homogeneity of variance, you can still conduct a parametric *t*-test because the standard `t.test()` function in `R` includes Welch's adjustment to correct for unequal variances [@FieldDiscoveringstatisticsusing2012: 373].\n\nA related assumption is made in correlation tests and linear regression and is called the assumption of **homoscedasticity** (see also @sec-Residuals). To check the assumption of homoscedasticity for a correlation, we can visualise the variance (or variability) around the correlation (the linear regression line) with the help of a scatter plot. In @fig-VocabEduPlot, we can see that this variance is much larger for higher `Vocab` scores than for low to medium scores.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"See code to generate plot.\"}\nDabrowska.data |> \n  filter(Group == \"L1\") |> \n  ggplot(aes(x = Vocab, y = EduTotal)) +\n  geom_point() +\n  geom_smooth(method = \"lm\",\n              se = FALSE) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![The relationship between vocabulary and total education years](11_Inferential_files/figure-html/fig-VocabEduPlot-1.png){#fig-VocabEduPlot fig-alt='A scatter plot showing the relationship between vocabulary on the x-axis and total education years on the y-axis. Each black dot represents a data point, and a blue line indicates a positive correlation; however, as Vocab scores become higher, the dots appear to be futher removed from the blue regression line.' width=576}\n:::\n:::\n\n\nIn other words, these data do not meet the assumption of homoscedasticity. In this case, we should therefore use a non-pamametric correlation statistic, such as Spearman's ρ ('rho') and Kendall's τ ('tau'). As we are looking at a relatively small dataset here (only L1 speakers) and some speakers performed equally well on the `Vocab` test (in other words, we have some ties in the ranks), Kendall's τ is recommended:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor.test(formula = ~ Vocab + EduTotal,\n         data = L1.data,\n         method = \"kendall\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tKendall's rank correlation tau\n\ndata:  Vocab and EduTotal\nz = 3.5406, p-value = 0.0003992\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n      tau \n0.2761618 \n```\n\n\n:::\n:::\n\n\nIt returns a correlation coefficient τ of `0.28` and a *p*-value of \\< 0.05, which allows us to reject the null hypothesis of no association between `Vocab` and `EduTotal` scores among L1 English speakers.\n\n::: {.callout-caution collapse=\"true\"}\n#### [Optional (fun) task]{.unnumbered style=\"color:green;\"} 🦖\n\nInspired by the Anscombe Quartet, @matejkaSameStatsDifferent2017 created a set of 12 pairs of variables that have the same descriptive statistics as the data that produce a scatter plot representing a tyrannosaurus (see @fig-DinoPLot).\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Scatter plot depicting a tyrannosaurus (originally created by [Alberto Cairo](https://web.archive.org/web/20240620205540/http://www.thefunctionalart.com/2016/08/download-datasaurus-never-trust-summary.html))](11_Inferential_files/figure-html/fig-DinoPLot-1.png){#fig-DinoPLot fig-alt='A scatter plot with the x-axis ranging from 20 to 100, and the y-axis ranging from 0 to 100. The data points form a pattern that looks like a tyrannosaurus.' width=384}\n:::\n:::\n\n\n[**1.**]{style=\"color:green;\"} Install and load the `datasauRus` package to access the `R` data object `datasaurus_dozen`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"datasauRus\")\nlibrary(datasauRus)\n\ndatasaurus_dozen\n```\n:::\n\n\n[**2.**]{style=\"color:green;\"} Run the following code to compare the descriptive statistics of all three datasets within the `datasaurus_dozen`. The fourth set, `dino`, is the one visualised in @fig-DinoPLot. Note that there is a very small, negative correlation between all pairs of variables of `-0.0656`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndatasaurus_dozen |> \n  group_by(dataset) |> \n  summarise(\n    mean_x = mean(x), \n    mean_y = mean(y), \n    std_dev_x = sd(x), \n    std_dev_y = sd(y), \n    corr_x_y = cor(x, y))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 13 × 6\n   dataset    mean_x mean_y std_dev_x std_dev_y corr_x_y\n   <chr>       <dbl>  <dbl>     <dbl>     <dbl>    <dbl>\n 1 away         54.3   47.8      16.8      26.9  -0.0641\n 2 bullseye     54.3   47.8      16.8      26.9  -0.0686\n 3 circle       54.3   47.8      16.8      26.9  -0.0683\n 4 dino         54.3   47.8      16.8      26.9  -0.0645\n 5 dots         54.3   47.8      16.8      26.9  -0.0603\n 6 h_lines      54.3   47.8      16.8      26.9  -0.0617\n 7 high_lines   54.3   47.8      16.8      26.9  -0.0685\n 8 slant_down   54.3   47.8      16.8      26.9  -0.0690\n 9 slant_up     54.3   47.8      16.8      26.9  -0.0686\n10 star         54.3   47.8      16.8      26.9  -0.0630\n11 v_lines      54.3   47.8      16.8      26.9  -0.0694\n12 wide_lines   54.3   47.8      16.8      26.9  -0.0666\n13 x_shape      54.3   47.8      16.8      26.9  -0.0656\n```\n\n\n:::\n:::\n\n\n[**3.**]{style=\"color:green;\"} Run the following code to visualise the relationships between the other 12 pairs of variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(datasaurus_dozen, \n       aes(x = x, y = y, colour = dataset)) + \n  geom_point() + \n  facet_wrap(~ dataset, ncol = 3) +\n  theme_bw() + \n  theme(legend.position = \"none\") + \n  labs(x = NULL,\n       y = NULL)\n```\n:::\n\n\n[**4.**]{style=\"color:green;\"} Add a `geom_` layer (see @sec-geoms) to the following {ggplot2} code to add a blue linear regression line in all 13 panels.\n\n\n::: {.cell source-line-numbers='4:6'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"View code to complete Part 4 of the task.\"}\nggplot(datasaurus_dozen, \n       aes(x = x, y = y, colour = dataset)) + \n  geom_point() + \n  facet_wrap(~ dataset, ncol = 3) +\n  geom_smooth(method = \"lm\",\n              se = FALSE,\n              colour = \"blue\") +\n  theme_bw() + \n  theme(legend.position = \"none\") + \n  labs(x = NULL,\n       y = NULL)  \n```\n:::\n\n\nClearly, if we had only used correlation statistics to describe the relationships between these 13 pairs of variables, we would have missed some literally dinosaur-sized patterns! 🤯\n:::\n\n::: column-margin\n![Hex sticker of the [{datasauRus}](https://jumpingrivers.github.io/datasauRus/) package](images/hex_datasauRus.png){#fig-hexdatasauRus width=\"100\" fig-alt=\"The hexagonal logo of the datasauRus package features a scatter plot in which the dots look like a dinosaur.\"}\n:::\n\n![Remember that the best way to check test assumptions is to visualise your data! (artwork by [Allison Horst](https://twitter.com/allison_horst) CC BY 4.0)](images/AHorst_NotNormal.png){#fig-NotNormal fig-alt=\"Cartoon of a normal distribution looking skeptically at an excited looking bimodal / negatively skewed distribution. The first says to the second, \\\"you're not normal.\\\"\" width=\"448\"}\n\n## Multiple testing problem {#sec-pHacking}\n\nIn @sec-PValues we saw that conducting a statistical test with an α-level of 0.05 means that we accept a 5% risk of falsely rejecting the null hypothesis when it is actually true. Falsely rejecting the null hypothesis leads to a **false positive** result, also referred to as making a **Type I error**. It is crucial to understand that if we perform multiple tests on the same data, we dramatically increase the risk of reporting such false positive results. This is known as the multiple testing or multiple comparisons problem.\n\nImagine that we want to test 20 independent null hypotheses on a single dataset using α = 0.05 as our significance threshold. Even if all null hypotheses are actually true, the probability of obtaining at least one significant result, i.e. at least one *p*-value \\<  0.05, by chance is equal to 64%.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n1 - (1 - 0.05)^20\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.6415141\n```\n\n\n:::\n:::\n\n\nThus, having conducted 20 independent tests, we have a 64% chance of finding at least one statistically significant result purely by chance! As the number of tests increases, this probability approaches certainty [see also @BaayenAnalyzinglinguisticdata2008: 106-107]. With 100 tests, we are at 99%:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n1 - (1 - 0.05)^100\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9940795\n```\n\n\n:::\n:::\n\n\nTo avoid reporting false positive results, it is therefore recommended that we correct our α-level to account for the number of tests performed on the data. The simplest (but also most conservative) approach to do so is called the **Bonferroni correction**. It consists in adjusting our chosen α-level by dividing it by the number of tests that we are conducting on the data. Hence, if we want to use 0.05 as our significance level and conduct 20 independent tests on the same data, we divide 0.05 by 20:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n0.05 / 20\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0025\n```\n\n\n:::\n:::\n\n\nThis means that we would now only report the outcome of statistical test as \"statistically significant\" if its *p*-value is \\< 0.0025.\n\nOther correction methods for multiple testing exist. **Holm's** method, for example, is a popular, slightly less conservative alternative to the Bonferroni correction. The base `R` function `p.adjust()` can be used to automatically adjust *p*-values using different methods including Bonferroni's and Holm's.\n\nHere are the seven *p*-values that we obtained from the seven independent statistical tests that we performed on the @DabrowskaExperienceAptitudeIndividual2019 data as part of this chapter:\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_values <- c(1.956e-05, 0.000007032, 0.5525, 0.0005581, 0.2782, 0.026, 0.0003992)\n```\n:::\n\n\nAt our chosen α-level of 0.05, five of these *p*-values were statistically significant, leading us to reject the corresponding five null hypotheses. However, if we apply Holm's correction using `p.adjust()`, we find that we can only reject four of these null hypotheses.\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_values.adjusted <- p.adjust(p_values, method = \"holm\")\n\n# The adjusted p-values in scientific notation:\np_values.adjusted\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.1736e-04 4.9224e-05 5.5640e-01 2.2324e-03 5.5640e-01 7.8000e-02 1.9960e-03\n```\n\n\n:::\n\n```{.r .cell-code}\n# The adjusted p-values in standard notation:\nformat(p_values.adjusted, scientific = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"0.000117360\" \"0.000049224\" \"0.556400000\" \"0.002232400\" \"0.556400000\"\n[6] \"0.078000000\" \"0.001996000\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Which adjusted p-values are below 0.05, indicating that we can reject the corresponding null hypothesis:\np_values.adjusted < 0.05\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  TRUE  TRUE FALSE  TRUE FALSE FALSE  TRUE\n```\n\n\n:::\n:::\n\n\nIt is crucial to understand that every *p*-value represents a probabilistic finding, not a definitive statement about reality. A *p*-value of 0.03 does *not* mean there is a 97% chance that the alternative hypothesis is true. Instead, it indicates that, if the null hypothesis were true, we would observe data at least as extreme as our sample only 3% of the time. Given the probabilistic nature of statistical inference and the multiple testing problem, **replication** of findings across independent studies is essential for building scientific knowledge (see @sec-Reproducibility). No single study, regardless of its sample size or *p*-value, can ever provide us with definitive evidence.\n\nBecause of the multiple testing problem, only reporting statistically significant results and failing to report the number of tests conducted to find them is considered a **Questionable Research Practice (QRP)**. It is one way to [*p*-hack](https://forrt.org/glossary/english/p_hacking/) results. A great resource for definitions and links to further literature related to conducting research is the [FORRT Glossary](https://forrt.org/glossary) — a community-sourced glossary of Open Scholarship terms [@parsonsCommunitysourcedGlossaryOpen2022]. Below is the FORRT Glossary's entry for QRPs.\n\n> #### Questionable Research Practices or Questionable Reporting Practices (QRPs) {.unnumbered}\n>\n> **Also available in:** [Arabic](https://forrt.org/glossary/arabic/questionable_research_practices_or_questionable_reporting_practices/) \\| [German](https://forrt.org/glossary/german/questionable_research_practices_or_questionable_reporting_practices/) \\|\n>\n> **Definition:** A range of activities that intentionally or unintentionally distort data in favour of a researcher's own hypotheses - or omissions in reporting such practices - including; selective inclusion of data, hypothesising after the results are known (HARKing), and *p*-hacking. Popularized by John et al. (2012).\n>\n> **Related terms:** Creative use of outliers, Fabrication, [File-drawer](https://forrt.org/glossary/english/publication_bias/), [Garden of forking paths](https://forrt.org/glossary/english/garden_of_forking_paths/), [HARKing](https://forrt.org/glossary/english/harking/), Nonpublication of data, [*p*-hacking](https://forrt.org/glossary/english/p_hacking/), *p*-value fishing, Partial publication of data, Post-hoc storytelling, [Preregistration](https://forrt.org/glossary/english/preregistration/), [Questionable Measurement Practices (QMP)](https://forrt.org/glossary/english/questionable_measurement_practices/), [Researcher degrees of freedom](https://forrt.org/glossary/english/researcher_degrees_of_freedom/), [Reverse *p*-hacking](https://forrt.org/glossary/english/reverse_p_hacking/), [Salami slicing](https://forrt.org/glossary/english/salami_slicing/)\n>\n> **Reference:** Banks et al. (2016); Fiedler and Schwartz (2016); Hardwicke et al. (2014); John et al. (2012); Neuroskeptic (2012); Sijtsma (2016); Simonsohn et al. (2011)\n>\n> **Drafted and Reviewed by:** Mahmoud Elsherif, Tamara Kalandadze, William Ngiam, Sam Parsons, Mariella Paul, Eike Mark Rinke, Timo Roettger, Flávio Azevedo\n\n::: callout-tip\n#### Your turn! {.unnumbered}\n\nThe dataset from @DabrowskaExperienceAptitudeIndividual2019 includes the results of numerous additional tests that we have not yet examined.\n\nIn this task, we imagine that a friend of yours has decided to test whether, on average, male and female speakers performed equally well on four additional English grammar tests. The four variables that he selected correspond to participants' scores (adjusted for guessing) on English tests assessing participants' mastery of:\n\n-   the active voice (`Active`)\n-   the passive voice (`Passive`)\n-   post-modified subjects (`Postmod`)\n-   subject relatives (`SubRel`)\n\nYour friend conducted the following four *t*-tests to find out whether he could reject the the null hypothesis of no difference in the average performance of male and female English speakers in these four grammar tests. He chose 0.05 as his significance threshold and, on the basis of these four tests, concluded that he could reject the null hypothesis in two out of four tests: the one concerning the active voice and the one about subject relatives.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(formula = Active ~ Gender, \n       data = Dabrowska.data)\n\nt.test(formula = Passive ~ Gender, \n       data = Dabrowska.data)\n\nt.test(formula = Postmod ~ Gender, \n       data = Dabrowska.data)\n\nt.test(formula = SubRel ~ Gender, \n       data = Dabrowska.data)\n```\n:::\n\n\n[**Q11.14**]{style=\"color:green;\"} Your friend tells you that, based on the results of the two significant t-tests, he has decided to write a term paper focussing on how men have a better understanding of the active voice and subject relative constructions than women. You warn him that this approach reminds you of a questionable research practice (QRP) that you read about in the [FORRT glossary](https://forrt.org/glossary/english/). Which one?\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<form name=\"form_53211\" onsubmit=\"return validate_form_53211()\" method=\"post\">\n<label>\n<input type=\"radio\" name=\"answer_53211\" id=\"answer_53211_1\" value=\"Creative use of outliers\"/>\nCreative use of outliers\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_53211\" id=\"answer_53211_2\" value=\"Salami slicing\"/>\nSalami slicing\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_53211\" id=\"answer_53211_3\" value=\"Optional stopping\"/>\nOptional stopping\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_53211\" id=\"answer_53211_4\" value=\"HARKing\"/>\nHARKing\n</label>\n<br/>\n<input type=\"submit\" value=\"Check answer\"/>\n<div id=\"result_53211\"></div>\n</form>\n<script>function validate_form_53211() {var x, text; var x = document.forms['form_53211']['answer_53211'].value;if (x == 'HARKing'){text = 'That’s right! This sounds like a typical case of HARKing (Hypothesizing After the Results are Known), which means presenting a hypothesis that is based on or informed by your results in a research report as if it was, in fact, a hypothesis that you came up with before looking at the data.';} else {text = 'No, that’s not the problem, here.';} document.getElementById('result_53211').innerHTML = text; evaluate_final_score(); return false;}function evaluate_final_score(){\n         element = document.getElementById('checkdown_final_score');\n         if(element === null){return false;} else {var element, text, res1, res2, res3, res4, res5, res6, res7, res8, res9, res10, res11, res12, res13, res14;res1 = document.getElementById('result_12592').innerText == 'Correct! Random sampling (also called probability sampling) gives every individual an equal, known chance of being selected.'; res2 = document.getElementById('result_5683').innerText == 'Correct. Even when a convenience sample is “quota‑matched” on observable demographics, it remains a convenience sample because participants first volunteered to take part (e.g., they signed up on a platform like MTurk). This means that unobserved factors, such as motivation, socioeconomic status, and language background, can still render the sample unrepresentative, thus limiting the external validity of inferential data analyses made on the basis of these data.'; res3 = document.getElementById('result_15824').innerText == 'That’s right, well done!'; res4 = document.getElementById('result_95146').innerText == 'Correct!'; res5 = document.getElementById('result_23300').innerText == 'Correct!'; res6 = document.getElementById('result_73587').innerText == 'Correct!'; res7 = document.getElementById('result_4666').innerText == 'That’s right, well done!'; res8 = document.getElementById('result_27514').innerText == 'Correct!'; res9 = document.getElementById('result_66174').innerText == 'That’s right, very well done! The results of this test show the value of statistical significance testing: Here, the difference between the male and female participants is very large, but because we have such a small sample size, the test warns us that this large difference could very well be due to chance and not actually correspond to a real difference in the population.'; res10 = document.getElementById('result_35485').innerText == 'That’s right, well done!'; res11 = document.getElementById('result_22435').innerText == 'That’s right, well done!'; res12 = document.getElementById('result_6341').innerText == 'That’s right, very well done!'; res13 = document.getElementById('result_29482').innerText == 'That’s right, very well done!'; res14 = document.getElementById('result_53211').innerText == 'That’s right! This sounds like a typical case of HARKing (Hypothesizing After the Results are Known), which means presenting a hypothesis that is based on or informed by your results in a research report as if it was, in fact, a hypothesis that you came up with before looking at the data.';text = res1 + res2 + res3 + res4 + res5 + res6 + res7 + res8 + res9 + res10 + res11 + res12 + res13 + res14;element.innerHTML = text;\n         return false;\n         }}</script>\n```\n\n:::\n\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"hint_92832\" onclick=\"return show_hint_92832()\">🐭 Click on the mouse for a hint.</div>\n<div id=\"result_92832\" onclick=\"return show_hint_92832()\"></div>\n<script>function show_hint_92832(){var x = document.getElementById('result_92832').innerHTML; if(!x){document.getElementById('result_92832').innerHTML = 'You will need to go to the FORRT glossary to find out what these terms mean.';} else {document.getElementById('result_92832').innerHTML = '';}}</script>\n```\n\n:::\n:::\n\n\n[**Q11.15**]{style=\"color:green;\"} You explain to your friend that, if he wants to conduct four separate tests on the same dataset, he needs to correct the *p*-values for multiple testing. Which reason(s) can you use to explain this to your friend?\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<form name=\"form_67659\" onsubmit=\"return validate_form_67659()\" method=\"post\">\n<label>\n<input type=\"checkbox\" id=\"answer_67659_1\" value=\"Running multiple tests on the same dataset increases your chances of accidentally getting a false positive result.\"/>\nRunning multiple tests on the same dataset increases your chances of accidentally getting a false positive result.\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_67659_2\" value=\"The sample size gets split up when you conduct multiple tests, which results in less accurate results.\"/>\nThe sample size gets split up when you conduct multiple tests, which results in less accurate results.\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_67659_3\" value=\"Even though you chose an α-level of 0.05 for each test, your overall risk of erroneously rejecting the null hypothesis in at least one of these four tests is actually much higher than 5%.\"/>\nEven though you chose an α-level of 0.05 for each test, your overall risk of erroneously rejecting the null hypothesis in at least one of these four tests is actually much higher than 5%.\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_67659_4\" value=\"Running multiple tests on the same dataset makes each individual test weaker and less reliable.\"/>\nRunning multiple tests on the same dataset makes each individual test weaker and less reliable.\n</label>\n<br/>\n<label>\n<input type=\"checkbox\" id=\"answer_67659_5\" value=\"Multiple tests on the same dataset make it harder to detect real differences when they actually exist.\"/>\nMultiple tests on the same dataset make it harder to detect real differences when they actually exist.\n</label>\n<br/>\n<input type=\"submit\" value=\"Check answer\"/>\n<div id=\"result_67659\"></div>\n</form>\n<script>function validate_form_67659() {var text; var x1 = document.getElementById('answer_67659_1'); var x2 = document.getElementById('answer_67659_2'); var x3 = document.getElementById('answer_67659_3'); var x4 = document.getElementById('answer_67659_4'); var x5 = document.getElementById('answer_67659_5'); if (x1.checked == true&x2.checked == false&x3.checked == true&x4.checked == false&x5.checked == false){text = 'Well done, your friend will be well-informed!';} else {text = 'No, not quite.';} document.getElementById('result_67659').innerHTML = text; evaluate_final_score(); return false;}function evaluate_final_score(){\n         element = document.getElementById('checkdown_final_score');\n         if(element === null){return false;} else {var element, text, res1, res2, res3, res4, res5, res6, res7, res8, res9, res10, res11, res12, res13, res14, res15;res1 = document.getElementById('result_12592').innerText == 'Correct! Random sampling (also called probability sampling) gives every individual an equal, known chance of being selected.'; res2 = document.getElementById('result_5683').innerText == 'Correct. Even when a convenience sample is “quota‑matched” on observable demographics, it remains a convenience sample because participants first volunteered to take part (e.g., they signed up on a platform like MTurk). This means that unobserved factors, such as motivation, socioeconomic status, and language background, can still render the sample unrepresentative, thus limiting the external validity of inferential data analyses made on the basis of these data.'; res3 = document.getElementById('result_15824').innerText == 'That’s right, well done!'; res4 = document.getElementById('result_95146').innerText == 'Correct!'; res5 = document.getElementById('result_23300').innerText == 'Correct!'; res6 = document.getElementById('result_73587').innerText == 'Correct!'; res7 = document.getElementById('result_4666').innerText == 'That’s right, well done!'; res8 = document.getElementById('result_27514').innerText == 'Correct!'; res9 = document.getElementById('result_66174').innerText == 'That’s right, very well done! The results of this test show the value of statistical significance testing: Here, the difference between the male and female participants is very large, but because we have such a small sample size, the test warns us that this large difference could very well be due to chance and not actually correspond to a real difference in the population.'; res10 = document.getElementById('result_35485').innerText == 'That’s right, well done!'; res11 = document.getElementById('result_22435').innerText == 'That’s right, well done!'; res12 = document.getElementById('result_6341').innerText == 'That’s right, very well done!'; res13 = document.getElementById('result_29482').innerText == 'That’s right, very well done!'; res14 = document.getElementById('result_53211').innerText == 'That’s right! This sounds like a typical case of HARKing (Hypothesizing After the Results are Known), which means presenting a hypothesis that is based on or informed by your results in a research report as if it was, in fact, a hypothesis that you came up with before looking at the data.'; res15 = document.getElementById('result_67659').innerText == 'Well done, your friend will be well-informed!';text = res1 + res2 + res3 + res4 + res5 + res6 + res7 + res8 + res9 + res10 + res11 + res12 + res13 + res14 + res15;element.innerHTML = text;\n         return false;\n         }}</script>\n```\n\n:::\n\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"hint_14552\" onclick=\"return show_hint_14552()\">🐭 Click on the mouse for a hint.</div>\n<div id=\"result_14552\" onclick=\"return show_hint_14552()\"></div>\n<script>function show_hint_14552(){var x = document.getElementById('result_14552').innerHTML; if(!x){document.getElementById('result_14552').innerHTML = 'Two of these reasons are correct. The other options correspond to common misconceptions about statistical signifiance testing.';} else {document.getElementById('result_14552').innerHTML = '';}}</script>\n```\n\n:::\n:::\n\n\n[**Q11.16**]{style=\"color:green;\"} How high is your friend's risk of erroneously rejecting the null hypothesis in at least one of his four tests?\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<form name=\"form_51573\" onsubmit=\"return validate_form_51573()\" method=\"post\">\n<label>\n<input type=\"radio\" name=\"answer_51573\" id=\"answer_51573_1\" value=\"5%\"/>\n5%\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_51573\" id=\"answer_51573_2\" value=\"7%\"/>\n7%\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_51573\" id=\"answer_51573_3\" value=\"19%\"/>\n19%\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_51573\" id=\"answer_51573_4\" value=\"20%\"/>\n20%\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_51573\" id=\"answer_51573_5\" value=\"almost 100%\"/>\nalmost 100%\n</label>\n<br/>\n<input type=\"submit\" value=\"Check answer\"/>\n<div id=\"result_51573\"></div>\n</form>\n<script>function validate_form_51573() {var x, text; var x = document.forms['form_51573']['answer_51573'].value;if (x == '19%'){text = 'That’s right and that’s definately more than the 5% he originally intended to take!';} else {text = '❌';} document.getElementById('result_51573').innerHTML = text; evaluate_final_score(); return false;}function evaluate_final_score(){\n         element = document.getElementById('checkdown_final_score');\n         if(element === null){return false;} else {var element, text, res1, res2, res3, res4, res5, res6, res7, res8, res9, res10, res11, res12, res13, res14, res15, res16;res1 = document.getElementById('result_12592').innerText == 'Correct! Random sampling (also called probability sampling) gives every individual an equal, known chance of being selected.'; res2 = document.getElementById('result_5683').innerText == 'Correct. Even when a convenience sample is “quota‑matched” on observable demographics, it remains a convenience sample because participants first volunteered to take part (e.g., they signed up on a platform like MTurk). This means that unobserved factors, such as motivation, socioeconomic status, and language background, can still render the sample unrepresentative, thus limiting the external validity of inferential data analyses made on the basis of these data.'; res3 = document.getElementById('result_15824').innerText == 'That’s right, well done!'; res4 = document.getElementById('result_95146').innerText == 'Correct!'; res5 = document.getElementById('result_23300').innerText == 'Correct!'; res6 = document.getElementById('result_73587').innerText == 'Correct!'; res7 = document.getElementById('result_4666').innerText == 'That’s right, well done!'; res8 = document.getElementById('result_27514').innerText == 'Correct!'; res9 = document.getElementById('result_66174').innerText == 'That’s right, very well done! The results of this test show the value of statistical significance testing: Here, the difference between the male and female participants is very large, but because we have such a small sample size, the test warns us that this large difference could very well be due to chance and not actually correspond to a real difference in the population.'; res10 = document.getElementById('result_35485').innerText == 'That’s right, well done!'; res11 = document.getElementById('result_22435').innerText == 'That’s right, well done!'; res12 = document.getElementById('result_6341').innerText == 'That’s right, very well done!'; res13 = document.getElementById('result_29482').innerText == 'That’s right, very well done!'; res14 = document.getElementById('result_53211').innerText == 'That’s right! This sounds like a typical case of HARKing (Hypothesizing After the Results are Known), which means presenting a hypothesis that is based on or informed by your results in a research report as if it was, in fact, a hypothesis that you came up with before looking at the data.'; res15 = document.getElementById('result_67659').innerText == 'Well done, your friend will be well-informed!'; res16 = document.getElementById('result_51573').innerText == 'That’s right and that’s definately more than the 5% he originally intended to take!';text = res1 + res2 + res3 + res4 + res5 + res6 + res7 + res8 + res9 + res10 + res11 + res12 + res13 + res14 + res15 + res16;element.innerHTML = text;\n         return false;\n         }}</script>\n```\n\n:::\n\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"hint_43986\" onclick=\"return show_hint_43986()\">🐭 Click on the mouse for a hint.</div>\n<div id=\"result_43986\" onclick=\"return show_hint_43986()\"></div>\n<script>function show_hint_43986(){var x = document.getElementById('result_43986').innerHTML; if(!x){document.getElementById('result_43986').innerHTML = 'Section 11.7 showed how to calculate this risk for 20 individual tests. Can you adapt the formula to four tests?';} else {document.getElementById('result_43986').innerHTML = '';}}</script>\n```\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show sample code to answer Q11.16\"}\n1 - (1 - 0.05)^4 \n```\n:::\n\n\n[**Q11.17**]{style=\"color:green;\"} Use Holm's correction to correct the four *p*-values that your friend obtained to account for the fact that he conducted four tests on the same dataset. After correction, which null hypotheses can be rejected at α-level = 0.05?\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<form name=\"form_70134\" onsubmit=\"return validate_form_70134()\" method=\"post\">\n<label>\n<input type=\"radio\" name=\"answer_70134\" id=\"answer_70134_1\" value=\"All four of them.\"/>\nAll four of them.\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_70134\" id=\"answer_70134_2\" value=\"Only the one concerning the passive voice.\"/>\nOnly the one concerning the passive voice.\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_70134\" id=\"answer_70134_3\" value=\"Only the one concerning the active voice.\"/>\nOnly the one concerning the active voice.\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_70134\" id=\"answer_70134_4\" value=\"The one concerning the active voice and the one concerning subject relatives.\"/>\nThe one concerning the active voice and the one concerning subject relatives.\n</label>\n<br/>\n<label>\n<input type=\"radio\" name=\"answer_70134\" id=\"answer_70134_5\" value=\"None of them.\"/>\nNone of them.\n</label>\n<br/>\n<input type=\"submit\" value=\"Check answer\"/>\n<div id=\"result_70134\"></div>\n</form>\n<script>function validate_form_70134() {var x, text; var x = document.forms['form_70134']['answer_70134'].value;if (x == 'Only the one concerning the active voice.'){text = 'That’s right, now only one of the four null hypotheses can be rejected.';} else {text = 'No, that’s incorrect.';} document.getElementById('result_70134').innerHTML = text; evaluate_final_score(); return false;}function evaluate_final_score(){\n         element = document.getElementById('checkdown_final_score');\n         if(element === null){return false;} else {var element, text, res1, res2, res3, res4, res5, res6, res7, res8, res9, res10, res11, res12, res13, res14, res15, res16, res17;res1 = document.getElementById('result_12592').innerText == 'Correct! Random sampling (also called probability sampling) gives every individual an equal, known chance of being selected.'; res2 = document.getElementById('result_5683').innerText == 'Correct. Even when a convenience sample is “quota‑matched” on observable demographics, it remains a convenience sample because participants first volunteered to take part (e.g., they signed up on a platform like MTurk). This means that unobserved factors, such as motivation, socioeconomic status, and language background, can still render the sample unrepresentative, thus limiting the external validity of inferential data analyses made on the basis of these data.'; res3 = document.getElementById('result_15824').innerText == 'That’s right, well done!'; res4 = document.getElementById('result_95146').innerText == 'Correct!'; res5 = document.getElementById('result_23300').innerText == 'Correct!'; res6 = document.getElementById('result_73587').innerText == 'Correct!'; res7 = document.getElementById('result_4666').innerText == 'That’s right, well done!'; res8 = document.getElementById('result_27514').innerText == 'Correct!'; res9 = document.getElementById('result_66174').innerText == 'That’s right, very well done! The results of this test show the value of statistical significance testing: Here, the difference between the male and female participants is very large, but because we have such a small sample size, the test warns us that this large difference could very well be due to chance and not actually correspond to a real difference in the population.'; res10 = document.getElementById('result_35485').innerText == 'That’s right, well done!'; res11 = document.getElementById('result_22435').innerText == 'That’s right, well done!'; res12 = document.getElementById('result_6341').innerText == 'That’s right, very well done!'; res13 = document.getElementById('result_29482').innerText == 'That’s right, very well done!'; res14 = document.getElementById('result_53211').innerText == 'That’s right! This sounds like a typical case of HARKing (Hypothesizing After the Results are Known), which means presenting a hypothesis that is based on or informed by your results in a research report as if it was, in fact, a hypothesis that you came up with before looking at the data.'; res15 = document.getElementById('result_67659').innerText == 'Well done, your friend will be well-informed!'; res16 = document.getElementById('result_51573').innerText == 'That’s right and that’s definately more than the 5% he originally intended to take!'; res17 = document.getElementById('result_70134').innerText == 'That’s right, now only one of the four null hypotheses can be rejected.';text = res1 + res2 + res3 + res4 + res5 + res6 + res7 + res8 + res9 + res10 + res11 + res12 + res13 + res14 + res15 + res16 + res17;element.innerHTML = text;\n         return false;\n         }}</script>\n```\n\n:::\n\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"hint_2882\" onclick=\"return show_hint_2882()\">🐭 Click on the mouse for a hint.</div>\n<div id=\"result_2882\" onclick=\"return show_hint_2882()\"></div>\n<script>function show_hint_2882(){var x = document.getElementById('result_2882').innerHTML; if(!x){document.getElementById('result_2882').innerHTML = 'You need to first extract the p-values from the outputs of the four t-tests and then apply the correction using the p.adjust() function.';} else {document.getElementById('result_2882').innerHTML = '';}}</script>\n```\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show sample code to answer Q11.17\"}\n# Running the four t-tests and saving the output as four R objects to the local environment:\nactive.ttest <- t.test(formula = Active ~ Gender, \n       data = Dabrowska.data)\n\npassive.ttest <- t.test(formula = Passive ~ Gender, \n       data = Dabrowska.data)\n\npostmod.ttest <- t.test(formula = Postmod ~ Gender, \n       data = Dabrowska.data)\n\nsubjrel.ttest <- t.test(formula = SubRel ~ Gender, \n       data = Dabrowska.data)\n\n# Extracting just the p-values from the test outputs\np.values <- c(active.ttest$p.value, passive.ttest$p.value, postmod.ttest$p.value, subjrel.ttest$p.value)\n\n# The original p-values:\np.values\n\n# Applying Holm's correction to the p-values\np_values.adjusted <- p.adjust(p.values, method = \"holm\")\n\n# The adjusted p-values in scientific notation:\np_values.adjusted\n\n# Now only the first p-value is below 0.05:\np_values.adjusted < 0.05\n```\n:::\n\n\n \n:::\n\n::: callout-note\n# In search of the truth...\n\nInferential statistics, even if used correctly, when all test assumptions are met and *p*-values corrected for multiple comparisons, tell us nothing about the validity or reliability of our results. Fancy statistics cannot fix inaccurate measurements or inconsistent annotations! In her study, Ewa Dąbrowska largely relied on tests that had been tested for **validity** and **reliability** in previous studies, which is why we trust that these test instruments mostly *do* measure what they claim to measure (which makes them valid) and that the results that they produce are consistent across participants (which makes them reliable). However, if this is not the case, we need to be extremely careful about the claims that we are making based on such data!\n\nRecall how we observed that, for both L1 and L2 participants, there was a positive correlation between the number years they were in formal education and their English grammar comprehension test scores: the longer they were in formal education, the higher their test scores. Even if we show that, post-adjustment for multiple testing, this correlation is statistically significant at a conventional level of significance, this result could well be an artefact of our measuring instrument: perhaps the participants who spent less time in formal education simply have less practice completing academic-style tests. Maybe, as a result, they did not fully understand the instructions or were unable to complete the test within the given time. Not being good at completing this test, however, may not be reflective of how well they can actually understand complex grammatical structures in English because understanding language typically doesn't happen in artificial test contexts!\n:::\n\n## What's next?\n\nThis chapter has given you the keys to understanding the basics of inferential statistics following the frequentist null hypothesis significance testing (NHST) framework. We looked at how we can use *t*-tests and correlation tests to infer information about a population based on a random sample from the population. We introduced *p*-values, standardised effect sizes, and confidence intervals. These are complex concepts that take time to understand. Frequentist inferential statistics can be very powerful and useful, but it isn't intuitive. Most people will need to read this chapter and other resources (see recommended readings) several times to really get to grips with these concepts.\n\nIn the following two chapters, we will move from single statistical tests to multiple linear regression models. In @sec-SLR, we will first see how *t*-tests and correlation tests can be understood and computed as simple linear regression models before exploring the potential of multiple linear regression models in @sec-MLR. The latter allow us to quantify and test the effects of several variables in a single model. Multiple linear regression naturally handles multiple predictors simultaneously, thus providing more nuanced insights into the contributions of each variable and their potential interactions.\n\n::: callout-tip\n### Recommended readings 📚 {.unnumbered}\n\n-   Çetinkaya-Rundel, Mine & Johanna Hardin. 2021. **Foundations of inference**. In *Introduction to Modern Statistics 2^e^*. Open Access online book: <https://openintro-ims.netlify.app/foundations-of-inference>\n\n-   Jané, Matthew B., Qinyu Xiao, Siu Kit Yeung, Flavio Azevedo, Mattan S. Ben-Shachar, Aaron R Caldwell, Denis Cousineau, et al. 2024. **Guide to effect sizes and confidence intervals**. An Open Educational Resource: <https://doi.org/10.17605/OSF.IO/D8C4G>.\n\n-   Lakens, Daniël. 2022. **Improving Your Statistical Inferences**. <https://doi.org/10.5281/ZENODO.6409077>. An Open Educational Resource: <https://lakens.github.io/statistical_inferences/>\n:::\n\n### Check your progress 🌟 {.unnumbered}\n\nWell done! You have successfully completed this chapter introducing the complex topic of inferential statistics. You have answered [`<span id=\"checkdown_final_score\">0</span>`{=html} out of 17 questions]{style=\"color:green;\"} correctly.\n\nAre you confident that you can...?\n\n-   [ ] Differentiate between different methods of sampling (@sec-Sampling)\n-   [ ] Formulate null hypotheses and alternative hypotheses (@sec-NHST)\n-   [ ] Test null hypotheses using *t*-tests in `R` (@sec-ttest)\n-   [ ] Explain what statistical significance and *p*-values mean (@sec-PValues)\n-   [ ] Calculate Cohen's *d* in `R` (@sec-EffectSize)\n-   [ ] Calculate, test, and interpret correlations in `R` (@sec-Correlations)\n-   Check that the main underlying assumptions of the most common statistical tests are met (@sec-Assumptions)\n-   [ ] Correct *p*-values for multiple comparisons and explain why this is important (@sec-pHacking)\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}