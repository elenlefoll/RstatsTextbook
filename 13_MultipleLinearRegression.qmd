---
bibliography: references.bib
code-annotations: hover
---

# Multiple linear reg`R`ession modelling {#sec-MLR}

::: callout-warning
## Warning

As with the rest of this textbook (see [Preface](https://elenlefoll.github.io/RstatsTextbook/)), this chapter is very much **work in progress**. Feedback is very welcome.
:::

```{r}
#| include: false
library(checkdown)
library(performance)
library(qqplotr)
library(visreg)
library(tidyverse)
select <- dplyr::select
```

### Chapter overview {.unnumbered}

In this chapter, you will learn how to:

-   Fit a linear regression model with multiple predictors
-   Center numeric predictor variables when meaningful
-   Interpret the coefficient estimates of a multiple linear regression model
-   Interpret a model's coefficients of determination (multiple R^2^ and adjusted R^2^)
-   Compute and plot the relative importance of predictors
-   Fit interaction effects and interpret their coefficient estimates
-   Visualise the predictions of a multiple linear regression model and its (partial) residuals
-   Report the results of multiple linear regression model in tabular and graphical formats
-   Check that the assumptions of multiple linear regression models are met.

## From simple to multiple linear regression models

In @sec-SLR, we used the `lm()` function to fit simple linear regression models. We saw that, like the `t.test()` and the `cor.test()` functions, the `lm()` function takes a formula as its first argument. Schematically, the formula syntax for a simple linear regression model takes the form of:

```         
outcome.variable ~ predictor
```

In the second half of this chapter, we will continue to try to predict (i.e., try to better understand) `Vocab` scores among L1 and L2 English speakers, but this time, we will do so with *multiple* predictors. To this end, we will use the `+` operator to add predictors to our model formula like this:

```         
outcome.variable ~ predictor1 + predictor2 + predictor3
```

A linear regression model can include as many predictors as we like -- or rather, as is meaningful and we have data for! The predictors can be a mixture of numeric and categorical predictors. Multiple linear regression modelling is much more powerful than conducting individual statistical tests (as in @sec-Inferential) or several simple linear regression models (as in @sec-SLR) because it enables us to quantify the strength of the association between an outcome variable and a predictor, while **controlling for the other predictors** -- in other words, while holding all the other predictors constant. Moreover, it also reduces the risk of reporting false positive results (i.e. Type 1 error, see @sec-pHacking). In this chapter, we will see that we can learn much more about our data from a single multiple linear regression model than from a series of single statistical tests or simple regression models.

::: {.callout-warning collapse="false"}
### Prerequisites

As with previous chapters, all the examples, tasks, and quiz questions from this chapter are based on data from:

> DƒÖbrowska, Ewa. 2019. Experience, Aptitude, and Individual Differences in Linguistic Attainment: A Comparison of Native and Nonnative Speakers. Language Learning 69(S1). 72‚Äì100. <https://doi.org/10.1111/lang.12323>.

Our starting point for this chapter is the wrangled combined dataset that we created and saved in @sec-DataWrangling. Follow the instructions in @sec-filter to create this `R` object.

Alternatively, you can download `Dabrowska2019.zip` from [the textbook's GitHub repository](https://github.com/elenlefoll/RstatsTextbook/blob/main/Dabrowska2019.zip){.uri}. To launch the project correctly, unzip the file and then double-click on the `Dabrowska2019.Rproj` file.

To begin, load the `combined_L1_L2_data.rds` file that we created in @sec-DataWrangling. This file contains the full data of all the L1 and L2 participants from @DabrowskaExperienceAptitudeIndividual2019. The categorical variables are stored as factors, and obvious data entry inconsistencies and typos have been corrected.

```{r}
library(here)

Dabrowska.data <- readRDS(file = here("data", "processed", "combined_L1_L2_data.rds"))
```

Before you get started, check that you have correctly imported the data by examining the output of `View(Dabrowska.data)` and `str(Dabrowska.data)`. In addition, run the following lines of code to load the {tidyverse} and create "clean" versions of both the L1 and L2 datasets as separate `R` objects.

```{r}
library(tidyverse)

L1.data <- Dabrowska.data |> 
  filter(Group == "L1")

L2.data <- Dabrowska.data |> 
  filter(Group == "L2")
```

Once you are satisfied that the data has been correctly imported and that you are familiar with the dataset, you are ready to tap into the potential of multiple linear regression modelling! üöÄ
:::

## Combining multiple predictors {#sec-MultipleLM}

In the following, we will attempt to model the variability in the receptive English vocabulary test scores of L1 and L2 English participants from @DabrowskaExperienceAptitudeIndividual2019. To this end, we will use multiple numeric and categorical predictors:

a.  participants' native-speaker status (`Group`)
b.  their `Age`,
c.  their occupational group (`OccupGroup`),
d.  their `Gender`
e.  their non-verbal IQ test score (`Blocks`), and
f.  the number of years they were in formal education (`EduTotal`).

We use the `+` operator to construct the model formula. It doesn't matter in which order we list the predictors, as the model will consider them all simultaneously.

```{r}
model4 <- lm(Vocab ~ Group + Age + OccupGroup + Gender + Blocks + EduTotal, 
             data = Dabrowska.data)
```

We can then examine the model summary just like we did in @sec-SLR using the `summary()` function:

```{r}
summary(model4)
```

As with the simple linear regression models, we begin our interpretation of the model summary with the coefficient estimate for the **intercept** (see @sec-Ttestsregression). The first question we ask ourselves is:

-   In this model, what does the intercept correspond to?

Remember that the reference levels of **categorical predictors** correspond to the **first level** of these variables. This is why, here, the coefficient estimate for the intercept corresponds to a female English native speaker with a clerical occupation:

```{r}
levels(Dabrowska.data$Gender) # 'Female' is the first level.
levels(Dabrowska.data$Group) # 'L1' is the first level.
levels(Dabrowska.data$OccupGroup) # 'C' corresponding to a clerical professional occupation is the first level.
```

The reference level of the **numeric predictors**, by contrast, corresponds to the value of¬†**zero**. In `model4`, therefore, the estimated coefficient for the intercept corresponds to the predicted `Vocab` score of an adult English native speaker who belongs to the occupational group "C", is female, who is aged 0 (!), scored 0 on the Blocks test, and spent 0 years (!) in formal education. Needless to say that trying to interpret this value is utterly meaningless! This is why, in this case, it makes sense to **center** our numeric predictor variables before entering them into our model.

## Centering numeric predictors

Centering involves subtracting a variable's average from each value in the variable. Typically, we subtract the mean from each value but, given that we know that many of the numeric variables in our dataset are not normally distributed (see @sec-DistributionsNumeric), here, we will subtract the median instead.

To this end, we use the `mutate()` function to add three columns to the `R` data object `Dabrowska.data`. These new columns contain **transformed** versions of the variable predictors that we previously entered into our model:

```{r}
Dabrowska.data <- Dabrowska.data |> 
  mutate(Age_c = Age - median(Age),
         Blocks_c = Blocks - median(Blocks),
         EduTotal_c = EduTotal - median(EduTotal))
```

We have centred the values of the three numeric predictor variables so that a value of zero in the transformed version corresponds to the variable's original median value (see @tbl-Untransformed).

```{r}
#| echo: false
#| label: "tbl-Untransformed"
#| tbl-cap: "Comparison of the original, untransformed variables with the new, centered ones in a random sample of observations from the data"

library(kableExtra)

highlight_cell <- function(x, target, colour = "pink") {
  ifelse(x == target,
         cell_spec(x, background = colour, bold = TRUE),
         as.character(x))
}

Dabrowska.data |>  
  select(Age, Age_c, Blocks, Blocks_c, EduTotal, EduTotal_c) |>  
  slice(c(1, 12, 24, 7, 45, 64, 79, 133, 157, 145)) |> 
  mutate(Age = highlight_cell(Age, median(Dabrowska.data$Age)),
         Age_c = highlight_cell(Age_c, 0),
         Blocks = highlight_cell(Blocks,  median(Dabrowska.data$Blocks)),
         Blocks_c = highlight_cell(Blocks_c, 0), 
         EduTotal   = highlight_cell(EduTotal, median(Dabrowska.data$EduTotal)),
         EduTotal_c = highlight_cell(EduTotal_c, 0)) |> 
  kable("html", escape = FALSE, align = "c") |> 
  column_spec(1, background = "#F8F8F8") |> 
  column_spec(2, background = "#F8F8F8") |> 
  column_spec(5, background = "#F8F8F8") |> 
  column_spec(6, background = "#F8F8F8") |>
  column_spec(1:6, width = "16%") 
```

For each pair of variables, the value of¬†0 in the centered variable corresponds to the median value of the untransformed variable. As a result, in the centered variables, each data point is expressed in terms of how much it is either above the median (positive score) or below the median (negative score).

## Interpreting a model summary

We can now fit a new multiple linear regression model that attempts to predict `Vocab` scores with the same predictors as `model4` above, except that we are now entering the centered numeric predictor variables instead of the original untransformed ones. The categorical predictor variables remain the same.

```{r}
model4_c <- lm(Vocab ~ Group + Age_c + OccupGroup + Gender + Blocks_c + EduTotal_c, 
             data = Dabrowska.data)

summary(model4_c)
```

If you compare the summary of `model4` with that of `model4_c`, you will notice that, whilst the intercept coefficient estimate has changed, all the other coefficient estimates have stayed the same.

Let's decipher the summary of `model4_c` step-by-step:

-   In this model, the estimated coefficient for the **intercept** corresponds to the predicted `Vocab` score of an English native speaker (`Group`¬†=¬†L1) who is aged `r median(Dabrowska.data$Age)` (the median `Age`), belongs to the occupational group `C`, is female, scored `r median(Dabrowska.data$Blocks)` on the Blocks test (the median `Blocks` score), and was in formal education for `r median(Dabrowska.data$EduTotal)` years (the median number of years).

-   As with the simple linear regression models that we computed in @sec-SLR, the **coefficient estimates of the numeric predictors** (`Age_c`, `Blocks_c`, and `EduTotal_c`) correspond to increases or decreases in `Vocab` scores for each additional unit of that predictor variable, whilst keeping all other predictors at their reference level. Remember that, for numeric predictors, the reference level is 0, which, given that we have centered them, corresponds to the variable's median value. Looking at the coefficient estimate for `Blocks_c` (`1.3011`), this means that someone who scored one point more than the median score in the Blocks test is predicted to have a `Vocab` test result that is `1.3011` points higher than the intercept, namely:

    ```{r}
    70.5852 + 1.3011
    ```

-   To calculate the predicted `Vocab` score of an L1 female speaker in a clerical occupation, of median age, who spent the median number of years in formal education, but scored an impressive 26 points on the Blocks test, we multiply the estimated `Blocks_c` coefficient (`1.3011`) by 26 minus the median Blocks score (`r median(Dabrowska.data$Blocks)`) and add this to the intercept coefficient:

    ```{r}
    70.5852 + 1.3011*(26 - median(Dabrowska.data$Blocks))
    ```

-   You'll be pleased to read that the interpretation of **coefficient estimates of categorical predictors** is less involved. Recall that, for categorical variables, the reference level is always the variable's first level (see @sec-MultipleLM). If we want to make a prediction for an L2 instead of an L1 female speaker, but keep all other predictors at the reference level (i.e. clerical occupation, median age, median number of years in formal education, and median Blocks score), all we need to do is add the coefficient estimate for `GroupL2` (`-22.2896`) to the intercept coefficient. Given that this coefficient is negative, this addition will result in a predicted `Vocab` score that is lower than the model's reference level:

    ```{r}
    70.5852 + -22.2896
    ```

-   Since very few people are perfectly average, let us now calculate the predicted `Vocab` score of an actual person: the 154^th^ participant in our dataset. As shown below using the {tidyverse} function `slice()`, the 154^th^ participant is a 46-year-old Polish male driver who scored 23 points on the Blocks test and attended formal education for 10 years.

    ```{r}
    Dabrowska.data |> 
      slice(154) |> 
      select(Age, Gender, Occupation, OccupGroup, Blocks, EduTotal)
    ```

    To obtain the model's predicted `Vocab` score for a 46-year-old male L2 speaker with a manual occupation (`M`) who scored 23 points on the `Blocks` test and was in formal education for 10 years (`EduTotal`), we combine the coefficient estimates as follows:

    ```{r}
    70.5852 +                                       # <1>     
    -22.2896 +                                      # <2>
    0.3718 * (46 - median(Dabrowska.data$Age)) +    # <3>
    -3.9993 +                                       # <4>
    -3.7177 +                                       # <5>
    1.3011 * (23 - median(Dabrowska.data$Blocks)) + # <6>
    2.4308 * (10 - median(Dabrowska.data$EduTotal)) # <7>
    ```

    1.  Intercept coefficient
    2.  L2 speaker
    3.  46 years old
    4.  Manual occupation (OccupGroupM)
    5.  Male
    6.  23 points on Blocks test
    7.  10 years in formal education

    ::: {.callout-tip .content-visible when-format="html"}
    You can hover your mouse over the circled numbers to find out how this predicted score was calculated. All the coefficient estimates were copied from the model summary output by `summary(model4_c)`.
    :::

-   We can check that we did the maths correctly by outputting the model's prediction for the 154^th^ observation directly. To this end, we apply the `predict()` function to the model object `model4_c` and extract the model's predicted score for the 154^th^ data point:

    ```{r}
    predict(model4_c)[154]
    ```

    As you can see, we obtain the same predicted `Vocab` score. The minor difference after the decimal point is due to us using coefficient estimates rounded-off to four decimal places (as displayed in the model's summary output) rather than the exact values to which the `predict()` function has access.

-   This is all very well, but how accurate is this model prediction? To find out, we can compare this *predicted* score (that our model predicts for any male 46-year old with a manual occupation, a Blocks score of 23, and 10 years in formal education) to the 46-year-old Polish driver's *actual* `Vocab` score:

    ```{r}
    Dabrowska.data |> 
      slice(154) |> 
      select(NativeLg, Age, Occupation, Gender, Blocks, EduTotal, Vocab)
    ```

-   Our model prediction is close to our Polish driver's real `Vocab` test score, but we can see that our model has slightly underestimated his performance. This underestimation results in a positive **residual**. The residual is positive because there are points "left over" by the model. Model residuals are calculated by subtracting a model's prediction from the real, observed value of the outcome variable for any specific data point. In our case, we substract our model's predicted `Vocab` score for a male 46-year old with a manual occupation, a Blocks score of 23, and 10 years in formal education from the Polish 46-year-old driver's actual `Vocab` score:

    ```{r}
    48.88889 - 45.53989
    ```

    Residuals are also stored in the model object and be accesssed like this:

    ```{r}
    model4_c$residuals[154]
    ```

-   If we compare this particular residual (of the 154^th^ participant in the dataset) with the **summary statistics of all residuals** (corresponding to all participants) which is displayed at the top of the output of `summary(model4_c)`, we can see that this particular residual is slightly higher than the average (median) residual. This means that, on average, our model does a slightly better job of predicting `Vocab` scores than it does for this particular combination of predictors.

    ```         
    Residuals:
        Min      1Q  Median      3Q     Max 
    -62.825 -10.838   1.831  12.185  38.345 
    ```

    Looking at the minimum and maximum residuals, we can see that our model overestimates at least one person's `Vocab` score by 63 points (this is the most negative residual: `Min`), whilst in another case it underestimates it by 38 points (this is the largest positive residual: `Max`).

    ```{r}
    #| code-fold: true
    #| code-summary: "Click here to find out whose score was most underestimated!"
    #| eval: false

    # Run the following code to find out more about the participant whose Vocab score was most dramatically underestimated by our model:

    Dabrowska.data |> 
      slice(which.max(model4_c$residuals)) |> 
      select(NativeLg, Age, Occupation, Gender, Blocks, EduTotal, Vocab)
    ```

-   The ***p*****-values** associated with each coefficient estimate in the model summary indicate which predictor variables (or predictor variable *levels*, in the case of categorical variables) make a statistically significant contribution to the model. You may recall that, in @sec-Categoricalpredictor, we fitted a simple linear (`model3`) which included only `OccupGroup` as a single predictor. In this simple model, the coefficient estimate for `OccupGroupI` made a statistically significant contribution to the model at an Œ±-level of 0.05 (*p*¬†=¬†0.0335). By contrast, in the present multiple linear regression model, the predictor level `OccupGroupI` does *not* make a statistically significant contribution (*p*¬†=¬†0.077600 which is higher than 0.05). This is because our multiple regression model `model4_c` includes more predictors that can better account for the variance in `Vocab` scores in the data than occupational groups can.

-   From the **adjusted R-squared** value (`0.3276`) displayed at the bottom of the model summary output, we can see that our multiple linear regression model accounts for around 33% of the total variance in `Vocab` scores found in the @DabrowskaExperienceAptitudeIndividual2019 data. This is considerably more than we achieved with any of the simple linear models that we fitted in @sec-SLR.

### Interpreting model predictions {#sec-ModelPredictions}

Whilst it is important to understand how to interpret the coefficients of a multiple linear regression model from the model summary, in practice, it is also always a very good idea to visualise model predictions. On the one hand, this reduces the risk of making any obvious interpretation errors and, on the other, it is much easier to interpret the residuals of a model when they are visualised alongside model predictions.

To fully visualise the predictions of a multiple linear model, we would need to be able to plot as many dimensions as there are predictors in the model. The trouble is that, as humans, we find it difficult to interpret data visualisations with more than two dimensions. Indeed, although 3D plots can sometimes be useful (and can easily be generated in `R`), we are much better at interpreting two-dimensional plots. To bypass this inherent human weakness, we will plot the values predicted by our model on several plots: one for each predictor variable (see @fig-VisregVocab). Run the following command and then follow the instructions displayed in the Console pane to view the plots one by one. You may need to resize your Plot pane for the plots to be displayed. If you have a small screen, you may find the "üîé Zoom" option in RStudio's Plots pane useful as it allows you to view the plots in a separate window.

```{r}
#| eval: false
library(visreg)

visreg(model4_c)
```

```{r}
#| echo: false
#| fig-width: 8
#| label: "fig-VisregVocab"
#| fig-cap: "`Vocab` scores as predicted by `model4_c` (blue lines) as a function of various predictor variables and partial model residuals (grey points)"

model4Group <- visreg(model4_c, "Group", whitespace = 0.3, gg = TRUE) + theme_bw()
model4Age <- visreg(model4_c, "Age_c", gg = TRUE) + theme_bw()
model4OccupGroup <- visreg(model4_c, "OccupGroup", whitespace = 0.4, gg = TRUE) + theme_bw()
model4Gender <- visreg(model4_c, "Gender", whitespace = 0.3, gg = TRUE) + theme_bw()
model4Blocks <- visreg(model4_c, "Blocks_c", gg = TRUE) + theme_bw()
model4EduTotal <- visreg(model4_c, "EduTotal_c", gg = TRUE) + theme_bw()

library(patchwork)
model4Group + model4Age + model4OccupGroup + model4Gender + model4Blocks + model4EduTotal
```

On plots produced by the `visreg()` function, as in @fig-PredictedVocabByOccupation, the model's predicted values are displayed as blue lines. By default, these predictions are surrounded by 95% confidence bands, which are visualised as grey areas. Now that we have several predictors in our model, the points in `visreg()` plots represent the model's **partial residuals**. Partial residuals are the left-over variance (i.e., the residual) relative to the predictor that we are examining after having subtracted off the contribution of all the other predictors in the model.

When interpreting the numeric predictors plotted in @fig-VisregVocab above, it is important to remember that we entered centered numeric predictors in `model4_c`. This means that an `Age_c` value of¬†0 corresponds to the median age in the dataset: 31 years. This is why @fig-VisregVocab features negative ages: the negative values correspond to participants who are younger than 31. By contrast, positive scores correspond to participants who are older than 31 years. This is hardly intuitive so, for the purposes of visualising and interpreting our model, it is best to transform these variables back to their original scale (see @fig-PredictedVocabAge). This is achieved by adding the following `xtrans` argument within the `visreg()` function:

```{r}
#| eval: false
visreg(model4_c, 
       xvar = "Age_c", 
       xtrans = function(x) x + median(Dabrowska.data$Age), 
       gg = TRUE) + 
  labs(x = "Age (in years)",
       y = "Predicted Vocab scores") +
  theme_bw()
```

```{r}
#| echo: false
#| fig-asp: 0.8
#| label: "fig-PredictedVocabAge"
#| fig-cap: "`Vocab` scores as predicted by `model4_c` for speakers of various ages (blue line) and partial model residuals represented as points with two data points highlighted: one representing a very large positive residual and one representing a very small residual"

vr <- visreg(model4_c, 
       xvar = "Age_c", 
       xtrans = function(x) x + median(Dabrowska.data$Age), 
       points=list(size = 2),
       gg = TRUE)

PolishDriver <- data.frame(x = Dabrowska.data$Age_c[154] + median(Dabrowska.data$Age), 
                           y = vr$data$y[154])

LithuanianBarStaff <- data.frame(x = Dabrowska.data$Age_c[141] + median(Dabrowska.data$Age), 
                           y = vr$data$y[141])
vr +
  geom_point(
    data = PolishDriver,
    aes(x = x, y = y),
    colour = "#8338ec",
    size = 2) +  
  geom_point(
    data = LithuanianBarStaff,
    aes(x = x, y = y),
    colour = "#ff006e",
    size = 2) +    
  
## Curved arrow pointing to the Polish driver point
geom_curve(
  data = PolishDriver,
  aes(x = x,          # start of the arrow (the point itself)
      y = y + 2,
      xend = x + 2,   # tip of the arrow ‚Äì move a little to the right
      yend = y + 8),  # and a little up
  curvature = -0.2,
  arrow = arrow(length = unit(0.02, "npc"),
                ends = "first"),
  colour = "#8338ec"
) +

## Label for the Polish driver point
geom_text(
  data = PolishDriver,
  aes(x = x + 2, y = y + 11, label = "Polish driver"),
  colour = "#8338ec",
  hjust = 0.5,          
  vjust = 0.5,
  size = 3.5
) +
  
## Purple line down to regression line from Polish driver point
geom_segment(
  data = PolishDriver,
  aes(x = x, 
      y = y, 
      xend = x, 
      yend = 75),
  colour = "#8338ec",
  linetype = "dotted",
  linewidth = 0.5
) +  

## Curved arrow pointing to the Lithuanian bar‚Äëstaff point
geom_curve(
  data = LithuanianBarStaff,
  aes(x = x,
      y = y + 2,
      xend = x - 1,
      yend = y + 8),
  curvature = 0.2,
  arrow = arrow(length = unit(0.02, "npc"),
                ends = "first"),
  colour = "#ff006e"
) +

## Label for the Lithuanian bar‚Äëstaff point
geom_text(
  data = LithuanianBarStaff,
  aes(x = x - 2, y = y + 11, label = "Lithuanian bar staff"),
  colour = "#ff006e",
  hjust = 0.5,
  vjust = 0.5,
  size = 3.5
) +
  
## Pink line down to regression line from Lithuanian bar‚Äëstaff point
geom_segment(
  data = LithuanianBarStaff,
  aes(x = x, 
      y = y, 
      xend = x, 
      yend = 66),
  colour = "#ff006e",
  linetype = "dotted",
  linewidth = 0.5
) +
  
  labs(x = "Age (in years)",
       y = "Predicted Vocab scores") +
  theme_bw()
```

In @fig-PredictedVocabAge, we focus on one predictor from our model: `Age`. We know from our model summary that the coefficient estimate for age is positive (`0.3718`), hence the upward trend. But we also know that this coefficient is relatively small, which is why the blue line is not very steep. Recall that the points on @fig-PredictedVocabAge represent the partial residuals. Hence, there are as many points as there are participants in the dataset used to fit the model. The further away a point is from the predicted value (the blue line), the poorer the prediction is for that participant.

We can see that many of the points on @fig-PredictedVocabAge are not within the grey 95% confidence band, which means that our model cannot account for all of the variance in `Vocab` scores. Some of the partial residuals are particularly large: the `Vocab` score of the 23-year-old Lithuanian bar staff, for instance, is vastly underestimated. By contrast, the 46-year-old Polish driver's predicted score is well within the confidence band of the predicted `Vocab` score for his age, indicating that our model makes a pretty good prediction for this L2 speaker.

```{r}
#| echo: false
#| eval: false
i <- 154 # row index in the original data

# x‚Äëcoordinate: the centerd value + the median that you added in xtrans
x_i <- Dabrowska.data$EduTotal_c[i] + median(Dabrowska.data$EduTotal)

vr <- visreg(model4_c, 
       xvar = "EduTotal_c", 
       xtrans = function(x) x + median(Dabrowska.data$EduTotal), 
       gg = TRUE)

# y‚Äëcoordinate: the *partial residual* that visreg plotted.
y_i <- vr$data$y[i]

# Put them in a tiny data frame (required by geom_point)
highlight_df <- data.frame(x = x_i, y = y_i)

vr +
  geom_point(
    data = highlight_df,
    aes(x = x, y = y),
    colour = "purple",
    size = 0.8) +  
  labs(x = "Time spent in formal education (in years)",
       y = "Predicted Vocab scores") +
  theme_bw()

```

::: {.callout-tip collapse="false"}
#### Your turn! {.unnumbered}

The dataset from @DabrowskaExperienceAptitudeIndividual2019 includes a variable that we have not explored yet: `ART`. It stands for Author Recognition Test [@acheson2008] and is a measure of print exposure to \[almost exclusively Western, English-language\] literature. DƒÖbrowska [-@DabrowskaExperienceAptitudeIndividual2019:¬†9] explains the testing instrument and her motivation for including it in her battery of tests as follows:

> The test consists of a list of 130 names, half of which are names of real authors. The participants‚Äô task is to mark the names that they know to be those of published authors. To penalize guessing, the score is computed by subtracting the number of foils \[wrong guesses\] from the number of real authors selected. Thus, the maximum possible score is 65, and the minimum score could be negative if a participant selects more foils than real authors. When this happened, the negative number was replaced with 0.
>
> The ART has been shown to be a valid and reliable measure of print exposure, which, unlike questionnaire-based measures, is not contaminated by socially desirable responses and assesses lifetime reading experience as opposed to current reading (see Acheson et al., 2008; Stanovich & Cunningham, 1992).

[**Q13.1**]{style="color:green;"} For this task, you will fit a new multiple linear regression model that predicts `Vocab` scores among L1 and L2 speakers using all of the predictors that we used in `model4_c` plus `ART` scores. Which formula do you need to specify inside the `lm()`function to fit this model?

```{r echo=FALSE}
#| label: "Q13.1"

check_question("Vocab ~ Group + Age_c + OccupGroup + Gender + Blocks_c + EduTotal_c + ART",
               options = c("Vocab ~ Group + Age_c + OccupGroup + Gender + Blocks_c + EduTotal_c + ART",
                           "ART ~ Group + Age_c + OccupGroup + Gender + Blocks_c + EduTotal_c",
                           "Vocab ~ model4_c + ART",
                           "Vocab ~ Group + Age_c + OccupGroup + Gender + Blocks_c + EduTotal_c * ART"),
               random_answer_order = TRUE,
               type = "radio",
               q_id = "Q13.1",
               button_label = "Check answer",
               right = "That's right, well done!",
               wrong = "No, try it out for yourself and see which formula generates the right model.")  
```

[**Q13.2**]{style="color:green;"} Fit a model using the correct formula from above. Examine the summary of your new model. Which of these statements is/are true?

```{r echo=FALSE}
#| label: "Q13.2"

check_question(c("Recognising more real authors on the ART is associated with higher Vocab scores.",
                 "ART makes a statistically significant contribution to the model at the Œ±-level of 0.05.",
                 "The model that includes ART accounts for more of the variance in Vocab scores than the model that does not (model4_c)."),
               options = c("Recognising more real authors on the ART is associated with higher Vocab scores.",
                 "ART makes a statistically significant contribution to the model at the Œ±-level of 0.05.",
                 "The model that includes ART accounts for more of the variance in Vocab scores than the model that does not (model4_c).",
                 "Recognising more real authors on the ART is associated with lower Vocab scores.",
                 "ART does not make a statistically significant contribution to the model at the Œ±-level of 0.05.",
                 "The model that does not include ART (model4_c) accounts for more of the variance in Vocab scores than the model that does."),
               random_answer_order = TRUE,
               type = "check",
               q_id = "Q13.2",
               button_label = "Check answer",
               right = "Excellent!",
               wrong = "No, not quite. Check the code below if you need some help.")
check_hint("Three of these statements are correct.", 
           hint_title = "üê≠ Click on the mouse for a hint.")
```

```{r}
#| code-fold: true
#| code-summary: "Show sample code to answer Q13.2."
#| eval: false

# First, it is always a good idea to visualise the data before fitting a model to have an idea of what to expect:
Dabrowska.data |> 
  ggplot(mapping = aes(y = Vocab,
                       x = ART)) +
  geom_point() +
  geom_smooth(method = "lm") +
  theme_bw()

# We fit the new model with ART as an additional predictor using the formula from Q13.1:
model.ART <- lm(Vocab ~ Group + Age_c + OccupGroup + Gender + Blocks_c + EduTotal_c + ART, 
             data = Dabrowska.data)

# We examine the coefficients of this new model:
summary(model.ART)

# To find out how much variance the model accounts for compared to our prior model, compare the adjusted R-squared values of the two models:
summary(model4_c)
summary(model.ART)
```

Have you noticed how, having added `ART` as a predictor to your model, `Age` no longer makes a statistically significant contribution to the model (*p*¬†=¬†0.070994)? How come? Well, it turns out that `ART` and `Age` are correlated (*r*¬†=¬†0.32, see @fig-AgeArtPlot). This moderate positive correlation makes intuitive sense: the older you are, the more likely you are to have come across more authors and therefore be able to correctly identify them in the Author Recognition Test. Of the two predictors, our model suggests that ART scores are a more useful predictor of `Vocab` scores than age. In other words, if we know the ART score of a participant from the @DabrowskaExperienceAptitudeIndividual2019 dataset, their age does not provide additional information that can help us to better predict their `Vocab` score.

```{r}
#| echo: false
#| fig-asp: 0.8
#| label: "fig-AgeArtPlot"
#| fig-cap: "Observed relationship between participants' age and their Author Recognition Test (ART) scores. The blue line shows that there is a moderate positive correlation between these two variables."

Dabrowska.data |> 
  ggplot(mapping = aes(x = Age,
                       y = ART)) +
  geom_point() +
  geom_smooth(method = "lm") +
  theme_bw()
```

Note that when two predictor variables are strongly correlated, we say that they are collinear. **Collinearity** can cause issues in statistical modelling so it's something to watch out for (see @sec-ModelAssumptions).

[**Q13.3**]{style="color:green;"} What is the predicted `Vocab` score of a female L1 speaker aged 31 (the median age) with a manual occupation, who scored 16 on the Blocks test (the median score), spent 14 years in formal education (median time period), and scored 20 on the Author Recognition Test?

```{r echo=FALSE}
#| label: "Q13.3"

check_question("71.1133",
               options = c("71.1133",
                           "74.1007",
                           "74.3556",
                           "8.5405",
                           "123.3076",
                           "73.6248"),
               random_answer_order = TRUE,
               type = "radio",
               button_label = "Check answer",
               q_id = "Q13.3",
               right = "Absolutely, well done!",
               wrong = "No, that's it. Check the hint below.")
check_hint("The person described corresponds to the reference levels of all the predictors except for their occupational group and ART score.", 
           hint_title = "üê≠ Click on the mouse for a hint.")

```

```{r}
#| code-fold: true
#| code-summary: "Show sample code to answer Q13.3."
#| eval: false

# We must add together the coefficient for the intercept, the coefficient for the manual occupation (which is negative), and the ART coefficient multiplied by 20 (as we did not center this predictor):
62.5728 + -2.5115 + 0.5526*20
```

[**Q13.4**]{style="color:green;"} By default, the `visreg()` function displays a 95% confidence band around predicted values, corresponding to a significance level of 0.05. How can you change this default to display a 99% confidence band instead? Run the command `?visreg()` to find out how to achieve this from the function's help file.

```{r echo=FALSE}
#| label: "Q13.4"

check_question("visreg(model.ART, xvar = \"ART\", alpha = 0.01)",
               options = c("visreg(model.ART, xvar = \"ART\", alpha = 0.01)",
                           "visreg(model.ART, xvar = \"ART\", alpha = 99)",
                           "visreg(model.ART, xvar = \"ART\", alpha = \"99%\")",
                           "visreg(model.ART, xvar = \"ART\", CI = 0.01)",
                           "visreg(model.ART, xvar = \"ART\", trans = CI(0.99))"),
               random_answer_order = TRUE,
               type = "radio",
               q_id = "Q13.4",
               button_label = "Check answer",
               right = "That's right, well done!",
               wrong = "No, that's not it. Have you tried to run the command to see if it works?")
check_hint("99% expressed as a probability is 0.99. The significance level is equal to 1 minus the confidence level expressed as a probability.", 
           hint_title = "üê≠ Click on the mouse for a hint.")

```

```{r}
#| code-fold: true
#| code-summary: "Show sample code to answer Q13.4."
#| eval: false

# The argument that you need to change is called "alpha". This is because the significance level is also called the alpha level (see Section 11.3). 

# The alpha value that corresponds to a 99% confidence interval is equal to:
1 - 0.99

# Hence the code should (minimally) include these arguments:
visreg(model.ART, 
       xvar = "ART", 
       alpha = .01)
```

Observe how the width of the confidence bands changes when you increase the significance level from 0.01 to 0.1: the higher the significance level, the narrower the confidence band becomes. Remember that the significance level corresponds to the risk that we are willing to take of wrongly rejecting the null hypothesis when it is actually true. Therefore, when we lower this risk by choosing a lower significance level (i.e., 0.01), we end up with wider confidence bands that are more likely to allow us to draw a flat line (see @sec-Correlations). If we can draw a flat line through a 99% confidence band, this means that we do not have enough evidence to reject the null hypothesis at the significance level of 0.01.
:::

## Relative importance of predictors

When interpreting a model summary, it is important to remember that the coefficient estimates of each predictor correspond to a change in prediction for a one-unit change in that predictor, e.g. for the predictor `Age`, an increase of one year. However, within a single model, it's quite common for the predictor variables to be measured in different units. For example, in `summary(model4_c)`, the coefficient estimate for `Age_c` is `0.3718`, which means that, holding all other predictors constant, for every year that a participant is older than the median age, their predicted `Vocab` score increases by `0.3718`. By contrast, the coefficient estimate for the `Blocks_c` predictor (`1.3011`) corresponds to an increase in `Vocab` scores for every additional point that participants score on the non-verbal IQ Blocks test. Its unit is therefore test points. For categorical predictor variables, a single-unit change represents a change from one predictor level to another, e.g. from L1 to L2, or from female to male.

Because they are in different units that represent different quantities or levels, the raw coefficient estimates of a model cannot be compared to each other. Indeed, depending on the predictor, a one-unit change may correspond to either a big or a small change. This is where measures of the relative importance of predictors come in handy. In this chapter, we will use the **lmg** metric [that was first proposed by **L**indeman, **M**erenda & **G**old in -@lindemanIntroductionBivariateMultivariate1980: 119 ff.] to compare the importance of predictors within a single multiple linear regression model.

Although not currently widely used in the language sciences [but see, e.g. @DabrowskaExperienceAptitudeIndividual2019: 14], lmg has a number of advantages. Its interpretation is fairly intuitive because it is similar to a coefficient of determination (R^2^): a value of¬†0 means that, in this model, a predictor accounts for 0% of the variance in the outcome variable, while a value of¬†1 would mean that it can be used to perfectly predict the outcome variable. Calculating lmg values is computationally involved because the metric includes both direct effects and is adjusted for all the other predictors of the model. But this need not worry us because a researcher and statistician, Ulrike Gr√∂mping, has developed and published an `R` package that will do the computation for us. üòä

Once we have installed and loaded the {[relaimpo](https://cran.r-universe.dev/relaimpo/doc/manual.html)} package [@gr√∂mping2006], we can use its `calc.relimp()` function to calculate a range of relative importance metrics for linear models. With the argument "type", we specify that we are interested in the lmg metric:

```{r}
#install.packages("relaimpo")
library(relaimpo)

rel.imp.metric <- calc.relimp(model4_c, 
                              type = "lmg")
```

The output of the function is long. We have therefore saved it as a new `R` object (`rel.imp.metric`) in order to retrieve only the part that we are interested in, namely the lmp values for each predictor, as a single data frame (`rel.imp.metric.df`):

```{r}
rel.imp.metric.df <- data.frame(lmg = rel.imp.metric$lmg) |> 
    rownames_to_column(var = "Predictor")
```

We display this data frame in descending order of lmp values, rounded to two decimal values:

```{r}
rel.imp.metric.df |> 
  mutate(lmg = round(lmg, digits = 2)) |> 
  arrange(-lmg)
```

These values indicate that, on average, whether or not a participant is a native or non-native speaker of English (`Group`) makes the biggest difference in terms of predicted `Vocab` scores. Gender, by contrast, is practically irrelevant in this model.

We can also visualise these values in the form of a bar plot (see @fig-ModelLmg). Note that another nice thing about the lmg metric is that the lmg values for all the predictors entered in `model4_c` add up to the model's (unadjusted) multiple R^2^ (`0.3621`).

```{r}
#| label: "fig-ModelLmg"
#| fig-cap: "Relative importance of predictors in `model 4_c`"

rel.imp.metric.df |> 
  ggplot(mapping = aes(x = fct_reorder(Predictor, lmg),
                       y = lmg)) +
  geom_col() +
  theme_minimal() +
  labs(x = "Predictor in model4_c") +
  coord_flip() 
```

Many studies will compare the coefficient estimates of multiple regression models using **standardised coefficients**. To understand what standardised coefficients represent and how to use them yourself, I recommended reading Winter [-@winterStatisticsLinguistsIntroduction2020: Section 6.2].

::: {.callout-tip collapse="false"}
#### Your turn! {.unnumbered}

In the last [Your turn!]{style="color:green;"} section, you fitted a multiple linear regression model to predict `Vocab` scores among L1 and L2 speakers using all of the predictors that we used in `model4_c` plus `ART` scores.

[**Q13.5**]{style="color:green;"} Calculate the relative importance of each of the predictors in this model using the {relaimpo} package. What is the relative importance of the predictor `ART` in this model as estimated by the lmg metric and rounded to two decimal places?

```{r echo=FALSE}
#| label: "Q13.5"

check_question(c("0.11", "0,11"),
               button_label = "Check answer",
               q_id = "Q13.5",
               right = "That's right! Note that entering this predictor in the model changes the relative importance of all the other predictors, too.",
               wrong = "No, that's incorrect.")  
```

```{r}
#| code-fold: true
#| code-summary: "Show sample code to answer Q13.5."
#| eval: false

# Fit the model:
model.ART <- lm(Vocab ~ Group + Age_c + OccupGroup + Gender + Blocks_c + EduTotal_c + ART, 
             data = Dabrowska.data)

# Compute the lmg values and extract them as a data frame
library(relaimpo)
rel.imp.metric <- calc.relimp(model.ART, 
                              type = "lmg")

rel.imp.metric.df <- data.frame(lmg = rel.imp.metric$lmg) |> 
    rownames_to_column(var = "Predictor")

rel.imp.metric.df |> 
  mutate(lmg = round(lmg, digits = 2)) |> 
  arrange(-lmg)
```
:::

## Modelling interactions between predictors

One of the strengths of multiple linear regression is that we can also model interactions between predictors. This is important because a predictor's relationship with the outcome variable may depend on another predictor. Consider `Age` as a predictor of `Vocab` scores. In `model4_c`, we saw that this predictor made a statistically significant contribution to the model. The coefficient was positive, which means that the model predicts that the older the participants, the higher their receptive English vocabulary.

However, if you completed [**Task 11.1**]{style="color:green;"}, you might remember that `Age` correlates positively with `Vocab` scores among L1 participants, but does not among L2 participants (see @fig-CorVocabAge below). If anything, the correlation visualised in the right-hand panel of @fig-CorVocabAge is slightly negative. Moreover, the confidence band is wide enough to draw a straight line through it, which suggests that the data is also compatible with the null hypothesis of no correlation. As a result, we can conclude that this slightly negative correlation is not statistically significant.

```{r}
#| label: "fig-CorVocabAge"
#| fig-cap: "Observed correlation between participants' vocabulary scores and their age in both the L1 and L2 groups"

Dabrowska.data |> 
   ggplot(mapping = aes(x = Age, 
                        y = Vocab)) +
     geom_point() +
     facet_wrap(~ Group) +
     geom_smooth(method = "lm", 
                 se = TRUE) +
  theme_bw()
```

@fig-CorVocabAge informs us that what we really need is a model that can account for the fact that `Age` is probably a useful predictor of `Vocab` scores for L1 speakers, but not so much for L2 speakers. In other words, we'd like to model an **interaction** between the predictors `Age` and `Group`.

In `R`'s formula syntax, interaction terms are denoted with a semicolon (`:`) or an asterisk (`*`). In the following model, therefore, we are attempting to predict `Vocab` scores on the basis of a person's native-speaker status (`Group`), their age, occupational group, gender, Blocks test score, number of years in formal education, and the interaction between their age and native-speaker status (`Age_c:Group`):

```{r}
model5 <- lm(Vocab ~ Group + Age_c + OccupGroup + Gender + Blocks_c + EduTotal_c + Age_c:Group, 
             data = Dabrowska.data)

summary(model5)
```

In the model summary above, the interaction term between `Age` and `Group` is the last coefficient listed (`GroupL2:Age_c`). The values confirm our intuition based on our descriptive visualisation of the data (@fig-CorVocabAge): there is a statistically significant interaction between age and native-speaker status (*p*¬†=¬†0.002089) and the interaction coefficient for this interaction term (`Age_c:Group`) is negative (`-0.8982`). This means that, whilst being older is generally associated with higher `Vocab` scores for the reference level of L1 speakers, if a participant is an L2 English speaker, this trend is reversed.

To understand how this works in practice, let's compare the predicted `Vocab` scores of two 35-year-olds: one a native English speaker and the other a non-native. For the purposes of this illustration, we will assume that, apart from their native-speaker status, all their other characteristics correspond to the reference level in our model (i.e. they are both female, have a clerical occupation, scored average on the Blocks test and were in formal education for the median number of years). For the L1 speaker, we calculate their predicted `Vocab` as before. We take the intercept as our starting point and then add the `Age_c` coefficient multiplied by the difference between their age and the median age (which is the reference level of our centered predictor) (i.e., 35¬†-¬†31¬†=¬†4). This means that their predicted `Vocab` score is:

```{r}
69.1314 + 0.5466*4
```

```{r}
#| eval: false
#| include: false

# Just checking I've not made a mistake with the manual calculation of the predicted value:
new_obsL1 <- data.frame(
  Group      = factor("L1", levels = levels(model5$model$Group)),
  Gender     = factor("F",  levels = levels(model5$model$Gender)),
  Age_c      = 4,
  OccupGroup = factor("C", levels = levels(model5$model$OccupGroup)),
  Blocks_c   = 0,
  EduTotal_c = 0)

predict(model5, newdata = new_obsL1, type = "response")
```

For the L2 speaker, we do the same thing but, this time, we also add the `GroupL2` coefficient, and the interaction coefficient (`GroupL2:Age_c`) multiplied by the difference between the speaker's age and the median age (which remains 4 years). Their predicted score is therefore:

```{r}
69.1314 + 0.5466*4 + -19.9128 + -0.8982*4
```

```{r}
#| eval: false
#| include: false

# Just checking I've not made a mistake with the manual calculation of the predicted value:
new_obsL2 <- data.frame(
  Group      = factor("L2", levels = levels(model5$model$Group)),
  Gender     = factor("F",  levels = levels(model5$model$Gender)),
  Age_c      = 4,
  OccupGroup = factor("C", levels = levels(model5$model$OccupGroup)),
  Blocks_c   = 0,
  EduTotal_c = 0)

predict(model5, newdata = new_obsL2, type = "response")
```

::: callout-caution
#### Important {.unnumbered}

Whenever we have a model with an interaction, we can no longer interpret the individual coefficient estimates of the predictors that enter into the interaction by themselves: instead, we must interpret our model's main effects together with their interaction!

For example, in `model5`, if we only took account of the model's main effect for age, we would be misled into thinking that age is *always* positively associated with `Vocab` scores. However, as illustrated in @fig-CorVocabAge, we know that this is not true for L2 speakers ‚Äî hence the statistically significant interaction between `Age_c` and `Group` in this model in `model5`.
:::

The best way to avoid misinterpreting interaction effects is to make sure you always visualise the predictions of models that involve interactions. The `visreg()` function includes a "by" argument, which is ideal for this:

```{r}
#| fig-asp: 0.8
#| label: "fig-PredictedVocabAgeInteraction"
#| source-line-numbers: "3"
#| fig-cap: "`Vocab` scores as predicted by `model4_c` for L1 and L2 speakers of various ages (blue lines) and partial model residuals (grey points)"

visreg(model5, 
       xvar = "Age_c", 
       by = "Group",
       xtrans = function(x) x + median(Dabrowska.data$Age), 
       gg = TRUE) + 
  labs(x = "Age (in years)",
       y = "Predicted Vocab scores") +
  theme_bw()
```

If we compare @fig-PredictedVocabAgeInteraction (in which the points represent the model's partial residuals) to @fig-CorVocabAge (in which the points represent the actual, observed values), we can see that the partial residuals are, on average, smaller than the differences between the observed values and the regression lines of @fig-CorVocabAge. This is because the regression lines of @fig-CorVocabAge correspond to a model that only includes three coefficients, `Age_c`, `Group` and their interaction (`Age_c:Group`), whereas the partial residuals in @fig-PredictedVocabAgeInteraction correspond to the left-over variance in `Vocab` scores once all other predictors of the `model5` have been taken into account.

Comparing the model summaries of `model4_c` and `model5`, we can see that adding the interaction term between age and native-speaker status considerably boosted the amount of variance in `Vocab` scores that our model now accounts for: whereas the adjusted R^2^ of the model without the interaction was `0.3276` (33%), it has now reached `0.3654` (37%) thanks to the added interaction.

::: {.callout-tip collapse="false"}
#### Your turn! {.unnumbered}

In this task, we will explore the possibility of an interaction between participants' native-speaker status (`Group`) and the number of years they spent in formal education (`EduTotal`). The underlying idea is that, for the L1 speakers, their formal education will most likely have been in English; hence the longer they were in formal education, the higher they might score on the `Vocab` test. For the L2 speakers, however, their formal education will more likely have been in a language other than English. As a result, years in formal education may be a weaker predictor of `Vocab` scores than for L1 speakers.

[**Q13.6**]{style="color:green;"} To begin, generate a plot to visualise this interaction in the observed data: map the `EduTotal` variable onto the *x*-axis and `Vocab` onto the *y*-axis, and split the plot into two facets, one for L1 and the other for L2 participants. Based on your interpretation of your plot, which of these statements is/are correct?

```{r echo=FALSE}
#| label: "Q13.6"

check_question(c("For both L1 and L2 participants, the number of years that they spent in formal education is positively correlated with their Vocab scores."),
               options = c("For both L1 and L2 participants, the number of years that they spent in formal education is positively correlated with their Vocab scores.",
                           "The number of years that L1 participants spent in formal education is positively correlated with their Vocab scores. This is not the case for L2 participants.",
                           "For both L1 and L2 participants, the number of years that they spent in formal education is negatively correlated with their Vocab scores.",
                           "For both L1 and L2 participants, the correlation between the number of years that they spent in formal education and their Vocab scores is not statistically significant."),
               random_answer_order = TRUE,
               type = "check",
               q_id = "Q13.6",
               button_label = "Check answer",
               right = "That's right, well done!",
               wrong = "No, that's incorrect. Check the code below if you're struggling to generate the plot.")
check_hint("Only one of these statements are correct.", 
           hint_title = "üê≠ Click on the mouse for a hint.")           
```

```{r}
#| code-fold: true
#| code-summary: "Show code to generate the plot needed to complete Q13.6"
#| eval: false

Dabrowska.data |> 
   ggplot(mapping = aes(x = EduTotal, 
                        y = Vocab)) +
     geom_point() +
     facet_wrap(~ Group) +
     geom_smooth(method = "lm", 
                 se = TRUE)
```

[**Q13.7**]{style="color:green;"} Below is the model formula for `model5`:

```         
Vocab ~ Group + Age_c + OccupGroup + Gender + Blocks_c + EduTotal_c + Age_c:Group
```

What do you need to add to this formula to also model the interaction that you just visualised?

```{r echo=FALSE}
#| label: "Q13.7"

check_question("+ EduTotal_c:Group",
                 options = c("+ Vocab ~ EduTotal_c:Group",
                           "+ EduTotal_c:Group",
                           "+ EduTotal:Group",
                           "+ EduTotal_c:Group:Vocab"),
               random_answer_order = TRUE,
               type = "radio",
               q_id = "Q13.7",
               button_label = "Check answer",
               right = "That's right. Now fit the model and check the model summary to see if this interaction makes a significant contribution to the model!",
               wrong = "No, that's not it.")
```

[**Q13.8**]{style="color:green;"} Fit the model with the added interaction. Does the interaction make a statistically significant contribution to the model at the significance level of 0.05?

```{r echo=FALSE}
#| label: "Q13.8"

check_question("No, it does not.",
               options = c("No, it does not.",
                           "Yes, it does."),
               button_label = "Check answer",
               type = "radio",
               q_id = "Q13.8",
               right = "That's right. The *p*-value associated with the interaction coefficient is greater than 0.05.",
               wrong = "No. The *p*-value associated with the interaction coefficient is greater than 0.05.")
```

```{r}
#| code-fold: true
#| code-summary: "Show code to complete Q13.8"
#| eval: false

model5.int <- lm(Vocab ~ Group + Age_c + OccupGroup + Gender + Blocks_c + EduTotal_c + Age_c:Group + EduTotal_c:Group,
                 data = Dabrowska.data)

summary(model5.int)
```

[**Q13.9**]{style="color:green;"} Use the {visreg} package to visualise the interaction effect `EduTotal_c:Group` as predicted by your model. Looking at your plot of predicted values and partial residuals, which conclusion can you draw?

```{r echo=FALSE}
#| label: "Q13.9"

check_question("Years in formal education is a statistically significant predictor of Vocab scores for both L1 and L2 speakers, and there is no striking difference in the strength of this association between the L1 and the L2 group.",
               options = c("Years in formal education is a statistically significant predictor of Vocab scores for both L1 and L2 speakers, and there is no striking difference in the strength of this association between the L1 and the L2 group.", 
                           "Years in formal education is a statistically significant predictor of Vocab scores for L1 speakers, but not L2 speakers.", 
                           "Years in formal education is a statistically significant predictor of Vocab scores for L2 speakers, but not L1 speakers.", 
                           "The model makes almost perfect predictions for L1 speakers, but not L2 speakers.",
                           "None of these conclusions can be drawn from the plot alone."),
               button_label = "Check answer",
               type = "radio",
               q_id = "Q13.9",
               right = "That's right, well done! We can tell from the plot that years in education is a statistically significant predictor because, in both the L1 and the L2 panel, it is impossible to draw a straight line that stays within the confidence band. A straight line would represent the null hypothesis of no association and, if we cannot draw it within the confidence band, this means that we can reject the null hypothesis. Concerning the statistical significance of the interaction, however, it is not obvious that the slopes of the two lines differ much. All the predicted Vocab scores for L2 speakers are lower than for L1 speakers, but the increase associated with more years of formal education, as represented by the gradients of the two slopes, look about the same. This is why the interaction coefficient turned out not to be statistically significant.",
               wrong = "No. Remember that the strength of the association is represented by the gradient of the slope of the regression lines.")
check_hint("Think about whether a straight line indicating no correlation (i.e. the null hypothesis) could be drawn within the confidence bands.", 
           hint_title = "üê≠ Click on the mouse for a hint.")

```

```{r}
#| code-fold: true
#| code-summary: "Show code to help you complete Q13.9"
#| eval: false

visreg(model5.int, 
       xvar = "EduTotal_c", 
       by = "Group",
       xtrans = function(x) x + median(Dabrowska.data$EduTotal), 
       gg = TRUE) + 
  labs(x = "Years in formal education",
       y = "Predicted Vocab scores") +
  theme_bw()
```
:::

### Interactions between two numeric predictors

We can also model interactions between two numeric predictors. For instance, we may hypothesize that the positive effect of time spent in formal education is moderated by age. If this interaction effect were negative, this would mean that, as people get older, the fact that they spent longer in formal education becomes less relevant to predict their current vocabulary knowledge.

Let's add an `Age_c:EduTotal_c` interaction to our model to find out if our data supports this hypothesis.

```{r}
model6 <- lm(Vocab ~ Group + Age_c + OccupGroup + Gender + Blocks_c + EduTotal_c + Group:Age_c + Age_c:EduTotal_c,
             data = Dabrowska.data)

summary(model6)
```

Looking at the last coefficient estimate in the model summary (`Age_c:EduTotal_c`), we can see that this second interaction coefficient is negative -- in line with our hypothesis. However, this coefficient is also very small (`-0.01776`). Moreover, its associated *p*-value (`0.694947`) warns us that, if there were no interaction effect (i.e. under the null hypothesis), we would have a 69% probability of observing such a small effect in a dataset this size simply due to random variation alone.

The model summary also tells us that the amount of variance in `Vocab` scores that `model6` accounts for is 36.18% (`Adjusted R-squared:  0.3618`). This is actually slightly *less* than `model5` (`Adjusted R-squared:  0.3654`), which did not include this interaction. In other words, this additional interaction does not help us to model the association between our predictor variables and `Vocab` scores more accurately.

In @fig-PredictedVocabAgeByEdu, we visualise this statistically non-significant interaction to better understand what it corresponds to. When used to visualise an interaction effect between two numeric predictors, the `visreg()` function automatically splits the second predictor variable (the "by" variable) into three categories corresponding to low, middle, and high values of the variable. It therefore shows the predicted effect of the numeric predictor predicted on the *x*-axis in these three different contexts. If, as in @fig-PredictedVocabAgeByEdu, the three slopes are of the same gradient and the regression lines therefore run parallel to each other, this indicates that there is no noteworthy interaction between the two numeric predictors.

```{r}
#| label: "fig-PredictedVocabAgeByEdu"
#| fig-cap: "`Vocab` scores as predicted by `model4_c` for English speakers of various ages who were in formal education for a below-average, average, and above-average length of time (in blue) and partial model residuals (grey points)"

visreg(model6,
       xvar = "Age_c", 
       by = "EduTotal_c",
       xtrans = function(x) x + median(Dabrowska.data$Age), 
       gg = TRUE) + 
  labs(x = "Age",
       y = "Predicted Vocab scores") +
  theme_bw()
```

Examining @fig-PredictedVocabAgeByEdu, we can therefore conclude that the effect of age on `Vocab` scores is not moderated by the number of years that participants spent in formal education.

The `visreg()` function provides two ways to visualise interaction effects between two numeric variables: in @fig-PredictedVocabAgeRL, below-average, average, and above-average number of years in formal education were visualised across three panels. @fig-PredictedVocabAgeRL shows the same predictions but, this time, the code includes the argument "overlay = TRUE", which results in a single panel with three coloured regression lines superimposed. This can make it easier to check whether the lines are parallel.

```{r}
#| source-line-numbers: "4"
#| label: "fig-PredictedVocabAgeRL"
#| fig-cap: "`Vocab` scores as predicted by `model4_c` for English speakers of various ages who were in formal education for a below-average (red line), average (green line), and above-average (blue line) period of time and partial model residuals (coloured points)."

visreg(model6,
       xvar = "Age_c", 
       by = "EduTotal_c",
       overlay = TRUE,
       xtrans = function(x) x + median(Dabrowska.data$Age), 
       gg = TRUE) + 
  labs(x = "Age",
       y = "Predicted Vocab scores") +
  theme_bw()
```

In order to interpret the output of a model featuring a significant interaction between two numeric predictors, we will now fit a simpler model predicting the `Vocab` scores of the L1 participants only (`L1.data`) using only two predictor variables: years in formal education (`EduTotal`) and Author Recognition Test (`ART`) scores.

The Author Recognition Test [@acheson2008] is a measure of print exposure to literature, which is known to be a strong predictor of vocabulary knowledge (see [**Your turn!**]{style="color:green;"} in @sec-ModelPredictions above). This is confirmed in the L1 dataset as `ART` and `Vocab` scores are strongly correlated:

```{r}
cor(L1.data$ART, L1.data$Vocab)
```

At the same time, we also know that there is a positive, though less strong, correlation between the number of years that L1 participants were in formal education and their `Vocab` test results:

```{r}
cor(L1.data$EduTotal, L1.data$Vocab)
```

In the following, we explore the possibility that this positive association between Author Recognition Test (`ART`) scores and `Vocab` scores may be moderated by the number of years spent in formal education (`EduTotal`). If this interaction effect were negative, this would mean that, if two individuals both have the same high `ART` score, but one spent longer in formal education, then the effect of those extra years of education would not be as strong as they would be for someone with a lower `ART` score. To test this hypothesis, we formulate the following null hypothesis:

-   **H~0~**: The number of years spent in formal education does not moderate the association of L1 English speakers' `ART` scores with their `Vocab` scores.

We now fit a model to find out if we have enough evidence to reject this null hypothesis:

```{r}
# First, we center the numeric variables in the L1 dataset:
L1.data <- L1.data |> 
   mutate(EduTotal_c = EduTotal - median(EduTotal),
          Blocks_c = Blocks - median(Blocks),
          Age_c = Age - median(Age))

# Second, we fit the model with both variables and their two-way interaction as predictors:
L1.model <- lm(Vocab ~ ART + EduTotal_c + ART:EduTotal_c,
              data = L1.data)

# Finally, we inspect the model:
summary(L1.model)
```

The model summary confirms that both predictors individually (i.e. as main effects) make statistically significant, positive contributions to the prediction of `Vocab` scores. Crucially, the summary also indicates that the interaction effect (`ART:EduTotal_c`) is statistically significant at Œ±¬†=¬†0.05 (*p*¬†=¬†`0.001017`). The negative coefficient estimate of `-0.1803` means that, for each additional year of formal education, our model predicts that the effect of a participant's `ART` score on `Vocab` decreases by 0.1803 points. In simpler terms, the positive effect of reading literature, as measured by the Author Recognition Test [@acheson2008], decreases slightly for every year spent in formal education.

Remember that, whenever we report an interaction, we can no longer interpret the estimated coefficients of the individual predictors in isolation. That is because we must also consider the interaction effect. To understand what this means in practice, let's compare two speakers from the L1 dataset:

```{r}
L1.data |> 
  slice(2, 18) |> 
  select(Occupation, OccupGroup, ART, EduTotal, Vocab)
```

-   Participant No. 2 is a student and support worker with an ART score of 31 points, who has (so far) spent 13 years in formal education.

-   Participant No. 18 is a housewife who also scored 31 points on the ART, but who was in formal education for a total of 17 years.

In general, we can calculate the `Vocab` scores that our `L1.model` predicts by combining the model's coefficient estimates like this:

```         
Intercept + 
`ART` coefficient\*`ART` score + 
`EduTotal` coefficient\*(`EduTotal` in years - median `EduTotal`) +
`ART:EduTotal` coefficient\*`ART` score\*(`EduTotal` in years - median `EduTotal`)
```

Based on the coefficient estimates of the model summary, we can therefore calculate the predicted `Vocab` score of the student/support worker as follows:

```{r}
51.9162 + # <1>
1.3088 * 31 + # <2>
5.2626 * (13 - median(L1.data$EduTotal)) + # <3>
-0.1803 * 31 * (13 - median(L1.data$EduTotal)) # <4>
```

1.  Intercept coefficient
2.  Main effect of 31 points on the ART test
3.  Main effect of 13 years in formal education
4.  Interaction effect between 31 points on the ART and having spent 13 years in formal education

And similarly for the housewife with the same ART score:

```{r}
51.9162 + # <1>
1.3088 * 31 + # <2>
5.2626 * (17 - median(L1.data$EduTotal)) + # <3>
-0.1803 * 31 * (17 - median(L1.data$EduTotal)) # <4>
```

1.  Intercept coefficient
2.  Main effect of 31 points on ART test
3.  Main effect of 17 years in formal education
4.  Interaction effect between scoring 31 points on the ART and having spent 17 years in formal education

We can check that we did the maths correctly by checking the model's prediction for these two individuals. Remember that minor differences after the decimal point are due to us using rounded coefficient estimates.

```{r}
predict(L1.model)[2]
predict(L1.model)[18]
```

Notice how these two predicted scores are very similar, even though the housewife spent longer in formal education than the student/support worker. This is because, for L1 speakers, greater print exposure and more education generally lead to a larger vocabulary (as indicated by the positive main-effect coefficient estimates in `L1.model`), but the increase in vocabulary for each additional year of education is predicted to be smaller for individuals with higher ART scores.

@fig-VocabEduART visualises the predictions of `L1.model`. How can we interpret these three regression lines?

```{r}
#| code-fold: true
#| code-summary: "Show code to generate plot."
#| label: "fig-VocabEduART"
#| fig-cap: "`Vocab` scores as predicted by `model4_c` (in blue) and partial model residuals (grey points) as a function of the number of years speakers were in formal education and for three ART test scores"

visreg(L1.model,
       xvar = "EduTotal_c", 
       by = "ART",
       xtrans = function(x) x + median(L1.data$EduTotal), 
       gg = TRUE) + 
  labs(x = "Years in formal education",
       y = "Predicted Vocab scores") +
  theme_bw()
```

For low and mid-level ART scores, the model predicts a positive correlation between the number of years a participant has spent in formal education and their predicted `Vocab` scores. However, this is not the case for participants who scored high on the ART test (third panel). The three regression lines representing the model's predicted scores are clearly not parallel, and it would not be possible to draw three parallel lines that each stay within their respective 95% confidence bands. This confirms that, in this L1 model, the interaction between ART scores and years in formal education is statistically significant at Œ±¬†=¬†0.05. The main effects of these two predictors cannot be meaningfully interpreted without considering this interaction.

## Model selection

Typically, the more predictors we enter in a model, the better the model fits the data. However, having predictors that contribute very little to the model and/or whose contributions are not statistically significant risks lowering the accuracy of the model's predictions on new data. In this case, we say that the model **overfits**. A model that overfits the sample data risks not generalising to other data. If the aim of our statistical modelling is to infer from our sample to the general population (see @sec-Inferential), it can make sense to try to find an 'optimal' model that accounts for as much of the variance in the outcome variable as possible whilst relying only on association effects that we can be fairly confident could not have occurred due to chance only. This is where model selection comes into play.

Some people consider model selection to be a bit of an art. This chapter only aims to introduce the topic and does not cover ‚Äî let alone compare ‚Äî different model selection procedures. Ultimately, what really matters is that, if we intend to apply a model selection procedure, we decide on the method *before* analysing the data. This is because the method selection procedure bears the very real risk of (consciously or unconsciously) "fishing" for statistically significant results. Fishing is a highly Questionable Research Practice (QRP, see @sec-pHacking) that consists in trying out different methods until we obtain results that match our theory and/or hypotheses.

In this chapter, we will use the **adjusted R^2^** as our model selection decision criterion to arrive at an optimal model of `Vocab` scores among L1 and L2 speakers of English. Recall that R^2^ values correspond to the amount of variance in the outcome variable that a model can predict. The adjusted R^2^ is particularly useful because it is adjusted for the number of predictors entered in the model; models with more predictors are penalised.

> When using adjusted R^2^ as the decision criterion, we seek to eliminate or add predictors depending on whether they lead to the largest improvement in adjusted and we stop when adding or eliminating another predictor does not lead to further improvement in adjusted R^2^. Adjusted R^2^ describes the strength of a model fit, and it is a useful tool for evaluating which predictors are adding value to the model, where *adding value* means they are (likely) improving the accuracy in predicting future outcomes [@cetinkaya-rundelIntroductionModernStatistics2021: [Section 8.4](https://openintro-ims.netlify.app/model-mlr#sec-model-selection)].

There are two common ways to add or remove predictors in a multiple regression model. These are called backward elimination and forward selection. They are often called **stepwise selection** because they add or remove one variable at a time.

> **Backward elimination** starts with the full model ‚Äì the model that includes all potential predictor variables. Predictors are eliminated one-at-a-time from the model until we cannot improve the model any further.
>
> **Forward selection** is the reverse of the backward elimination technique. Instead of eliminating predictors one-at-a-time, we add predictors one-at-a-time until we cannot find any predictors that improve the model any further [@cetinkaya-rundelIntroductionModernStatistics2021: [Section 8.4](https://openintro-ims.netlify.app/model-mlr#sec-model-selection)].

We will use backward elimination as it allows us to start with a **full model**, that includes all the predictors and any interactions that we believe are justified on the basis of theory and/or prior research. At this stage, it is absolutely crucial to think about which variables and which interactions are genuinely meaningful and which are not!

With the @DabrowskaExperienceAptitudeIndividual2019 data, several full models can be justified. In this section, we will attempt to model `Vocab` scores among L1 participants using four predictors (`ART`, `Blocks_c`, `Age_c`, `EduTotal_c`, and `OccupGroup`) and the following four interactions (`ART:EduTotal_c`, `Blocks_c:EduTotal_c`, `ART:Age_c`, and `Blocks:Age_c`). We can justify this choice of predictors based on our current understanding of language learning.

```{r}
L1.model.full <- lm(Vocab ~ ART + Blocks_c + Age_c + EduTotal_c + OccupGroup + ART:EduTotal_c + Blocks_c:EduTotal_c + ART:Age_c + Blocks_c:Age_c, 
           data = L1.data)

summary(L1.model.full)
```

Our full model has an adjusted R^2^ of `0.5632`, which means that the combination of these variables and the four interactions account for about 56% of the variance in `Vocab` scores in the L1 data. However, many of the model coefficients in `model.full` are not statistically significant, so we could try to remove them to see whether this leads to a lower adjusted R^2^ or not. Strictly speaking, a stepwise selection procedure would entail removing each interaction one-by-one. To save space here, we remove all three non-significant interaction terms in a single backward step:

```{r}
L1.model.back1 <- lm(Vocab ~ ART + Blocks_c + Age_c + EduTotal_c + OccupGroup + ART:EduTotal_c, 
           data = L1.data)

summary(L1.model.back1)
```

This procedure has actually led to a (very) small increase in our adjusted R^2^, which is now `0.5679`, or 57%. Can we simplify our model even further and still account for as much variance in `Vocab` scores by dropping categorical predictor variable `OccupGroup`?

```{r}
L1.model.back2 <-  lm(Vocab ~ ART + Blocks_c + Age_c + EduTotal_c + ART:EduTotal_c, 
           data = L1.data)

summary(L1.model.back2)
```

This new model has a slightly higher adjusted R^2^ (`0.5788`), so it looks like this was another sensible simplification of our model. All of the remaining coefficient estimates in `L1.model.back2` make statistically significant contributions to the model. If we try to remove one, we can expect that the amount of variance that our model can account for will drop. For example, we can try to remove `Age` as a predictor from our model to see what happens:

```{r}
L1.model.back3 <- lm(Vocab ~ ART + Blocks_c + EduTotal_c + ART:EduTotal_c, 
           data = L1.data)

summary(L1.model.back3)
```

Indeed, `L1.model.back3` accounts for 49% of the variance (adjusted R^2^ = `0.4864`), which is considerably less than `L1.model.back2`. We will therefore report and interpret `L1.model.back2`.

::: callout-note
## Reporting a multiple regression model

There are many ways to report the **numerical** results of a statistical model. Researchers typically include a table reporting the model's coefficient estimates (sometimes referred to as Œ≤, "beta"), together with a measure of variability around these estimates (e.g., standard error or confidence intervals), as well as their associated *p*-values. In addition, it is important to report the accuracy of the model. Different so-called **goodness-of-fit measures** are used for this; one of the most common being the adjusted coefficient of determination, R^2^. The output of the `summary()` function includes all of these statistics and is therefore suitable for a research report.

Alternatively, the [{sjPlot}](https://strengejacke.github.io/sjPlot/) library [@l√ºdecke2020] includes a handy function that produces nicely formatted tables to report all kinds of models, including multiple linear regression models. When you run the `tab_model()` function in RStudio, the table will be displayed in the Viewer pane. By default, it includes the model's coefficient estimates and 95% confidence intervals around these estimates, as well as *p*-values formatted in bold if they are below 0.05.

```{r}
#| eval: false
install.packages("sjPlot")
library(sjPlot)

tab_model(L1.model.back2)
```

```{r}
#| echo: false
library(sjPlot)
tab_model(L1.model.back2)
```

::: column-margin
![Hex sticker of the [{sjPlot}](https://strengejacke.github.io/sjPlot/) package](images/hex_sjPlot.png){width="100"}
:::

Check the documentation of the `tab_model()` function and the [sjPlot website](https://strengejacke.github.io/sjPlot/) to learn about its many useful formatting options:

```{r}
#| eval: false
?tab_model
```

It is also recommended to **visualise** the model's predictions and its (partial) residuals. Again, there are many ways to achieve this in `R`, but we will stick to using the {visreg} library. Run the following command and follow the instructions displayed in the Console to view all the plots in RStudio. You may need to resize your Plot pane or use the Zoom button to properly view the plots.

```{r}
#| eval: false
visreg(L1.model.back2)
```

Running the `visreg()` function on this multiple regression model outputs several warning messages in the Console. These messages are important and should not be ignored:

```         
Note that you are attempting to plot a 'main effect' in a model that contains an interaction. This is potentially misleading; you may wish to consider using the 'by' argument.
```

The message warns us that we should not attempt to interpret `ART` and `EduTotal_c` as **main effects** because our model includes an **interaction** that involves these two variables. Indeed, the fourth plot (see @fig-PredictedVocabEduL1) suggests that the more years an L1 speaker spends in formal education, the greater their `Vocab` score; however, because our model includes an interaction effect, the authors of the {visreg} package are warning us that the strength or even the direction of this effect could change depending on individuals' `ART` score.

```{r}
#| label: "fig-PredictedVocabEduL1"
#| fig-cap: "Predicted predicted vocabulary scores for L1 speakers as a function of the number of years that they were in formal education (in blue) and partial residuals (grey points)."

visreg(fit = L1.model.back2, 
       xvar = "EduTotal_c",
       xtrans = function(x) x + median(L1.data$EduTotal), 
       gg = TRUE) +
  labs(x = "Years in formal education",
       y = "Predicted Vocab scores") +
  theme_bw()    
```

As suggested by the warning message, we can (and should!) visualise interactions like this one using the "by" argument.

```{r}
#| label: "fig-PredictedVocabEduLit"
#| fig-cap: "Predicted predicted vocabulary scores for L1 speakers as a function of the number of years that they were in formal education and across three different ART scores (in blue) and partial residuals (grey points)."
visreg(fit = L1.model.back2, 
       xvar = "EduTotal_c", 
       xtrans = function(x) x + median(L1.data$EduTotal), 
       by = "ART",
       gg = TRUE) +
  labs(x = "Years in formal education",
       y = "Predicted Vocab scores") +
  theme_bw()  
```

@fig-PredictedVocabEduLit shows the predicted effect of the number of years in formal education on `Vocab` scores for participants who scored 3, 10, and 31 points on the `ART`. The regression line shows a positive relation between `Vocab` scores and years in formal education for below-average and average `ART` scores, but the regression line is almost flat for above-average `ART` scores. This indicates that, for individuals who score very high on the `ART`, years in education is no longer a useful predictor of their `Vocab` scores.

Finally, we should also report the outcome of our model assumption checks. This is covered in the following section.
:::

## Checking model assumptions {#sec-ModelAssumptions}

As we saw in @sec-AssumptionsLR, it is crucial that we check that our models meet the assumptions of linear regression models before we interpret them because, if they don't, our models may be unreliable. In some cases, modelling issues may already be visible from the model summary. Warning signs include very large model residuals, coefficient estimates reported as `NA`, and warning messages stating that the model is "singular" or that it has "failed to converge".

These problems can occur for a number of reasons, but most often because the model is too complex given the data available: the sample size may be too small or the data too sparse for certain combinations of predictors (e.g. if you try to enter gender and native language as predictors in a model, but for some languages only female native speakers are represented in the dataset). There are often ways around these issues, but they are beyond the scope of this introductory textbook (see recommended readings below and [next-step resources](https://elenlefoll.github.io/RstatsTextbook/A_FurtherResources.html)).

The model assumptions for multiple linear regression models are the same as for simple linear regression models (see @sec-AssumptionsLR):

-   Independence of the data points (see @sec-Independence and @sec-IndependenceSLR)
-   Linear relationships between the predictors (see @sec-Linearity and @sec-LinearitySLR)
-   Homogeneity of the model residuals (see @sec-Homoscedasticity and @sec-Residuals)
-   Normality of the model residuals (see @sec-Normality)
-   No overly influential outliers (see @sec-Linearity)

There is only one additional assumption that is specific to models that include multiple predictors:

-   No multicollinearity

In the following, we use the `check_model()` function from the {[performance](https://easystats.github.io/performance)} package [@ludeckePerformancePackageAssessment2021] to check these assumptions graphically. The `check_model()` function also requires the installation of the {[qqplotr](https://aloy.github.io/qqplotr/index.html)} [@almeidaGgplot2CompatibleQuantilequantile2018] package to work.

::: column-margin
![Hex sticker of the [{performance}](https://easystats.github.io/performance/) package](images/hex_performance.png){width="100"}
:::

```{r}
#| eval: false
install.packages(c("performance", "qqplotr"))
library(performance)
library(qqplotr)
```

Running the function on the saved model object should generate a large figure comprising six plots[^13_multiplelinearregression-1] (see @fig-6plots). In the following, we will learn to interpret them one-by-one.

[^13_multiplelinearregression-1]: If your version of RStudio does not display the six-panelled figure but instead only a white canvas, this is probably because your Plots pane is too small. In this case, you will need to make it as large as possible and then try running the function again. If that doesn't work either, don't worry as we will save and examine individual plots from now on.

```{r}
#| fig-width: 8
#| fig-asp: 1.5
#| label: "fig-6plots"
#| fig-cap: "Six plots output by the `check_model()` function for assessing key assumptions of linear regression models"
check_model(L1.model.back2)
```

By default, `check_model()` outputs a single figure that allows us to check the most important model assumptions. This large figure is useful for reporting purposes, but it is rather unwieldy for interpretation. Changing the "panel" argument of the `check_model()` function to `FALSE` returns a list of {ggplot} objects that we can save to our local environment as `diagnostic.plots`:

```{r}
diagnostic.plots <- plot(check_model(L1.model.back2, panel = FALSE))
```

Then we can use the double square bracket operator to display a single diagnostic plot from this saved list:

```{r}
#| label: "fig-PosteriorPredictiveCheck"
#| fig-cap: "Comparing the distribution of observed vocabulary scores (green) with data simulated based on model predictions (blue)"
diagnostic.plots[[1]]
```

This first plot (@fig-PosteriorPredictiveCheck) is another way to compare the model's predicted values (represented as here blue distributions) with the real-life (i.e observed) outcome variable (represented here in green). We can see that the center of the distribution of observed `Vocab` scores is slightly shifted to the right compared to most distributions of model-predicted data. That said, the simulated distributions are close to the real distribution and largely follow a similar shape. If they didn't, this would suggest that a linear regression model may not be suitable for our data.

The second plot (@fig-Linearity) is designed to check the assumption of **linearity**. If the predictors are linearly related, the green reference line should be flat and horizontal. Our reference line is slightly curved, but it remains possible to draw a straight line through the grey band, hence we can conclude that the assumption of linearity is not severely violated.

```{r}
#| label: "fig-Linearity"
#| fig-cap: "The relationship between model residuals and model predictions (fitted values)"
diagnostic.plots[[2]]
```

Note that @fig-Homoscedasticity is actually the same kind of plot as @fig-VocabResidual that we generated to check the assumption of equal (or constant) variance, i.e. **homoscedasticity**. Fitted values is another term for predicted values. Thus, in @fig-Homoscedasticity, the *x*-axis represents the `Vocab` scores predicted by the model. When the assumption of homoscedasticity is met, the model residuals are randomly distributed above and below 0, i.e. they do not notably increase or decrease as predicted values increase.

The {performance} package proposes a different kind of diagnostic plot to check the assumption **homoscedasticity** with the square-root of the absolute values of residuals on the *y*-axis (@fig-Homoscedasticity). A roughly flat and horizontal green reference line indicates homoscedasticity. Again, although the reference line in @fig-Homoscedasticity is by no means perfectly flat, a flat line can be drawn within the grey band.

```{r}
#| label: "fig-Homoscedasticity"
#| fig-cap: "The relationship between the square root of absolute standardized residuals and the model predictions (fitted values)"
diagnostic.plots[[3]]
```

The fourth diagnostic plot (@fig-Influential) helps to detect **outliers** in the data that may have a particularly strong influence on our model. It is based on Cook's distance [see @LevshinaHowlinguisticsData2015: Section 7.2.4; @sondereggerRegressionModelingLinguistic2023: Section 5.7.3]. Any points that fall outside the dashed green lines fall outside Cook's distance and are considered **influential observations**.

```{r}
#| label: "fig-Influential"
#| fig-cap: "The relationship between standardized residuals and leverage as estimated using Cook's distance"
diagnostic.plots[[4]]
```

In @fig-Influential, no data point falls outside of Cook's distance so we are not concerned about influential observations violating the assumptions of our model. Still, it is interesting to briefly explore the most influential observations, which are labelled by their index number in the dataset. In @fig-Influential, one of these is participant number¬†21:

```{r}
L1.data |> 
  slice(21) |> 
  select(Age, Gender, Occupation, Blocks, EduTotal, ART, Vocab)
```

As you can see, this female senior lecturer is rather unusual: she performed below average in the non-verbal IQ test (`Blocks`), yet achieved the second-highest `Vocab` score. She also performed far above average on the author recognition test (`ART`) and reported the longest period in formal education among the L1 participants (`EduTotal`).

In some cases, it may be justified to remove overly influential outliers; however, this should typically only be done when we are fairly certain that the outliers were caused by a technical error, such as a measuring instrument not functioning properly, or a human error, such as a participant misunderstanding the direction of a response scale. All other outliers may be theoretically interesting: if our aim is to generalise our model to the full population, we must be prepared to include some unusual observations that also belong to that population. In the case of L1 English speakers, this includes people who spent more than 20 years in formal education and are seemingly much more into languages than the kind of abstract puzzles typically found in non-verbal IQ tests!

```{r}
#| eval: false
#| include: false

# In this chunk, we check what happens if we remove the senior lecturer from the model
L1.modelback2.minus21 <-  L1.data |> 
  slice(-21) |> 
  lm(formula = Vocab ~ ART + Blocks_c + Age_c + EduTotal_c + ART:EduTotal_c)

summary(modelback2.minus21) # As expected, not much.
```

The fifth diagnostic plot output by the `check_model()` function (@fig-Collinearity) serves to check the assumption of a no **multicollinearity**. Multicollinearity, or **high collinearity**, refers to a strong linear dependence between predictors such that they do not contribute unique or independent information to the model. Multicollinearity should not be confused with a strong correlation between two individual predictors as measured using the `cor.test()` function (see @sec-Correlations). What matters here is the association between one or more predictor variables, conditional on the other variables in the model. This is considerably more complex to calculate, but luckily, there are several functions that allow us to do just that in `R`.

The {performance} package relies on the Variance Inflation Factor (VIF) to quantify collinearity. Typically, VIF scores above 10 are considered to indicate a problematic degree of collinearity between some predictors. @fig-Collinearity indicates that our model does not suffer from multicollinearity.

```{r}
#| label: "fig-Collinearity"
#| fig-cap: "VIF factor and 95% confidence interval for each predictor in the model"
diagnostic.plots[[5]]
```

Finally, the sixth plot output by the `check_model()` function serves to check the assumption of the normality of the residuals. In @sec-NormalityResiduals, we achieved this by visualising the distribution of residuals as a density plot (see @fig-VocabResidualDensity). The diagnostic plot presented in @fig-Normality, by contrast, is a so-called Q-Q plot (quantile-quantile plot). These plots are designed to compare the shapes of distributions. If the residuals follow a perfect normal distribution, the points should all fall along the straight reference line (in green).

```{r}
#| label: "fig-Normality"
#| fig-cap: "Q-Q plot"
diagnostic.plots[[6]]
```

In @fig-Normality, we see that most residuals follow a normal distribution, except some observations at the tails of the distribution. This is fairly typical and, if the other model assumptions are not violated, minor deviations from normality like this are unlikely to be a problem, especially with larger sample sizes.

::: callout-note
The `check_model()` vignette provides more detailed information on these diagnostic plots, how to interpret them, and what to do if the assumptions are not met: <https://easystats.github.io/performance/articles/check_model.html>.
:::

::: {.callout-tip collapse="false"}
#### Your turn! {.unnumbered}

So far in this chapter, we have fitted a model of L1 speakers' `Vocab` scores. We have found that participants' print exposure (as measured by the ART), non-verbal intelligence (as measured by the Blocks test), their age, the number of years they were in formal education, and the interaction between ART scores and years in education all contribute to being able to predict how well they performed on the `Vocab` test. The model that we selected as the optimal model (`L1.model.back2`) was able to predict 58% of the variance in `Vocab` scores among L1 speakers.

In this task, your aim is to find out which predictors and interactions among predictors can be used to predict `Vocab` scores among L2 speakers and to what degree of accuracy. In addition to the predictors that we entered in our full L1 model, you can enter three additional predictors in your model that are relevant to L2 speakers only:

-   Age at which they started living in an English-speaking country (`Arrival`)
-   Total number of years they have been living in an English-speaking country (`LoR`)
-   Number of hours they spend reading in English in a typical week (`ReadEng`)

Run the following code to median-center some of these numeric variables in `L2.data`:

```{r}
L2.data <- L2.data |> 
  mutate(Blocks_c = Blocks - median(Blocks),
         Age_c = Age - median(Age),
         EduTotal_c = EduTotal - median(EduTotal),
         Arrival_c = Arrival - median(Arrival),
         LoR_c = LoR - median(LoR))
```

Then, fit the following multiple linear regression model on the L2 dataset and examine the model summary:

```{r}
#| output: false

L2.model <- lm(formula = Vocab ~ ART + Blocks_c + Age_c + EduTotal_c + Arrival_c + LoR_c + ReadEng + ART:EduTotal_c + ART:ReadEng + EduTotal_c:ReadEng + Arrival_c:Age_c + LoR_c:Arrival_c,
             data = L2.data)

summary(L2.model)
```

[**Q13.10**]{style="color:green;"} How many interaction terms contribute to the predictions made by `L2.model`?

```{r}
#| echo: false
#| label: "Q13.10"

check_question("5",
               options = c("0", "1", "2", "3", "4", "5"),
               button_label = "Check answer",
               q_id = "Q13.10",
               right = "That's right. The model formula includes five interactions and they all contribute to the model predictions. That said, their coefficient estimates are very small and none of their contributions to the model are statistically significantly different from zero.",
               wrong = "No, that's incorrect.")
check_hint("By definition, all of the interactions entered in a model make a contribution, however small, to the model. Note that the question here is not about whether these contributions are statistically significantly different from zero.", 
           hint_title = "üê≠ Click on the mouse for a hint.")
```

[**Q13.11**]{style="color:green;"} Use the `check_model()` function to visually check whether `L2.model` meets the most important assumptions of multiple linear regression models. Which assumption(s) are obviously violated?

```{r}
#| echo: false
#| label: "Q13.11"
check_question("No multicollinearity",
               options = c("Independence of the data points",
                           "Linear relationships between the predictors",
                           "Homogeneity of the model residuals",
                           "Normality of the model residuals",
                           "No overly influential outliers",
                           "No multicollinearity",
                           "None of these assumptions"),
               type = "check",
               button_label = "Check answer",
               q_id = "Q13.11",
               right = "Yes, well done!",
               wrong = "No, not quite.")
check_hint("From the six diagnostic plots produced by the check_model() function, it transpires that one of these assumptions is severely violated.", 
           hint_title = "üê≠ Click on the mouse for a hint.")
```

```{r}
#| code-fold: true
#| code-summary: "Show sample code to answer Q13.11."
#| eval: false

diagnostic.plots.L2 <- plot(check_model(L2.model, panel = FALSE))

diagnostic.plots.L2[[1]] # No issues here.

diagnostic.plots.L2[[2]] # Also no issues here.

diagnostic.plots.L2[[3]] # Nothing major to worry about: the reference line is not perfectly flat, but it's not far off.

diagnostic.plots.L2[[4]] # No observation points appear to have an overly large influence on the model

diagnostic.plots.L2[[5]] # Here we have three coefficients with VIF scores > 10. This is problematic!

diagnostic.plots.L2[[6]] # The residuals are not normally distributed, but the largest deviation is at lowest end of the distribution which it not terribly unusual.
```

We can display the exact VIF values of a model using the `vif()` function from the {car} package:

```{r}
#install.packages("car")
library(car)

vif(L2.model)
```

From this list of VIF values, we can see that three predictors display high collinearity: `Age_c`, `Arrival_c`, and `LoR_c`. This makes sense because, for most L2 participants recruited in the UK, if we know how old they were when they arrived in the UK (`Arrival`), and their length of residence in the country in years (`LoR`), then we can probably almost predict their age (`Age`) to a high degree of accuracy. We can easily check this hypothesis in a random sample of six participants from our `L2.data`:

```{r}
#| include: false
set.seed(2025)
```

```{r}
L2.data |> 
  select(Arrival, LoR, Age) |> 
  slice_sample(n = 6)
```

As you can see, in most cases, participants' age is equal to their age when they first arrived in the country plus the number of years they've been living there since. This is why `Age` and the combination of the predictors `Arrival` and `LoR` are almost perfectly correlated:

```{r}
cor(L2.data$Age, (L2.data$Arrival + L2.data$LoR))
```

To remedy this, we must remove one of these three predictors from our model. Fit a new model that does *not* include participants' age when they first arrived in the UK (`Arrival`). Save this new model as `L2.model.b`. Check that it now meets all the model assumptions and then examine its summary.

[**Q13.12**]{style="color:green;"} Adjusted for the number of predictors entered into `L2.model.b`, what percentage of the variance in `Vocab` scores in the L2 dataset does this model accurately predict?

```{r}
#| echo: false
#| label: "Q13.12"
check_question("About 20%",
               options = c("Practically 0%",
                           "About 3%",
                           "About 15%",
                           "About 20%",
                           "About 22%",
                           "About 31%",
                           "About 57%"),
               button_label = "Check answer",
               q_id = "Q13.12",
               right = "Yes, well done!",
               wrong = "No, this is not the correct value.")

```

```{r}
#| code-fold: true
#| code-summary: "Show sample code to answer Q13.12."
#| eval: false

L2.model.b <- lm(formula = Vocab ~ ART + Blocks_c + Age_c + EduTotal_c + LoR_c + ReadEng + ART:EduTotal_c + ART:ReadEng + EduTotal_c:ReadEng,
             data = L2.data)

check_model(L2.model.b) # Now all looking much better than earlier!

summary(L2.model.b) # Adjusted R-squared:  0.2027
```

[**Q13.13**]{style="color:green;"} Which main-effect predictor(s) make a statistically significant contribution to `L2.model.b` at Œ±¬†=¬†0.05?

```{r}
#| echo: false
#| label: "Q13.13"
check_question("ReadEng",
               options = c("ART",
                           "Blocks_c",
                           "Age_c",
                           "EduTotal_c",
                           "Arrival_c",
                           "LoR_c",
                           "ReadEng"),
               type = "check",
               button_label = "Check answer",
               q_id = "Q13.13",
               right = "That's right, only one main-effect predictor makes a statistically significant contribution to this model of receptive vocabularly knowledge among L2 speakers of English.",
               wrong = "Humm, not quite. Which coefficient estimate(s) are associated with p-values that are lower than 0.05?")
```

[**Q13.14**]{style="color:green;"} Does the model's coefficient estimate for `ReadEng` make intuitive sense?

```{r}
#| echo: false
#| label: "Q13.14"
check_question("Yes, the ReadEng coefficient is positive, which means that the more time L2 speakers regularly spend reading in English, the higher their predicted receptive English vocabulary test scores.",
               options = c("Yes, the ReadEng coefficient is positive, which means that the more time L2 speakers regularly spend reading in English, the higher their predicted receptive English vocabulary test scores.",
                           "No, the ReadEng coefficient is larger than all other coefficients, which violates the model assumption of no overly influential outliers."),
               type = "radio",
               button_label = "Check answer",
               q_id = "Q13.14",
               right = "That's right: Reading is a great way to learn new vocabularly! And reading a statistics textbook is therefore doubly beneficial: you get to learn about quantitative data analysis *and* expand your English vocabulary! ü§ì",
               wrong = "No, only individual observations (i.e. data points) can be considered outliers, not coefficient estimates.")
```

[**Q13.15**]{style="color:green;"} Which interaction effect(s) make a statistically significant contribution to `L2.model.b` at Œ±¬†=¬†0.05?

```{r}
#| echo: false
#| label: "Q13.15"
check_question("None of them",
               options = c("ART:EduTotal_c",
                           "ART:ReadEng",
                           "EduTotal_c:ReadEng",
                           "None of them"),
               type = "check",
               button_label = "Check answer",
               q_id = "Q13.15",
               right = "That's right, none of them make a statistically significant contribution to the model. However, that doesn't necessarily mean that their coefficients do not point to any real associations: we may not have enough data to observe any statistically significant interactions.",
               wrong = "Hummm, are you sure? Check the model summary again.")
```

[**Q13.16**]{style="color:green;"} Use the {visreg} library to visualise the model's predicted `Vocab` scores (on the *y*-axis) as a function of all possible `ReadEng` values (on the *x*-axis) and subdivide the figure into three panels corresponding to low, mid-level, and high ART scores. Which arguments do you have to use inside the `visreg()` function to achieve this?

```{r}
#| echo: false
#| label: "Q13.16"
check_question(c("fit = L2.model.b", 
                 "xvar = \"ReadEng\"", 
                 "by = \"ART\""),
               options = c("fit = L2.model.b",
                           "xvar = \"ART\"", 
                           "xvar = \"ReadEng\"", 
                           "xtrans = function(x) x + median(L2.data$ReadEng)", 
                           "by = \"ReadEng\"",
                           "by = \"ART\""),
               type = "check",
               button_label = "Check answer",
               q_id = "Q13.16",
               right = "Yes, well done! Now you can focus on interpreting your model's predicted values (the blue lines) and examine the partial residuals (the grey points that represent the variance in Vocab scores that is not accounted for by this model). It should be clear from this plot that this L2 model is not able to predict `Vocab` scores as well as our L1 model could (hence the considerably lower adjusted coefficient of determination (R-squared). Is this because L2 learners are more diverse and therefore less predictable? Or is it (also) because the sample size is smaller (67 instead of 90 participants)? We cannot tell from this data alone. What is certain is that more research is needed to answer these fascinating questions!",
               wrong = "No, that won't quite do.")
```

```{r}
#| code-fold: true
#| code-summary: "Show sample code to answer Q13.16."
#| eval: false

visreg(fit = L2.model.b,
       xvar = "ReadEng",
       by = "ART")
```
:::

## Tapping into the potential of statistical modelling

This chapter aimed to provide first insights into the potential of statistical modelling with multiple predictors. It has demonstrated the importance of considering interactions between predictors and of visualising both the observed, sample data, as well as simulated values based on the predictions of a statistical model.

You may noted that *p*-values were *not* at the heart of this chapter. Instead, we focused on model accuracy (as measured by the adjusted R^2^), coefficient estimates and their relative importance (as measured by the lmg metric), predicted values, and (partial) model residuals. This is because there is an unfortunate tendency among some students and researchers to be misled into thinking that, if a result turns out to be statistically significant, it must be true. As statistician Andrew Gelman puts it, some confuse statistics for "a form of modern alchemy, transforming the uncertainty and variation of the laboratory and field measurements into clean scientific conclusions that can be taken as truth" [@gelmanEthicsStatisticalPractice2018: 43].

I hope that working through @sec-Inferential to [-@sec-MLR] has shown you that a big part of learning to work with quantitative data in the language sciences is really about "embracing variation and accepting uncertainty" [@gelmanEmbracingVariationAccepting2019]. If we intend to report inferential statistics based on sample data, it is important that we decide on significance level thresholds, model selection criteria, and other such parameters *before* conducting our data analysis [on the benefits of preregistrating protocols and methods in the language sciences, see e.g. @mertzenBenefitsPreregistrationHypothesisdriven2021; @roettgerPreregistrationExperimentalLinguistics2021]. We must strive to test well-defined hypotheses and to quantify (and ideally also visualise!) **uncertainty**. Ultimately, it is worth remembering that "in almost all practical data analysis situations ‚Äì we can only draw uncertain conclusions from data, regardless of whether we manage to obtain statistical significance or not" [@vasishthHowEmbraceVariation2021: 1311].

As explained in @sec-Inferential, statistical inference based on NHST and *p*-values comes from a statistical framework called **frequentism**, which happens to (currently) be the most widely used framework in the language sciences. Taking a frequentist approach means that the statistical properties of the hypothesis tests that we conduct are considered under hypothetical replications of our study with new sample data. This is because, in the frequentist framework, we estimate the long-run probability of observing certain effects, were we to repeat the study many times. This is one of the reasons why **replication** (see @sec-Reproducibility) is key to advancing our knowledge of linguistics and language teaching and learning. Whenever we have small sample sizes, small effect sizes (i.e. small coefficient estimates in statistical models), large measurement error, and/or lots of variability among the target population -- as is often the case in the language sciences -- we simply cannot get **reliable** results from a single sample.

Does this mean that we should give up with quantitative data analysis and statistics all together? No, of course not. On the contrary, this uncertainty makes research in the language sciences all the more interesting and worth pursuing! It also means that there is much more to be learnt in terms of methods. In this chapter, you have learnt about **frequentist fixed-effects linear regression models**. These models are incredibly useful and can be used in many contexts, but they make a number of important assumptions that do not always hold:

-   Perhaps the most obvious, yet one that we have not discussed so far, is that the **outcome variable** of a linear regression model must be **continuous**, like the `Vocab` variable in `Dabrowska.data`, which ranges from -13.33 to 95.56 (see @sec-Variables). Other types of statistical models can be used to model other types of outcome variables, including:

    -   **Binomial (or binary) logistic regression models** allow us to predict **binary** outcomes (e.g. whether or not a verb is negated) [to find out more, see e.g. @LevshinaHowlinguisticsData2015: Chapter¬†12; @sondereggerRegressionModelingLinguistic2023: Chapter¬†6; @WinterStatisticsLinguistsIntroduction2019: Chapter¬†12].

    -   **Multinomial logistic regression models** can be used to model **categorical** outcome variables with more than two levels (e.g. which modal verb is used in certain constructions) [to find out more, see e.g. @LevshinaHowlinguisticsData2015: Chapter¬†13].

    -   **Poisson regression models** are used to model **count** variables, i.e. discrete numeric variables such as the frequency of fillers (such as *uh* and *oh*) in certain contexts [to find out more, see e.g. @WinterStatisticsLinguistsIntroduction2019: Chapter¬†13].

-   The observations (i.e. the data points) used to fit a **fixed-effect model** must be independent of each other. In the language sciences, such a situation is actually quite rare (see @sec-IndependenceSLR). To model interdependencies between observations, we can fit **mixed-effects models** (also called multilevel or hierarchical models) [to learn more, see e.g. @griesStatisticsLinguisticsPractical2021: Chapter¬†6; @sondereggerRegressionModelingLinguistic2023: Chapters¬†8-10; @WinterStatisticsLinguistsIntroduction2019: Chapters¬†14-15].

-   Linear regression models assume **linear** relationships between the predictors. There are different ways to circumvent this problem. In some cases, predictors can be **transformed** to meet this assumption [see @WinterStatisticsLinguistsIntroduction2019: Chapter¬†5]. In others, it may be wiser to model non-linear associations with other kinds of models such as Generalised Additive Mixed Models [GAMMs, see e.g. @soskuthyGeneralisedAdditiveMixed2017].

-   Finally, we need not stick to the **frequentist** school of statistics. In fact, quantitative linguists are increasingly turning to **Bayesian** statistics and finding that Bayesian models help them work with the particularities of linguistic data [to learn more about Bayesian statistics, see e.g. @levshinaComparingBayesianFrequentist2022; @nicenboimIntroductionBayesianData2026].

::: callout-note
## Recommended further reading üìö

In this textbook, we have only just scratched the surface of statistical modelling. We can do much, much more with these kinds of models, and there are lots of additional things to take into account. The good news is that there are lots of excellent resources to help you continue your statistical modelling journey. Here are some good places to get started (in alphabetical order):

-   Gries, Stefan Thomas. 2021. *Statistics for linguistics with R: A practical introduction* (De Gruyter Mouton Textbook). 3^rd^ revised edition. De Gruyter Mouton.
-   Levshina, Natalia. 2015. *How to do linguistics with R: Data exploration and statistical analysis*. John Benjamins.
-   Nicenboim, Bruno, Daniel Schad & Shravan Vasishth. 2026. Introduction to Bayesian Data Analysis for cognitive science (Chapman & Hall/CRC Statistics in the Social and Behavioral Sciences Series). CRC Press. Open Access version: <https://bruno.nicenboim.me/bayescogsci/>.
-   Sonderegger, Morgan. 2023. *Regression modeling for linguistic data*. Cambridge, Massachusetts: The MIT Press. Open Access version: <https://osf.io/pnumg/>.
-   Winter, Bodo. 2019. Statistics for Linguists: An Introduction Using R. Routledge.

Although they go further than the present textbook, you will also find that these resources begin by explaining many of the things already covered in this chapter and previous chapters, and that's actually a good thing. There's no harm in revising these complex topics from a different perspective, with different examples, `R` packages, and coding styles.
:::

### Check your progress üåü {.unnumbered}

Congratulations: you have successfully completed the most difficult chapter of this textbook! You have answered [`r checkdown::insert_score()` out of 16 questions]{style="color:green;"} correctly.

Are you confident that you can...?

-   [ ] Fit a linear regression model in `R` with multiple numeric and categorical predictors and interpret the intercept and predictor coefficients
-   [ ] Center numeric predictors to make the intercept more meaningful and improve model interpretability
-   [ ] Assess the importance of predictors using metrics like lmg
-   [ ] Visualise and interpret model predictions (with confidence bands) and partial residuals using the {visreg} library
-   [ ] Model and interpret interactions between predictors in multiple linear regression models, and visualise these interactions to see how one predictor moderates the effect of another on the outcome variable
