[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Analysis for the Language Sciences",
    "section": "",
    "text": "Preface\nLast updated on 25 February 2026",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-is-this-book-about",
    "href": "index.html#what-is-this-book-about",
    "title": "Data Analysis for the Language Sciences",
    "section": "What is this book about?",
    "text": "What is this book about?\nThis textbook is intended as a hands-on introduction to data management, statistics, and data visualisation for students and researchers in the language sciences. It relies exclusively on freely accessible, open-source tools, focusing primarily on the programming language and environment R.\nIt is often claimed that learning R is ‚Äúnot for everyone‚Äù, or that it has ‚Äúa steep learning curve‚Äù. This textbook aims to prove that the opposite is true. There are many reasons why it is worth investing the time and effort to learn how to do research in R, and it is no more difficult than learning any other new skill. In fact, the results of a recent study suggests that language aptitude is a much stronger predictor of programming aptitude than numeracy (i.e., ‚Äúbeing good at numbers‚Äù) (Prat et al. 2020). So if you have successfully learnt a foreign language in the past, there is no reason why you shouldn‚Äôt succeed in learning a programming language!\n\nLearning R is like learning a foreign language. If you enjoy learning languages, then ‚ÄòR‚Äô is just another one. [‚Ä¶] You have to learn vocabulary, grammar and syntax. Similar to learning a new language, programming languages also have steep learning curves and require quite some commitment. (Dauber 2024)\n\nThe rationale for this textbook is based on my personal observations, in both teaching and consulting, that many ‚Äòintroductory‚Äô textbooks to statistics and/or R are not suitable for many humanities students and researchers, who typically have little to no prior programming experience and for whom the word ‚Äústatistics‚Äù often evokes little more than unpleasant memories of school mathematics. It is worth stressing that is not a matter of generation (I have observed this phenomenon across all age groups), intelligence (I have taught people far more intelligent than me), or an innate inability to deal with numbers and/or computers (although these are beliefs that, sadly, some have deeply internalised). Instead, I am convinced that, for many people, it is simply a matter of finding a sturdy, first stepping stone and gathering up the courage to step on it to begin this learning journey.\nThe aim of this textbook is by no means to replace any of the brilliant, existing textbooks aimed at imparting statistical literacy for linguistics research, but rather to provide a stepping stone to be able to access these wonderful resources.1",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#who-is-this-book-for",
    "href": "index.html#who-is-this-book-for",
    "title": "Data Analysis for the Language Sciences",
    "section": "Who is this book for?",
    "text": "Who is this book for?\nThe target audience for this book are students and researchers in the language sciences, including (applied) linguistics, (first and second) language teaching, and language education research. All examples are taken from these research areas. Ultimately, however, this textbook may be of use to anyone who feels they could benefit from a maximally accessible stepping stone, whichever discipline they come from.\nThis textbook is intended to be read linearly, chapter by chapter. Apart from the first introductory chapter, all other chapters will require several hours of commitment. They include quizzes and short practical tasks. Completing these tasks is essential to genuinely assimilate the textbook‚Äôs contents. That‚Äôs because the best way to learn a new skill is to try things out. So, with this in mind, let‚Äôs get cracking!\n\n\n\n\n\n\nFigure¬†1: Artwork encouraging beginner R learners by @allison_horst.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#about-the-author",
    "href": "index.html#about-the-author",
    "title": "Data Analysis for the Language Sciences",
    "section": "About the author",
    "text": "About the author\nI started learning about statistics and R in 2017 when I realised that it would be important for me to conduct the kind of quantitative analyses that I wanted to do as part of my PhD in applied linguistics/English language teaching (Le Foll 2022). I had no previous experience in either and there were no such courses at my university. Even though I mostly learnt by myself, it would be incorrect to say that I am self-taught: I learnt from some of the resources listed in appendix of next-step resources, attended bootcamps and summer schools, read countless posts on StackOverflow and various blogs, and exchanged with like-minded people on social media (#Rstats, #dataviz, #TidyTuesday). This why it is probably fairer to say that I am community-taught.\nI now like to describe myself as an ‚Äúadvanced beginner‚Äù in R and statistics. I am not a programmer, nor a statistician, but rather an applied linguist and committed educator. I enjoy teaching data literacy, statistics, and data visualisation to current and future generations of linguists, language education scholars, and teachers. I teach regular methods courses at the University of Cologne that are attended not just by M.A.¬†and M.Ed. students, but also by some doctoral and post-doctoral researcher colleagues. In addition, I teach workshops for both doctoral and post-doctoral researchers at other institutions on a freelance basis.\nThis textbook was partly designed on the basis of materials that I have developed for these courses and workshops. Publishing these materials is my way to contribute to the wonderful community of people who have helped me on my leaRning journey. :hugging_face:Ô∏è\n\n\n\nMe back in 2017, proudly presenting at my first international conference.2",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Data Analysis for the Language Sciences",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis textbook has benefited greatly from the generous, critical feedback I have received from both novice and expert users of R throughout this project. Many thanks to my colleagues Nick Bearman, Marie Flesch, Ben Golub, Fritjof Lammers, Akira Murakami, and Sonja Eisenbei√ü for their friendly critical peer review and to my (former) students at the University of Cologne, Jan Hollmann, Rose H√∂rsting, Vishar Kavehamoli, Marie Kl√ºnter, Vijaya Lakshmi, Paula Raabe, Gina Reinhard, Matteo Schmelzer, Poppy Siahaan, Veronika Strobl, Clara Stumm, Katja Wiesner, Ali Yƒ±ldƒ±z, and Isabel Zimmer, for their highly valuable learner feedback on earlier drafts of various chapters of this textbook.\nSpecial thanks also go out to the researchers whose works are used as case studies in this textbook, in particular Sarah Schimke and Ewa DƒÖbrowska, and to Allison Horst whose beautiful and witty artworks illustrate many of the chapters of this textbook (e.g., Figure¬†1).\nIn addition, I would like to thank everyone who has contributed to my own data analysis learning journey. At the risk of forgetting someone, I would like to extend special thanks to Vaclav Brezina, Guillaume Desagulier, Stephanie Evert, Stefan Gries, Dani√´l Lakens, Natalia Levshina, Luke Tudge, Bodo Winter, the RLadies Stack group, the R package developers and maintainers of all the packages that I use, as well as the many generous contributors to Stack Overflow and to the #Rstats community on social media.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#get-in-touch",
    "href": "index.html#get-in-touch",
    "title": "Data Analysis for the Language Sciences",
    "section": "Get in touch! üì©Ô∏è",
    "text": "Get in touch! üì©Ô∏è\nIf (parts of) this textbook helped you on your leaRning journey or for your teaching, do drop me a line to let me know!\nIf you‚Äôve spotted an error or if you have any other suggestion to improve this resource, I would love to hear from you. ‚úâÔ∏èÔ∏è",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#a-word-about-the-license",
    "href": "index.html#a-word-about-the-license",
    "title": "Data Analysis for the Language Sciences",
    "section": "A word about the license üîìÔ∏è",
    "text": "A word about the license üîìÔ∏è\nThe online version of this textbook is published under a CC BY-NC-SA license, which means that these materials can be shared and adapted for free as long as the original source is cited, the use is non-commerical, and any adaptations are shared under the same or a compatible license.\nI have chosen this license because I explicitly object to tech companies scraping my work and reselling (mashed-up versions of) it as ‚ÄúAI‚Äù. If you work for a for-profit educational institution or are a freelance trainer who would like to use (parts of) this textbook, please send me a brief e-mail to explain your context of (re-)use; I will most likely be very happy to grant you permission to do so.\n\n\n\n\n\n\nNoteHow to cite this textbook\n\n\n\n\n\nPlease cite the current version of the web version of the textbook as:\n\n\nLe Foll, Elen. 2025. Data Analysis for the Language Sciences: A very gentle introduction to statistics and data visualisation in R. Open Educational Resource. https://elenlefoll.github.io/RstatsTextbook/ (accessed DATE).\n\n\nTo cite a specific passage, please quote the corresponding chapter or section number(s). Note that the case-study chapters include their how ‚Äúhow to cite‚Äù text boxes.\n\n\n\n\n\n\n\n\nDauber, Daniel. 2024. R for non-programmers: A guide for social scientists. Open Education Resource. https://bookdown.org/daniel_dauber_io/r4np_book/.\n\n\nLe Foll, Elen. 2022. Textbook English: A corpus-based analysis of the language of EFL textbooks used in secondary schools in France, Germany and Spain. Osnabr√ºck University PhD thesis. https://doi.org/10.48693/278.\n\n\nPrat, Chantel S., Tara M. Madhyastha, Malayka J. Mottarella & Chu-Hsuan Kuo. 2020. Relating natural language aptitude to individual differences in learning programming languages. Scientific Reports. Nature 10(1). 3817. https://doi.org/10.1038/s41598-020-60661-8.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Data Analysis for the Language Sciences",
    "section": "",
    "text": "A (work-in-progress) list of next-step resources can be found in the Appendix.‚Ü©Ô∏é\nI chose this picture because I vividly remember two professors pointing out that I had written ‚Äúp¬†=¬†0.00‚Äù on my poster (which I had copied-and-pasted from the output of the statistics software that I had used) and laughing among themselves ‚Äîbut well within earshot‚Äî at how stupid that was. Learning quantitative data analysis skills certainly requires a lot of effort on the part of the learner, but it also requires an academic culture that strives to include rather than exclude. This textbook explicitly aims for an inclusive approach to teaching the basics of data analysis in R and I have included this photo as a reminder to always persevere, whether in the face of seemingly insurmountable error message or snarky remarks! For those of you who are curious, a p-value can never equal exactly zero. But p-values can be extremely close to zero so that the value may be rounded off to 0.00. In this case, however, it is standard practice to report p &lt;¬†0.001.‚Ü©Ô∏é",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "1_OpenScholarship.html",
    "href": "1_OpenScholarship.html",
    "title": "1¬† Open Scholarship",
    "section": "",
    "text": "Chapter overview\nThis book aims to provide a stepping stone for students and scholars of traditionally less quantitative and computational disciplines to gather first (hopefully positive!) experiences with statistical and computational approaches to working with empirical data1. The underlying belief is that these methods ought to be accessible to all, regardless of their academic background or personal circumstances. To this end, this book embraces the principles of Open Scholarship.\nIn this chapter, you will learn about the relevance of Open Scholarship in learning how to manage, manipulate, analyse, and visualise research data. In doing so, the following aspects of Open Scholarship will be introduced:",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Open Scholarship</span>"
    ]
  },
  {
    "objectID": "1_OpenScholarship.html#sec-OpenScience",
    "href": "1_OpenScholarship.html#sec-OpenScience",
    "title": "1¬† Open Scholarship",
    "section": "1.1 Open Science",
    "text": "1.1 Open Science\nOpen Science is a major component of Open Scholarship and the two terms are frequently used synonymously. Open Scholarship, however, is broader in that it includes all kinds of knowledge, whereas Open Science focuses on what is conventionally considered ‚Äúscientific knowledge‚Äù. Open Science covers many different aspects including:\n\n\n\n\n\n\n\n\n\nOpen materials\nGiving free, unrestricted, public access to research materials in a way that allows others to replicate the results of published studies and to conduct new studies based on these existing materials.\nMaterials may include questionnaire items, all kinds of experimental stimuli, annotation schemes, inclusion and exclusion criteria, etc. (see Task 2 in Section 2.4).\n\n\n\n\nOpen data\nGiving free, unrestricted, public access to scientific data, whenever ethically and legally possible (see Berez-Kroeker et al. 2022). An important principle of Open Science is the sharing of FAIR data; that is data that are Findable, Accessible, Interoperable, and Reusable.\nIn Section 2.1, we will see that studies in the language sciences can involve many different types of data including texts, tables, images, and videos.\n\n\n\n\nOpen code\nMaking computer code freely and publicly available with appropriate documentation to make research methods and data analyses transparent.\nOpen code can include source code for custom software and packages, code for stimuli generation, data collection and processing, statistical analysis, and data visualisation. Sharing code allows for collaborations, while sharing both code and data allows others to reproduce published results.\n\n\n\n\nOpen access\nGiving free, unrestricted, public access to scientific outputs, foremost publications. Contrary to a frequent misunderstanding, authors or their institutions do not necessarily have to pay article processing fees (APCs) to publish their work in open access. Publishing open access can instead involve uploading a pre-copyedit version of a publication on a public repository (see Section 2.4) or publishing in a so-called ‚Äòdiamond‚Äô (typically non-profit) open access publication outlet (see section on Open Access in The Turing Way Community 2022).\n\n\n\n\n\nSharing research data allows us to reproduce the analyses reported in research publications based on the authors‚Äô original data and to test whether different analysis methods would have led to different conclusions. Sharing research materials and code means that we can replicate studies to check the robustness of published results and/or their generalisability across different populations. For example, if a journal article reports on the effectiveness of a new language teaching method based on a study conducted at a British university, we can test whether the same effect can be observed when replicating the study at a Nigerian university or an Indonesian secondary school.\nOpen Science advocates argue that scientific knowledge ‚Äú[should], where appropriate, be openly accessible, transparent, rigorous, reproducible, replicable, accumulative, and inclusive, all which are considered fundamental features of the scientific endeavour‚Äù (Parsons et al. 2022). This corresponds to an ideal that, although probably impossible to fully achieve, is nonetheless worth striving for at all times.\n\nOpen science consists of principles and behaviors that promote transparent, credible, reproducible, and accessible science. (Parsons et al. 2022)\n\nTo conduct open science, a sound understanding of data management and of effective data analysis workflows is crucial. This textbook aims to provide a gentle, practical introduction to these foundational skills using examples from the language sciences. Published as an Open Educational Resource (see Section 1.3), it showcases linguistics and Second Language Acquisition (SLA) publications that include open data, open code and/or materials and teaches data analysis using exclusively open-source software and programming languages (see Section 1.2).",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Open Scholarship</span>"
    ]
  },
  {
    "objectID": "1_OpenScholarship.html#sec-OpenSource",
    "href": "1_OpenScholarship.html#sec-OpenSource",
    "title": "1¬† Open Scholarship",
    "section": "1.2 Open Source",
    "text": "1.2 Open Source\nIn line with its aim to provide an accessible introduction to statistics and data visualisation, this textbook relies exclusively on open-source software and programming languages, foremost LibreOffice Calc, R and RStudio. Open source refers to software whose source code is available under a license that grants anyone the rights to study, modify, and distribute the software to anyone and for any purpose. If we think of a software application as a cake, the source code is like its recipe. It contains the list of ingredients and the steps to bake the cake. Open source means that the recipe is publicly available. You can access it, read it, and use it to bake the cake. You can also modify it to add your own twist, such as adding a new ingredient or making it vegan, and share it with others. In the context of software, this allows many people to collaborate, make improvements, and share their versions, resulting in better and more diverse software (see Figure¬†1.1).\n\n\n\n\n\n\nFigure¬†1.1: Open software development (CC-BY 4.0 Scriberia with The Turing Way community, DOI: 10.5281/zenodo.3332807)\n\n\n\nUsing open-source software in this introductory textbook means that anyone2 can download, install and use the required software at no cost. However, it is very important to note that not all free software (also called ‚Äòfreeware‚Äô) is open source.\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nThis quiz encourages you to do some quick internet searches to find out more about open-source software.\nQ1.1 Which of these is an open-source alternative to Microsoft Word?\n\n\n\n\n\nGoogle Docs\n\n\n\n\nPages\n\n\n\n\nLibreOffice Writer\n\n\n\n\n\n\n\n\n¬†\nQ1.2 Which of these is an open-source alternative to Microsoft Powerpoint?\n\n\n\n\n\nLibreOffice Impress\n\n\n\n\nGoogle Slides\n\n\n\n\nKeynote\n\n\n\n\n\n\n\n\n¬†\nQ1.3 Not only can software be open source, programming languages can, too. In fact, most modern programming languages are open source. In this book, we will focus on the open-source programming language R. Which of these is not an open-source programming language?\n\n\n\n\n\nJavaScript\n\n\n\n\nPython\n\n\n\n\nMATLAB\n\n\n\n\n\n\n\n\n¬†\nQ1.4 There are also many open-source operating systems. Which of these is an open-source alternative to the operating system Windows?\n\n\n\n\n\nMacOS\n\n\n\n\nUbuntu\n\n\n\n\niOS\n\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nYour first practical task is to download and install the open-source software suite LibreOffice. This is important as we will use its spreadsheet editor, LibreOffice Calc, in the following two chapters.\n\n\nLibreOffice is available for Windows, Mac and Linux. You can download it from here: https://www.libreoffice.org/download/download-libreoffice/.\nDetailed installation instructions can be found here: https://www.libreoffice.org/get-help/install-howto/.\nOn the official LibreOffice website you can choose either:\n\nthe latest version for ‚Äútechnology enthusiast, early adopter or power user‚Äù\nor the ‚Äúslightly older‚Äù but more tested version.\n\nIn drafting this textbook, I used the latest version which, at the time, was version 24.2.2. The one that you download will be higher than that as the developers regularly publish updates. If you already have LibreOffice installed on your computer, now is a good time to check that your version is up-to-date.\nDetailed documentation is available in many different languages: https://documentation.libreoffice.org/en/english-documentation/\n\n\n\n\n\n\n\n\n\n\nNoteGoing further\n\n\n\n\n\nIn this introductory textbook, we have simplified things considerably. To be considered open source, software distributions actually have to comply with ten criteria. You can read up on them here:\n\nhttps://opensource.org/osd\n\nTo find out more about the benefits of open-source software in the context of research, I recommend reading:\n\nhttps://book.the-turing-way.org/reproducible-research/open/open-source",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Open Scholarship</span>"
    ]
  },
  {
    "objectID": "1_OpenScholarship.html#sec-OpenEducation",
    "href": "1_OpenScholarship.html#sec-OpenEducation",
    "title": "1¬† Open Scholarship",
    "section": "1.3 Open Education",
    "text": "1.3 Open Education\nOpen Education is a key component of Open Scholarship (see Chapter 1). Open Education aims to stimulate collaborative teaching and learning and to provide high-quality Open Educational Resources (OERs) that are accessible for all.\nAs illustrated in Figure¬†1.2, OERs are licensed in such a way that everyone has the right to engage in ‚Äú5 Rs‚Äù when using OERs. The 5 Rs of OERs are:\n\nRetain - the right to make, own, and control copies of the content (e.g.¬†download, duplicate, and store copies of an OER).\nReuse - the right to use the content in a range of ways (e.g.¬†as teaching materials on a course, as part of a website, or in a video).\nRevise - the right to adapt, adjust, modify, or alter the content itself (e.g.¬†translate the content into another language, create a version for a different programming language).\nRemix - the right to combine the original or revised content with other open materials to create something new.\nRedistribute - the right to share copies of the original content, any revisions, and remixes with others (e.g.¬†give a copy of the content to a friend).\n\n\n\n\n\n\n\nFigure¬†1.2: OER sketch note CC-BY 4.0 Yvonne Stry\n\n\n\nOERs may be published under different licenses and, in engaging in the 5 Rs, the exact terms of an OER‚Äôs license must be respected. For example, the web-based version of this textbook is published as an OER under the Creative Commons license CC BY-NC-SA. This means that anyone can engage in the 5 Rs with it (i.e.¬†users are free to read and use, edit, remix, and expand upon the textbook) as long as:\n\nthe original author and source is mentioned (hence you should specify who this resource is BY),\nany derived version is not made into a commercial product (NC stands for non-commercial), and that\nany derived versions of this textbook (e.g.¬†a translated version or a version adapted for history scholars) are also shared with this same license (SA stands for share alike).\n\nIn line with the principles of Open Education, all of the datasets used as case studies in this textbook have been published in open access. We will analyse real data from published research studies in the fields of applied linguistics and language education to learn about data management, statistics, and data visualisation.\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nQ1.5 Is it possible to reuse Figure¬†1.2 on a company website?\n\n\n\n\n\nYes, but only after having obtained written consent from the artist.\n\n\n\n\nYes, but you must mention the image's sell-by date.\n\n\n\n\nYes, but only if it is the website of a non-profit company.\n\n\n\n\nYes, but you must mention the name of the artist.\n\n\n\n\n\n\n\n\n¬†\nQ1.6 Which of these resources can be published as OERs?\n\n\n\n\n\nSerious games\n\n\n\n\nCourse syllabi\n\n\n\n\nLecture slides\n\n\n\n\nLesson plans\n\n\n\n\nHomework tasks\n\n\n\n\nMassive Open Online Courses\n\n\n\n\nCourse assessments\n\n\n\n\nTextbooks\n\n\n\n\n\n\n\n\n¬†\n\n\n\n\n\n\n\n\n\nNoteGoing further\n\n\n\n\n\nThere are thousands of high-quality Open Educational Resources (OERs) out there, yet few people are aware of them. OER databases are good starting points to start exploring OERs, e.g.:\n\nhttps://oercommons.org/\nhttps://www.twillo.de/oer/web/\n\nAn appendix of next-step resources also lists recommended next-step OERs on data management, data analysis in R, statistics, data visualisation, Open Science, and reproducibility.\nIf you want to share your own research materials, data, or OER but you‚Äôre unsure about which license to use, this handy license chooser tool is a great starting point. In addition, librarians are usually very happy to advise students and researchers on these topics.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Open Scholarship</span>"
    ]
  },
  {
    "objectID": "1_OpenScholarship.html#check-your-progress",
    "href": "1_OpenScholarship.html#check-your-progress",
    "title": "1¬† Open Scholarship",
    "section": "Check your progress üåü",
    "text": "Check your progress üåü\nYou have successfully completed 0 out of 6 questions in this chapter.\nAre you confident that you can‚Ä¶?\n\nExplain the basic principles of Open Science (Section 1.1)\nFind out if software and programming languages are open-source or proprietary (Section 1.2)\nInstall open-source software on your own computer (Section 1.2)\nExplain the basic principles of Open Education (Section 1.3)\nWork out if you can reuse something that is licensed with a Creative Commons license (Section 1.3).\n\nThe next two chapters are devoted to research data management. While you may be keen to get cracking with data analysis in R, it is crucial that we first ensure that we understand what kind of research data we are dealing with, how and where they are saved, under which name, etc. otherwise nothing will work! Or at least not for very long‚Ä¶ :pensive_face:\n\n\n\n\nBerez-Kroeker, Andrea L., Bradley McDonnell, Eve Koller & Lauren B. Collister. 2022. The Open Handbook of Linguistic Data Management. MIT Press. https://doi.org/10.7551/mitpress/12200.001.0001.\n\n\nParsons, Sam, Fl√°vio Azevedo, Mahmoud M. Elsherif, Samuel Guay, Owen N. Shahim, Gisela H. Govaart, Emma Norris, et al. 2022. A community-sourced glossary of open scholarship terms. Nature Human Behaviour. Nature 6(3). 312‚Äì318. https://doi.org/10.1038/s41562-021-01269-4.\n\n\nThe Turing Way Community. 2022. The Turing Way: A handbook for reproducible, ethical and collaborative research (1.0.2). Zenodo. https://doi.org/10.5281/zenodo.3233853.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Open Scholarship</span>"
    ]
  },
  {
    "objectID": "1_OpenScholarship.html#footnotes",
    "href": "1_OpenScholarship.html#footnotes",
    "title": "1¬† Open Scholarship",
    "section": "",
    "text": "Empirical data are based on what is experienced or observed rather than on theory alone.‚Ü©Ô∏é\nProvided that they have access to the internet and a functioning personal computer.‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Open Scholarship</span>"
    ]
  },
  {
    "objectID": "2_Data.html",
    "href": "2_Data.html",
    "title": "2¬† Data files and formats",
    "section": "",
    "text": "Chapter overview\nThis chapter first considers what data means in the context of language research, before turning to how these data are formatted and stored. You will learn about:\nAlong the way, you will get insights into an eye-tracking study involving cute Playmobil figures and a meta-science investigation that highlights the utmost importance of data literacy for research.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data files and formats</span>"
    ]
  },
  {
    "objectID": "2_Data.html#sec-DataLanguageSciences",
    "href": "2_Data.html#sec-DataLanguageSciences",
    "title": "2¬† Data files and formats",
    "section": "2.1 Data in the language sciences",
    "text": "2.1 Data in the language sciences\nIn this book, we are concerned with empirical research in the language sciences, in other words, with research that is based on the analysis of data. But what are data exactly? Data can be collected via surveys, measurements, or observations. To begin with, however, these collected datasets are ‚Äúraw‚Äù. Data only becomes information once we have analysed and interpreted the data in a meaningful way. Hence, just like uncooked pasta does not make a flavourful meal, we must learn to ‚Äúcook‚Äù the raw data to obtain meaningful information.\nWhat kind of data are analysed in the language sciences? To get a rough idea of the range of data types analysed in the language sciences, let us take a look at the IRIS database.\n\nIRIS is a collection of instruments, materials, stimuli, data, and data coding and analysis tools used for research into languages, including first, second, and beyond, and signed language learning, multilingualism, language education, language use, and language processing. Materials are freely accessible and searchable, easy to upload (for contributions) and download (for use). (iris-database.org 2011)\n\nAs such, IRIS supports Open Science and Open Scholarship (see Chapter 1).\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nIn this task and many future tasks, we will make use of the IRIS database.\n\nConnect to the IRIS website and navigate to its Search and Download page.\nScroll down to the filter option ‚ÄòData Type‚Äô.\nClick on ‚ÄòData Type‚Äô and browse through the different data types that are most commonly used in language-related research.\n\n\n\n\n\n\n\nFigure¬†2.1: Screenshot from the IRIS database search page (accessed on 17 April 2024)\n\n\n\nQ2.1 For which kinds of studies could these different types of data have been collected? Think about both experimental and observational studies.\nQ2.2 Which of these data types is most likely to be measured in milliseconds (ms)?",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data files and formats</span>"
    ]
  },
  {
    "objectID": "2_Data.html#sec-ResearchData",
    "href": "2_Data.html#sec-ResearchData",
    "title": "2¬† Data files and formats",
    "section": "2.2 Types of research data",
    "text": "2.2 Types of research data\nGiven the wide range of methods used in language research, it is no surprise that they are so many different types of research data (see e.g. Good 2022). Although the data types listed on the IRIS search page (see Figure¬†2.1 for extract) are very broad and the categories not clearly defined, the list illustrates the breadth of research data types typically analysed in language studies.\nThe first data type category, ‚ÄúOral production‚Äù, for instance, can equally refer to text transcriptions of language users‚Äô oral production, audio, or video files. It can also refer to either raw data or to (more or less) processed data. For example, a transcript of a conversation could have been automatically annotated for part-of-speech, meaning that every word would be marked for their word class (e.g.¬†This_DT is_VBZ not_RB raw_JJ text_NN data_NN ._PUNC), or it could have been manually anonymised by adding placeholders (e.g.¬†Is &lt;NAME&gt; going out with &lt;NAME&gt;?) indicating that certain words have been retracted for data protection reasons.\nThe second most frequent data type category, ‚ÄúClosed response format‚Äù, includes different kinds of questionnaires and tests. Questionnaires may ask study participants to disclose personal information relevant to the research questions using single or multiple-choice questions, such as what language(s) they use at home, how long they have studied a language for, or how old they are. Tests may be designed to assess participants‚Äô language competences (e.g.¬†in the form of a vocabulary or grammar test), as well as other aspects relevant to the research questions being investigated (e.g.¬†short-term memory or baseline reaction times).\nIn this book, we will focus on the research processes that take place after the data have been collected. However, it is vital that we are aware of the conditions and context in which the data we are analysing were collected and pre-processed. It is no exaggeration to say that these steps in the research process can entirely change the results of the data analysis. Suppose we decide to compare the abilities of two groups of French L2 learners. To do this, we administered a language production test to two whole classes of secondary school students learning French as a second language using two different teaching methods. If one group had 15 minutes to complete the test and the other had up to 60 minutes, the results would not be comparable.\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nQ2.3 Which other reasons could potentially jeopardise the comparison of test results data from two different groups of pupils?\n\n\n\n\n\nOne group having a classroom decorated with French flags and posters about France.\n\n\n\n\nOne group having French lessons on Tuesday mornings, the other on Friday afternoons.\n\n\n\n\nOne group having a single native speaker of French, whilst the other has none.\n\n\n\n\nOne group having a higher proportion of pupils from migrant families.\n\n\n\n\nThe two groups having different teachers.\n\n\n\n\nOne group having a higher proportion of pupils with reading difficulties.\n\n\n\n\n\n\n\n\n\n\n\nWhilst there are many ways to ensure that as many factors as possible are controlled for, not all can be controlled for. What is crucial is that all aspects of the data collection process are well documented so that all factors, whether controlled or not, can be taken into account when analysing the data.\nIn research, we usually distinguish between primary data, which are the data that you collected yourself, and secondary data, which are data that were collected by others. Hence if you were to carry out a new study based on data that you found on IRIS, you would be conducting a secondary data analysis. Especially when conducting secondary data analyses, it is crucial that we have enough information about the data itself, i.e.¬†metadata. Metadata is crucial for finding, sharing, evaluating, and reusing datasets (Trippel 2025). For some data and projects, it makes sense to create separate metadata files that contain additional or more detailed information about the collected data. For language-related data, various metadata tools and standards exist (see e.g. Paquot et al. 2024; Consortium 2025; Windhouwer & Goosen 2022; Withers 2012) and it makes sense to try to stick to these standards as far as possible to ensure greater comparability and comparability across different datasets (for more information on how to develop a Data Management Plan for a linguistics study, see Kung 2022).",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data files and formats</span>"
    ]
  },
  {
    "objectID": "2_Data.html#sec-FileExtensions",
    "href": "2_Data.html#sec-FileExtensions",
    "title": "2¬† Data files and formats",
    "section": "2.3 Data formats and file extensions",
    "text": "2.3 Data formats and file extensions\nDifferent data types come in different data formats. For audio files, you may be familiar with the MP3 format, but this is by no means the only format in which audio files can be saved. Many other audio file formats exist, such as Waveform Audio File Format (WAVE) and Free Lossless Audio Codec (FLAC).\nWe can usually tell in what format a file is in by looking at its file extension. The file extension is the suffix of the file name. It comes at the end of the file name and is preceded by a dot. The file extension of a WAVE file is .wav, whereas that of an MP3 file is .mp3; hence the file recording.wav is a WAVE file, whereas recording.mp3 is an MP3 file.\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nQ2.4 In which format are Microsoft Word files typically saved?\n\n\n\n\n\n.docx\n\n\n\n\n.odt\n\n\n\n\n.msword\n\n\n\n\n.docs\n\n\n\n\n\n\n\n\n¬†\nQ2.5 Which of these files are audio files?\n\n\n\n\n\ndialog_01.csv\n\n\n\n\n001_dialog.flac\n\n\n\n\ndialogue001.mp3\n\n\n\n\nDIALOGUE.audio\n\n\n\n\ndialogue001.wav\n\n\n\n\n\n\n\n\nüòá Hover for a hint\n\n\n\n\n\n\n\nUnfortunately, many modern operating systems have a tendency to hide file extensions by default. This results in the files recording.wav and recording.mp3 both being displayed as recording in File Finder/Explorer windows (compare Figure¬†2.2 (a) and Figure¬†2.2 (b)). This is misleading and can lead to all kinds of problems.\n\n\n\n\n\n\n\n\n\n\n\n(a) Displaying file names without file extensions\n\n\n\n\n\n\n\n\n\n\n\n(b) Displaying file names with their extensions\n\n\n\n\n\n\n\nFigure¬†2.2: Demonstrating the importance of seeing file extensions.\n\n\n\nTo ensure that you can always see the extensions of the files on your computer in the File Explorer (on Windows) or the File Finder (on macOS), follow these instructions:\n\nOn Windows: https://www.howtogeek.com/205086/beginner-how-to-make-windows-show-file-extensions/.\nOn macOS: https://support.apple.com/en-gb/guide/mac-help/mchlp2304/mac (select the version of your operating system at the top of the page).",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data files and formats</span>"
    ]
  },
  {
    "objectID": "2_Data.html#sec-Sharing",
    "href": "2_Data.html#sec-Sharing",
    "title": "2¬† Data files and formats",
    "section": "2.4 Sharing research data and materials",
    "text": "2.4 Sharing research data and materials\nIn line with the principles of Open Science (see Chapter 1), it is important to ensure that both the materials that were used to collect research data (e.g.¬†questionnaire items, audio, image or video stimuli, language aptitude tests, etc.) and the data themselves are made openly available to the research community, whenever legally possible and ethically responsible. Sharing materials ensures that studies can be replicated, for example with new participants or in a different language. Sharing research data also allows independent researchers to reproduce the results of studies, allowing them to verify the reported results and to conduct additional analyses that may confirm, contradict, or extend the conclusions of the original studies.\nYou may be wondering how linguists and language education researchers can make their research data and materials publicly available. Table¬†2.1 provides a non-exhaustive list of public repositories where researchers can upload research data and materials (with figures collected in early June 20241). Some are specific to the language sciences, while others cater to all research disciplines. All of the examples, tasks, and exercises in this book are based on research data and materials that researchers have made available in open access on one or more of these repositories.\n\n\n\n\nTable¬†2.1: Non-exhaustive list of public repositories of research data and materials.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRepository\nDiscipline\nNb. of entries\nProvides DOI\nOnline since\n\n\n\n\nDryad\nAll\n60000\nYes\n2008\n\n\nFigshare\nAll\n8000000\nYes\n2012\n\n\nHAL\nAll\n5000000\nNo\n2001\n\n\nHarvard Dataverse\nAll\n160000\nYes\n2006\n\n\nIRIS\nLinguistics\n3500\nNo\n2011\n\n\nOpen Science Repository, OSF\nAll\n153663\nYes\n2012\n\n\nTroms√∏ Repository of Language and Linguistics, TROLLing\nLinguistics\n4500\nYes\n2014\n\n\nVivil\nClinical research\n7000\nYes\n2013\n\n\nZenodo\nAll\n3750000\nYes\n2013\n\n\n\n\n\n\n\n\n\nIn the following Your Turn! sections, we will look at a study by Schimke et al. (2018) (see Figure¬†2.3 (a)), which is an example of a publication which was awarded the Open Data and the Open Materials badges (see Figure¬†2.3 (b)). This means that the research materials and data associated with this study can be found in an open, online repository:\n\n‚ÄúThis article has been awarded Open Materials and Open Data badges. All materials and data are publicly accessible via the IRIS Repository at https://www.iris-database.org/iris/app/home/detail?id=york:934337. Learn more about the Open Practices badges from the Center for Open Science: https://osf.io/tvyxz/wiki‚Äù Schimke et al. (2018).\n\nThe authors could have chosen to upload their materials and data to any of the online repositories listed in Table¬†2.1 but, in this case, they chose IRIS.\n\n\n\n\n\n\n\n\n\n\n\n(a) Title page of the Schimke et al. (2018)\n\n\n\n\n\n\n\n\n\n\n\n(b) The Open Data and Open Materials badges\n\n\n\n\n\n\n\nFigure¬†2.3: An example of a publication for which both research materials and data have been published.\n\n\n\nAmong other results, Schimke et al. (2018) report on two eye-tracking experiments. One of these experiments involved Spanish-speaking participants listening to ambiguous sentences in Spanish whilst looking at images of Playmobil figures (see Figure¬†2.4 for an example).\n\n\n\n\n\n\nFigure¬†2.4: Image from Experiment 1 in Schimke et al. (2018)\n\n\n\n\n\n\n\n\n\nNote¬†2.1: How did the experiment work?\n\n\n\nIn this eye-tracking experiment, participants were instructed to decide whether the sentences they heard matched the Playmobil images or not. Consider the following two sentences from the experiment:\n\n\nEl barrendero se encontr√≥ con el cartero antes de que recogiera las cartas.\n[The street sweeper met the postman before he fetched the letters.]\nEl barrendero se encontr√≥ con el cartero antes de que recogiera la escoba.\n[The street sweeper met the postman before he fetched the broom.]\n\n\nUp until the point at which either las cartas [the letters] or la escoba [the broom] are heard, it is unclear who is doing the fetching. From a grammatical point of view, it could be either the street sweeper or the postman.\nParticipants were presented with Figure¬†2.4 as they were listening to either Sentence 1 or Sentence 2. At the same time, the researchers measured how long it took for the participants to look at the subject governing the verb recogiera. In other words, for Sentence 1, they were interested in how long it took participants to focus on the postman Playmobil figure and, in Sentence 2, on the street sweeper. Such fine measurements are made in milliseconds, i.e.¬†in thousandths of seconds, using a special eye-tracking device.\n\n\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nImagine that you want to run an experiment similar to the one carried out in Schimke et al. (2018). You can reuse the Playmobil image files created by the researchers as they helpfully uploaded them to the IRIS database.\nIn which file format do you think the images are archived? To find out, click here to go directly to the list of data and materials associated with the study. There are four entries in the IRIS database that are associated with this study. Select the ‚ÄúPictorial‚Äù entry which contains the images. It allows you to download a ZIP file called Images_online.zip. ZIP is an archive file format that can contain one or more compressed files. Download this ZIP file.\nOnce the download was successful, navigate to the folder where the file was saved on your computer and unzip the file, i.e.¬†decompress it and extract its contents:\n\nTo unzip on Windows, double-click the .zip file\n\nselect ‚ÄòExtract All‚Äô,\nselect a folder,\nand then click ‚ÄòExtract‚Äô.\n\nOn a Mac, simply double-click the .zip file to unzip it.\nIf you are using the Linux command line, use the command unzip followed by the name of the file to unzip it.\n\nYou should find that the ZIP file contains a folder entitled ‚ÄòImages‚Äô, which contains 58 pictures of different combinations of Playmobil figures that correspond to the experiment‚Äôs stimulus sentences.\nQ2.6 In which file format are these Playmobil image files?\n\n\n\n\n\nBMP\n\n\n\n\nGIMP\n\n\n\n\nJPEG\n\n\n\n\nPNG\n\n\n\n\nGIF\n\n\n\n\n\n\n\n\n¬†\nImage files typically contain metadata that is embedded in the image files themselves. This metadata may include the dimensions of the image and its colour profile. To view this metadata, right-click on one of the image files that you have extracted from the ZIP file and select the option to get more information about the file, e.g.¬†‚ÄúGet Info‚Äù or ‚ÄúProperties‚Äù.\nQ2.7 How wide are these Playmobil images in pixel?",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data files and formats</span>"
    ]
  },
  {
    "objectID": "2_Data.html#sec-TabularData",
    "href": "2_Data.html#sec-TabularData",
    "title": "2¬† Data files and formats",
    "section": "2.5 Working with tabular data",
    "text": "2.5 Working with tabular data\nThe measurements made by the eye-tracking device in Schimke et al. (2018)‚Äôs eye-tracking experiments were stored in the form of tables. Table¬†2.2 is an extract of a table that contains processed eye-tracking data from Schimke et al. (2018). It forms part of the study‚Äôs supplementary materials and can also be downloaded from the IRIS database.\nIn this table, each row corresponds to the data associated with one participant‚Äôs eye movements while listening to a single stimulus sentence and looking at the corresponding Playmobil image (e.g. Figure¬†2.4). The extract displayed as Table¬†2.2 only shows the data associated with the first six stimulus sentences (items) that participant ‚Äús1‚Äù, a Spanish L2 learner, listened to. The columns crit1, crit2 and crit3 contain values derived from the measurements made using the eye-tracking device.2 From Table¬†2.2, we can also see that participant ‚Äús1‚Äù was 19 years old when they started formally learning Spanish (AoO stands for ‚Äúage of onset of formal instruction‚Äù) and that they were 20 when the experiment was conducted.\n\n\n\n\nTable¬†2.2: Extract of table containing eye-tracking data from Schimke et al.¬†(2018)'s appendix\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlanguage\nsubject\ndisambiguation\nitem\ncrit1\ncrit2\ncrit3\nAoO\nage\n\n\n\n\nS\ns1\n1\n1\n0.3451355\n-0.5618789\n0.7036070\n19\n20\n\n\nS\ns1\n2\n2\n-0.2679332\n-1.5849625\n0.1852149\n19\n20\n\n\nS\ns1\n1\n3\n-1.1563420\n0.9898042\n-1.5849625\n19\n20\n\n\nS\ns1\n2\n4\n-1.5849625\n-0.0874628\n-1.5849625\n19\n20\n\n\nS\ns1\n1\n5\n1.5849625\n0.1831223\n1.5849625\n19\n20\n\n\nS\ns1\n2\n6\n-0.7824086\n-0.8548021\n-1.1758498\n19\n20\n\n\n\n\n\n\n\n\n\nWhen working with data, tables are ubiquitous. Data stored in tables are called tabular data. Hence, learning to work with tabular data is a crucial data literacy skill.\nIn the language sciences, the results of most studies (whether experimental or corpus studies) are stored in tables. For example, when researchers conduct an online survey, the data collected by the online survey platform (e.g.¬†Qualtrics, LimeSurvey, SoSci Survey) are automatically stored in the form of one or more table(s). These can then be exported from the survey platform in various tabular file formats (e.g.¬†.csv, .json, .xlsx).\nIn some cases, data may be collected by analogue means, e.g.¬†by getting participants to answer a paper questionnaire or collecting school children‚Äôs work on paper. However, for quantitative analysis, analogue research data are first digitalised. Then, the data are typically stored as text files in file formats such as .txt or .csv.\n\n2.5.1 Delimiter-separated values (DSV) files\nTables can be stored in many data formats but the simplest and most widely used in linguistic research are text files with delimiter-separated values (DSV). For sharing and archiving research data, DSV files are favoured over formats specific to propriety software such as .xslx (Microsoft Excel files) or .numbers (Apple Numbers files). This is because DSV files can be ‚Äúunderstood‚Äù by many different programs and on all operating systems. The fact that they are simple text files means that we will also be able to reliably read them in the future, even if programs such as Excel or Numbers have evolved or have been discontinued. Reliability and compatibility are fundamental to maintaining the integrity of research data and ensuring that data can be reused, even in the distant future.\nIn DSV files, each value (e.g.¬†measurement or response) is separated by a specific separator character. In principle, any character can be used to separate values, but the most common separators are the comma (,), tab (\\t), colon (:), and semicolon (;). Below is the .csv file corresponding to Table¬†2.1.\nRepository,Discipline,Nb. of entries,Provides DOI,Online since\nDryad,All,60000,Yes,2008\nFigshare,All,8000000,Yes,2012\nHAL,All,5000000,No,2001\nHarvard Dataverse,All,160000,Yes,2006\nIRIS,Linguistics,3500,No,2011\n\"Open Science Repository, OSF\",All,153663,Yes,2012\n\"Troms√∏ Repository of Language and Linguistics,TROLLing\",Linguistics,4500,Yes,2014\nVivil,Clinical research,7000,Yes,2013\nZenodo,All,3750000,Yes,2013\nAs you can see, the values are separated by commas.3 Additionally, some of the values are enclosed in, or delimited by, double quotation marks (\"). This prevents any commas that may occur within an actual field value, e.g.¬†the comma in the field Open Science Repository, OSF, from being interpreted as a separator character.\nGiven that DSV files are text files, it is possible to open them in a free plain-text editor (e.g.¬†Notepad++ or BBEdit) or a text-processing program (e.g.¬†Microsoft Word or LibreOffice Writer). However, these programmes will typically display DSV files as in Figure¬†2.5.\n\n\n\n\n\n\nFigure¬†2.5: The .csv file corresponding to Table¬†2.1 opened in Microsoft Word\n\n\n\nWe can probably agree that what we are seeing in Figure¬†2.5 is not a very reader-friendly way to display tabular data! This is why DSV files are more often opened in spreadsheet programs (e.g.¬†LibreOffice Calc, Google Sheets, Microsoft Excel, Numbers) than in text-editing programs. Let‚Äôs find out how in the next section.\n\n\n2.5.2 Opening DSV files in LibreOffice Calc\nThere are several ways to open a DSV file in LibreOffice Calc but the safest is to launch LibreOffice (see Task 1.1 in Section 1.2 if you have not yet installed LibreOffice) and, from the list of options under ‚ÄòCreate‚Äô, click on ‚ÄòCalc Spreadsheet‚Äô to open up a blank spreadsheet. Then, from the ‚ÄòFile‚Äô drop-down menu, select ‚ÄòOpen‚Ä¶‚Äô or use the keyboard shortcut Ctrl/Cmd + O and locate the DSV file that you wish to open.\nOn opening a DSV file in LibreOffice Calc, we get a dialogue box with various options (see Figure¬†2.6).\n\n\n\n\n\n\nFigure¬†2.6: Text import dialogue in LibreOfficeCalc\n\n\n\nTo correctly import this particular DSV file, it is necessary to specify that the separator character is the comma (,) and that the delimiter character is the double quotation mark (\") (see selected options in Figure¬†2.6). With these settings in LibreOffice Calc, the table is rendered as in Figure¬†2.7.\n\n\n\n\n\n\nFigure¬†2.7: CSV file opened in LibreOffice Calc\n\n\n\nNote that if you open a DSV file in Excel or Google Sheets, you will not be shown such a dialogue box. Instead, these programs assume that they can guess which separator and delimiter characters your file uses. Whilst this may, at first, sound convenient, this is not good news: you should be the one in control of how your data files are interpreted, not the program! In the next section, you will learn why opening DSV files such as .csv and .tsv files in Microsoft Excel, Google Sheets, or Numbers can be very dangerous. In some cases, these programs will ‚Äòcorrupt‚Äô, i.e.¬†permanently damage, your DSV files, which can lead to irreversible data loss!\nThe bad news is that, if you are using Windows or MacOS, it is very likely that either Excel or Numbers is your default app to open DSV files. This means that if you double click on a .csv and .tsv file in your Finder/Explorer window, the file will likely automatically open up in either Excel or Numbers. This is why it is important you do not double-click on such files to open them: Opening a file just once with these programs can lead to data loss! If this happens to you with a file that you have downloaded from a repository, your best bet is to delete your local version of the file and download a fresh version so that you can start again from scratch.\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nIn this task, we will practice opening a DSV file in LibreOffice Calc. Our example file is a real dataset from Schimke et al. (2018). We will begin by downloading it from the public repository IRIS.\nIn addition to the eye-tracking experiments, Schimke et al. (2018) conducted two further experiments in which participants completed a gap-filling task via an online survey platform. In the first of these experiments, the participants were native (L1) speakers of French, German, and Spanish. In the second, they were French- and Spanish-speaking learners (L2) of German.\nIn both experiments, the L1 and L2 participants were shown ambiguous sentences similar to the ones used in the eye-tracking experiment with the Playmobil images (see Note¬†2.1). After having read each stimulus, the participants were asked to complete a gap-fill task according to their understanding of the preceding ambiguous sentence. Participants were told ‚Äúthat there were no incorrect responses and that they should answer spontaneously‚Äù (Schimke et al. 2018: 755). Below is an example questionnaire item in the three languages examined:\n\n\n1. Der Brieftr√§ger ist dem Stra√üenfeger begegnet, bevor er schnell ein Sandwich geholt hat. ___________________ hat ein Sandwich geholt.\n2. Le facteur a rencontr√© le balayeur avant qu‚Äôil prenne rapidement un sandwich. ___________________ a pris un sandwich.\n3a. El cartero se reuni√≥ con el barrendero antes de que √©l recogiera velozmente un emparedado. ___________________ recogi√≥ un emparedado.\n3b. El cartero se reuni√≥ con el barrendero antes de que recogiera velozmente un emparedado. ___________________ recogi√≥ un emparedado.\n\n\nNote that, for Spanish, there were two types of stimuli: one with an overt pronoun (as in 3a. with √©l) and one without (as in 3b. with a null pronoun), as both variants are possible in Spanish. All three examples translate as:\n\n\nThe postman encountered the street sweeper before he quickly fetched a sandwich. ___________________ fetched a sandwich.\n\n\nTo complete the gap, participants could either select ‚ÄòThe postman‚Äô or ‚ÄòThe street sweeper‚Äô.\n\nGo back to the study‚Äôs page on IRIS and select the second entry entitled ‚ÄòOther questionnaire‚Äô which, among other things, contains ‚ÄòWritten production data‚Äô.\n\nNote that this database entry includes both research data and research materials: the file sentences_offline_task.xlsx contains the full list of questionnaire items, including both experimental and filler items, with which we could reconstruct the experiment to replicate it with a new set of participants. For now, however, we are not interested in obtaining materials to replicate the study, but rather in examining the study‚Äôs original data.\nThis IRIS entry also contains three data files. The last file (logoddslearnersfinal.txt) is the DSV file that was used to create Table¬†2.2 above.\nIn this task, we are going to look at the questionnaire data corresponding to the gap-filling task experiment conducted with German L2 learners, which is contained in the data file offlinedataLearners.txt:\n\nDownload the offlinedataLearners.txt file (which is the second listed) and save it on your computer (see Section 3.3).\nLaunch LibreOffice (see Section 1.2 if you have not yet installed LibreOffice) and, from the list of options under ‚ÄòCreate‚Äô, click on ‚ÄòCalc Spreadsheet‚Äô to open up a blank spreadsheet.\nFrom the ‚ÄòFile‚Äô drop-down menu, select ‚ÄòOpen‚Ä¶‚Äô or use the keyboard shortcut ‚ÄòCtrl/Cmd + O‚Äô. Find offlinedataLearners.txt in the folder where you saved it and click on ‚ÄòOpen‚Äô.\nA ‚ÄòText Import‚Äô dialogue box will pop up. This a DSV file, not a fixed-width file, so ensure that the option ‚ÄòSeparated by‚Äô is selected. If not already set by default, it is also a good idea to select ‚ÄòUnicode (UTF-8)‚Äô for the ‚ÄòCharacter set‚Äô.\nExperiment with the different ‚ÄòSeparator Options‚Äô until the preview at the bottom of the dialogue box looks like a table.\nEnsure that, apart from the ‚ÄòSeparator Options‚Äô, all other options in the dialogue box are unselected and then click on ‚ÄòOK‚Äô.\n\nQ2.8 What is the separator character in the file offlinedataLearners.txt?\n\n\n\n\n\nTab\n\n\n\n\nComma\n\n\n\n\nSemicolon\n\n\n\n\nSpace\n\n\n\n\nAll of them\n\n\n\n\n\n\n\n\n¬†\nQ2.9 What is the delimiter character in the file offlinedataLearners.txt?\n\n\n\n\n\n'\n\n\n\n\n\"\n\n\n\n\nBoth \" and '\n\n\n\n\nThere is none.\n\n\n\n\n\n\n\n\n¬†\nQ2.10 How many observations does the file offlinedataLearners.txt contain?\n\n\n\n\n\n3505\n\n\n\n\n701\n\n\n\n\n3500\n\n\n\n\n5\n\n\n\n\n700\n\n\n\n\n\n\n\n\n¬†\nQ2.11 In this table, what does each observation correspond to?\n\n\n\n\n\nAll the responses from a single participant.\n\n\n\n\nAll the responses in a single language.\n\n\n\n\nAll the responses to a single sentence in a single language.\n\n\n\n\nA single participant's response to a single sentence gap.\n\n\n\n\n\n\n\n\n¬†\n\n\n\n\n\n\n\n\n\nImportantWhat if I absolutely have to open a DSV file in Excel? üò∞\n\n\n\n\n\nIf you absolutely must open a DSV file (e.g.¬†a .csv or .tsv file) in Excel (for example because you do not have sufficient permissions to install LibreOffice on the computer that you are using), do not open the file by double clicking on the file as this will automatically trigger Excel‚Äôs problematic auto-formatting behaviour (see Section 2.6)! Instead, first launch Excel and create a new blank workbook. Then navigate to the ‚ÄòData‚Äô tab, select the ‚ÄòGet Data‚Äô option, and then ‚ÄòFrom Text/CSV‚Äô (see Figure¬†2.8). In the following dialogue, you can specify how the data should be imported. The options are very similar to the ones offered in LibreOffice (see above).\nNote that with this method it may be possible to prevent Excel from automatically (and irreversibly!) applying transformations to your data. However, sadly, this may not suffice. Read on to find out more‚Ä¶\n\n\n\n\n\n\nFigure¬†2.8: Import data into excel",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data files and formats</span>"
    ]
  },
  {
    "objectID": "2_Data.html#sec-ExcelWarning",
    "href": "2_Data.html#sec-ExcelWarning",
    "title": "2¬† Data files and formats",
    "section": "2.6 A word of warning about spreadsheet programs ‚ö†Ô∏èÔ∏è",
    "text": "2.6 A word of warning about spreadsheet programs ‚ö†Ô∏èÔ∏è\nYou should be aware that opening DSV files in spreadsheet programs can corrupt the files! Once a file is corrupted, it is often not possible to retrieve the original data so this is very bad news, indeed. Such problems are particularly frequent when opening DSV files with Microsoft Excel and Google Sheets. This is because the default settings in these programs surreptitiously modify files upon opening.\nThese ‚Äòauto-format‚Äô modifications include replacing certain values by dates (e.g.¬†changing 3-4 to March, 4th) or numbers (e.g.¬†changing 1.23E5 to 123000)4, removing leading zeros (e.g.¬†changing 001 to 1), or misinterpreting certain characters (e.g.¬†the value -ism will generate an error because the hyphen is interpreted as minus sign).\nNot only can these auto-format modifications lead to inaccurate data analysis but, in the worst of cases, they can even cause data loss. The crux of the problem is that often users do not realise what the program has done in the background. How bad can this be? Find out by completing the task below.\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nIn this task, you will find out how genetics researchers who use spreadsheets for their analyses regularly have their data so badly damaged that it affects the results of their publication. Though we have no statistics on how spreadsheet errors affect the work of linguists, it is (unfortunately) very likely to be just as bad as in genetics.\nZiemann, Eren & El-Osta (2016) reported that a fifth of genetics publications with supplementary .xls or .xlsx files with gene lists contained errors caused by Excel‚Äôs auto-formatting behaviour. The results of this study shocked the research community and a report about it went viral. Click on the link below to read the open-access article ‚ÄúGene name errors: Lessons not learnt‚Äù by Abeysooriya et al. (2021) to find out whether the situation has improved since 2016 and answer the questions below.\n\nAbeysooriya, Mandhri, Megan Soria, Mary Sravya Kasu & Mark Ziemann. 2021. Gene name errors: Lessons not learned. PLOS Computational Biology. Public Library of Science 17(7). e1008984. https://doi.org/10.1371/journal.pcbi.1008984.\n\nQ2.12 Has the proportion of genetics publications with Excel gene lists affected by auto-formatting errors decreased since 2016?\n\n\n\n\n\nNo, it increased between 2016 and 2020.\n\n\n\n\nNo, it has remained stable.\n\n\n\n\nYes, it decreased between 2016 and 2020.\n\n\n\n\n\n\n\n\n¬†\nQ2.13 Does using LibreOffice Calc (see Section 1.2) also cause these same issues?\n\n\n\n\n\nYes, LibreOffice is just as likely to cause such errors.\n\n\n\n\nNo, if you cannot afford Excel, then LibreOffice Calc is an excellent open-source alternative.\n\n\n\n\nNo, but whilst LibreOffice is better than Excel or Google Sheets, it is still less than ideal for data analysis.\n\n\n\n\n\n\n\n\n¬†\nQ2.14 Did highly reputable journals publish fewer articles with erroneous Excel gene lists?\n\n\n\n\n\nNo, publications with problematic Excel files were found in more or less equal proportions in all journals.\n\n\n\n\nYes, they published fewer.\n\n\n\n\nNo, they published more.\n\n\n\n\n\n\n\n\n¬†\n\n\n\nIt is worth noting that, for some Windows users, these auto-formatting issues can corrupt files that they have never actively opened in Excel! ü§ØÔ∏è This happens when Windows applies Excel‚Äôs default settings to all CSV files, regardless of what program they are actually opened with. To ensure that this does not happen to you, check that Excel is definitely not your default app to open .csv and .tsv files (see below for instructions).\n\n\n\n\n\n\nTipOpening a .csv or .tsv file in LibreOffice from a File Finder/Explorer window\n\n\n\n\n\nRemember that to open a .csv or .tsv file on your computer, should never ever double-click on it and let the default program open it! As we saw in Section 2.6, this can break or ‚Äòcorrupt‚Äô the file. To avoid accidentally double-clicking on a .csv or .tsv file and having the file corrupted, I recommend making either LibreOffice or a plain-text editor (e.g.¬†Notepad++ or BBEdit) your default application to open up such files.\nOn MacOS, you can change the default application used to open files of any file extensions by right-clicking a file name with this particular extension and than selecting ‚ÄòGet Info‚Äô (Figure¬†2.9 (a)). In the example below, Numbers is the default application for all .csv files (see Figure¬†2.9 (b)). In the dropdown menu ‚ÄòOpen with:‚Äô, you can then select LibreOffice (provided you have installed it beforehand!) and finally click on ‚ÄòChange All‚Ä¶‚Äô (Figure¬†2.9 (c)). You will be asked to confirm your choice.\n\n\n\n\n\n\n\n\n\n\n\n(a) ¬†\n\n\n\n\n\n\n\n\n\n\n\n(b) ¬†\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) ¬†\n\n\n\n\n\n\n\nFigure¬†2.9: Changing the default application for a file extension on MacOS\n\n\n\nIf your operating system is Windows, you should look in your Windows‚Äô settings for the option ‚ÄòDefault Apps‚Äô (see Figure¬†2.10).\n\n\n\n\n\n\nFigure¬†2.10: Default apps in Windows settings\n\n\n\nIn the next step, select ‚ÄòChoose default apps by file type‚Äô. Here, you can search for .csv as a file type, and choose which program you want to set as the default program for opening .csv files. If Excel is currently your default (as in Figure¬†2.11 (a)), you can click on Excel and choose a different program. LibreOffice is a sensible, open-source alternative (see Figure¬†2.11 (b)). A plain-text editor such as Notepad would also be fine (also listed on Figure¬†2.11 (b)).\n\n\n\n\n\n\n\n\n\n\n\n(a) Excel as the default programme for .csv files\n\n\n\n\n\n\n\n\n\n\n\n(b) Changing the default programme for .csv files\n\n\n\n\n\n\n\nFigure¬†2.11: Changing the default app for opening .csv files in Windows\n\n\n\nIf it is not possible to adjust the default app settings, either due to insufficient permissions or because you only have temporary access to this PC, do not to open .csv or .tsv files with the default program. Instead, right-click on the file name and, using the ‚ÄòOpen with‚Äô option, select the option to open the file with LibreOffice, if available, or else with a plain-text editor.\n\n\n\n\nCheck your progress üåü\nYou have successfully completed 0 out of 14 questions in this chapter.\nAre you confident that you can‚Ä¶?\n\nDistinguish different types of research data (Section 2.2)\nFind and download openly available research data and materials (Section 2.4)\nDistinguish different data formats using the file extensions (Section 2.3)\nOpen delimiter-separated values (DSV) files in LibreOffice Calc (Section 2.5.1) - (Section 2.5.2)\nExplain the risks of opening DSV files in Microsoft Excel and Google sheets (Section 2.6)\n\nIn Chapter 3, you will learn how to name, save, and back-up research data files so as to facilitate sound data analysis.\n\n\n\n\nAbeysooriya, Mandhri, Megan Soria, Mary Sravya Kasu & Mark Ziemann. 2021. Gene name errors: Lessons not learned. PLOS Computational Biology. Public 17(7). e1008984. https://doi.org/10.1371/journal.pcbi.1008984.\n\n\nConsortium, TEI. 2025. TEI P5: Guidelines for electronic text encoding and interchange. Zenodo. https://doi.org/10.5281/zenodo.17161156.\n\n\nGood, Jeff. 2022. The scope of linguistic data. In Andrea L. Berez-Kroeker, Bradley McDonnell, Eve Koller & Lauren B. Collister (eds.), The open handbook of linguistic data management, 27‚Äì47. MIT Press. https://doi.org/10.7551/mitpress/12200.001.0001.\n\n\niris-database.org. 2011. IRIS. https://iris-database.org/.\n\n\nKung, Susan Smythe. 2022. Developing a data management plan. In Andrea L. Berez-Kroeker, Bradley McDonnell, Eve Koller & Lauren B. Collister (eds.), The open handbook of linguistic data management, 101‚Äì115. MIT Press. https://doi.org/10.7551/mitpress/12200.001.0001.\n\n\nPaquot, Magali, Alexander K√∂nig, Egon W. Stemle & Jennifer-Carmen Frey. 2024. The core metadata schema for learner corpora (LC-meta): Collaborative efforts to advance data discoverability, metadata quality and study comparability in L2 research. International Journal of Learner Corpus Research 10(2). 280‚Äì300. https://doi.org/10.1075/ijlcr.24010.paq.\n\n\nSchimke, Sarah, Israel de la Fuente, Barbara Hemforth & Saveria Colonna. 2018. First language influence on second language offline and online ambiguous pronoun resolution. Language Learning 68(3). 744‚Äì779. https://doi.org/10.1111/lang.12293.\n\n\nTrippel, Thorsten. 2025. Metadata for research data. In Piotr Ba≈Ñski, Ulrich Heid & Laura Herzberg (eds.), Harmonizing language data: Standards for linguistic resources, 251‚Äì279. De Gruyter. https://www.degruyterbrill.com/document/doi/10.1515/9783112208212-011/html.\n\n\nWindhouwer, Menzo & Twan Goosen. 2022. Component metadata infrastructure. In Darja Fi≈°er & Andreas Witt (eds.), CLARIN: The infrastructure for language resources, 191‚Äì222. De Gruyter. https://doi.org/10.1515/9783110767377-008.\n\n\nWithers, Peter. 2012. Metadata management with arbil. In V. D. Arranz, B. Broeder, M. Gaiffe, M. Gavrilidou & M. Monachini (eds.), Proceedings of the workshop describing LRs with metadata: Towards flexibility and interoperability in the documentation of LR, 72‚Äì75. European Language Resources Association (ELRA). http://www.lrec-conf.org/proceedings/lrec2012/workshops/11.LREC2012%20Metadata%20Proceedings.pdf#page=79.\n\n\nZiemann, Mark, Yotam Eren & Assam El-Osta. 2016. Gene name errors are widespread in the scientific literature. Genome Biology 17(1). 177. https://doi.org/10.1186/s13059-016-1044-7.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data files and formats</span>"
    ]
  },
  {
    "objectID": "2_Data.html#footnotes",
    "href": "2_Data.html#footnotes",
    "title": "2¬† Data files and formats",
    "section": "",
    "text": "Note that, for some repositories, the number of entries includes other types of research outputs, e.g.¬†preprints and figures.‚Ü©Ô∏é\nDetails of what these values mean are not relevant here but, for those of you who are curious, they correspond to the ‚Äúlog odds of looks‚Äù that participant made towards one or the other Playmobil figure whilst listening to the experimental stimulus sentences at three time points, called ‚Äúcritical regions‚Äù. These critical regions include the time window between the onset of the pronoun and 480 milliseconds after the onset of the disambiguating information. Schimke et al. (2018: 768‚Äì769) explain that ‚Äú[a] positive value of the log odds indicates more looks to the subject than to the object antecedent, while a negative value indicates the reverse pattern.‚Äù‚Ü©Ô∏é\nNote that the file extension .csv stands for ‚Äúcomma-separated values‚Äù. Confusingly, however, DSV files are often given a .csv extension even when the separator character is not the comma. As a result, even though the .tsv extension stands for ‚Äútab-separated values‚Äù, .csv files are frequently separated by a tab (\\t) rather than comma. Isn‚Äôt that fun? üôÉ‚Ü©Ô∏é\nIn scientific notation, ‚ÄúE‚Äù stands for ‚Äúexponent‚Äù, which refers to the number of times a number needs to be multiplied by 10. This notation is used as a shorthand way of writing very large or very small numbers. This is why ‚Äú1.23E5‚Äù is interpreted by Excel as 1.23 multiplied by 10 to the power of 5, which is to say: 1.23 multiplied by 100,000. This operation shifts the decimal point five places to the right, resulting in the number 123000.‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data files and formats</span>"
    ]
  },
  {
    "objectID": "3_DataManagement.html",
    "href": "3_DataManagement.html",
    "title": "3¬† Data management",
    "section": "",
    "text": "Chapter overview\nEven if you are confident that you have no trouble managing your computer files, it is still worth taking a few minutes to read up on the basics of data management. This is especially true if you consider yourself a ‚Äúdigital native‚Äù as modern operating systems have made the way that computers deal with files very opaque.\nIn this chapter, you will learn about:",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data management</span>"
    ]
  },
  {
    "objectID": "3_DataManagement.html#recipes-for-successful-data-management",
    "href": "3_DataManagement.html#recipes-for-successful-data-management",
    "title": "3¬† Data management",
    "section": "3.1 Recipes for successful data management",
    "text": "3.1 Recipes for successful data management\nData management is hardly a ‚Äúhot‚Äù topic that people like to dwell on. That‚Äôs a shame because good file management is absolutely central to be able to conduct research and poor file management has the potential to seriously ‚Äòspice things up‚Äô‚Ä¶ but not in a good way! üå∂Ô∏èÔ∏è Whether you are working on a short course assignment, your Master‚Äôs or PhD thesis, or as part of a large research project team: research-related files must be named appropriately and safely stored in meaningful places.\nImagine trying to make a curry in an utterly disorganised kitchen that contains dozens of different spices, scattered across different cabinets and drawers, with vague or misleading labels. For example, you might have three jars labelled ‚ÄúChilli‚Äù and no way of knowing which is mild ‚ÄúKashmiri Chilli‚Äù as opposed to the extra hot ‚ÄúThai Bird‚Äôs Eye Chilli‚Äù. The third might not be chilli at all, but actually a jar of paprika that has been entirely mislabelled. Some of these spices have been gathering dust for decades but the labels have no best-before dates so there is no way of knowing which are still fragrant. Cooking in such a kitchen would turn even the simplest cooking task into a tedious, time-consuming, and error-prone chore: If you‚Äôre not extremely careful, you could easily end up serving something that is bland or, in the worst of cases, entirely inedible! Similarly, in research, if your files are poorly named or stored haphazardly, it will make your work far less efficient, considerably more error-prone, and ultimately utterly frustrating.\n\n\n\n\n\n\nFigure¬†3.1: An inefficient and potentially dangerous workflow (artwork CC BY 4.0 Allison Horst)\n\n\n\nBut the good news is: just as a tidy, well-organized kitchen can greatly enhance your cooking experience, good file management can streamline your research process, help you avoid making mistakes, and reduce stress. In the following sections, we will cook up some good practices for file naming, data management, and project organisation. We will start with basic recipes for naming and managing your files. See the ‚ÄòGoing further‚Äô boxes for tips on learning the ‚Äògourmet skills‚Äô needed to handle more complex projects. So, let‚Äôs don the chef‚Äôs hat and learn how to create a user-friendly computer workspace. And remember, as with cooking, practice makes perfect!\n\n\n\n\n\n\nFigure¬†3.2: A tidy and efficient workflow (artwork CC BY 4.0 Allison Horst)",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data management</span>"
    ]
  },
  {
    "objectID": "3_DataManagement.html#sec-FileNaming",
    "href": "3_DataManagement.html#sec-FileNaming",
    "title": "3¬† Data management",
    "section": "3.2 Naming conventions",
    "text": "3.2 Naming conventions\nFile names are labels. They tell us what is inside a file and helps us identify the correct file quickly and reliably. If you had to run to the printing shop to get your thesis printed in time for a tight deadline, which of these sets of files would you rather have to choose from? Which is more likely to lead you to getting the wrong version printed?\n\n\n\n\n\n\n\n\n\n\n\n(a) ¬†\n\n\n\n\n\n\n\n\n\n\n\n(b) ¬†\n\n\n\n\n\n\n\nFigure¬†3.3: Two sets of file names, one clearly better than the other.\n\n\n\nLike the labels on your neatly organised spice jars, file and folder names should be clear, concise, and easily readable. Good file and folder names should be both human-friendly and computer-friendly.\nBy human-friendly we mean that you and any other human being should easily be able to understand what a folder or file contains. Just like you wouldn‚Äôt want a label on a spice jar to be a random string of numbers (e.g.¬†0171) or only include the best-before date but nothing else (e.g.¬†31 Jan 2028), you also wouldn‚Äôt want to guess what a file contains based on an ambiguous or unclear name like Chili. Labels should be informative but succinct (e.g.¬†Thai Bird's Eye Chilli 31 Jan 2028 not Thai Bird's Eye Chilli bought on December 19, 2023 whilst Christmas shopping with mum, note that the best before date is 31 January 2028)! Unless you and all your colleagues read Thai, do not be tempted to write the part of the file name in Thai as this could also lead to misunderstandings.\nAnother reason for not including Thai characters in your file name is that it would not be computer-friendly. In general, computers are not good at dealing with names that contain anything else but Latin alphanumeric characters, e.g.¬†the letters A to Z and a to z with no accents and the numbers 0 to 9. Hyphens (-) and underscores (_) can also be used, but not spaces. The dot (.) is reserved for the file extension and should ideally not be used elsewhere in the file name.\nHence, whilst Thai Bird's Eye Chilli 31 Jan 2028 is human-friendly, it is not computer-friendly. To make it a computer-friendly label, we need to remove the apostrophe. Whilst spaces are not strictly forbidden, they can cause all kinds of issues and are therefore also best avoided. Space characters can be replaced by hyphens (-) and underscores (_) and the two can be combined in a meaningful way. For example, in the label Thai-Birds-Eye-Chilli_31-Jan-2028, the _ distinguishes between two different pieces of information, whilst the - helps humans to parse individual words within a piece of information. Using such patterns consistently not only helps humans to read file names efficiently, it also means that computers can easily ‚Äòparse‚Äô, i.e.¬†break down such names into meaningful items. This can be very useful to search for files or automatically extract metadata from file names.\n\n\n\n\n\n\nFigure¬†3.4: To help us remember the different, systematic ways to use letter case, hyphens, and underscores in naming conventions, these patterns have fun names (art work by @allison_horst).\n\n\n\nIt is fine to use both lower-case and upper-case letters in file and folder names. However, some operating systems will treat upper-case and lower-case letters as the same, whilst others will not. This means that you should avoid having file names that are only distinguishable by case.\nFinally, it is worth noting that file names cannot be infinitely long! The maximum length of a file name depends on the operating system and the application that you use1 but, as a rule of thumb, if you can display the entire file name in a reasonably sized Finder window (on macOS) or File Explorer window (on Windows), its length is unquestionably both human- and computer-friendly.\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nQ3.1 In which case is this file name? my_first_file_name.R\n\n\n\n\n\ncamelCase\n\n\n\n\nkebab-case\n\n\n\n\nUPPER_SNAKE\n\n\n\n\nUpperCamel\n\n\n\n\nlower_snake\n\n\n\n\n\n\n\n\n¬†\nQ3.2 Why is this file name problematic? MyDocument final.1a.docx\n\n\n\n\n\nLack of clarity\n\n\n\n\nUse of special character other than _ or -\n\n\n\n\nMixed capitalisation\n\n\n\n\nSpaces in file name\n\n\n\n\n\n\n\n\n¬†\nQ3.3 Which of these file names are both human-friendly and computer-friendly?\n\n\n\n\n\n2024-01-05_TermPaper.docx\n\n\n\n\n05.01.24_Draft.docx\n\n\n\n\nMC1.png\n\n\n\n\nAnalysis_24April.R\n\n\n\n\nMANUSCRIPT_CORRECTIONS.docx\n\n\n\n\n\n\n\n\n\n\n\nIt is also important to ensure that file names are easily sortable. If you have a series of files that document a process, consider beginning each file name with a number that correspond to the order of the process, e.g.¬†01_DataPreparation.R, 02_DataAnnotation.R, 03_AnnotationEvaluation.R. Left-padding the numbers with one or more 0 will mean that the files are sorted numerically, even when files are listed alphabetically (see Figure¬†3.5 (b)).\n\n\n\n\n\n\n\n\n\n\n\n(a) File names without additional zeros numbers\n\n\n\n\n\n\n\n\n\n\n\n(b) File names with left-padded numbers\n\n\n\n\n\n\n\nFigure¬†3.5: Why left-padding file names is good file naming practice.\n\n\n\nIt is sometimes useful to include the date in file names. However, many date formats are not easily sortable (see Figure¬†3.6 (a)). Formatting dates using the ‚ÄòYYYY-MM-DD‚Äô format as in Figure¬†3.6 (b) will allow you to easily sort your files in chronological order.\n\n\n\n\n\n\n\n\n\n\n\n(a) File names with a non-ordered date format\n\n\n\n\n\n\n\n\n\n\n\n(b) File names with an ordered date format\n\n\n\n\n\n\n\nFigure¬†3.6: Why using the YYYY-MM-DD is good file naming practice.\n\n\n\nYYYY-MM-DD is the internationally recommended standard for dates (ISO 8601). It provides a clear, computer-readable representation of dates that avoids the confusion that can arise from different cultures using different day-month conventions (e.g.¬†12/13 can mean both 12 December and 13, November).\nEven though computers have gotten much better at dealing with folder and file names containing spaces and special characters, using anything other than basic Latin alphanumeric characters (A-Z, a-z, and 0-9), - and _ in file and folder names will - sooner or later - cause you or your colleagues some serious issues. This is especially true when you start coding. Do not delay getting used to using systematic, human- and computer-friendly folder and file names! In the long run, these simple guidelines will make your digital life much smoother and save you much time and unnecessary stress.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data management</span>"
    ]
  },
  {
    "objectID": "3_DataManagement.html#sec-FoldersPaths",
    "href": "3_DataManagement.html#sec-FoldersPaths",
    "title": "3¬† Data management",
    "section": "3.3 Folders and paths",
    "text": "3.3 Folders and paths\nNow that you know how to name your files and folders sensibly, we can turn to best practices for organising these files and folders. Returning to our kitchen analogy, imagine that, over many years, you collected hundreds of recipes from friends and family. These recipes are jotted down on individual sheets of paper, all of which have been thoughtlessly tossed into a large kitchen drawer called ‚ÄòDocuments‚Äô, which also happens to contain receipts for kitchen appliances still under warranty, takeaway brochures, and various other bits of paper. In such a kitchen, finding Aunt Sophie‚Äôs famous caramelised apple cake could take a while! If, however, you had a dedicated kitchen drawer for recipes which contained neatly labelled folders of different types of dishes, you would know to look for this cake recipe in the Desserts folder. Within the Desserts folder, you could have sub-folders for different types of desserts (e.g.¬†cakes, ice creams, trifles). This would make finding Aunt Sophie‚Äôs recipe an absolute piece of cake!\nThinking about how to structure folders and sub-folders for your projects is about creating a kind of road map that should be readily interpretable by both humans and computers. This is where the concept of ‚Äòpaths‚Äô arises. Paths, in simple terms, describe the location of a file or a folder in a computer‚Äôs filesystem. There are different types of paths. An absolute path provides a complete path from the computer‚Äôs ‚Äúroot folder‚Äù. If our house were our root folder, the absolute path to Aunt Sophie‚Äôs recipe would be \"/Kitchen/Recipes/Desserts/Cakes/Apple-Cake_Aunt-Sophie\". Hence, just like your home‚Äôs postal address, which ideally specifies your home‚Äôs absolute location worldwide, an absolute path provides a complete path from a computer‚Äôs root folder to the file or folder in question.\nBy contrast, a relative path represents the location of a file or folder relative to another folder. Hence, if we already have the Dessert folder open in front of us, the relative path to the apple cake recipe would simply be \"Cakes/Apple-Cake_Aunt-Sophie\". However, if we wanted to access a recipe in the Starters folder from the Cakes folder, we would first have to go ‚Äúback up the path‚Äù from the Cakes folder to the Recipes drawer. This is achieved by adding ../ to the front of the relative path, e.g.¬†\"../Starters/Soups/Pea-Mint-Soup_Barbara\".\nFor example, \"/Users/lefoll/Documents/Teaching/RstatsTextbook/ToDo.txt\" is the absolute path from my computer‚Äôs root folder to the file containing my to-do list in relation to this textbook project. By contrast, a ‚Äúrelative path‚Äù represents the location of a file or folder relative to another folder. Hence, if I am already in the directory \"/Users/lefoll/Documents/Teaching/RstatsTextbook/\", the relative path to my to-do list is only \"ToDo.txt\".\nNote that, here, we use the term ‚Äúfolder‚Äù as a metaphor for a computer file directory. Most modern operating systems use folder icons that look like the kind of paper file folders that office workers use to have piled up on their desks as a means of visually representing directories in computer file systems.\nTo complicate things a little, the way file paths are written varies depending on the computer‚Äôs operating system. In Unix-based systems like Linux and macOS, paths are written using forward slashes (e.g.¬†\"/Users/elen/Documents/Teaching/RstatsTextbook/ToDo.txt\"), whereas on Windows, paths are written using backslashes (e.g.¬†\"C:\\Users\\elen\\Documents\\Teaching\\RstatsTextbook\\ToDo.txt\").\nThere are many ways to find out where your files are stored on your computer. Let us begin by opening a Finder window (on macOS) or a File Explorer window (on Windows). Navigate to the folder which contains the file for which you want to find the absolute path. Alternatively you could use your computer‚Äôs search function to search for the file. Once you have found it:\n\non Windows: Right-click on the file (in some older Windows versions, you may also need to press the ‚Äúshift‚Äù key). Among the options presented to you, click on the one to copy the file path (e.g.¬†‚ÄúCopy as path‚Äù or similar in the language of your operating system).\non macOS: Right-click on the file and then press the Option/‚å• key on your keyboard. Pressing down this key will change the options you are given after having right-clicked. One of these options should now be ‚ÄúCopy ‚Ä¶ as Pathname‚Äù (or something equivalent in the language of your operating system). Click on this option.\n\nThen, open any text-editing programme (e.g.¬†LibreOffice Writer, Microsoft Word, TextEdit, or NotePad++) and use the shortcut Ctrl/Cmd + V to paste your file‚Äôs path in the empty document. If you are on Windows, your path should have backslashes, whereas if you are on Linux or macOS, your path should have forward slashes.\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†3.7: Screenshot of a Finder window showing a hierarchical folder structure\n\n\n\nQ3.4 What is the absolute path to the highlighted file in Figure¬†3.7?\n\n\n\n\n\nUsers/lefoll/Documents/UzK/2024_SoSe_Stats/Rscripts/2_ErrorsAreFun.R\n\n\n\n\n../UzK/2024_SoSe_Stats/Rscripts/2_ErrorsAreFun.R\n\n\n\n\nUzK/2024_SoSe_Stats/Rscripts/2_ErrorsAreFun.R\n\n\n\n\nUsers\\lefoll\\Documents\\UzK\\2024_SoSe_Stats\\Rscripts\\2_ErrorsAreFun.R\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\nQ3.5 From the ‚ÄúUzK‚Äù folder, what is the relative path to the highlighted file in Figure¬†3.7?\n\n\n\n\n\n../UzK/2024_SoSe_Stats/Rscripts/2_ErrorsAreFun.R\n\n\n\n\nRscripts/2_ErrorsAreFun.R\n\n\n\n\nUzK/2024_SoSe_Stats/Rscripts/2_ErrorsAreFun.R\n\n\n\n\n2024_SoSe_Stats/Rscripts/2_ErrorsAreFun.R\n\n\n\n\n\n\n\n\n¬†\nQ3.6 From the ‚ÄúRscripts‚Äù folder, what is the relative path to the folder ‚Äú2023_SoSe_CADS‚Äù (see Figure¬†3.7)?\n\n\n\n\n\n../../2023-SoSe-CADS\n\n\n\n\n../2023_SoSe_CADS\n\n\n\n\n../../../2023_SoSe_CADS\n\n\n\n\n../../2023_SoSe_CADS\n\n\n\n\n\n\n\n\nHint: From the Rscript folder, you will need to go ‚Äúback up the path‚Äù twice: once to get to the course folder 2024_SoSe_Stats and a second time to get to the UzK folder, before you can move to the 2023_SoSe_CADS folder. Going back up the path is achieved with ../.\n\n\n\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nRead the abstract of the following academic article. What was this experimental study about?\n\nTerai, Masato, Junko Yamashita & Kelly E. Pasich. 2021. Effects of Learning Direction in Retrieval Practice on EFL Vocabulary Learning. Studies in Second Language Acquisition 43(5). 1116‚Äì1137. https://doi.org/10.1017/S0272263121000346.\n\nQ3.7 According to the study, which is the most effective way of learning vocabulary in a foreign language?\n\n\n\n\n\nBy first reading a word in one's native language, and then reading a translation in the target language.\n\n\n\n\nBy first reading a word in the target language, and then a translation in one's native language.\n\n\n\n\nBeginners learn better if they are first exposed to a word in their native language and then in the target language. The opposite is true for more proficient learners.\n\n\n\n\nIt's impossible to tell as all human learners are different.\n\n\n\n\n\n\n\n\n¬†\nThe authors of this article have published the data and materials associated with this study on IRIS. You can find them here: https://iris-database.org/search/?s_publicationAPAInlineReference=Terai%20et%20al.%20(2021)\nQ3.8 In which format are the video files associated with this publication?\n\n\n\n\n\n.mov\n\n\n\n\n.mxf\n\n\n\n\n.avi\n\n\n\n\n.mp4\n\n\n\n\n\n\n\n\n¬†\nQ3.9 In which format is the analysis code which they shared on IRIS?\n\n\n\n\n\nPython\n\n\n\n\nRmarkdown\n\n\n\n\nR\n\n\n\n\nHTML\n\n\n\n\n\n\n\n\n¬†\nQ3.10 The associated materials also include a section entitled ‚ÄúScores on measures / tests‚Äù. Download the file dataset1_ssla_20210313.csv from this section. Which character is used as the separator in this delimiter-separated values (DSV) file?\n\n\n\n\n\nColon\n\n\n\n\nSpace\n\n\n\n\nSemicolon\n\n\n\n\nComma\n\n\n\n\nTab",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data management</span>"
    ]
  },
  {
    "objectID": "3_DataManagement.html#sec-BackupData",
    "href": "3_DataManagement.html#sec-BackupData",
    "title": "3¬† Data management",
    "section": "3.4 Backing up data: ‚ÄòFire safety‚Äô measures in the digital kitchen üßØÔ∏è",
    "text": "3.4 Backing up data: ‚ÄòFire safety‚Äô measures in the digital kitchen üßØÔ∏è\nA basic principle of sound data management consists in keeping a copy of all your files in more than one place. This ensures that, should something go awry, your research is not lost forever but instead can be recovered and restored promptly. There are many ways things could go wrong: laptops can get stolen or permanently damaged (laptops are not terribly keen on hot chocolate as it turns out‚Ä¶), computer files can be corrupted and become unusable, you or someone else may accidentally delete files, your computer can become infested with a nasty virus, etc.\nAn effective way to protect your projects is to abide by the 3-2-1 rule (Schweinberger 2022). It‚Äôs simple:\n\nEnsure that you have at least three copies of your data (e.g.¬†one that you work with on your personal computer and two back-up copies).\nSplit the backup copies between two different storage media (e.g.¬†a hard-drive stored in your office and online in a secure cloud service).\nStore one of these copies in a secure place off-site (i.e.¬†not where your computer usually is).\n\nOne solution is to store your three copies on:\n\nyour personal laptop or computer,\na backup hard drive stored in a secure location, and\na secure online repository such as the data management system provided by your institution, e.g.¬†Sciebo, ownCloud, or GitLab.\n\nChoosing an online repository will protect your data if your computer malfunctions or is damaged or stolen, but remember that it can also potentially make your data accessible to others. This is particularly true of commercial back-up solutions such as Microsoft‚Äôs OneDrive, Google‚Äôs Drive, Apple‚Äôs iCloud, or Dropbox, which although convenient and very user-friendly, should not be used to store sensitive data (e.g.¬†data that may be used to identify individuals, contain financial information, health records, location data, or proprietary research data). Always check if your institution has its own, secure cloud option. If not, keeping a second hard-drive copy in a separate, secure location is likely the safest solution.\nWhilst the 3-2-1 rule stipulates that you should keep at least three copies of each file, in an optimal scenario, each file should exist only once at each location (e.g.¬†on your laptop, a separate hard-drive, and the server of an online repository). It is quite easy to (often unknowingly) end up with several duplicates of the same file on any one machine but this can cause issues if, for example, you end up updating the wrong version of the file. Avoiding and eliminating file duplicates is therefore an important step towards proficient data management.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data management</span>"
    ]
  },
  {
    "objectID": "3_DataManagement.html#conclusion",
    "href": "3_DataManagement.html#conclusion",
    "title": "3¬† Data management",
    "section": "3.5 Conclusion",
    "text": "3.5 Conclusion\nSound data management - comprising of both good folder and file naming practices and the smart organisation of these folders and files - is the foundation for efficient research workflow. Understanding and applying these basic principles of file management will ensure that everything in your digital ‚Äòkitchen‚Äô has its place, is well labelled, and easy to find. By ensuring that we keep our kitchens clean, tidy, and safe, we can whip out some truly delicious dishes!\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure¬†3.8: The kitchen workflow metaphor (artwork CC BY 4.0 Allison Horst).\n\n\n\n\n\n\n\n\n\nNoteGoing further\n\n\n\n\n\nThis short online module is ideal to learn more about smarter ways to work with files and data:\n\nThe University of Queensland Library. 2023. Work with Data and Files. The University of Queensland. https://uq.pressbooks.pub/digital-essentials-data-and-files/. (14 May, 2024).\n\nTo go further, here are some great, open-access resources to dive deeper into data management and data standards in linguistics and education research:\n\nBerez-Kroeker, Andrea L., Bradley McDonnell, Eve Koller & Lauren B. Collister. 2022. The Open Handbook of Linguistic Data Management. MIT Press. https://doi.org/10.7551/mitpress/12200.001.0001.\nHeid, Ulrich, Piotr Ba≈Ñski & Laura Herzberg (eds.). 2025. Harmonizing Language Data: Standards for Linguistic Resources (Digital Linguistics 4). Boston: De Gruyter. https://doi.org/10.1515/9783112208212.\nLewis, Crystal. Data Management in Large-Scale Education Research. https://datamgmtinedresearch.com/. (14 May, 2024).\n\n\n\n\n\nCheck your progress üåü\nYou have successfully completed 0 out of 10 questions in this chapter.\nAre you confident that you can‚Ä¶?\n\nGive your files and folders names that are both human- and computer-readable (Section 3.2)\nOrganise your files and folders in a sensible manner (Section 3.3)\nRemember the 3-2-1 rule on backups (Section 3.4)\n\nIf that‚Äôs the case, you are now all set to install R and RStudio in Chapter 4 and learn how to get started in R in Chapter 5!\n\n\n\n\nalvinashcraft, alexbuckgit, ArcticLampyrid & bearmannl. 2022. Maximum path length limitation. Learn Microsoft. https://learn.microsoft.com/en-us/windows/win32/fileio/maximum-file-path-limitation.\n\n\nSchweinberger, Martin. 2022. Data management, version control, and reproducibility. https://ladal.edu.au/repro.html.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data management</span>"
    ]
  },
  {
    "objectID": "3_DataManagement.html#footnotes",
    "href": "3_DataManagement.html#footnotes",
    "title": "3¬† Data management",
    "section": "",
    "text": "For example, many Windows applications have a maximum file path length of 260 characters (alvinashcraft et al. 2022).‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data management</span>"
    ]
  },
  {
    "objectID": "4_InstallingR.html",
    "href": "4_InstallingR.html",
    "title": "4¬† Installing R and RStudio",
    "section": "",
    "text": "Chapter overview\nThis chapter is designed to help you get started using R and RStudio, assuming no prior use of either. We will be covering the following topics:\nIf you already have some experience of using R and RStudio, please ensure that both are up-to-date before continuing (see Section 4.5). Whilst parts of this chapter will likely be revision, others may be the opportunity to learn some new tips about setting up and using R in RStudio, installing and citing packages.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Installing `R` and RStudio</span>"
    ]
  },
  {
    "objectID": "4_InstallingR.html#why-learn-r",
    "href": "4_InstallingR.html#why-learn-r",
    "title": "4¬† Installing R and RStudio",
    "section": "4.1 Why learn R?",
    "text": "4.1 Why learn R?\nIn short, because R can do it all! This statement is only a slight exaggeration: R is indeed a highly versatile programming language and environment that allows us to do a multitude of tasks relevant to the language sciences. These include data handling and processing, statistical analysis, creating effective and appealing data visualisations, web scraping, text analysis, generating reports in various formats, designing web pages, and interactive apps, and much, much more! üí™\nWhilst some will claim that R has a steep learning curve, this textbook aims to prove that the opposite is true! Whilst it‚Äôs fair to say that, as with all new things, it will take you a while to get the hang of it, once you‚Äôve got started, you will see that your possibilities are (pretty much) endless and that learning how to do new things in R makes for fun and very rewarding challenges. What‚Äôs more, this textbook introduces the tidyverse approach to programming in R (see Section 9.1), which is particularly accessible to beginners. We will also use RStudio to access R, which makes things considerably more intuitive and generally easier to work with.\nCrucially, both R and the RStudio Desktop version that we will be using are free and open source (see Section 1.2), which means that they are accessible to all, regardless of their institutional affiliation or professional status. This is in contrast to proprietary statistical software such as SPSS, for which you or your university needs to buy an expensive license. To get started in R, all you will need is access to the internet, a computer (unfortunately, a tablet will probably not suffice), and the intrinsic motivation to work your way through the fundamental skills taught in this textbook.\n\n[Using R is] like the green and environment-friendly gardening alternative to buying plastic wrapped tomatoes in the supermarket that have no taste anyway (Martin Schweinberger 2002 in ‚ÄòWhy R?‚Äô).\n\n\n\n\n\n\nFigure¬†4.1: ‚ÄúTomato Harvest, Yellow & Red‚Äù by OakleyOriginals (CC BY 2.0).\n\n\n\n\nLast but not least, in choosing to learn R, you are entering a vibrant community of users. As an open-source programming environment, R is the product of many different people‚Äôs contributions. Everyday, new packages, functions, and resources are being developed, improved, and shared with the community. Given that R has evolved into one of the most popular languages for scientific programming (and has become ‚Äúthe de facto standard in the language sciences‚Äù Winter 2020: xiii), many of these have been created by scientists and are particularly well-suited to research workflows. Moreover, the R community is known for being welcoming, supportive, and inclusive. This is reflected in the strong presence of many community-led initiatives such as RLadies+, RainbowR, and LatinR, that encourage under-represented groups to participate in and contribute to the R community.\n\n\n\n\n\n\nFigure¬†4.2: Logo of the RLadies Ribeir√£o Preto, one of many RLadies+ chapters.\n\n\n\n\n‚ÄúI am studying a language/linguistics so why should I learn to code?‚Äù ü§î\nUsing scripts rather than GUI software will help you make your research less error-prone, more transparent, and sustainable. Being open-source, there are no restrictions as to who can run R code and older versions are available ensuring that reproduction is feasible, even years later. As many other linguists use R (see e.g. Mizumoto & Plonsky 2016), you will be able to collaborate with others and understand other researchers‚Äô R code. As we will see in Chapter 14, in RStudio, it is also very easy to export R code and share your scripts, for example as part of an appendix to your research publication, in various formats (including .html that can be opened in any browser and .pdf, see Chapter 14).\nIn addition, learning to code in R provides an excellent foundation in data literacy and statistical reasoning. These are skills that are highly valued among employers, both in academia and the industry. Many companies, public institutions (e.g.¬†ministries, hospitals, national agencies) and NGOs hire data scientists who often work in R. And, even if you end up doing little to no coding yourself, understanding the basic principles of programming is a highly useful skill in the modern world; it is crucial to be able to effectively communicate and collaborate with programming colleagues. More generally, computational thinking skills are highly transferable (see e.g. Ye, Lai & Wong 2022). For instance, they can help us to structure our work processes more systematically, identify patterns more effectively, and formulate our thoughts more precisely.\n\n\n\n\n\n\nNoteWhat about learning Python instead? üêç\n\n\n\n\n\nSome of you may be wondering whether you should be learning Python rather than R. Both are widely used languages in scientific programming and data science. At the time of writing, there are more resources specifically aimed at linguists and education researchers in R than there are in Python simply because it is currently the most widely used language in these disciplines. Should you wish to learn Python at a later stage (see Next-step Resources), many of the same principles that you will have learned in this textbook will apply: it should feel somewhat like learning Italian when you already speak Spanish or French.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Installing `R` and RStudio</span>"
    ]
  },
  {
    "objectID": "4_InstallingR.html#installing-r-and-rstudio",
    "href": "4_InstallingR.html#installing-r-and-rstudio",
    "title": "4¬† Installing R and RStudio",
    "section": "4.2 Installing R and RStudio",
    "text": "4.2 Installing R and RStudio\n\n4.2.1 What are R and RStudio? And why do I need both?\nAs a beginner, it‚Äôs easy to confuse R and RStudio, but it‚Äôs important to understand that they are two very different things. R is a programming environment for statistical computing and graphics that uses the programming language R. Think of it as the engine with which we will learn to perform lots of different tasks. RStudio, by contrast, is a set of tools, a so-called ‚Äòintegrated development environment‚Äô (IDE). It makes working in R much more intuitive and efficient. If R is the engine of our car, you can imagine RStudio as our dashboard. Hence, even though we will later on appear to only be working in RStudio, R will actually be doing the heavy-lifting, under the hood.\n\n\n\n\n\n\n\n\n\n\n\n(a) Logo of the programming language and environment R\n\n\n\n\n\n\n\n\n\n\n\n(b) Logo of the IDE RStudio (RStudio¬Æ is a trademark of Posit Software, PBC)\n\n\n\n\n\n\n\nFigure¬†4.3: Even the two logos are easy to confuse, but remember that R and RStudio are two very different things!\n\n\n\n\n\n\n\n\n\nNoteUsing other IDEs to work in R\n\n\n\n\n\nAt the time of writing, RStudio is the most widely used Integrated Development Environment (IDE) to work in R. However, it is worth noting that many other IDEs that can be used to access R. These include:\n\nJupyter notebook\nVisual Studio Code\nPyCharm\nEclipse\n\nWhilst this textbook will assume that everyone is working in RStudio, if you are already familiar with another IDE that works well with R, you are welcome to continue working in that IDE. Each IDE has a different feel to it and offers different functions so, ultimately, it‚Äôll be up to you to find the one that suits you best!\n\n\n\n\n\n4.2.2 Installing R\n\nGo to the website of the Comprehensive R Archive Network (CRAN).\nClick on the ‚ÄúDownload R for ‚Ä¶‚Äù link that matches your operating system (Linux, macOS or Windows), then:\n\nFor Windows, click on the top ‚Äòbase‚Äô link, also marked as ‚Äúinstall R for the first time‚Äù (Note that you should also use this link if you are updating your R version). On the next page, click on the top ‚ÄúDownload R‚Äù link.\nFor MacOS, click on either the top .pkg link if you have an Apple silicon Mac (e.g.¬†M1, M2, M3) or the second .pkg link, if you have an older Intel Mac.\nFor Linux, click on your Linux distribution and then follow the instructions on the following pages.\n\nOnce you have downloaded one of these R versions, navigate to the folder where you have saved it (by default, this will be your Downloads folder), and double click on the executable file to install R.\nFollow the on-screen instructions to install R.\nTest that R is correctly installed. On Windows and MacOS, navigate to your Applications folder and double click on the R icon. On Linux, open up R by typing R in your terminal. This should open up an R Console. You can type R commands into the Console after the command prompt &gt;. Type the following R code after the command prompt and then press enter: plot(1:10).\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Test command in the R Console\n\n\n\n\n\n\n\n\n\n\n\n(b) Resulting plot (note that the proportions of your plot may be different depending on the size of your window)\n\n\n\n\n\n\n\nFigure¬†4.4: Testing R\n\n\n\n‚úÖÔ∏è If you see the plot above, you have successfully installed and tested R and you can go on to installing RStudio.\n‚ö†Ô∏èÔ∏è If that‚Äôs not the case, make a note of the errors produced (copy and paste them into a text document or take a screenshot) and search for solutions on the internet. It is very likely that many other people have already encountered the same problem as you and that someone from the R community has posted a solution online.\n\n\n\n\n\n\nNoteWhat to do if you cannot get R and/or RStudio working on your computer\n\n\n\n\n\nThe aim of this chapter is to install both R and R Studio on your own computer so that you can write and run your own scripts locally (i.e.¬†on your own computer without the need for an internet connection). In some cases, however, this might not be possible. For example, because the programmes are not available for your operating system, or because you do not have admin rights on your computer, or because your disk is full and you cannot delete anything. None of these situations are ideal to do research, but don‚Äôt give up on learning R: there is an alternative!\nYou can sign up to Posit Cloud. Posit Cloud will allow you to run R in RStudio in a browser (e.g.¬†Firefox or Chrome) without having to install anything on your computer. Although Posit Cloud‚Äôs free plan is limited, it will suffice to learn the contents of this textbook. You will be able to follow the textbook in exactly the same way as everyone else. However, you will need a stable internet connection and you may find that you need to be a bit more patient as things are likely to run a little slower. If you decide to opt for the Posit Cloud solution, create a free account and then go straight to Section 4.3.\n\n\n\n\n\n4.2.3 Installing RStudio\nWhen you head over to their website, it may be confusing to you that the company that provides RStudio, Posit, also offers paid-for versions of RStudio and other paying services. Do not worry, we will not need any of these: These are products designed for companies and large organisations. The version of RStudio Desktop that we will be using, however, is free and, given that it is open source, even if Posit decided to stop working on this product one day, others in the R community would take over. Such is the beauty of open-source software! :hugging_face:\n\nHead over to Posit‚Äôs download page to download the latest version of RStudio Desktop.\nAs you have already installed R, you can jump straight to the section entitled ‚Äú2: Install RStudio‚Äù. The website should have detected which operating system your computer is running on, so that you can most likely simply click on the ‚ÄúDownload RStudio Desktop‚Ä¶‚Äù button. Your download should start straight away.\n\nIf an incorrect operating system is detected, scroll down the page to find your operating system and download the corresponding version of RStudio.\n\nOnce you have downloaded RStudio, navigate to the folder where the downloaded file has been saved (by default, this will be your Downloads folder), and double-click on the executable file to install RStudio.\nFollow the on-screen instructions to install RStudio.\n\nIf you run into any issues that you cannot solve with existing online posts, the Posit Community forums are a good place to ask for help.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Installing `R` and RStudio</span>"
    ]
  },
  {
    "objectID": "4_InstallingR.html#sec-SettingupR",
    "href": "4_InstallingR.html#sec-SettingupR",
    "title": "4¬† Installing R and RStudio",
    "section": "4.3 Setting up RStudio",
    "text": "4.3 Setting up RStudio\nFrom now on, we will only be accessing R through RStudio. When you open up RStudio for the first time, you might find the layout rather intimidating. The application window is divided into several sections, which we call panes. Each pane also has several tabs. Although it may seem overwhelming at first, you will soon see that these different panes and tabs will actually make life much easier.\n\n4.3.1 Global options\nBefore we get staRted properly, we need to change some of the default settings of RStudio. The first set of changes that we are going to make ensure that, each time we launch a new R session in RStudio, we start afresh.\nTo do so, head over to the ‚ÄòTools‚Äô drop-down menu and click on ‚ÄòGlobal Options‚Äô. Make sure that the first three boxes are unticked (see Figure¬†4.5 (a)). Under ‚ÄúSave workspace to .RData on exit‚Äù, select the option ‚ÄúNever‚Äù. Always starting afresh is good programming practice. It avoids any problems being carried over from previous R sessions. You can think of it like cooking in a freshly cleaned, tidy kitchen. It‚Äôs much safer than preparing a meal in a messy, possibly even contaminated kitchen!\n\n\n\n\n\n\n\n\n\n\n\n(a) General tab\n\n\n\n\n\n\n\n\n\n\n\n(b) Code tab\n\n\n\n\n\n\n\nFigure¬†4.5: RStudio‚Äôs Global Options\n\n\n\nNext, under the ‚ÄòGlobal Options‚Äô tab ‚ÄòCode‚Äô of the ‚ÄòGlobal Options‚Äô window, ensure that the fourth option ‚ÄúUse native pipe operator‚Äù is ticked (see Figure¬†4.5 (b)). This is a new feature in R that is very useful so we will make use of it from Section 7.5.1 onwards. The other options are not relevant for now.\nFinally, head over to the ‚ÄòPane Layout‚Äô tab. From here, you can rearrange the panes of your RStudio window. To do so, click on the downward arrow symbols to get a drop-down menu corresponding to each pane. You can also select which tabs you would like to see in each pane. If you are already familiar with RStudio, feel free to stick to your favourite set-up. Personally, I use the panes layout below and, if you are new to R, I recommend that you select this layout, too. You can always go back to these settings to change this set-up at any stage. Don‚Äôt forget to click on ‚ÄòOK‚Äô at the bottom of the ‚ÄòGlobal Options‚Äô page to save your settings. Then, the panes in your RStudio window should be ordered as in Figure¬†4.6 (b).\n\n\n\n\n\n\n\n\n\n\n\n(a) Panes Layout tab\n\n\n\n\n\n\n\n\n\n\n\n(b) Customised panes layout\n\n\n\n\n\n\n\nFigure¬†4.6: Recommended RStudio panes layout\n\n\n\n\n\n4.3.2 Testing RStudio\nIt is now time we tested whether RStudio is communicating well with R. To do so, let‚Äôs run the same test as in the R Console. This time, head over to the Console tab in the top right pane of your RStudio window and, after the command prompt &gt;, type: plot(1:10) and then press enter. You should see the same plot as earlier on (see Figure¬†4.4 (b)), appearing in the Plots tab of the bottom-right pane of your RStudio window.\nIf you get the following error message Error in plot.new() : figure margins too large, this is because your bottom-right pane is hidden from view or too small for the plot to be printed there. Click on the small two-window icon in the bottom-right corner if it is hidden (see Figure¬†4.7 (a)). Or, if it is too small, click on the dividing line between the two right-hand side panes and, whilst still holding down the mouse button, drag up the line until it is about halfway up. Then, re-type the command plot(1:10) in the Console pane and press enter again. The plot should appear as in Figure¬†4.7 (b).\n\n\n\n\n\n\n\n\n\n\n\n(a) Hidden (minimised) bottom-right pane\n\n\n\n\n\n\n\n\n\n\n\n(b) Now the dividing line between the two panes is halfway up and the plot has been successfully output in the Plots pane\n\n\n\n\n\n\n\nFigure¬†4.7: Testing that RStudio is communicating well with your R installation.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Installing `R` and RStudio</span>"
    ]
  },
  {
    "objectID": "4_InstallingR.html#installing-r-packages",
    "href": "4_InstallingR.html#installing-r-packages",
    "title": "4¬† Installing R and RStudio",
    "section": "4.4 Installing R packages",
    "text": "4.4 Installing R packages\n\n4.4.1 What are packages?\nYou now have a base installation of R. Base R is very powerful and comes with many standard packages and functions that R users use on a daily basis. If you click on the Packages tab in the bottom-right pane and scroll down, you will see that there are many packages available. Only a few are selected. These are part of the base R installation.\nYou can think of base R as a fully functional student kitchen. It is rather small and only has the most essential ingredients and equipment, but it still has everything you need to cook simple, delicious meals. Downloading and installing additional packages is like buying more sophisticated and specialised kitchen devices (i.e.¬†packages that provide additional functions) or fancier ingredients (i.e.¬†packages that provide access to specific datasets).\nIn addition to the members of the R Core Team who develop and maintain base R, thousands of R users develop and share additional R packages every day. These enable us to vastly increase the capacities of base R. Packages are a very helpful way to bundle together a set of functions, data, and documentation files so that other R users can easily download these bundles and add them to their local R installation.\nThroughout this textbook, the names of packages will be enclosed in curly brackets like this: {ggplot2}.\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nQ4.1 Which of these packages is not part of base R?\n\n\n\n\n\n{datasets}\n\n\n\n\n{graphics}\n\n\n\n\n{stats}\n\n\n\n\n{ggplot2}\n\n\n\n\n\n\n\n\n¬†\nQ4.2 Is it possible to create an R package that provides access to the full texts of all of Jane Austen‚Äôs published novels for computational text analysis in R?\n\n\n\n\n\nYes, pretty much anything is possible in R!\n\n\n\n\nNo way, that sounds impossible!\n\n\n\n\n\n\n\n\n¬†\nQ4.3 Is the {janeaustenr} package installed as part of base R?\n\n\n\n\n\nYes\n\n\n\n\nNo\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.4.2 Installing packages\nTo install a package, you will first need to download it from the internet. Packages can be stored on any website, but the most trustworthy online repository for R packages and the easiest to work with is CRAN (Comprehensive R Archive Network). To install the {janeaustenr} package from CRAN, type the following command in the Console pane and then type enter:\n\ninstall.packages(\"janeaustenr\")\n\nThis command will take a few seconds to run (or longer depending on how slow your internet connection is). You should then see a message in red in the Console indicating, among other things that you can ignore, that the package has been successfully downloaded and how big it is (here: 1.5 megabyte). The message also gives you path to where the package‚Äôs content has been saved on your computer (see Figure¬†4.8). You do not need to worry about any of the other information.\n\n\n\n\n\n\nFigure¬†4.8: Screenshot showing that the package has been correctly installed.\n\n\n\nTo check that the package has been successfully downloaded and installed, head over to the Packages tab of the bottom-right pane and scroll down to the {janeaustenr} package, or search for it using the search window within this same tab. The {janeaustenr} package should now be visible, which tells us that the package is installed on your computer. Note, however, that the checkbox next to it is currently empty. This means that the package hasn‚Äôt been loaded in our current R session and therefore cannot be used yet.\n\n\n\n\n\n\nNoteSolving package installation problems :double_exclamation_mark:\n\n\n\n\n\nWhen you install or update an R package, you will often encounter a message that informs you that so-called package dependencies, i.e.¬†other packages that are required for the new/updated package to function, also need updating. You will be asked if you want to update these packages as part of the installation/update process. I recommend that you update them ‚ÄúAll‚Äù or at least ‚ÄúCRAN packages only‚Äù. To do so, enter either 1 or 2 in the Console (as specified in the instructions displayed in the Console) and press enter.\nThese packages have more recent versions available.\nIt is recommended to update all of them.\nWhich would you like to update?\n\n1: All                            \n2: CRAN packages only             \n3: None                           \n4: stringi (1.7.2 -&gt; 1.8.7) [CRAN]\nRStudio may be prompt you to restart your R session for the update process to be completed. RStudio should recover your session work on launching the new session.\nIn some cases, you will find that the most recent version of a package needs to be installed from source. Installing binary versions is faster than compiling from source and it is less error-prone for beginners so, if you get asked Do you want to install from sources the packages which need compilation? (Yes/no/cancel) and you have no specific need to install the latest source code version, selecting ‚Äúno‚Äù is fine.\nIf you get an ‚Äúunable to access index for repository‚Äù message when trying to install a package, this is probably because the default CRAN mirror for your location is unavailable. This is likely to be a temporary connection issue. In the meantime, you can enter the following command to choose a different mirror (preferably one close to your own location) and try the installation again:\n\nchooseCRANmirror()\n\nIn some cases, R may be unable to install a package and the following message will appear in your Console:\nWarning message:\npackage ‚ÄòPackageName‚Äô is not available for this version of R\nFirst, check that you haven‚Äôt made a typo in the package name, bearing in mind that R is a case-sensitive programming language so that, for example, attempting to install the {JaneAustenr} package will fail because the package is spelt {janeaustenr}.\nIf the package name is correctly spelt, this error probably means that the default package version on CRAN is not compatible with your current R version or that the package is not available on CRAN. Check which version of R you are currently running by entering R.version in your Console. Compare this version number with the required R version indicated on the package‚Äôs info page on CRAN. For example, Figure¬†4.9 informs us that the {janeaustern} package ‚Äúdepends‚Äù on a R version that is 3.5 or higher. All CRAN packages are listed on: https://cran.r-project.org/web/packages/available_packages_by_name.html.\n\n\n\n\n\n\nFigure¬†4.9: Screenshot of the top part of &lt;https://cran.r-project.org/web/packages/janeaustenr/index.html&gt; (accessed on 17 December 2025)\n\n\n\nIf the package requires a version of R that is more recent than you what you currently have, you will need to first update R (see Section 4.5.2) before you can install this package. If you cannot update R or if the package requires an older version of R than you currently have, you can use the {remotes} package (which you will first need to install) to install a specific package version that is compatible with your R version.\n\ninstall.packages(\"remotes\")\nlibrary(remotes)\n\ninstall_version(\"PackageName\", \"VersionNumber\") \n\nYou can find older package versions on the CRAN archive at https://cran.r-project.org/src/contrib/Archive/. Some packages or package versions are not available on CRAN. Alternative repositories for R packages include Bioconductor, GitHub, and GitLab. The latter two are also where you can typically find packages and package versions that are currently under development. To install packages from these repositories, read Section 1.5 in Douglas et al. (2024).\n\n\n\n\n\n4.4.3 Loading packages\nIf you want to use the new kitchen device or fancy ingredient that was delivered in the package that you installed, you first need to get it out of the fridge or the cupboard and place it on your kitchen counter. This is the equivalent of loading a package. The command to load a package is library(). This is because, once they are unpacked (i.e.¬†installed), packages are stored in a directory on your computer and this directory is referred to as a ‚Äúlibrary‚Äù.\nTo load the {janeaustenr} package, enter the following command in the Console:\n\nlibrary(janeaustenr)\n\nIf you correctly installed the package and have not misspelt the command, it may look like nothing has happened, as the Console returns nothing (see first red ellipse on Figure¬†4.10). However, if you go back to your Packages tab and scroll down to the {janeaustenr} package, you will see that the box next to it is now ticked (see second ellipse on Figure¬†4.10). This means that the package is loaded and ready to be used.\n\n\n\n\n\n\nFigure¬†4.10: Loading a library\n\n\n\nNote that whilst you only need to install each package once, you will need to load it every time we want to use it in a new R session. This is because (if you set RStudio up as explained in Section 4.3.1), when we close R and start a new R session, our kitchen is perfectly clean and tidy. Everything is back in storage and the good news is that we didn‚Äôt even need to do the washing-up! üôÉ\n\n\n4.4.4 Package documentation\nTo find out more about any package or function, use the handy help() function or its shortcut ?. For example, to find out more about the {janeaustenr} package, enter the command help(janeaustenr) or ?janeaustenr in the Console. The help file will open up in the Help tab of the bottom-right pane (see Figure¬†4.6 (b)). It contains the name of the package and a short description, as well as the name of the package maintainer, Julia Silge, and some additional links.\nOne of these links takes us to the package creator‚Äôs GitHub repository. This is where we can find a source code for the package, should we want to check how it works under the hood, or amend it in any way. Click on this link and scroll down the package‚Äôs GitHub page to consult its README file. This document informs us that the package includes plain text versions of Jane Austen‚Äôs six completed, published novels and tells us under what name they are stored within the library. For example, to access Pride and Prejudice, we need to load the library object prideprejudice.\nPick your favourite Jane Austen novel and enter its corresponding object name in the Console, e.g.¬†emma. The entire novel will be printed in the Console output! You can print only a few lines by selecting them within square brackets. For example, the command emma[20:25] will print lines 20 to 25 of the object emma (see Figure¬†4.11).\n\n\n\n\n\n\nFigure¬†4.11: Screenshot showing a selection of lines from the object emma (note that you can adjust the size of the Console pane to see more or less of the text at any one time).\n\n\n\nTo find out more about a dataset or function within a package, use the functions help() or ?, e.g.¬†help(emma) or ?emma. In this case, the help file provides us with a short description of this object and a link to the original source from which the package creator obtained the novel (which is in the public domain, otherwise it would not be possible to share it in this way).\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nQ4.4 Which is the correct R object name to access Jane Austen‚Äôs novel ‚ÄòSense and Sensibility‚Äô?\n\n\n\n\n\nsensesensibility\n\n\n\n\nSense&Sensibility\n\n\n\n\nSense and Sensibility\n\n\n\n\nsenseandsensibility\n\n\n\n\nSensesensibility\n\n\n\n\n\n\n\n\n¬†\nQ4.5 What is the source for the R object containing Jane Austen‚Äôs novel ‚ÄòSense and Sensibility‚Äô?\n\n\n\n\n\nhttps://jasna.org/austen/works/sense-sensibility/\n\n\n\n\nhttps://www.goodreads.com/book/show/14935.Sense_and_Sensibility\n\n\n\n\nhttp://www.gutenberg.org/ebooks/161\n\n\n\n\nhttps://en.wikipedia.org/wiki/Sense_and_Sensibility\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\nQ4.6 What is the first word of the 66th line in the R object containing Jane Austen‚Äôs novel ‚ÄòSense and Sensibility‚Äô?\n\n\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\n\n\n\n\n\n4.4.5 Citing R packages\nWhen we use a package that is not part of base R, it is very important to reference the package properly. There are two main reasons for doing this. For a start, the people who create and maintain these packages largely do so in their free time and they deserve full credit for their incredibly valuable work and contribution to science. Hence, whenever you use a package for your research, you should cite it, just like you would other sources.\nThe help page of the {janeaustenr} package already informed us that the maintainer of the package is Julia Silge. To get a full citation, however, we should use the citation() function. Enter citation(\"janeaustenr\") in the Console to find out how to cite this package.\nNote that the recommended bibliographic reference also includes the package version, which is important for reproducibility as the package may evolve and someone wanting to reproduce your analysis (and this may well be future you!) will need to know which version you used. This is the second main reason why we should be diligent about citing the exact packages that we used to ensure that others can reproduce our analyses (sec-Reproducibility). In a research report, thesis, or academic article, you could cite the {janeaustenr} package like this:\n\nWe used the janeaustenr package (Silge 2022) to access Jane Austen‚Äôs six published novels in R (R Core Team 2024).\n\nNote that you can see the full references by hovering on the in-text citation links or by going to the References section of this book. Chapter 14 explains how to insert bibliographic references in documents that include R code.\n\n\n\n\n\n\nNoteMore about referencing packages\n\n\n\n\n\nYou may also want to install the {report} package, which includes a number of useful functions for citing R versions and R packages:\n\nreport::report_system()\n\nAnalyses were conducted using the R Statistical language (version 4.5.2; R Core\nTeam, 2025) on macOS Sequoia 15.4.1\n\nreport::cite_packages()\n\n  - Barr D, DeBruine L (2023). _webexercises: Create Interactive Web Exercises in 'R Markdown' (Formerly 'webex')_. doi:10.32614/CRAN.package.webexercises &lt;https://doi.org/10.32614/CRAN.package.webexercises&gt;, R package version 1.1.0, &lt;https://CRAN.R-project.org/package=webexercises&gt;.\n  - Makowski D, L√ºdecke D, Patil I, Th√©riault R, Ben-Shachar M, Wiernik B (2023). \"Automated Results Reporting as a Practical Tool to Improve Reproducibility and Methodological Best Practices Adoption.\" _CRAN_. doi:10.32614/CRAN.package.report &lt;https://doi.org/10.32614/CRAN.package.report&gt;, &lt;https://easystats.github.io/report/&gt;.\n  - Moroz G (2020). _Create check-fields and check-boxes with checkdown_. &lt;https://CRAN.R-project.org/package=checkdown&gt;.\n  - R Core Team (2025). _R: A Language and Environment for Statistical Computing_. R Foundation for Statistical Computing, Vienna, Austria. &lt;https://www.R-project.org/&gt;.\n  - Silge J (2022). _janeaustenr: Jane Austen's Complete Novels_. doi:10.32614/CRAN.package.janeaustenr &lt;https://doi.org/10.32614/CRAN.package.janeaustenr&gt;, R package version 1.0.0, &lt;https://CRAN.R-project.org/package=janeaustenr&gt;.\n  - Xie Y (2025). _knitr: A General-Purpose Package for Dynamic Report Generation in R_. R package version 1.51, &lt;https://yihui.org/knitr/&gt;. Xie Y (2015). _Dynamic Documents with R and knitr_, 2nd edition. Chapman and Hall/CRC, Boca Raton, Florida. ISBN 978-1498716963, &lt;https://yihui.org/knitr/&gt;. Xie Y (2014). \"knitr: A Comprehensive Tool for Reproducible Research in R.\" In Stodden V, Leisch F, Peng RD (eds.), _Implementing Reproducible Computational Research_. Chapman and Hall/CRC. ISBN 978-1466561595.\n\nreport::report_packages()\n\n  - webexercises (version 1.1.0; Barr D, DeBruine L, 2023)\n  - report (version 0.6.2; Makowski D et al., 2023)\n  - checkdown (version 0.0.13; Moroz G, 2020)\n  - R (version 4.5.2; R Core Team, 2025)\n  - janeaustenr (version 1.0.0; Silge J, 2022)\n  - knitr (version 1.51; Xie Y, 2025)\n\n\nTo find out more, I recommend reading Steffi LaZerte‚Äôs blog post on ‚ÄúHow to cite R and R packages‚Äù: https://ropensci.org/blog/2021/11/16/how-to-cite-r-and-r-packages/ (see also Section 14.10).",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Installing `R` and RStudio</span>"
    ]
  },
  {
    "objectID": "4_InstallingR.html#sec-Updates",
    "href": "4_InstallingR.html#sec-Updates",
    "title": "4¬† Installing R and RStudio",
    "section": "4.5 Keeping things up to date",
    "text": "4.5 Keeping things up to date\nAs with all software, it is a good idea to keep your installations of RStudio and R up-to-date. New features are constantly being added, bugs are fixed, and updates may include important security patches.\n\n4.5.1 Updating RStudio\nBy default, RStudio will let you know when a new version is available in a pop-up window. To update RStudio follow the same instructions as for the first installation (see Section 4.2.3). When you add RStudio to your apps, you will get a message warning you that an older version of this programme already exists on your computer (see Figure¬†4.12). You can safely click on the option ‚ÄúReplace‚Äù. All of your previous Global Options settings (@sec-SettingupR) will be transferred to your updated RStudio version so this should be a quick-and-easy process.\n\n\n\n\n\n\nFigure¬†4.12: Warning message on MacOS when installing an updated version of RStudio\n\n\n\nYou can also check which version of RStudio you are running by clicking on the ‚ÄúHelp‚Äù menu in RStudio‚Äôs top toolbar and then selecting the option ‚ÄúAbout RStudio‚Äù. In the ‚ÄúHelp‚Äù drop-down menu, you also have an option to ‚ÄúCheck for Updates‚Äù.\n\n\n4.5.2 Updating R\nUpdating R is a little more complex because you will also need to update all of your R packages, too. Some of the packages that you use may not (yet) be available for the latest R version. This is why, for beginners, I do not recommend updating R in the middle of a project. That said, it is a good idea to keep your R version up-to-date. To find out which version of R you are currently working with, run this command in the Console.\n\nR.version.string\n\n[1] \"R version 4.5.2 (2025-10-31)\"\n\n\nCompare this version number with the number of the latest version available on CRAN (see Figure¬†4.13). If the version that you are running is not the same as the latest R version available on CRAN, you might want to update it. As a rule of thumb, it is a good idea to do an update if your version is more than six months old. To proceed with the update, close RStudio on your computer. Then, follow the same instructions as for the first-time installation of R (see Section 4.2.2).\n\n\n\n\n\n\nFigure¬†4.13: CRAN R for macOS page using the latest recommended R version\n\n\n\n\n\n4.5.3 Updating R packages\nOnce you have updated R, it is important that you also update your installed packages. To do so, run the following command in the Console:\n\nupdate.packages(ask = FALSE, checkBuilt = TRUE)\n\nAlternatively, you can also go to the Packages tab of RStudio and click on the button ‚ÄúUpdate‚Äù. A pop-up window will appear with a list of the packages that need updating. Click on ‚ÄúSelect All‚Äù and ‚ÄúInstall Updates‚Äù.\n\n\n\n\n\n\n\n\n\n\n\n(a) ‚ÄòUpdate‚Äô button in RStudio‚Äôs Packages tab\n\n\n\n\n\n\n\n\n\n\n\n(b) ‚ÄòUpdate Packages‚Äô dialogue in RStudio\n\n\n\n\n\n\n\nFigure¬†4.14: Updating packages using RStudio‚Äôs Graphical User Interface (GUI)\n\n\n\nNote that, if you have installed a lot of packages, this updating operation could take a while. It requires a stable internet connection and a bit of patience. :person_in_lotus_position:\n\n\n\n\n\n\nNoteAn easier way to update R using {installr} (for Windows only)\n\n\n\n\n\nThe {installr} package simplifies updating R on Windows. To install the package use the usual commands:\n\ninstall.packages(\"installr\") # Run this command the first time you use the package.\n\nlibrary(installr) # Run this command everytime you want to update R using this package.\n\nThen, run the updateR() function, which automates the updating process by detecting your current R version, comparing it with the latest available version, and guiding you through the process of downloading and installing the latest version.\nIt is also possible to customise the update process with arguments like updateR(update_packages = FALSE) to skip package updates. For more details, check the documentation using the command ?updateR.\n\n\n\n\n\nCheck your progress üåü\nYou have successfully completed 0 out of 6 questions in this chapter.\nAre you confident that you can‚Ä¶?\n\nInstall R and RStudio (Chapter 4)\nSet up and test RStudio (Section 4.3)\nInstall and load R packages (Section 4.4.2)\nFind out more about R packages and functions (Section 4.4.4)\nCite R packages (Section 4.4.5)\nUpdate RStudio, R, and R packages (Section 4.5.2)\n\nOnce you have successfully completed all the steps outlined in this chapter, you are ready to get staRted with Chapter 5, which provides a hands-on introduction to R in RStudio. ü§ì\n\n\n\n\nDouglas, Alex, Deon Roos, Francesca Mancini & David Lusseau. 2024. An introduction to R. https://intro2r.com/.\n\n\nMizumoto, Atsushi & Luke Plonsky. 2016. R as a lingua franca: Advantages of using r for quantitative research in applied linguistics. Applied Linguistics 37(2). 284‚Äì291. https://doi.org/10.1093/applin/amv025.\n\n\nR Core Team. 2024. R: A language and environment for statistical computing. R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nSilge, Julia. 2022. Janeaustenr: Jane Austen‚Äôs complete novels. https://CRAN.R-project.org/package=janeaustenr.\n\n\nWinter, Bodo. 2020. Statistics for linguists: An introduction using R. Routledge. https://doi.org/10.4324/9781315165547.\n\n\nYe, Jiachu, Xiaoyan Lai & Gary Ka-Wai Wong. 2022. The transfer effects of computational thinking: A systematic review with meta-analysis and qualitative synthesis. Journal of Computer Assisted Learning 38(6). 1620‚Äì1638. https://doi.org/10.1111/jcal.12723.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Installing `R` and RStudio</span>"
    ]
  },
  {
    "objectID": "5_GettingStaRted.html",
    "href": "5_GettingStaRted.html",
    "title": "5¬† Getting staRted in R",
    "section": "",
    "text": "Chapter overview\nNow that you have installed and tested R and RStudio, in this chapter, you will learn how to:\nIf you are already familiar with the basics of R and are keen to learn more about doing statistics in R, you can skip most of this chapter. That said, it‚Äôs probably not a bad idea to have a go at the quiz questions and the final task to refresh your memory.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Getting sta`R`ted in `R`</span>"
    ]
  },
  {
    "objectID": "5_GettingStaRted.html#sec-Console",
    "href": "5_GettingStaRted.html#sec-Console",
    "title": "5¬† Getting staRted in R",
    "section": "5.1 Using the Console",
    "text": "5.1 Using the Console\nThis is what you did in Section 4.2.2 when you tested that RStudio was working properly (using the command: plot(1:10)).\nOne way to write R code in RStudio is to use the Console. If you set up RStudio as recommended here, the Console should be in your top-right pane. You can type a line of code immediately after the command prompt &gt; and press ‚ÄúEnter‚Äù.\nData input is the most basic operation in R. Try inputting a number by typing it out in the Console and then pressing ‚ÄúEnter‚Äù. R will interpret the number and return it. You can input both integers (whole numbers, e.g.¬†13) and decimal numbers (e.g.¬†0.5).\n\n\n\n\n\n\nFigure¬†5.1: Inputting numbers in the Console\n\n\n\nR can handle not only numbers but also text data, known as ‚Äúcharacter strings‚Äù or just ‚Äústrings‚Äù. Strings must always be enclosed in quotation marks. You can choose to use either double quotation marks \" \" or single quotation marks ' ', but it is important to be consistent. In this textbook, we will use double quotation marks throughout.\nTry first inputting a single word and then an entire sentence in the Console as in Figure¬†5.2.\n\n\n\n\n\n\nFigure¬†5.2: Inputting strings in the Console\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\n\nWord processors such as Microsoft Word, Pages, and LibreOffice Writer should not be used to write, edit, or share R scripts. Among other problems, such programmes frequently enable an automatic ‚Äòsmart quotes‚Äô feature, which silently replaces straight quotation marks (\") with curly ones (‚Äú and ‚Äù). These correspond to different Unicode characters, which R does not recognise as valid string delimiters. Hence, curly quotes result in broken code that returns errors, e.g.:\n\ntext &lt;- ‚ÄúThis line of code was edited in Word.‚Äù\n\nError: unexpected input in \"text &lt;- ‚Äú\"\nWord processors are also known to introduce problematic line breaks and cause havoc with automatic capitalisation. To ensure reliability and portability, programming scripts therefore should always be written and edited in plain-text editors such as RStudio, textEdit, or Notepad++. They should be saved and shared in plain-text formats such as .R or .txt (see Section 2.5.1 and Section 5.4.3). This also applies to any notes that you take from this textbook or any programming class that you attend.\n\n\n\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nQ5.1 What happens if you type a word without quotation marks in the Console and then press ‚Äúenter‚Äù?\n\n\n\n\n\nR returns an error suggesting that you probably mistyped the word.\n\n\n\n\nR automatically wraps the word in quotation marks and processes it as a string.\n\n\n\n\nR returns an error message because it interprets the word as an object name or command.\n\n\n\n\nR returns an error message indicating that it expected a number.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Getting sta`R`ted in `R`</span>"
    ]
  },
  {
    "objectID": "5_GettingStaRted.html#sec-Maths",
    "href": "5_GettingStaRted.html#sec-Maths",
    "title": "5¬† Getting staRted in R",
    "section": "5.2 Doing maths in R",
    "text": "5.2 Doing maths in R\nR can also be used as a very powerful calculator. The lines of code in Figure¬†5.3 demonstrate mathematical operations involving addition (+), subtraction (-), division (/), and multiplication (*). Try out a few yourself!\n\n\n\n\n\n\nFigure¬†5.3: Using the R Console as a calculator\n\n\n\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nQ5.2 Try entering 13^2 in the Console. What does the ^ (caret) operator do?\n\n\n\n\n\nThe ^ operator creates a vector, here with 13 occurrences of the integer 2.\n\n\n\n\nThe ^ operator performs an exponentiation operation, here 13 to the power of 2.\n\n\n\n\nThe ^ operator calculates the modulus of a number, here of 13 with 2 as the base.\n\n\n\n\n\n\n\n\n¬†\nQ5.3 Compare 13*13 with 13 * 13. What is the difference in the output?\n\n\n\n\n\nAdding a space generates an error.\n\n\n\n\nIt is impossible to add a space in the R Console.\n\n\n\n\nThere is no difference.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Getting sta`R`ted in `R`</span>"
    ]
  },
  {
    "objectID": "5_GettingStaRted.html#sec-WorkingRObjects",
    "href": "5_GettingStaRted.html#sec-WorkingRObjects",
    "title": "5¬† Getting staRted in R",
    "section": "5.3 Working with R objects",
    "text": "5.3 Working with R objects\nSo far, we have used the Console like a calculator. It‚Äôs important to understand that, just like with a standard calculator, the output of all of our operations was not saved anywhere. If we want to store values, sequences of values, and the results of computations for later use, R allows us to store these as ‚ÄúR objects‚Äù.\n\n5.3.1 Creating objects\nWe use the assignment operator (&lt;-) to assign a value or sequence of values to an object name.\nWrite out the following line to create an object called my.favourite.number that contains your own favourite number.\n\nmy.favourite.number &lt;- 13\n\nWhen you enter this line in the Console and press ‚ÄúEnter‚Äù, it should look like nothing happened: R does not return anything in the Console. Instead, it saves the output in an object called my.favourite.number. However, if you look in your Environment pane, you should see that an object has appeared (Figure¬†5.4).\n\n\n\n\n\n\nFigure¬†5.4: Created object in the Environment pane\n\n\n\nTo save an object containing a character string, we use quotation marks. Create an object called my.favourite.word containing your favourite word (in any written language of your choice).\n\nmy.favourite.word &lt;- \"empathy\"\n\nYour Environment pane should now contain two objects. You can print the content of a stored object by entering the object name in the Console and then pressing ‚ÄúEnter‚Äù (see Figure¬†5.5).\n\n\n\n\n\n\nTip\n\n\n\nIf you‚Äôre feeling lazy or simply want to avoid making a typo, you can type just the first few letters of an object name and then press the ‚ÄúTab‚Äù key (‚Üπ or ‚á•). RStudio will then give you a drop-down menu with possible options. Select the one you want by clicking on it or pressing ‚ÄúEnter‚Äù.\n\n\n\n\n\n\n\n\nFigure¬†5.5: Calling up stored objects in the Console to view their content\n\n\n\n\n\n5.3.2 Object types\nThese two objects are of different types. We can use the class() function to find out which type of object an object is.\n\n\n\n\n\n\nFigure¬†5.6: Using the class() function\n\n\n\nHere, my.favourite.number is a numeric object, while my.favourite.word is a character object.\n\n\n5.3.3 Naming objects\nObject naming conventions in R are fairly flexible. We can use dots (.), underscores (_) and capital letters to make our object names maximally informative and easy for us humans to read. However, spaces and other symbols are not allowed. All of these options work:\n\nword2 &lt;- \"cheerful\"\nmy.second.word &lt;- \"cheerful\"\nmy_second_word &lt;- \"cheerful\"\nMySecondWord &lt;- \"cheerful\"\n\n\n\n\n\n\n\nFigure¬†5.7: Environment pane showing all of the objects currently stored in the R session environment\n\n\n\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nQ5.4 Which of these object names are not allowed in R? Try to create an object with each of these names and see if you get an error message or not.\n\n\n\n\n\nBestWordEver!\n\n\n\n\ntop word\n\n\n\n\nmy-favourite-word\n\n\n\n\nAgn√®s.Favourite.Word\n\n\n\n\n1TopWord\n\n\n\n\nTop1Word\n\n\n\n\n\n\n\n\nüòá Hover for a hint\n\n\n\n\n\n\n\nObject names should not contain spaces or symbols like !, nor should they contain hyphens as the hyphen is reserved for the mathematical operator ‚Äúminus‚Äù. Digits can be used anywhere except at the beginning of an object name. And whilst it is possible to have special characters such as accented letters like ‚Äú√®‚Äù, it is not recommended that you use them for object names.\n\n\n5.3.4 Overwriting and deleting objects\nObject names are unique. If you create a new object with an existing object name, it will overwrite the existing object with the new one. In other words, you will lose the values that you saved in the original object. Try it out by running this line and observing what happens in your Environment pane:\n\nword2 &lt;- \"surprised\"\n\nEarlier on, you created an object called word2 which contained the string ‚Äúcheerful‚Äù. But, by running this new line of code, ‚Äúcheerful‚Äù has been replaced by the string ‚Äúsurprised‚Äù - with no warning that you were about to permanently delete ‚Äúcheerful‚Äù! üò≤\nThe command to delete a single object from your environment is remove() or rm(). Hence, to permanently delete the object MySecondWord, you can use either of these commands:\n\nremove(MySecondWord)\nrm(MySecondWord)",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Getting sta`R`ted in `R`</span>"
    ]
  },
  {
    "objectID": "5_GettingStaRted.html#sec-RScirpt",
    "href": "5_GettingStaRted.html#sec-RScirpt",
    "title": "5¬† Getting staRted in R",
    "section": "5.4 Working with .R scripts",
    "text": "5.4 Working with .R scripts\nIf we shut down RStudio right now, we will lose all of our work so far. This is because the objects that we have created are only saved in the environment of our current R session. Whilst this might sound reckless, it is actually a good thing: In Section 4.3.1 we set our ‚ÄòGlobal Options‚Äô settings in RStudio such that, whenever we restart RStudio, we begin with a clean slate, or a perfectly clean and tidy kitchen. We don‚Äôt want any dirty dishes or stale ingredients lying around when we enter the kitchen! With this in mind, close RStudio now and open it again to start a new R session.\nYou should now have an empty history in your Console pane and an empty Environment pane. Whilst nobody wants to start cooking in a messy kitchen, it‚Äôs also true that, if we want to remember what we did in a previous cooking/baking session, we should write it down. The pages of our recipe book are .R scripts. In the following, we will see that writing scripts is much better than running everything from the Console. It allows us to save and rerun our entire analysis pipeline any time we want. It also ensures that our analyses are reproducible and saves us time as we don‚Äôt have to rewrite our code every time. Crucially, if we made a mistake at any stage, we can go back and correct it and rerun the entire corrected script at the click of a button.\n\n5.4.1 Creating a new .R script\nThere are three ways to create a new .R script in RStudio. Pick the one that you like best:\n\nNavigate to the top menu item ‚ÄúFile‚Äù, then select ‚ÄúNew File‚Äù, then click on ‚ÄúR Script‚Äù.\nClick on the icon with a white page and a green plus button in the top left corner of the tool bar.\nUse the keyboard shortcut Shift + Ctrl/Cmd + N.\n\nWhichever option you chose, RStudio should have opened an empty file in a fourth pane (see Figure¬†5.8). This is the ‚ÄúSource pane‚Äù and it should have appeared in the top-left corner of your RStudio window.\n\n\n\n\n\n\nFigure¬†5.8: RStudio window showing a new, empty .R script that has yet to be saved\n\n\n\n\n\n5.4.2 Running code from an .R script\nWe can now type our code in this empty .R script in the Source pane, just like we did in the Console. Type the following lines of code in the script (see Figure¬†5.9):\n\n13*13\nmy.favourite.number &lt;- 13\nmy.favourite.word &lt;- \"empathy\"\n\nYou will have noticed that when you pressed ‚ÄúEnter‚Äù after every line, nothing happened: Nowhere can we see the result of 13*13, nor have our two objects been saved to the environment as the Environment pane remains empty (see Figure¬†5.9). Just like a recipe for a cake is not an actual, delicious cake, but simply a set of instructions, a script is only a text file that contains lines of code as instructions. For these instructions to be executed, we need to send them to the R Console where they will be interpreted as R code.\n\n\n\n\n\n\nFigure¬†5.9: Writing code in an .R script\n\n\n\nTo send a line of code to the Console (also referred to as ‚Äúexecuting‚Äù or ‚Äúrunning‚Äù code), select the line that you want to execute, or place your mouse cursor anywhere within that line and then click on the ‚ÄúRun‚Äù button (in the top-right corner of the pane, see Figure¬†5.8) or use the keyboard shortcut Ctrl/Cmd + Enter. This shortcut is worth learning, as it will save you a lot of time and effort in the long term.\nTry out these two options to run the three lines of code of your script, then check that:\n\nyou are seeing the result of the mathematical operation in the Console output and\ntwo objects have been added to your environment.\n\nYou can also select several lines of code and run them all with a single click or shortcut. R will interpret the lines in the order that they are listed in your script.\n\n\n5.4.3 Saving an .R script\nIt is now very easy to rerun this script any time we want to redo this calculation and recreate these two R objects. However, our .R script is not yet saved! RStudio is warning us about this by highlighting the file name ‚ÄúUntitled1*‚Äù in red (see Figure¬†5.8). Just like with any unsaved computer file, if we were to shut RStudio down now, we would lose our work. So, let us save this .R script locally, that is on our own computer. To do so either:\n\nNavigate to the top menu item ‚ÄúFile‚Äù and then click on ‚ÄúSave‚Äù,\nClick on the save icon üíæ, or\nUse the keyboard shortcut Ctrl/Cmd+ S.\n\nGive your script a meaningful file name. Remember that file names should be both computer-readable and human-readable. If you navigate to the folder where you saved your .R script, you should see that its file extension is .R. You should also see that it is a tiny file because it contains nothing more than a few lines of text. If you double click on an .R file, RStudio should automatically open it. However, if you wanted, you could open .R files with any text-processing software, such as LibreOffice Writer or Microsoft Word.\n\n\n5.4.4 Writing comments in scripts\nJust like in a recipe book, in addition to writing the actual instructions, we can also write some notes, for example to remind ourselves of why we did things in a particular way or for what occasion we created a special dish. In programming, notes are called ‚Äúcomments‚Äù and they are typically preceded by the # symbol.\nThus, if a line starts with a # symbol, we say that it is ‚Äúcommented out‚Äù. RStudio helpfully displays lines that are commented out in a different colour. These lines will not be interpreted as code even if you send them to the Console. Write the following lines in your script and then try to run them.\n\n#13^13\n\n#StringObject3 &lt;- \"This line has been commented out so the object will not be saved in the environment even if you try to run it.\"\n\nAs you can see, nothing happens. You can also add comments next to a line of interpretable code. In this case, the code is interpreted up until the #. This can be helpful to make a note of what a line of code does, e.g.:\n\nsqrt(169) # Here the sqrt() function will compute the square root of 169.\n\n[1] 13\n\n\nIt is good practice to comment your code when working in an .R script. Comments are crucial for other people to understand what your code does and how it achieves that. But even if you are confident that you are the only person who will ever use your code, it is still a very good idea to use comments to make notes documenting your intentions and your reasoning as you write your script.\nFinally, writing comments in your code as you work through the examples in this book is a great way to reinforce what you are learning. From this chapter onwards, I recommend that, for each chapter, you create an .R script documenting what you have learnt, adding lots of comments to help you remember how things work. This is generally more efficient (and less error-prone!) than trying to take notes in a separate document (e.g.¬†in a Microsoft Word file) or on paper.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Getting sta`R`ted in `R`</span>"
    ]
  },
  {
    "objectID": "5_GettingStaRted.html#sec-RelationalOperators",
    "href": "5_GettingStaRted.html#sec-RelationalOperators",
    "title": "5¬† Getting staRted in R",
    "section": "5.5 Using relational operators",
    "text": "5.5 Using relational operators\nNow that we have saved some objects in our environment, we can use them in calculations. Try out the following operations (and any other that take your fancy) with your own favourite number:\n\nmy.favourite.number / 2\n\n[1] 6.5\n\nmy.favourite.number * my.favourite.number\n\n[1] 169\n\n\nIn additional to the mathematical operations that we saw in Section 5.2, we can also use relational operators such &gt;, &lt;, &lt;=, &gt;=, == and != to make all kinds of comparisons. Try out the following commands to understand how these relational operators work and then have a go at the quiz questions.\n\nmy.favourite.number &gt; 10\nmy.favourite.number &lt; 10\nmy.favourite.number == 25\nmy.favourite.number &gt;= 13\nmy.favourite.number &lt;= -13\nmy.favourite.number != 25\n\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nQ5.5 What is the relational operator that checks whether a value is ‚Äúmore than or equal to‚Äù another value?\n\n\n\n\n\n\n\n\n\n\nüòá Hover for a hint\n\n\n\n\n¬†\nQ5.6 What is the relational operator that checks whether a value ‚Äúis not equal to‚Äù another value?\n\n\n\n\n\n\n\n\n\n\nüòá Hover for a hint\n\n\n\n\n\n\n\nThe relational operators == and != can also be used with character objects. Find out how they work by first creating a new character object with a word that was added to the 2025 edition of the popular French dictionary Petit Larousse:\n\nNew.French.Word &lt;- \"√©cogeste\"\n\nThen copy these lines of code to test how these relational operators work with string characters.\n\nNew.French.Word == \"√©cogeste\" \nNew.French.Word != \"trottinettiste\"\n\nYou will have noticed that the relational operator == tests whether two strings are the same and returns TRUE if that‚Äôs the case. In contrast, != tests whether two strings are different and will therefore return FALSE if they are not different.\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nAbove, we created the following R character string object:\n\nNew.French.Word &lt;- \"√©cogeste\"\n\nQ5.7 Why does this line of code return FALSE even though New.French.Word was assigned the character string ‚Äú√©cogeste‚Äù?\n\nNew.French.Word == \"ecogeste\"\n\n\n\n\n\n\nBecause == cannot be used to compare character strings in French.\n\n\n\n\nBecause \"√©cogeste\" and \"ecogeste\" are two different strings in R.\n\n\n\n\nBecause R automatically removed the accent as object names must be in English.\n\n\n\n\n\n\n\n\n¬†\nQ5.8 Why does this line of code return FALSE?\n\nNew.French.Word == \" √©cogeste\"\n\n\n\n\n\n\nBecause this string includes an additional space character.\n\n\n\n\nBecause R is case-sensitive.\n\n\n\n\nBecause string objects cannot include any special characters. This includes spaces.\n\n\n\n\n\n\n\n\n¬†\nQ5.9 Why does this line of code return FALSE?\n\nNew.French.Word == \"√âcogeste\"\n\n\n\n\n\n\nBecause strings should never start with a capital letter.\n\n\n\n\nBecause this word is not in the dictionary of the Acad√©mie Fran√ßaise.\n\n\n\n\nBecause R is case-sensitive.\n\n\n\n\n\n\n\n\n¬†\nQ5.10 Why does this line of code return FALSE?\n\nNew.French.Word != \"√©cogeste\"\n\n\n\n\n\n\nBecause √©cogeste is no longer a new French word.\n\n\n\n\nBecause this string is in a different text encoding.\n\n\n\n\nBecause this command asks whether New.French.Word is not equal to \"√©cogeste\".",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Getting sta`R`ted in `R`</span>"
    ]
  },
  {
    "objectID": "5_GettingStaRted.html#sec-Errors",
    "href": "5_GettingStaRted.html#sec-Errors",
    "title": "5¬† Getting staRted in R",
    "section": "5.6 Dealing with errors ü§¨",
    "text": "5.6 Dealing with errors ü§¨\nIf you try to run code that R cannot interpret, your Console will display an error message in red. A large part of learning to code is really about learning how to interpret these error messages, and making the most common errors often enough that you immediately know how to fix them. The process of fixing programming errors is called debugging and often involves an array of emotions (see Figure¬†5.10).\n\n\n\n\n\n\nFigure¬†5.10: The joys of debugging by @allison_horst.\n\n\n\nIn R, you will regularly encounter one particular problem that we will call the ‚Äúplus-situation‚Äù. Let‚Äôs take a closer look at this error. Copy and paste this exact line of code in your R Console and hit ‚ÄúEnter‚Äù to run it:\n\nsqrt(my.favourite.number\n\nNotice that, in this erroneous line of code, we have (intentionally) forgotten to include the final bracket. As a result, after you hit ‚ÄúEnter‚Äù, the Console output shows a ‚Äú+‚Äù instead of the result of the mathematical operation (see Figure¬†5.11). The ‚Äú+‚Äù indicates that the line is incomplete and therefore cannot be interpreted yet. Whenever you see a ‚Äú+‚Äù at the start of a command in the Console, R is asking you to complete your line of code.\n\n\n\n\n\n\nFigure¬†5.11: Incomplete function in console\n\n\n\nThere are two ways to fix this. The first method is to complete the line of code directly in the Console. In the above case, this means adding the closing bracket ‚Äú)‚Äù after the ‚Äú+‚Äù and hitting ‚ÄúEnter‚Äù again. Now that the line has been completed, R is able to interpret it as a valid R command and therefore outputs the expected result.\nIf you are running a line of code just once, from the Console, this first method is fine. As we have seen above, however, most of the time, you will write your code in a script rather than in the Console. So this first, on-the-fly method is only recommendable for lines of code that you will genuinely only need once. These include commands to install packages, like install.packages(\"janeaustenr\"), or to consult documentation files, e.g.¬†help(janeaustenr).\nGiven that we will mostly be working in scripts to ensure that our analyses are reproducible (see Chapter 14), let‚Äôs now generate this error from an .R script. To do so, copy and paste the erroneous line of code in your .R script and try to run it by either clicking on the ‚ÄúRun‚Äù icon or using the shortcut Ctrl/Cmd + Enter.\n\nsqrt(my.favourite.number\n\nAgain, our incomplete line of code cannot be interpreted and this generates a ‚Äúplus-situation‚Äù appears in the Console. Now, correct the error in your script by adding the missing closing bracket and try to run the command again.\n\nsqrt(my.favourite.number)\n\nAs shown in Figure¬†5.12, even though we have corrected the problem, we now get an error! ü§Ø At first sight, this does not make sense, but look carefully at what happened in the Console: The line of code that R tried to interpret (in blue) is sqrt(my.favourite.number + sqrt(my.favourite.number), i.e.¬†the combination of the incomplete version of the command plus the complete one. This is obviously nonsense and R tells us so by outputting an error message (see Figure¬†5.12)!\n\n\n\n\n\n\nFigure¬†5.12: Error message in console\n\n\n\nTo run a new line of code, we must see the command prompt &gt; in the Console. So, let‚Äôs generate the error again and learn how to fix it with the second method. Add this erroneous line to your script again and run it:\n\nsqrt(my.favourite.number\n\nThe plus-situation arises again, but we will now solve it using the second method. Head over to the Console and place your cursor next to the +. This time, instead of completing the line by adding a closing bracket, press the Escape key (‚ÄúEsc‚Äù) on your keyboard. This will cancel the incomplete line of code. Then, you can add the missing ) in your script and rerun the newly completed line of code from the Source pane.\nThis second method is the one you should use when you are documenting your code in a script. If you don‚Äôt make the changes immediately in your script, you will forget and you will run into this error again in the future. Think of it like a pastry chef who realises that they need to put a little more baking powder in a cake batter for the texture to be just right, but does not make a note of that change in their recipe book. It‚Äôs quite likely that the chef will forget the next time they bake the cake. If it is one of their assistants who prepares the batter, they will will have no way of knowing that the chef made that change!\nLearning to make sense of error messages is a very important skill that, like all skills, takes practice. Most errors are very easy to fix, if you keep your cool. In fact, 90% of errors are simply typos1, so really nothing worth stressing about!\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nQ5.11 Copy and paste the following lines of code in a new .R script. Try to run each line individually. Each line will generate an error of some kind. In some cases, RStudio will warn you in advance that a line of code is likely wrong by displaying a red cross icon to the left of the erroneous line. If you hover over the red cross icon, RStudio will display a message that may help you to fix the error.\nCan you decode the error messages to find out what is causing these errors and fix these ten erroneous commands?\n\nmy.favourite.word &lt;- \"empathy\"\nmy.favourite.number &lt;- 13\n\n# Error 1:\nmy.favourite.number + my.favorite.number\n\n# Error 2:\nNegin-Fav-Word &lt;- \"Ach so!\" \n\n# Error 3:\nmy.favourite.numbers^2\n\n# Error 4:\n√∂mers_favourite_ number &lt;- 52\n\n# Error 5:\n    √∂mers_favorite_number =   my.favourite..number\n\n# Error 6:\nmy.favourite.number*2 -&gt; half.my.fav.number\n\n# Error 7:\nrose's.favourite.number &lt;- 5\n\n# Error 8:\nBestWordEver &lt;- \"supercalifragilisticexpialidocious\n\n# Error 9:\n2FavNumbers &lt;- my.favourite.number + √∂mers_favourite_number\n\n# Error 10:\ngood.luck &lt;- ŸÖŸàŸÅŸÇ ÿ®ÿßÿ¥ŸäÿØ\"\n\n\n\n\nDebugging is an unavoidable part of writing code. If you‚Äôre stuck and starting to feel frustrated, the best thing you can usually do is to take a short break!\n\n\n\nTime to take a break? (artwork by @allison_horst).\n\n\n\n\n\n\n\n\nNoteClick here for the solutions to Q5.11\n\n\n\n\n\n\nThe first error was object 'my.favorite.number' not found. This means that the object my.favorite.number is not stored in your environment. If you think it is, the problem is most likely due to a typo. Here, my.favorite.number uses American English spelling, whereas we used British English spelling (with a ‚Äúu‚Äù) when we created the object. To correct the error, you need to use exactly the same spelling as when you created the object.\nThe second error is also object 'Negin' not found. However, here we do not expect an object called Negin to be in the environment because what we are actually trying to do is create and save a new object called Negin-Fav-Word! The problem is that R interprets the hyphens in this object name as ‚Äúminus‚Äù and therefore tries to find the object Negin in order to then subtract Fav and Word from it. To correct this error, you need to remove the hyphens or replace them by dots.\nThe third error is yet another object not found error. It is another typo: the correct object name is not in the plural form.\nThe fourth error is Error: unexpected symbol in \"√∂mers_favourite_ number\". In addition, RStudio warned us that there were some ‚Äúunexpected tokens‚Äù in this line of code. The unexpected item is the space between _ and number. To fix this error, you need to remove this space character.\nThe object my.favourite..number is not found because the name of the object saved in the environment does not have two consecutive dots. Note that the error does not come from the fact that this line begins with some white space and includes multiple space characters after the = sign. These added spaces make the line more difficult for us humans to read, but R simply ignores them. Hence, to fix this error, what you need to do is remove one of the consecutive dots in the object name.\nIt is also worth noting that, once you‚Äôve removed the extra dot, this line of code replaces the value originally stored in √∂mers_favourite_number with the value stored in my.favourite.number. If you check your environment pane, you will see that the command has changed √∂mers_favourite_number to 13 with no warning!! In other words, here, the equal sign = behaves in the same way as the assignment operator &lt;-.\nIf you tried to run this line, you will have noticed that it does not actually generate an error. However, you may have noticed that the assignment operator is in the opposite direction (-&gt; instead of &lt;-). This means that my.favourite.number times two is assigned to a new object called half.my.fav.number. Having observed this, you will hopefully want to either amend the line for it to make mathematical sense (my.favourite.number/2 -&gt; half.my.fav.number) or change the name of the new object for it to be meaningful (my.favourite.number*2 -&gt; twice.my.fav.number).\nRunning this line will have caused you to run into a + situation in the console. As explained in Section 5.6, to get out of it, first take your mouse cursor to the Console pane and press the Escape key on your keyboard to cancel the erroneous line. Whilst there is no error message to help you understand where the problem is coming from, you should see that RStudio helpfully displays a red cross icon to the left of the line; hovering over it displays a multi-line message. The first line is the relevant one: unexpected token 's.favourite.number &lt;- 5. This tells us that apostrophes are forbidden in object names. Remove the ' and the error will be fixed.\nThis line also causes a + situation. In this case, it is due to a missing quotation mark. To fix this error, first cancel the incomplete line of code by escaping it. Then, add the missing double quotation mark in your script and rerun the completed line.\nThe message Error: unexpected symbol in \"2FavNumbers\" is due to the fact that object names cannot start with a number. Change the object name to something like TwoFavNumbers or Fav2Numbers to fix this error.\nHere, too, the error message reads: unexpected symbol. However, it is important to remember that the unexpected symbol is not within the character string, but rather has to do with the code syntax used to assign the Persian string to good.luck. In other words, the problem has nothing to do with the fact that the string is in Persian, but rather that one of the quotation marks is missing. You can fix the error by ensuring that the phrase is enclosed in quotation marks.\n\n\n\n\n\nCheck your progress üåü\nYou have successfully completed 0 out of 11 questions in this chapter.\nAre you confident that you can‚Ä¶?\n\nUse the console in RStudio (Section 5.1)\nDo simple maths in R (Section 5.2)\nCreate, write, overwrite, and delete R objects (Section 5.3)\nCreate and save .R scripts (Section 5.4)\nUse relational operators in R (Section 5.5)\nKeep your calm when dealing with error messages in R (Section 5.6)\n\nIf that‚Äôs the case, you‚Äôre ready to go through Chapter 6 to learn how to import research data into R so that, in the following chapters, you can analyse real-life linguistics data!",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Getting sta`R`ted in `R`</span>"
    ]
  },
  {
    "objectID": "5_GettingStaRted.html#footnotes",
    "href": "5_GettingStaRted.html#footnotes",
    "title": "5¬† Getting staRted in R",
    "section": "",
    "text": "I must confess that I made this number up. I don‚Äôt have any reliable number, but it‚Äôs fair to say that it‚Äôs a very large proportion!‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Getting sta`R`ted in `R`</span>"
    ]
  },
  {
    "objectID": "6_ImpoRtingData.html",
    "href": "6_ImpoRtingData.html",
    "title": "6¬† ImpoRting data",
    "section": "",
    "text": "Chapter overview\nMany introductory R textbooks postpone this section until much later by relying on datasets that are directly accessible as R data objects. In real life, however, research data rarely come neatly packaged as an R data object. Your data will most likely be stored in a spreadsheet table or as text files of some kind. And -let‚Äôs be honest- they will be more messy than you would like to admit, making this chapter and the next crucial for learning to do data analysis in R.\nThis chapter will take you through the process of:\nIn future chapters, we will continue to work with these data. We will learn how to ‚Äúclean it up‚Äù for data analysis, before we begin to explore it using descriptive statistics and data visualisations.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Impo`R`ting data</span>"
    ]
  },
  {
    "objectID": "6_ImpoRtingData.html#sec-AccessingData",
    "href": "6_ImpoRtingData.html#sec-AccessingData",
    "title": "6¬† ImpoRting data",
    "section": "6.1 Accessing data from a published study",
    "text": "6.1 Accessing data from a published study\nAs we saw in Section 1.1, it is good practice to share both the data and materials associated with research studies so that others can reproduce and replicate the research.\nIn the following chapters, we will focus on data associated with the following study:\n\nDƒÖbrowska, Ewa. 2019. Experience, Aptitude, and Individual Differences in Linguistic Attainment: A Comparison of Native and Nonnative Speakers. Language Learning 69(S1). 72‚Äì100. https://doi.org/10.1111/lang.12323.\n\n\n\n\n\n\n\nFigure¬†6.1: Title page from the journal Language Learning\n\n\n\nFollow the DOI1 link above and read the abstract to find out what the study was about. You do not need to have institutional or paid access to the full paper to read the abstract.\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nQ6.1 What types of data were collected as part of this study?\n\n\n\n\n\nParticipants' results on a language analytic ability test\n\n\n\n\nParticipants' results on a nonverbal intelligence test\n\n\n\n\nParticipants' results on a vocabulary test\n\n\n\n\nParticipants' results on a collocations test\n\n\n\n\nParticipants' results on a grammar test\n\n\n\n\nSociodemographic information about the participants including their age and level of education\n\n\n\n\n\n\n\n\n¬†\nQ6.2 On average, how did the English L2 speakers perform compared to the native speakers?\n\n\n\n\n\nThe observed differences in performance between native speakers and L2 speakers were not statistically significant.\n\n\n\n\nOn average, native speakers outperformed L2 speakers on all language tasks, with the most significant difference observed in collocation tasks.\n\n\n\n\nOn average, both groups performed equally well on grammar and vocabulary tasks.\n\n\n\n\nOn average, L2 speakers performed better on grammar tasks than native speakers\n\n\n\n\n\n\n\n\n¬†\nQ6.3 Did all native speakers perform better than the L2 speakers in the English vocabulary and grammar tests?\n\n\n\n\n\nYes, the L1 speakers clearly outperformed the L2 speakers in both grammar and vocabulary.\n\n\n\n\nNo, while L1 speakers generally performed better, some L2 speakers demonstrated equally high proficiency in grammar and vocabulary.\n\n\n\n\nThis study only looked at average trends, so no conclusive statement can be made about individual participants.\n\n\n\n\n\n\n\n\n¬†\n\n\n\nThe author, Ewa DƒÖbrowska, has made the data used in this study available on an open repository (see Section 2.4). To find out on which repository, go back to the study‚Äôs DOI link and click on the drop-down menu ‚ÄúSupporting Information‚Äù. It links to a PDF file. Click on the link and scroll to the last page which contains the following information about the data associated with this study:\n\nAppendix S4: Datasets\nDƒÖbrowska, E. (2018). L1 data [Data set]. Retrieved from https://www.iris-database.org/iris/app/home/detail?id=york:935513\nDƒÖbrowska, E. (2018). L2 data [Data set]. Retrieved from https://www.iris-database.org/iris/app/home/detail?id=york:935514\n\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nQ6.4 On which repository/repositories can the data be found?\n\n\n\n\n\nZenodo\n\n\n\n\ndatabase.org\n\n\n\n\nIRIS\n\n\n\n\nResearchGate\n\n\n\n\nAll of the above.\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\n\n\n\nYou may have noticed that the datasets were published in 2018, whereas the article (DƒÖbrowska 2019) was published in the following year. This is very common in academic publications as it can take many months or even years for an article or book to be published, by which time the author(s) may have already made the data available on a repository. This particular article was actually first published on the journal‚Äôs website on 22 October 2018 as an ‚Äúadvanced online publication‚Äù, but was not officially published until March 2019 as part of Volume 69, Issue S1 of the journal (see https://doi.org/10.1111/lang.12323). This explains the discrepancy between the publication date of the datasets and the publication date of the article recorded in the bibliographic reference.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Impo`R`ting data</span>"
    ]
  },
  {
    "objectID": "6_ImpoRtingData.html#sec-SavingData",
    "href": "6_ImpoRtingData.html#sec-SavingData",
    "title": "6¬† ImpoRting data",
    "section": "6.2 Saving and examining the data",
    "text": "6.2 Saving and examining the data\nClick on the two links listed in Appendix S4 and download the two datasets. Note that the URL may take a few seconds to redirect and load. Save the two datasets in an appropriate place on your computer (see Section 3.3), as we will continue to work with these two files in the following chapters.\n\n\n\n\n\n\nNoteWhat‚Äôs a good place to save these files? :thinking-face:\n\n\n\n\n\nIf you haven‚Äôt already done so, I suggest that you create a folder in which you save everything that you create whilst learning from this textbook. This folder could be called something along the lines of DataLiteracyTextbook, 2024_data_literacy, or LeFoll_2024_DataLiteracy (see Section 3.2). Then, within this folder, I recommend that you create another folder called Dabrowska2019 (note how I have not included the ‚ÄúƒÖ‚Äù character in the folder name as this could cause problems), and within this folder, create another folder called data. This is the folder in which you can save these two files.\n\n\n\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nQ6.5 In which data format are these two files saved?\n\n\n\n\n\nThey are both .txt files.\n\n\n\n\nThey are both .csv files.\n\n\n\n\nThey are both .xslx files.\n\n\n\n\nThey are both .html files.\n\n\n\n\n\n\n\n\n¬†\n\n\n\nThe file L1_data.csv contains data about the study‚Äôs L1 participants. It is a delimiter-separated values (DSV) file (see Section 2.5.1). The first five lines of the file are printed below. Note that this is a very wide table as it contains many columns (you can scroll to the right to view all the columns).\nParticipant,Age,Gender,Occupation,OccupGroup,OtherLgs,Education,EduYrs,ReadEng1,ReadEng2,ReadEng3,ReadEng,Active,ObjCl,ObjRel,Passive,Postmod,Q.has,Q.is,Locative,SubCl,SubRel,GrammarR,Grammar,VocabR,Vocab,CollocR,Colloc,Blocks,ART,LgAnalysis\n1,21,M,Student,PS,None,3rd year of BA,17,1,2,2,5,8,8,8,8,8,8,6,8,8,8,78,95,48,73.33333333,30,68.75,16,17,15\n2,38,M,Student/Support Worker,PS,None,NVQ IV Music Performance,13,1,2,3,6,8,8,8,8,8,8,7,8,8,8,79,97.5,58,95.55555556,35,84.375,11,31,13\n3,55,M,Retired,I,None,No formal (City and Guilds),11,3,3,4,10,8,8,8,8,8,7,8,8,8,8,79,97.5,58,95.55555556,31,71.875,5,38,5\n4,26,F,Web designer,PS,None,BA Fine Art,17,3,3,3,9,8,8,8,8,8,8,8,8,8,8,80,100,53,84.44444444,37,90.625,20,26,15\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nQ6.6 Which character is used to separate the values in the file L1_data.csv?\n\n\n\n\n\nspace\n\n\n\n\ndouble quotation mark\n\n\n\n\ncomma\n\n\n\n\ntab\n\n\n\n\ncolon\n\n\n\n\n\n\n\n\n¬†\nQ6.7 Which character is used to delineate the values?\n\n\n\n\n\nspace\n\n\n\n\ndouble quotation mark\n\n\n\n\ndot\n\n\n\n\nnone\n\n\n\n\nsingle quotation mark",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Impo`R`ting data</span>"
    ]
  },
  {
    "objectID": "6_ImpoRtingData.html#sec-RProject",
    "href": "6_ImpoRtingData.html#sec-RProject",
    "title": "6¬† ImpoRting data",
    "section": "6.3 Using Projects in RStudio",
    "text": "6.3 Using Projects in RStudio\nOne of the advantages of working with RStudio is that it allows us to harness the potential of RStudio Projects. Projects help us to keep our digital kitchen nice and tidy. In RStudio, each project has its own directory, environment, and history which means that we can work on multiple projects at the same time and RStudio will keep them completely separate. This means that we can easily switch between cooking different dishes, say a gluten-free egg curry and vegan pancakes, without fear of accidentally setting the wrong temperature on the cooker or contaminating either dish.\nRegardless of whether or not you‚Äôre a keen multi-tasker, RStudio Projects are a great way to help you keep together all the data, scripts, and outputs associated with a single project in an organised manner. In the long run, this will make your life much, much easier. It will also be an absolute lifesaver as soon as you need to share your work with others (e.g.¬†your supervisor, colleagues, reviewers, etc.).\nTo create a new Project, you have two options. In RStudio, you can select ‚ÄòFile‚Äô, then ‚ÄòNew Project‚Ä¶‚Äô (see ‚Äúa‚Äù on Figure¬†6.2). Alternatively, you can click on the Project button in the top-right corner of your RStudio window and then select ‚ÄòNew Project‚Ä¶‚Äô (see ‚Äúb‚Äù on Figure¬†6.2).\n\n\n\n\n\n\nFigure¬†6.2: Create a new project in RStudio\n\n\n\nBoth options will open up a window with three options for creating a new project:\n\nNew Directory (which allows you to create an entirely new project for which you do not yet have a folder on your computer)\nExisting Directory (which allows you to create a project in an existing folder associated with your project)\nVersion Control (see Bryan 2018).\n\nIn Section 6.2, you should have already saved the data that we want to import in a dedicated folder on your computer. Here, a folder is the same as a directory. Hence, you can select the second option: ‚ÄòExisting Directory‚Äô.\nClicking on this option will open up a new window (Figure¬†6.2). Click on ‚ÄòBrowse‚Ä¶‚Äô to navigate to the folder where you intend to save all your work related to DƒÖbrowska (2019). If you followed my suggestions earlier on, this would be a folder called something along the lines of Dabrowska2019. Once you have selected the correct folder, select the option ‚ÄòOpen in a new session‚Äô and then click on ‚ÄòCreate Project‚Äô.\n\n\n\nNew project window\n\n\nCreating an RStudio project generates a new file in your project folder called Dabrowska2019.Rproj. You can see it in the Files pane of RStudio. Note that the extension of this newly created file is .Rproj. Such .Rproj files store information about your project options, which you will not need to edit. More usefully, .Rproj files can be used as shortcuts for opening your projects. To see how this works, shut down RStudio. Then, in your computer file system (e.g.¬†using a File Explorer window on Windows and a Finder window on macOS), navigate to your project folder to locate your .Rproj file (see Figure¬†6.3 (a)). Double-click on the file. This will automatically launch RStudio with all the correct settings for this particular project. Alternatively, you can use the Project button in the top-right corner of your RStudio window to open up a project from RStudio itself (see Figure¬†6.3 (b)).\n\n\n\n\n\n\n\n\n\n\n\n(a) Launching a Project from the File Finder\n\n\n\n\n\n\n\n\n\n\n\n(b) Launching a Project from RStudio\n\n\n\n\n\n\n\nFigure¬†6.3: The two options to open an RProject.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Impo`R`ting data</span>"
    ]
  },
  {
    "objectID": "6_ImpoRtingData.html#sec-WorkingDirectories",
    "href": "6_ImpoRtingData.html#sec-WorkingDirectories",
    "title": "6¬† ImpoRting data",
    "section": "6.4 Working directories",
    "text": "6.4 Working directories\nThe folder in which the .Rproj file was created corresponds to your project‚Äôs working directory. Once you have opened a Project, you can see the path to your project‚Äôs working directory at the top of the Console pane in RStudio. The Files pane should also show the content of this directory.\n\n\n\nContents of the project folder as displayed by RStudio\n\n\nClick on the ‚ÄúNew Folder‚Äù icon in your Files pane to create a new subfolder called analysis. Your folder Dabrowska2019 should now contain an .RProj file and two subfolders called analysis and data.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Impo`R`ting data</span>"
    ]
  },
  {
    "objectID": "6_ImpoRtingData.html#sec-ImportingDataCSV",
    "href": "6_ImpoRtingData.html#sec-ImportingDataCSV",
    "title": "6¬† ImpoRting data",
    "section": "6.5 Importing data from a .csv file",
    "text": "6.5 Importing data from a .csv file\nWe will begin by creating a new R script in which we will write the code necessary to import the data from DƒÖbrowska (2019)‚Äòs study in R. To do so, from the Files pane in RStudio, click on the analysis folder to open it and then click on the ‚ÄôNew Blank File‚Äô icon in the menu bar of the Files pane and select ‚ÄòR Script‚Äô. This will open a new, empty R script in your Source pane. It is best to always begin by saving a newly created file. Save this empty script with a computer- and human-friendly file name such as 1_DataImport.R (Section 3.2). It should now appear in your analysis folder in the Files pane.\nGiven that we want to import two .csv files, we are going to use the function read.csv(). You can find out what this function does by running the command ?read.csv or help(read.csv) in the Console to open up the documentation. This help file contains information about several base R functions used to import data. Scroll down to find the information about the read.csv() function. It reads:\nread.csv(file, header = TRUE, sep = \",\", quote = \"\\\"\",\n         dec = \".\", fill = TRUE, comment.char = \"\", ...)\nThis line from the documentation informs us that this function‚Äôs first argument is the path to the file from which we want to import the data. It also informs us that file is the only argument that does not have a default value (as it is not followed by an equal sign and a value). In this function, file is therefore the only argument that is compulsory. Hence, in theory, all we need to write to import the data is:\n\nL1.data &lt;- read.csv(file = \"data/L1_data.csv\")\n\nIn fact, we could shorten things even further as, unless otherwise specified, R will assume that the first value listed after a function corresponds to the function‚Äôs first argument which, here, is file. In other words, this command and the one above are equivalent:\n\nL1.data &lt;- read.csv(\"data/L1_data.csv\")\n\nThe file path \"data/L1_data.csv\" informs R that the data are located in a subfolder of the project‚Äôs working directory called data and that, within this data subfolder, the file that we want to import is called L1_data.csv. Note that the file extension must be specified (see Section 2.3). Note, also, the file path is separated with a single forward slash /. In R, this should work regardless of the operating system that you are using and, in order to be able to easily share your scripts with others, it is recommended that you use forward slashes even if you are running Windows (Section 3.3).\nAlthough the command above did the job, in practice, it is often safer to spell things out further to remind ourselves of some of the default settings of the function that we are using in case they need to be checked or changed at a later stage. In this example, we will therefore import the data with the following command:\n\nL1.data &lt;- read.csv(file = \"data/L1_data.csv\",\n                    header = TRUE,\n                    sep = \",\",\n                    quote = \"\\\"\",\n                    dec = \".\")\n\nIn the command above, header = TRUE, explicitly tells R to import the first row of the .csv table as column headers rather than values. This is not strictly necessary because, as we saw from the function‚Äôs help file, TRUE is already set as the default value for this argument, but it is good to remind ourselves of how this particular dataset is organised.\nThe arguments sep and quote specify the characters that, in this .csv file are used to separate the values on the one hand, and delineate them, on the other (see Section 2.5.1). As we saw above, DƒÖbrowska (2019)‚Äôs .csv files use the comma as the separator and the double quotation mark as the quoting character. Note that the \" character needs to be preceded by a backslash (\\) (we say it needs to be ‚Äúescaped‚Äù) because otherwise R will interpret it as part of the command syntax, which would lead to an error. Finally, the argument dec = \".\" explicitly tells R that this .csv file uses the dot as the decimal point. In some countries, e.g.¬†Germany and France, the comma is used to represent decimal places so, if you obtain data from a German or French colleague, this setting may need to be changed to dec = \",\" for the data to be imported correctly.\n\n\n\n\n\n\nImportant‚ÄúHell is empty, and all the devils are {here}.‚Äù :smiling-face-with-horns:\n\n\n\n\n\nThis section title borrows a quote from The Tempest by William Shakespeare to reflect the fact that file paths are perhaps the most frequent source of frustration among (beginner) coders. Section 6.6 explains how to deal with the most frequent error messages. Ultimately, however, these errors are typically due to poor data management (see Chapter 3). That‚Äôs because the devil‚Äôs in the detail (remember: no spaces, special characters, differences between different operating systems, etc.). As a result, even advanced users of R and other programming languages frequently find that file path issues continue to plague their work, if they fail to take file management seriously.\nTo make your projects more robust to such issues, I strongly recommend working with the {here} package in addition to RProjects. You will first need to install the package:\n\ninstall.packages(\"here\")\n\nWhen you load the package, it automatically runs the here() function with no argument, which returns the path to your project directory, as determined by the location of the .RProj file associated with your project.\n\nlibrary(here)\n\nYou can now use the here() function to build paths relative to this directory with the following syntax:\n\nhere(\"data\", \"L1_data.csv\")\n\n[1] \"/Users/lefoll/Documents/UzK/RstatsTextbook/data/L1_data.csv\"\n\n\nAnd you can embed this path in your import command like this:\n\nlibrary(here)\n\nL1.data &lt;- read.csv(file = here(\"data\", \"L1_data.csv\"))\n\nMuch like wearing a helmet for extra safety (Figure¬†6.4), {here} makes the paths that you include in your code far more robust. In other words, they are far less likely to fail and break your code when you share your scripts with your colleagues, or run them yourself from different directories or operating systems. For more reasons to use {here}, check out the blog post ‚ÄúWhy should I use the here package when I‚Äôm already using projects?‚Äù by Barrett (2018).\n\n\n\n\n\n\nFigure¬†6.4: Although a fairly common way of working with data in R, using setwd() (see Section 6.12.1) is dangerous and will, sooner or later, cause you and/or your colleagues some nasty accidents. A combination of using RProj and {here} as described above is much safer!",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Impo`R`ting data</span>"
    ]
  },
  {
    "objectID": "6_ImpoRtingData.html#sec-ImportingErrors",
    "href": "6_ImpoRtingData.html#sec-ImportingErrors",
    "title": "6¬† ImpoRting data",
    "section": "6.6 Import errors and issues :pleading-face:",
    "text": "6.6 Import errors and issues :pleading-face:\nIt is crucial that you check whether your data have genuinely been correctly imported. Here‚Äôs a list of things to check (in that order):\n\nWere you able to run the import command without producing any errors? If you are getting an error, remember that this is most likely due to a typo (see Section 5.6)!\n\nIf part of the error message reads ‚ÄúNo such file or directory‚Äù, this means that either the file path or the file name is incorrect. Carefully check the path that you specified in your import command (if you‚Äôre struggling to find the correct path, you may want to try out the import method explained in Section 6.12.2). To ensure that you are not misspelling the name of the file, you can press the tab key on your keyboard to get RStudio to auto-complete the file name for you.\nIf the error message includes the statement ‚Äúcould not find function‚Äù, this means that you have either misspelled the name of the function or this is not a base R function and you have forgotten to load the library to which this function belongs (see Section 6.8).\nAs usual whenever you get an error message, also check that you have included all of the necessary brackets and quotation marks (see Section 5.6).\n\nHas the R data object appeared in your Environment pane? Does it have the expected number of rows (observations) and columns (variables)? L1.data contains 90 observations and 31 variables. If you are getting different numbers, this might be because you previously opened the .csv file with Excel or that your computer converted it to Excel format automatically. To remedy this, ensure that you have followed all the steps described in Section 2.5.2.\nTo view the entire table, use the function View() with the name of your data object as the first and only argument, e.g.¬†View(L1.data)2. This will open up a new tab in your Source pane that displays the full table, much like in a spreadsheet programme. You can search and filter the table in this tab, but you cannot edit it in any way (and that‚Äôs a good thing because, if we want to edit things, we want to ensure that we keep track of our changes in a script!). Browse through the table and check that everything ‚Äúlooks healthy‚Äù. This is much like visually inspecting and smelling ingredients before using them in a recipe. It‚Äôs not perfect but if something is really off, you should notice it. Check that each cell appears to have one and only one value.\nFinally, use the str() function to view the structure of your data object in a more compact way. Using the command str(L1.data) will display a summary of the data.frame in the Console. The summary begins by informing us that this data object is a data.frame, that contains 90 observations and 31 variables. Then, it lists all of the variables, followed by the type of values stored in this variable (e.g.¬†character strings or integers) and then the first few values for each variable. Especially with very wide tables that contain a lot of variables, it is often easier to check the summary of the imported data with str() than with View(), though I would always recommend taking a few seconds to do both. This is time well spent!\n\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nImport both data files from DƒÖbrowska (2019) using the read.csv function as described above. Save the first as the R object L1.data (as in the example above) and the second as L2.data. Then, answer the following questions.\nQ6.8 In the two data files from DƒÖbrowska (2019), each row corresponds to one participant. How many L1 participants were included in this study?\n\n\n\n\n\n17\n\n\n\n\n221\n\n\n\n\n2790\n\n\n\n\n31\n\n\n\n\n90\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\nQ6.9 How many L2 participants were included in DƒÖbrowska (2019)‚Äôs study?\n\n\n\n\n\n67\n\n\n\n\n90\n\n\n\n\n45\n\n\n\n\n220\n\n\n\n\n306\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\nQ6.10 Compare the data frames containing the L1 and L2 data. Which dataset contains more variables?\n\n\n\n\n\nL2.data\n\n\n\n\nL1.data\n\n\n\n\nThey have the same number of variables\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\nQ6.11 You have saved the two datasets to your local R environment as L1.data and L2.data. What kind of R objects are L1.data and L2.data? You can find out by using the command class(). It simply takes the name of the object as its only argument.\n\n\n\n\n\ntable\n\n\n\n\ncharacter\n\n\n\n\ndata frame\n\n\n\n\nlist\n\n\n\n\ninteger\n\n\n\n\ntibble\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\nQ6.12 Why does the L2 dataset contain the variable NativeLg, but not the L1 dataset?\n\n\n\n\n\nBecause some or all of the L1 participants did not wish to answer this question.\n\n\n\n\nBecause this variable was removed from the dataset for data protection reasons.\n\n\n\n\nBecause Dr. DƒÖbrowska decided not to collect this information for L1 participants.\n\n\n\n\nBecause, in this study, all L1 participants have English as their native language.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Impo`R`ting data</span>"
    ]
  },
  {
    "objectID": "6_ImpoRtingData.html#sec-ImportingTabularData",
    "href": "6_ImpoRtingData.html#sec-ImportingTabularData",
    "title": "6¬† ImpoRting data",
    "section": "6.7 Importing tabular data in other formats",
    "text": "6.7 Importing tabular data in other formats\nWe have seen how to load data from a .csv file into R by creating an R data frame object that contains the data extracted from a .csv file. But, as we saw in Chapter 2, not all datasets are stored as .csv files. Fear not: there are many import functions in R, with which you can import pretty much all kinds of data formats! This section introduces a few of the most useful ones for research in the language sciences.\nWe begin with the highly versatile function read.table(). The read.csv() is actually a variant of read.table(). You recall that when we called up the help file for the former using ?read.csv(), we obtained a combined help file for several functions, the first of which was read.table(). By specifying the following arguments as we did earlier, we can actually use the read.table() function to import our .csv file with exactly the same results:\n\nL1.data &lt;- read.table(file = \"data/L1_data.csv\",\n                    header = TRUE,\n                    sep = \",\",\n                    quote = \"\\\"\",\n                    dec = \".\")\n\n\n6.7.1 Tab-separated file\nIn Task 2.3 in Section 2.5.2, you downloaded and examined a DSV file with a .txt extension that was separated by tabs: offlinedataLearners.txt from Schimke et al. (2018).\nIf we change the separator character argument to \\t for tab, we can also import this dataset in R using the read.table() function:\n\nOfflineLearnerData &lt;- read.table(file = \"data/offlinedataLearners.txt\",\n                                        header = TRUE,\n                                        sep = \"\\t\",\n                                        dec = \".\")\n\nFor the command above to work, you will first need to save the file offlinedataLearners.txt to the folder specified in the path. Otherwise, you will get an error message informing you that there is ‚ÄúNo such file or directory‚Äù (see Section 6.6).\n\n\n6.7.2 Semi-colon-separated file\nFigure¬†6.5 displays an extract of the dataset AJT_raw_scores_L2.csv from an experimental study by Busterud et al. (2023). Although this DSV file has a .csv extension, it is actually separated by semicolons. As you can see in Figure¬†6.5, in the file AJT_raw_scores_L2.csv, the comma is used to show the decimal place.\n\n\n\n\n\n\nFigure¬†6.5: Extract of data file AJT_raw_scores_L2.csv from\n\n\n\nIf you look carefully, you will also see that this dataset has some empty cells. These data can be downloaded from https://doi.org/10.18710/JBMAPT. It is delivered with a README text file. It is good practice to include a README file when publishing datasets or code and, as the name suggests, it is always a good idea to actually read README files! :upside-down-face: Among other things, this particular README explains that, in this dataset: ‚ÄúMissing data are represented by empty cells.‚Äù\nIf you call up the help file for the read.table() function again, you will see that there is an argument called na.strings. The default value is NA. When we import this dataset AJT_raw_scores_L2.csv from Busterud et al. (2023), we will therefore need to change this argument to ensure that empty cells are recognised as missing values.\nIn addition to the file path, the command to import this dataset specifies the separator character as the semicolon (sep = \";\"), the character used to represent decimals (dec = \",\"), and empty cells as missing values (na.strings = \"\"):\n\nAJT.raw.scores.L2 &lt;- read.table(file = \"data/AJT_raw_scores_L2.csv\",\n                          header = TRUE,\n                          sep = \";\",\n                          dec = \",\",\n                          na.strings = \"\")\n\nOnce we have run this command, we should check that the data have been correctly imported, for example by using the View() function:\n\nView(AJT.raw.scores.L2)\n\n\n\n           ID L3 Years.of.L3 Gender L3.selfasses L3.grade L2.selfasses L2.grade\n1     BKRE452  2           4      2          2.5        3            4        3\n2     SHEL876  2           4      2          3.0        5            6        5\n3     SVI√ò510  2           4      1          4.0        4            6        5\n4     EHEA194  2           4      1          2.0        2            6        4\n5     ERAO442  2           4      2          3.0        3            5        4\n6     SEIO103  2           4      1          2.0        3            4        3\n7     NMOI241  2           4      1          3.0        4            4        4\n8  BBIE911/77  2           4      1           NA        3           NA        4\n9     UUNO561  2           4      1          2.0        3            3        4\n10    SMAO470  2           4     NA          3.0     &lt;NA&gt;            6       NA\n11    SSID616  2           3      1          2.0        2            6        4\n12    SHRI714  2           1      2          4.0        3            3        3\n13    HALI620  2           1      2          5.0        6            4        4\n\n\nHere, we can see that the data have been correctly imported as a table. The commas have been correctly converted to decimal points and the empty cells are now labelled NA.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Impo`R`ting data</span>"
    ]
  },
  {
    "objectID": "6_ImpoRtingData.html#sec-readr",
    "href": "6_ImpoRtingData.html#sec-readr",
    "title": "6¬† ImpoRting data",
    "section": "6.8 Using {readr} from the {tidyverse} to import tabular files",
    "text": "6.8 Using {readr} from the {tidyverse} to import tabular files\nThe {tidyverse} is a family of packages that we will use a lot in future chapters. This family of package includes the {readr} package which features some very useful functions to import data into R. You can install and load the {readr} package either individually or as part of the {tidyverse} bundle:\n\n# Install the package individually:\ninstall.packages(\"readr\")\n\n# Or install the full tidyverse (this will take a little longer):\ninstall.packages(\"tidyverse\")\n\n# Load the library:\nlibrary(readr)\n\n\n6.8.1 Delimiter-separated values (DSV) files\nThe {readr} package includes functions to import DSV files that are similar, but not identical to the base R functions explained above. The main difference is that the {readr} functions load data into an R object of type ‚Äútibble‚Äù rather than ‚Äúdata frame‚Äù. In practice, this will not make a difference for our work in future chapters. Hence, the following two commands can equally be used to import L1_data.csv:\n\n# Import .csv file using the base R function read.csv():\nL1.data &lt;- read.csv(file = \"data/L1_data.csv\", \n                    header = TRUE, \n                    quote = \"\\\"\")\n\nclass(L1.data)\n\n[1] \"data.frame\"\n\n# Import .csv file using the {readr} function read_csv():\nL1.data &lt;- read_csv(file = \"data/L1_data.csv\", \n                    col_names = TRUE, \n                    quote = \"\\\"\")\n\nclass(L1.data)\n\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\n\nNote that instead of the argument header = TRUE, the {readr} function read_csv() takes the argument col_names = TRUE, which has the same effect.\nThere are a few more differences between the two functions that are worth noting:\n\nIf the column headers in your original data file contain spaces, these will be automatically replaced by dots (.) when you import data using base R functions. By contrast, with the {readr} functions, the spaces will, by default, be retained. As we will see later, this behaviour can represent both an advantage and disadvantage, depending on what you want to do.\nThe {readr} functions are quicker and are therefore recommended if you are importing large datasets.\nIn general, the behaviour of {readr} functions is more consistent across different operating systems and locale settings (e.g.¬†the language in which your operating system is set).\n\nNote that, just like read.csv() was a special case of read.table, the {readr} function read_csv() is a special variant of the more general function read_delim() that can be used to import data from all kinds of DSV files. Check the help file to find out all the options using ?read_delim.\nThe help file informs us that the package includes a function specifically designed to import semi-colon separated file with the the comma as the decimal point: read_csv2(). It further states that ‚Äú[t]his format is common in some European countries.‚Äù If you scroll down the help page, you will see that its usage is summarised in the following way:\nread_csv2(\n  file,\n  col_names = TRUE,\n  col_types = NULL,\n  col_select = NULL,\n  id = NULL,\n  locale = default_locale(),\n  na = c(\"\", \"NA\"),\n  quoted_na = TRUE,\n  quote = \"\\\"\",\n  comment = \"\",\n  trim_ws = TRUE,\n  skip = 0,\n  n_max = Inf,\n  guess_max = min(1000, n_max),\n  progress = show_progress(),\n  name_repair = \"unique\",\n  num_threads = readr_threads(),\n  show_col_types = should_show_types(),\n  skip_empty_rows = TRUE,\n  lazy = should_read_lazy()\n)\nThis overview of the read_csv2 function shows all of the arguments of the function and their default values. For instance, with na = c(\"\", \"NA\"), it tells us that, by default, both empty cells and cells with the value NA will be interpreted by the function as NA values.\nHaving checked the default values for all of the arguments of the read_csv2 function, we may conclude that we can safely use this {readr} function to import the file AJT_raw_scores_L2.csv from Busterud et al. (2023) without changing any of these default values. Hence, all we need is:\n\nAJT.raw.scores.L2 &lt;- read_csv2(file = \"data/AJT_raw_scores_L2.csv\")\n\nNote that, whereas when we used the base R function read.table() the header for the third variable in the file was imported as Years.of.L3, using the {readr} function, the variable is entitled Years of L3.\n\n\n6.8.2 Fixed width files\nFixed width files (with file extensions such as .gz, .bz2 or .xz) are a less common type of data source in the language sciences. In these text-based files, the values are separated not by a specific character such as the comma or the tab, but by a set amount of white/empty space other than a tab. Fixed width files can be loaded using the read_fwf() function from {readr}. Fields can be specified by their widths with fwf_widths() or by their positions with fwf_positions().",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Impo`R`ting data</span>"
    ]
  },
  {
    "objectID": "6_ImpoRtingData.html#sec-SpreadsheetSoftware",
    "href": "6_ImpoRtingData.html#sec-SpreadsheetSoftware",
    "title": "6¬† ImpoRting data",
    "section": "6.9 Importing files from spreadsheet software",
    "text": "6.9 Importing files from spreadsheet software\nIf your data are currently stored in a spreadsheet software (e.g.¬†LibreOffice Calc, Google Sheets, or Microsoft Excel), you can export them to .csv or .tsv. However, if you do not wish to do this (e.g.¬†because your colleague wishes to maintain the spreadsheet format that includes formatting elements such as bold or coloured cells), there are functions to import these file formats directly into R.\n\n6.9.1 LibreOffice Calc\nFor LibreOffice Calc (which you should have installed in Section 1.2), you can install the {readODS} package and use its read_ods() function to import .ods files. Details about all the options can be found here https://www.rdocumentation.org/packages/readODS/versions/2.3.0.\n\n# Install from CRAN (safest option):\ninstall.packages(\"readODS\")\n\n# Or install the development version from Github:\nremotes::install_github(\"ropensci/readODS\")\n\n# Load the library:\nlibrary(readODS)\n\n# Import your .ods data:\nMyLibreOfficeData &lt;- read_ods(\"data/MyLibreOfficeTable.ods\",\n                            sheet = 1,\n                            col_names = TRUE,\n                            na = NA)\n\n\n\n6.9.2 Microsoft Excel\nVarious packages can be used to import Microsoft Excel file formats, but the simplest is {readxl}, which is part of the {tidyverse}. It allows users to import data in both .xlsx and the older .xls format. You can find out more about its various options here: https://readxl.tidyverse.org/.\n\n# Install from CRAN (safest option):\ninstall.packages(\"readxl\")\n\n# Or install the development version from Github:\nremotes::install_github(\"tidyverse/readxl\")\n\n# Load the library:\nlibrary(readxl)\n\n# Import your .ods data:\nMyExcelData &lt;- read_excel(\"data/MyExcelSpreadsheet.xlsx\",\n                            sheet = 1,\n                            col_names = TRUE,\n                            na = NA)\n\n\n\n6.9.3 Google Sheets\nThere are also several ways to import data from Google Sheets. The simplest is to export your tabular data as a .csv, .tsv, .xslx, or .ods file by selecting Google Sheet‚Äôs menu option ‚ÄòFile‚Äô &gt; ‚ÄòDownload‚Äô. Then, you can simply import this downloaded file in R using the corresponding function as described above.\nHowever, if you want to directly import your data from Google Sheets and be able to dynamically update the analyses that you conduct in R even as the input data are amended on Google Sheets, you can use the {googlesheets4} package (which is part of the {tidyverse}):\n\n# Install from CRAN (safest option):\ninstall.packages(\"googlesheets4\")\n\n# Or install the development version from Github:\nremotes::install_github(\"tidyverse/googlesheets4\")\n\n# Load the library:\nlibrary(googlesheets4)\n\n# Import your Google Sheets data using your (spread)sheet's URL:\nMySheetsData &lt;- read_sheet(\"https://docs.google.com/spreadsheets/d/1U6Cf_qEOhiR9AZqTqS3mbMF3zt2db48ZP5v3rkrAEJY/edit#gid=780868077\",\n                            sheet = 1,\n                            col_names = TRUE,\n                            na = \"NA\")\n\n# Or import your Google Sheets data using just the sheet's ID:\nMySheetsData &lt;- read_sheet(\"1U6Cf_qEOhiR9AZqTqS3mbMF3zt2db48ZP5v3rkrAEJY/edit#gid=780868077\",\n                            sheet = 1,\n                            col_names = TRUE,\n                            na = \"NA\")\n\n\n\n\n\n\n\nFigure¬†6.6: Dialogue box to consent to the {tidyverse} API Packages having access to your Google Drive to import directly from a Google Sheet. Read this carefully before clicking on ‚ÄòContinue‚Äô.\n\n\n\n\n\n6.9.4 Importing spreadsheet files with multiple sheets/tabs\nNote that, as spreadsheet software typically allow users to have several ‚Äúsheets‚Äù or ‚Äútabs‚Äù within a file that each contains separate tables, the functions read_excel(), read_ods(), and read_sheet include an argument called sheet which allows you to specify which sheet should be imported. The default value is 1, which simply means that the first one is imported. If your sheets have names, you can also use its name as the argument value, e.g.:\n\nMyExcelData &lt;- read_excel(\"data/MyExcelSpreadsheet.xlsx\",\n                            sheet = \"raw data\",\n                            col_names = TRUE,\n                            na = NA)",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Impo`R`ting data</span>"
    ]
  },
  {
    "objectID": "6_ImpoRtingData.html#sec-SPPS",
    "href": "6_ImpoRtingData.html#sec-SPPS",
    "title": "6¬† ImpoRting data",
    "section": "6.10 Importing data files from SPSS, SAS and Stata",
    "text": "6.10 Importing data files from SPSS, SAS and Stata\nIf you‚Äôve recently switched from working in SPSS, SAS, or Stata (or are collaborating with someone who uses these programmes), it might be useful to know that you can also import the data files created by programmes directly into R using the {haven} package. Details of all the options can be found here: https://haven.tidyverse.org/.\n\n# Install from CRAN (safest option):\ninstall.packages(\"haven\")\n\n# Or install the development version from Github:\nremotes::install_github(\"tidyverse/haven\")\n\n# Load the library:\nlibrary(haven)\n\n# Import an SAS file\nMySASData &lt;- read_sas(\"MySASDataFile.sas7bdat\")\n\n# Import an SPSS file\nMySPSSDataFile &lt;- read_sav(\"MySPSSDataFile.sav\")\n\n# Import a Stata file\nMyStataData &lt;- read_dta(\"MyStataDataFile.dta\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Impo`R`ting data</span>"
    ]
  },
  {
    "objectID": "6_ImpoRtingData.html#sec-OtherFileFormats",
    "href": "6_ImpoRtingData.html#sec-OtherFileFormats",
    "title": "6¬† ImpoRting data",
    "section": "6.11 Importing other file formats",
    "text": "6.11 Importing other file formats\nIn this textbook, we will only deal with DSV files (Section 2.5.1) but, as you can imagine, there are many more R packages and functions that allow you to import all kinds of other file formats. These include .xml, .json and .html files, various database formats, and other files with complex structures (see, e.g.¬†https://rc2e.com/inputandoutput).\nIn addition, fellow linguists are constantly developing new packages to work with file formats that are specific to our discipline. In the spirit of Open Science (see Chapter 1), many are making these packages available to the wider research community by releasing them under open licenses. For example, Linguistics M.A.¬†students Katja Wiesner and Nicolas Werner wrote an R package to facilitate the import of .eaf files generated by the annotation software ELAN (Lausberg & Sloetjes 2009) into R as part of a seminar project supervised by Dr.¬†Fahime (Fafa) Same at the University of Cologne (https://github.com/relan-package/rELAN/?tab=readme-ov-file).",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Impo`R`ting data</span>"
    ]
  },
  {
    "objectID": "6_ImpoRtingData.html#quick-and-dirty-aka-bad-ways-to-import-data-in-r",
    "href": "6_ImpoRtingData.html#quick-and-dirty-aka-bad-ways-to-import-data-in-r",
    "title": "6¬† ImpoRting data",
    "section": "6.12 Quick-and-dirty (aka bad!) ways to import data in R",
    "text": "6.12 Quick-and-dirty (aka bad!) ways to import data in R\nFeel free to skip this section if you got on just fine with the importing method introduced above as the following two methods are problematic for a number of reasons. However, they may come in useful in special cases, which is why both are briefly explained below.\n\n6.12.1 Hardcoding file paths in R scripts :woozy-face:\nWhilst it is certainly not recommended (see e.g. Bryan 2017), it is nonetheless worth understanding this method of working with file paths in R as you may well come across it in other people‚Äôs code.\nInstead of creating an RProject to determine a project‚Äôs working directory (as we did in Section 6.3), it is possible to begin a script with a line of code that sets the working directory for the script using the function setwd(), e.g.:\n\nsetwd(\"/Users/lefoll/Documents/UzK/RstatsTextbook/Dabrowska2019\")\n\nAfterwards, data files can be imported using a relative path from the working directory just like we did earlier.\n\nL1.data &lt;- read.csv(file = \"data/L1_data.csv\")\n\nIf you have to work with an R script that uses this method, you will need to amend the path designated as the working directory by setwd() to the corresponding path on your own computer. This might not sound like much of an issue, but as data scientist, R expert, and statistics professor Jenny Bryan (2017) explains:\n\nThe chance of the setwd() command having the desired effect ‚Äì making the file paths work ‚Äì for anyone besides its author is 0%. It‚Äôs also unlikely to work for the author one or two years or computers from now. The project is not self-contained and portable.\n\nIn the language sciences, not everyone is aware of the severity of these issues. Hence, it is not uncommon for researchers to make their scripts even less reproducible by not setting a working directory at all and, instead, relying exclusively on absolute paths (Section 3.3). Hence, every time they want to import data (and, as we will see later on, export objects from R, too), they write out the full file path in the command like this:\n\nL1.data &lt;- read.csv(file = \"/Users/lefoll/Documents/UzK/RstatsTextbook/Dabrowska2019/data/L1_data.csv\")\n\nHaving to work with such a script is particularly laborious because it means that, if you inherit such a script from a colleague, you will have to manually change every single file path in the script to the corresponding file paths on your own computer. And, as Bryan (2017) points out, this will also apply if you change anything in your own computer directory structure! I hope I‚Äôve made clear that the potential for making errors in the process is far too important to even consider going down that route.\nHowever, should you have to use this method at some point for whatever reason, you can make use of Section 3.3 which explained how to copy full file paths from a file Explorer or Finder window. Note that if there are spaces or other special characters other than _ or - anywhere in your file path, your import command will fail (see Section 3.2 on naming conventions for folders and files). The following command, for instance, will fail and return an error (see Figure¬†6.7) because the folder ‚ÄúUni Work‚Äù contains a space.\n\nL1.data &lt;- read.csv(file = \"/Users/lefoll/Documents/Uni Work/RstatsTextbook/Dabrowska2019/data/L1_data.csv\")\n\n\n\n\n\n\n\nFigure¬†6.7: Error message due to an error in the file path\n\n\n\nThe only way to fix this issue is to remove the space in the name of the folder (in your File Finder or Navigator window) and then amend the file path in your R script accordingly.\n\n\n6.12.2 Importing data using RStudio‚Äôs GUI :face-with-rolling-eyes:\nYou may have noticed that, if you click on a data file from the Files pane in RStudio (Figure¬†6.8), RStudio will offer to import the dataset for you. This looks like (and genuinely is) a very convenient way to import data in an R session using RStudio‚Äôs GUI (graphical user interface).\n\n\n\n\n\n\nFigure¬†6.8: Importing a file from RStudio‚Äôs File pane\n\n\n\nClicking on ‚ÄòImport Dataset‚Äô opens up RStudio‚Äôs ‚ÄòImport Text Data‚Äô dialogue box, which is similar to the one that we saw in LibreOffice Calc (Section 2.5.2). It allows you to select the relevant options to correctly import the file and displays a preview to check that the options that you have selected are correct. You can also specify the name of the R object to which you want to assign the imported data. By default, the name of the data file (minus the file extension and any special characters) is suggested.\n\n\n\n\n\n\nFigure¬†6.9: RStudio‚Äôs ‚ÄòImport Text Data‚Äô dialogue\n\n\n\nAs soon as you click on the ‚ÄòImport‚Äô button, the data are imported and opened using the View() function for you to check the sanity of the data.\nThis importing method works a treat, so what‚Äôs not to like? Well, the first problem is that you are not in full control. You cannot select which import function is used; RStudio decides for you. You may have noticed that it chooses to use the {readr} import functions, rather than the base R ones. There are lots of good reasons to use the {readr} functions (see Section 6.8), but it may not be what you wanted to do. When we do research, it is important for us to be in control of every step of the analysis process.\nSecond, your data import settings are not saved in an .R script as the commands were only sent to the Console: they are part of your script. This means that if you import your data in this way, do some analyses, and then close RStudio, you will have no way of knowing with which settings you imported the data to obtain the results of your analysis! This can have serious consequences for the reproducibility of your work.\nWhilst there is no way of remedying the first issue, the second can easily be fixed. After you have successfully imported your data from RStudio‚Äôs Files pane, you can (and should!) immediately copy the import commands from the Console into your .R script. In this way, the next time you want to re-run your analysis, you can begin by running these import commands directly from your .R script rather than by via RStudio‚Äôs Files pane.\nIf you are running into errors due to incorrect file paths, it can be useful to try to import your data using RStudio‚Äôs GUI to see where you are going wrong by comparing your own attempts with the import commands that RStudio generated.\n\n\nCheck your progress üåü\nYou have successfully completed 0 out of 12 questions in this chapter.\nAre you confident that you can‚Ä¶?\n\nAccess, save, and examine data from published studies (Section 6.1) - (Section 6.2)\nSet up and use projects in RStudio (Section 6.3)\nImport data from .csv files into R (Section 6.5)\nCheck whether your data are correctly imported (Section 6.6)\nImport tabular data in other formats (Section 6.7)\nUse the {readr} package to import tabular files (Section 6.8)\nImport files with single and multiple tabs from spreadsheet software (Section 6.9) - (Section 6.9.4)\nFind out how to import data in other file formats (Section 6.10 - Section 6.11)\n\nNow it‚Äôs time to start exploring research data in R! In the next chapter, you will learn how to work with in-built R functions to find out more about the DSV data from DƒÖbrowska (2019) that you imported in this chapter.\n\n\n\n\nBarrett, Malcolm. 2018. Why should I use the here package when I‚Äôm already using projects? https://malco.io/articles/2018-11-05-why-should-i-use-the-here-package-when-i-m-already-using-projects.\n\n\nBryan, Jennifer. 2018. Let‚Äôs Git started: Happy Git and GitHub for the useR. Open Education Resource. https://happygitwithr.com/.\n\n\nBryan, Jenny. 2017. Project-oriented workflow. Tidyverse.org. https://www.tidyverse.org/blog/2017/12/workflow-vs-script/.\n\n\nBusterud, Guro, Anne Dahl, Dave Kush & Kjersti Faldet Listhaug. 2023. Verb placement in L3 french and L3 german: The role of language-internal factors in determining cross-linguistic influence from prior languages. Linguistic Approaches to Bilingualism. John 13(5). 693‚Äì716. https://doi.org/10.1075/lab.22058.bus.\n\n\nDƒÖbrowska, Ewa. 2019. Experience, aptitude, and individual differences in linguistic attainment: A comparison of native and nonnative speakers. Language Learning 69(S1). 72‚Äì100. https://doi.org/10.1111/lang.12323.\n\n\nLausberg, Hedda & Han Sloetjes. 2009. Coding gestural behavior with the NEUROGES-ELAN system. Behavior Research Methods 41(3). 841‚Äì849. https://doi.org/10.3758/BRM.41.3.841.\n\n\nParsons, Sam, Fl√°vio Azevedo, Mahmoud M. Elsherif, Samuel Guay, Owen N. Shahim, Gisela H. Govaart, Emma Norris, et al. 2022. A community-sourced glossary of open scholarship terms. Nature Human Behaviour. Nature 6(3). 312‚Äì318. https://doi.org/10.1038/s41562-021-01269-4.\n\n\nSchimke, Sarah, Israel de la Fuente, Barbara Hemforth & Saveria Colonna. 2018. First language influence on second language offline and online ambiguous pronoun resolution. Language Learning 68(3). 744‚Äì779. https://doi.org/10.1111/lang.12293.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Impo`R`ting data</span>"
    ]
  },
  {
    "objectID": "6_ImpoRtingData.html#footnotes",
    "href": "6_ImpoRtingData.html#footnotes",
    "title": "6¬† ImpoRting data",
    "section": "",
    "text": "‚ÄúDigital Object Identifiers (DOI) are alpha-numeric strings that can be assigned to any entity, including: publications (including preprints), materials, datasets, and feature films - the use of DOIs is not restricted to just scholarly or academic material. DOIs‚Äùprovides a system for persistent and actionable identification and interoperable exchange of managed information on digital networks.‚Äù (https://doi.org/hb.html). There are many different DOI registration agencies that operate DOIs, but the two that researchers would most likely encounter are Crossref and Datacite.‚Äù (Parsons et al. 2022)‚Ü©Ô∏é\nNote that, unlike the functions that we have used so far, the View() function begins with a capital letter. R is a case-sensitive programming language, which means that view() and View() are not the same thing!‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Impo`R`ting data</span>"
    ]
  },
  {
    "objectID": "7_VariablesFunctions.html",
    "href": "7_VariablesFunctions.html",
    "title": "7¬† VaRiables and functions",
    "section": "",
    "text": "Chapter overview\nIn this chapter, you will learn how to:",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Va`R`iables and functions</span>"
    ]
  },
  {
    "objectID": "7_VariablesFunctions.html#sec-InspectingData",
    "href": "7_VariablesFunctions.html#sec-InspectingData",
    "title": "7¬† VaRiables and functions",
    "section": "7.1 Inspecting a dataset in R",
    "text": "7.1 Inspecting a dataset in R\nIn Section 6.6, we saw that we can use the View() function to display tabular data in a format that resembles that of a spreadsheet programme (see Figure¬†7.2).\nThe two datasets from DƒÖbrowska (2019) are both long and wide so you will need to scroll in both directions to view all the data. RStudio also provides a filter option and a search tool (see Figure¬†7.2). Note that both of these tools can only be used to visually inspect the data. You cannot alter the dataset in any way using these tools (and that‚Äôs a good thing!).\n\nView(L1.data)\n\n\n\n\n\n\n\nFigure¬†7.2: The L1.data object as visualised using the View() function in RStudio\n\n\n\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nQ7.1 The View() function is more user-friendly than attempting to examine the full table in the Console. Try to display the full L2.dataset in the Console by using the command L2.data which is shorthand for print(L2.data). What happens?\n\n\n\n\n\nR only displays the first 22 rows and the columns are not aligned because the Console window is not wide enough.\n\n\n\n\nThe R Console prints out all the data, but not the column headers.\n\n\n\n\nR produces an error message because there are too many rows and the Console window is not long enough.\n\n\n\n\nThe R Console prints the data in a randomly jumbled way.\n\n\n\n\n\n\n\n\nüòá Hover for a hint\n\n\n\n\n¬†\n\n\n\nIn practice, it is often useful to printing subsets of a dataset in the Console to quickly check the sanity of the data. To do so, we can use the function head() that prints the first six rows of a tabular dataset.\n\nhead(L1.data)\n\n  Participant Age Gender             Occupation OccupGroup OtherLgs\n1           1  21      M                Student         PS     None\n2           2  38      M Student/Support Worker         PS     None\n3           3  55      M                Retired          I     None\n4           4  26      F           Web designer         PS     None\n5           5  55      F              Homemaker          I     None\n6           6  58      F                Retired          I     None\n                    Education EduYrs ReadEng1 ReadEng2 ReadEng3 ReadEng Active\n1              3rd year of BA     17        1        2        2       5      8\n2    NVQ IV Music Performance     13        1        2        3       6      8\n3 No formal (City and Guilds)     11        3        3        4      10      8\n4                 BA Fine Art     17        3        3        3       9      8\n5                    O'Levels     12        3        2        3       8      8\n6                    O'Levels     12        1        1        2       4      8\n  ObjCl ObjRel Passive Postmod Q.has Q.is Locative SubCl SubRel GrammarR\n1     8      8       8       8     8    6        8     8      8       78\n2     8      8       8       8     8    7        8     8      8       79\n3     8      8       8       8     7    8        8     8      8       79\n4     8      8       8       8     8    8        8     8      8       80\n5     8      8       8       8     8    7        8     8      8       79\n6     5      1       8       8     7    6        7     8      8       66\n  Grammar VocabR    Vocab CollocR Colloc Blocks ART LgAnalysis\n1    95.0     48 73.33333      30 68.750     16  17         15\n2    97.5     58 95.55556      35 84.375     11  31         13\n3    97.5     58 95.55556      31 71.875      5  38          5\n4   100.0     53 84.44444      37 90.625     20  26         15\n5    97.5     55 88.88889      36 87.500     16  31         14\n6    65.0     48 73.33333      21 40.625      8  15          3\n\n\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nQ7.2 Six is the default number of rows printed by the head() function. Have a look at the function‚Äôs help file using the command ?head to find out how to change this default setting. How would you get R to print the first 10 lines of L2.data?\n\n\n\n\n\nhead(L2.data n = 10L)\n\n\n\n\nhead(L2.data, n = 10)\n\n\n\n\nhead(L2.data, n = 10L)\n\n\n\n\nhead(L2.data, 10)\n\n\n\n\nhead(L2.data, rows = 10)\n\n\n\n\nhead(L2.data, L = 10)\n\n\n\n\n\n\n\n\nüòá Hover for a hint",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Va`R`iables and functions</span>"
    ]
  },
  {
    "objectID": "7_VariablesFunctions.html#sec-Variables",
    "href": "7_VariablesFunctions.html#sec-Variables",
    "title": "7¬† VaRiables and functions",
    "section": "7.2 Working with variables",
    "text": "7.2 Working with variables\n\n7.2.1 Types of variables\nIn statistics, we differentiate between numeric (or quantitative) and categorical (or qualitative) variables. Each variable type can be subdivided into different subtypes. It is very important to understand the differences between these types of data as we frequently have to use different statistics and visualisations depending on the type(s) of variable(s) that we are dealing with.\nSome numeric variables are continuous: they contain measured data that, at least theoretically, can have an infinite number of values within a range (e.g.¬†time). In practice, however the number of possible values depends on the precision of the measurement (e.g.¬†are we measuring time in years, as in the age of adults, or milliseconds, as in participants‚Äô reaction times in a linguistic experiment). Numeric variables for which only a defined set of values are possible are called discrete variables (e.g.¬†number of occurrences of a word in a corpus). Most often, discrete numeric variables represent counts of something.\n\nCategorical variables can be nominal or ordinal. Nominal variables contain unordered categorical values (e.g.¬†participants‚Äô mother tongue or nationality), whereas ordinal variables have categorical values that can be ordered meaningfully (e.g.¬†participants‚Äô proficiency in a specific language where the values beginner, intermediate and advanced or A1, A2, B1, B2, C1 and C2 have a meaningful order). However, the difference between each category (or level) is not necessarily equal. Binary variables are a special case of nominal variable which only has two mutually exclusive outcomes (e.g.¬†true or false in a quiz question).\n\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nQ7.3 Which type of variable is stored in the Occupation column in L1.data?\n\n\n\n\n\nBinary\n\n\n\n\nDiscrete\n\n\n\n\nNominal\n\n\n\n\nContinuous\n\n\n\n\nOther\n\n\n\n\n\n\n\n\n¬†\nQ7.4 Which type of variable is stored in the Gender column in L1.data?\n\n\n\n\n\nNominal\n\n\n\n\nBinary\n\n\n\n\nContinuous\n\n\n\n\nDiscrete\n\n\n\n\nOther\n\n\n\n\n\n\n\n\n¬†\nQ7.5 Which type of variable is stored in the column VocabR in L1.data?\n\n\n\n\n\nOther\n\n\n\n\nNominal\n\n\n\n\nDiscrete\n\n\n\n\nContinuous\n\n\n\n\nBinary\n\n\n\n\n\n\n\n\n¬†\n\n\n\n\n\n7.2.2 Inspecting variables in R\nIn tidy data tabular formats (see Chapter 8), each row corresponds to one observation and each column to a variable. Each cell, therefore, corresponds to a single data point, which is the value of a specific variable (column) for a specific observation (row). As we will see in the following chapters, this data structure allows for efficient and intuitive data manipulation, analysis, and visualisation.\nThe names() functions returns the names of all of the columns of a data frame. Given that the datasets from DƒÖbrowska (2019) are ‚Äòtidy‚Äô, this means that names(L1.data) returns a list of all the column names in the L1 dataset.\n\nnames(L1.data)\n\n [1] \"Participant\" \"Age\"         \"Gender\"      \"Occupation\"  \"OccupGroup\" \n [6] \"OtherLgs\"    \"Education\"   \"EduYrs\"      \"ReadEng1\"    \"ReadEng2\"   \n[11] \"ReadEng3\"    \"ReadEng\"     \"Active\"      \"ObjCl\"       \"ObjRel\"     \n[16] \"Passive\"     \"Postmod\"     \"Q.has\"       \"Q.is\"        \"Locative\"   \n[21] \"SubCl\"       \"SubRel\"      \"GrammarR\"    \"Grammar\"     \"VocabR\"     \n[26] \"Vocab\"       \"CollocR\"     \"Colloc\"      \"Blocks\"      \"ART\"        \n[31] \"LgAnalysis\" \n\n\n\n\n7.2.3 R data types\nA useful way to get a quick and informative overview of a large dataset is to use the function str(), which was mentioned in Section 6.6. It returns the ‚Äúinternal structure‚Äù of any R object. It is particular useful for large tables with many columns\n\nstr(L1.data)\n\n'data.frame':   90 obs. of  31 variables:\n $ Participant: chr  \"1\" \"2\" \"3\" \"4\" ...\n $ Age        : int  21 38 55 26 55 58 31 58 42 59 ...\n $ Gender     : chr  \"M\" \"M\" \"M\" \"F\" ...\n $ Occupation : chr  \"Student\" \"Student/Support Worker\" \"Retired\" \"Web designer\" ...\n $ OccupGroup : chr  \"PS\" \"PS\" \"I\" \"PS\" ...\n $ OtherLgs   : chr  \"None\" \"None\" \"None\" \"None\" ...\n $ Education  : chr  \"3rd year of BA\" \"NVQ IV Music Performance\" \"No formal (City and Guilds)\" \"BA Fine Art\" ...\n $ EduYrs     : int  17 13 11 17 12 12 13 11 11 11 ...\n $ ReadEng1   : int  1 1 3 3 3 1 3 2 1 2 ...\n $ ReadEng2   : int  2 2 3 3 2 1 2 2 1 2 ...\n $ ReadEng3   : int  2 3 4 3 3 2 3 3 1 2 ...\n $ ReadEng    : int  5 6 10 9 8 4 8 7 3 6 ...\n $ Active     : int  8 8 8 8 8 8 7 8 8 8 ...\n $ ObjCl      : int  8 8 8 8 8 5 8 4 7 5 ...\n $ ObjRel     : int  8 8 8 8 8 1 8 8 3 8 ...\n $ Passive    : int  8 8 8 8 8 8 8 8 2 8 ...\n $ Postmod    : int  8 8 8 8 8 8 7 7 6 8 ...\n $ Q.has      : int  8 8 7 8 8 7 8 1 3 0 ...\n $ Q.is       : int  6 7 8 8 7 6 7 8 7 8 ...\n $ Locative   : int  8 8 8 8 8 7 8 8 8 8 ...\n $ SubCl      : int  8 8 8 8 8 8 8 8 7 8 ...\n $ SubRel     : int  8 8 8 8 8 8 8 8 7 8 ...\n $ GrammarR   : int  78 79 79 80 79 66 77 68 58 69 ...\n $ Grammar    : num  95 97.5 97.5 100 97.5 65 92.5 70 45 72.5 ...\n $ VocabR     : int  48 58 58 53 55 48 39 48 31 42 ...\n $ Vocab      : num  73.3 95.6 95.6 84.4 88.9 ...\n $ CollocR    : int  30 35 31 37 36 21 29 33 22 29 ...\n $ Colloc     : num  68.8 84.4 71.9 90.6 87.5 ...\n $ Blocks     : int  16 11 5 20 16 8 8 10 7 9 ...\n $ ART        : int  17 31 38 26 31 15 7 10 6 6 ...\n $ LgAnalysis : int  15 13 5 15 14 3 4 5 2 6 ...\n\n\nAt the top of its output, the function str(L1.data) first informs us that L1.data is a data frame object, consisting of 90 observations (i.e.¬†rows) and 31 variables (i.e.¬†columns). Then, it returns a list of all of the variables included in this data frame. Each line starts with a $ sign and corresponds to one column. First, the name of the column (e.g.¬†Occupation) is printed, followed by the column‚Äôs R data type (e.g.¬†chr for a character string vector), and then its values for the first few rows of the table (e.g.¬†we can see that the first participant in this dataset was a ‚ÄúStudent‚Äù and the second a ‚ÄúStudent/Support Worker‚Äù).\nCompare the outputs of the str() and head() functions in the Console with that of the View() function to understand the different ways in which the same dataset can be examined in RStudio.\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nQ7.6 Use the str() function to examine the internal structure of the L2 dataset. How many columns are there in the L2 dataset?\n\n\n\n\n\n\n\n\n\n\n¬†\nQ7.7 Which of these columns can be found in the L2 dataset, but not the L1 one?\n\n\n\n\n\nNativeLg\n\n\n\n\nArrival\n\n\n\n\nEngWork\n\n\n\n\nOtherLgs\n\n\n\n\nFirstExp\n\n\n\n\nEduYrs\n\n\n\n\n\n\n\n\nüòá Hover for a hint\n\n\n\n\n¬†\nQ7.8 Which type of R object is the variable Arrival stored as?\n\n\n\n\n\nstring character\n\n\n\n\nindex\n\n\n\n\ninterest\n\n\n\n\ndigit\n\n\n\n\ninteger\n\n\n\n\nintelligence\n\n\n\n\n\n\n\n\nüòá Hover for a hint\n\n\n\n\n¬†\nQ7.9 How old was the third participant listed in the L2 dataset when they first moved to an English-speaking country?\n\n\n\n\n\n\n\n\n\n\nüòá Hover for a hint\n\n\n\n\n¬†\nQ7.10 In both datasets, the column Participant contains anonymised participant IDs. Why is the variable Participant stored as string character vector in L1.data, but as an integer vector in L2.data?\n\n\n\n\n\nBecause the L1 participants' IDs only contain whole numbers with no decimal points.\n\n\n\n\nBecause there are more L1 participants than L2 participants.\n\n\n\n\nBecause the L1 participants' IDs are written out as words rather than digits.\n\n\n\n\nBecause some of the L1 participants' IDs contain letters as well as numbers.\n\n\n\n\n\n\n\n\nüòá Hover for a hint\n\n\n\n\n¬†\n\n\n\n\n\n7.2.4 Accessing individual columns in R\nWe can call up individual columns within a data frame using the $ operator. This displays all of the participants‚Äô values for this one variable. As shown below, this works for any type of data.\n\nL1.data$Gender\n\n [1] \"M\" \"M\" \"M\" \"F\" \"F\" \"F\" \"F\" \"M\" \"M\" \"F\" \"F\" \"M\" \"M\" \"F\" \"M\" \"F\" \"M\" \"F\" \"F\"\n[20] \"F\" \"F\" \"F\" \"F\" \"F\" \"F\" \"M\" \"F\" \"M\" \"F\" \"M\" \"F\" \"F\" \"F\" \"M\" \"F\" \"F\" \"M\" \"F\"\n[39] \"F\" \"F\" \"F\" \"F\" \"M\" \"M\" \"F\" \"F\" \"M\" \"F\" \"F\" \"F\" \"F\" \"F\" \"F\" \"F\" \"M\" \"M\" \"M\"\n[58] \"F\" \"F\" \"M\" \"M\" \"M\" \"M\" \"F\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"F\" \"M\" \"F\" \"F\"\n[77] \"M\" \"M\" \"M\" \"F\" \"F\" \"M\" \"M\" \"F\" \"F\" \"M\" \"M\" \"M\" \"F\" \"M\"\n\nL1.data$Age\n\n [1] 21 38 55 26 55 58 31 58 42 59 32 27 60 51 32 29 41 57 60 18 41 60 21 25 26\n[26] 60 57 60 52 25 23 42 59 30 21 21 60 51 62 65 19 65 29 38 37 42 20 32 29 29\n[51] 27 28 29 25 33 25 25 25 52 25 53 22 65 60 61 65 65 61 30 30 32 30 39 29 55\n[76] 18 32 31 20 38 44 18 17 17 17 17 17 17 17 17\n\n\nBefore doing any data analysis, it is crucial to carefully visually examine the data to spot any problems. Ask yourself:\n\nDo the values look plausible?\nAre there any missing values?\n\nLooking at the Gender and Age variables, we can see that all the L1 participants declared being either ‚Äòmale‚Äô (\"M\") or ‚Äòfemale‚Äô (\"F\"), that the youngest were 17 years old, and that no participant was improbably old. A single improbable value is likely to be the result of a data entry error, e.g.¬†a participant or researcher accidentally entered 188 as an age, instead of 18. If you spot lots of improbable or outright weird values (e.g.¬†C, I and PS as age values!), something is likely to have gone wrong during the data import process (see Section 6.6).\nJust like we can save individual numbers and words as R objects to our R environment, we can also save individual variables as individual R objects. As we saw in Section 5.3, in this case, the values of the variable are not printed in the Console, but rather saved to our R environment.\n\nL1.Occupation &lt;- L1.data$Occupation\n\nIf we want to display the content of this variable, we must print our new R object by calling it up with its name, e.g.¬†L1.Occupation. Try it out! As listing all of the L1 participant‚Äôs jobs makes for a very long list, below, we only display the first six values using the head() function.\n\nhead(L1.Occupation)\n\n[1] \"Student\"                \"Student/Support Worker\" \"Retired\"               \n[4] \"Web designer\"           \"Homemaker\"              \"Retired\"",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Va`R`iables and functions</span>"
    ]
  },
  {
    "objectID": "7_VariablesFunctions.html#sec-SquareBrackets",
    "href": "7_VariablesFunctions.html#sec-SquareBrackets",
    "title": "7¬† VaRiables and functions",
    "section": "7.3 Accessing individual data points in R",
    "text": "7.3 Accessing individual data points in R\nWe can also access individual data points from a variable using the index operator, the square brackets ([]). For example, we can access the Occupation value for the fourth L1 participant by specifying that we only want the fourth element of the R object L1.Occupation.\n\nL1.Occupation[4]\n\n[1] \"Web designer\"\n\n\nWe can also do this from the L1.data data frame object directly. To this end, we use a combination of the $ and the [] operators.\n\nL1.data$Occupation[4]\n\n[1] \"Web designer\"\n\n\nWe can access a continuous range of data points using the : operator.\n\nL1.data$Occupation[10:15]\n\n[1] \"Housewife\"             \"Admin Assistant\"       \"Content Editor\"       \n[4] \"School Crossing Guard\" \"Carer/Cleaner\"         \"IT Support\"           \n\n\nOr, if they are not continuous, we can list the numbers of the values that we are interesting in using the combine function (c()) and commas separating each index value.\n\nL1.data$Occupation[c(11,13,29,90)]\n\n[1] \"Admin Assistant\"       \"School Crossing Guard\" \"Dental Nurse\"         \n[4] \"Student\"              \n\n\nIt is also possible to access data points from a table by specifying both the number of the row and the number of the column of the relevant data point(s) using the following pattern:\n[row,column]\nFor example, given that we know that Occupation is stored in the fourth column of L1.data, we can find out the occupation of the L1 participant in the 60th row of the dataset like this:\n\nL1.data[60,4]\n\n[1] \"Train Driver\"\n\n\nAll of these approaches can be combined. For example, here we access the values of the second, third, and fourth columns for the 11th, 13th, 29th, and 90th L1 participants.\n\nL1.data[c(11,13,29,90),2:4]\n\n   Age Gender            Occupation\n11  32      F       Admin Assistant\n13  60      M School Crossing Guard\n29  52      F          Dental Nurse\n90  17      M               Student\n\n\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nThe following two quiz questions focus on the NativeLg variables from the L2 dataset (L2.data).\nQ7.11 Use the index operators to find out the native language of the 26th L2 participant.\n\n\n\n\nCantonese\nChinese\nGerman\nItalian\nGreek\nLithuanian\nMandarin\nPolish\nRussian\nSpanish\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\nQ7.12 Which command(s) can you use to display only the Gender, Occupation, Native language, and Age of the last participant listed in the L2 dataset?\n\n\n\n\n\nL2.data[67:c(2,3,5,9)]\n\n\n\n\nL2.data[67,c(2,3,5,9)]\n\n\n\n\nL2.data[90,c(2:3,5,9)]\n\n\n\n\nL2.data[-1:c(2,3,5,9)]\n\n\n\n\nL2.data[67,c(2:3,5,9)]\n\n\n\n\nL2.data[67 , c(2,3,5,9)]\n\n\n\n\n\n\n\n\nüòá Hover for a hint\n\n\n\n\nüê≠ Click on the mouse for a second hint.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Va`R`iables and functions</span>"
    ]
  },
  {
    "objectID": "7_VariablesFunctions.html#sec-RFunctions",
    "href": "7_VariablesFunctions.html#sec-RFunctions",
    "title": "7¬† VaRiables and functions",
    "section": "7.4 Using built-in R functions",
    "text": "7.4 Using built-in R functions\nWe know from our examination of the L1 dataset from DƒÖbrowska (2019) that it includes 90 English native speaker participants. To find out their mean average age, we could add up all of their ages and divide the sum by 90 (see Section 8.1 for more ways to report the central tendency of a variable).\n\n(21 + 38 + 55 + 26 + 55 + 58 + 31 + 58 + 42 + 59 + 32 + 27 + 60 + 51 + 32 + 29 + 41 + 57 + 60 + 18 + 41 + 60 + 21 + 25 + 26 + 60 + 57 + 60 + 52 + 25 + 23 + 42 + 59 + 30 + 21 + 21 + 60 + 51 + 62 + 65 + 19 + 65 + 29 + 38 + 37 + 42 + 20 + 32 + 29 + 29 + 27 + 28 + 29 + 25 + 33 + 25 + 25 + 25 + 52 + 25 + 53 + 22 + 65 + 60 + 61 + 65 + 65 + 61 + 30 + 30 + 32 + 30 + 39 + 29 + 55 + 18 + 32 + 31 + 20 + 38 + 44 + 18 + 17 + 17 + 17 + 17 + 17 + 17 + 17 + 17) / 90\n\n[1] 37.54444\n\n\nOf course, we would much rather not write all of this out! Especially, as we are very likely to make errors in the process. Instead, we can use the base R function sum() to add up all of the L1 participant‚Äôs ages and divide that by 90.\n\nsum(L1.data$Age) / 90\n\n[1] 37.54444\n\n\nThis already looks much better, but it‚Äôs still less than ideal: What if we decided to exclude some participants (e.g.¬†because they did not complete all of the experimental tasks)? Or decided to add data from more participants? In both these cases, 90 will no longer be the correct denominator to calculate their average age! That‚Äôs why it is better to work out the denominator by computing the total number of values in the variable of interest. To this end, we can use the length() function, which returns the number of values in any given vector.\n\nlength(L1.data$Age)\n\n[1] 90\n\n\nWe can then combine the sum() and the length() functions to calculate the participants‚Äô average age.\n\nsum(L1.data$Age) / length(L1.data$Age)\n\n[1] 37.54444\n\n\nBase R includes lots of useful functions, especially to do statistics. Hence, it will come as no surprise to find that there is a built-in function to calculate mean average values. It is called mean() and is very simple to use.\n\nmean(L1.data$Age)\n\n[1] 37.54444\n\n\nIf you save the values of a variable to your R session environment, you do not need to use the name of the dataset and the $ sign to calculate its mean. Instead, you can directly apply the mean() function to the stored R object.\n\n# Saving the values of the Age variable to a new R object called L1.Age:\nL1.Age &lt;- L1.data$Age\n\n# Applying the mean() function to this new R object:\nmean(L1.Age)\n\n[1] 37.54444\n\n\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nQ7.13 How does the average age of the L2 participants in DƒÖbrowska (2019) compare to that of the L1 participants?\n\n\n\n\n\nOn average, the L2 participants are younger than the L1 participants.\n\n\n\n\nOn average, the L2 participants are older than the L1 participants.\n\n\n\n\nAge is not comparable across two different datasets.\n\n\n\n\nOn average, the L2 participants are the same age than the L1 participants.\n\n\n\n\n\n\n\n\n¬†\n\n\n\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nFor this task, you first need to check that you have saved the following two variables from the L1 dataset to your R environment.\n\nL1.Age &lt;- L1.data$Age\nL1.Occupation &lt;- L1.data$Occupation\n\nQ7.14 Below is a list of useful base R functions. Try them out with the variable L1.Age. What does each function do? Make a note by writing a comment next to each command (see Section 5.4.4). The first one has been done for you.\n\nmean(L1.Age) # The mean() function returns the mean average of a set of number.\nmin()\nmax()\nsort()\nlength()\nmode()\nclass()\ntable()\nsummary()\n\nQ7.15 Age is a numeric variable. What happens if you try these same functions with a character string variable? Find out by trying them out with the variable L1.Occupation which contains words rather than numbers.\n¬†\n\n\n\n\n\n\n\n\n\nNoteClick here for the solutions to Q7.14‚ÄîQ7.15.\n\n\n\n\n\nAs you will have seen, often the clue is in the name of the function - but not always! :winking-face:\n\nmean(L1.Age) # The mean() function returns the mean average of a set of number.\nmean(L1.Occupation) # It does not make sense to calculate a mean average value of a set of words, therefore R returns an 'NA' (not applicable) and a warning in red explaining that the mean() function expects a numeric or logical argument.\n\nmin(L1.Age) # For a numeric variable, min() returns the lowest numeric value.\nmin(L1.Occupation) # For a string variable, min() returns the first value sorted alphabetically.\n\nmax(L1.Age) # For a numeric variable, min() returns the highest numeric value.\nmax(L1.Occupation) # For a string variable, max() returns the last value sorted alphabetically.\n\nsort(L1.Age) # For a numeric variable, sort() returns all of the values of the variable ordered from the smallest to the largest.\nsort(L1.Occupation) # For a string variable, sort() returns of all of the values of the variable in alphabetical order.\n\nlength(L1.Age) # The function length() returns the number of values in the variable.\nlength(L1.Occupation) # The function length() returns the number of values in the variable.\n\nmode(L1.Age) # The function mode() returns the R data type that the variable is stored as.\nmode(L1.Occupation) # The function mode() returns the R data type that the variable is stored as.\n\nclass(L1.Age) # The function mode() returns the R object class that the variable is stored as.\nclass(L1.Occupation) # The function mode() returns the R object class that the variable is stored as.\n\ntable(L1.Age) # For a numeric variable, the function table() outputs a table that tallies the number of occurrences of each unique value in a set of values and sorts them in ascending order.\ntable(L1.Occupation) # For a string variable, the function table() outputs a table that tallies the number of occurrences of each unique value in a set of values and sorts them alphabetically.\n\nsummary(L1.Age) # For a numeric variable, the function summary() outputs six values that, together, summarise the set of values contained in this variable: the minimum and maximum values, the first and third quartiles (more on this in Chapter *), and the mean and median (more on this in Chapter *).\nsummary(L1.Occupation) # For a string variable, the summary() function only outputs the length of the string vector, its object class and data mode. \n\n\n\n\n\n7.4.1 Function arguments\nAll of the functions that we have looked at this chapter so far work with just a single argument: either a vector of values (e.g.¬†a variable from our dataset as in mean(L1.data$Age)) or an entire tabular dataset (e.g.¬†str(L1.data)). When we looked at the head() function, we saw that, per default, it displays the first six rows but that we can change this by specifying a second argument in the function. In R, arguments within a function are always separated by a comma.\n\nhead(L1.Age, n = 6)\n\n[1] 21 38 55 26 55 58\n\n\nThe names of the argument can be specified but do not have to be if they are listed in the order specified in the documentation. You can check the ‚ÄòUsage‚Äô section of a function‚Äôs help file (e.g.¬†using help(head) function or ?head) to find out the order of the arguments. Run the following commands and compare their output:\n\nhead(x = L1.Age, n = 6)\nhead(L1.Age, 6)\nhead(n = 6, x = L1.Age)\nhead(6, L1.Age)\n\nWhilst the first three return exactly the same output, the fourth returns an error because the argument names are not specified and are not in the order specified in the function‚Äôs help file. To avoid making errors and confusing your collaborators and/or future self, it‚Äôs good practice to explicitly name all the arguments except the most obvious ones.\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nLook at the following two lines of code and their (abbreviated) outputs.\n\nL1.data$Vocab\n\n\n\n[1] 73.33333 95.55556 95.55556 84.44444 88.88889 73.33333\n\n\n\nround(L1.data$Vocab)\n\n\n\n[1] 73 96 96 84 89 73\n\n\nQ7.16 Based on your observations, what does the round() function do?\n\n\n\n\n\nThe round() function displays fewer values for ease of reading.\n\n\n\n\nThe round() function rounds off numbers to the nearest whole number.\n\n\n\n\nThe round() function is designed to save screen space for smaller displays.\n\n\n\n\nThe round() function displays just the first two digits of any number.\n\n\n\n\n\n\n\n\n¬†\nQ7.17 Check out the ‚ÄòUsage‚Äô section of the help file on the round() function to find out how to round the Vocab values in the L1 dataset to two decimal places. How can this be achieved?\n\n\n\n\n\nround(L1.data$Vocab, digits = 2)\n\n\n\n\nround(L1.data$Vocab, 2)\n\n\n\n\nround(L1.data$Vocab, digits = -4)\n\n\n\n\nround(L1.data$Vocab: digits = 2)\n\n\n\n\n\n\n\n\nüòá Hover for a hint",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Va`R`iables and functions</span>"
    ]
  },
  {
    "objectID": "7_VariablesFunctions.html#combining-functions-in-r",
    "href": "7_VariablesFunctions.html#combining-functions-in-r",
    "title": "7¬† VaRiables and functions",
    "section": "7.5 Combining functions in R",
    "text": "7.5 Combining functions in R\nCombining functions is where the real fun starts with programming! In Section 7.4, we already combined two functions using a mathematical operator (/). But what if we want to compute L1 participant‚Äôs average age to two decimal places? To do this, we need to combine the mean() function and the round() function. We can do this in two steps.\n\n# Step 1:\nL1.mean.age &lt;- mean(L1.Age)\n\n# Step 2:\nround(L1.mean.age, digits = 2)\n\n[1] 37.54\n\n\nIn step 1, we compute the mean value and save it as an R object and, in step 2, we pass this object through the round() function with the argument digits = 2. There is nothing wrong with this method, but it often require lots of intermediary R objects, which can get rather tiresome.\nIn the following, we will look at two further ways to combine functions in R: nesting and piping.\n\n7.5.1 Nested functions\nThe first method involves lots of brackets (also known as ‚Äòparentheses‚Äô). This is because in nested functions, one function is placed inside another function. The inner function is evaluated first, and its result is passed to the next outer function. Here‚Äôs an example:\n\nround(mean(L1.Age))\n\n[1] 38\n\n\nIn this example, the mean() function is nested inside the round() function. The mean() function calculates the mean of L1.Age, and the result is passed to the round() function, which rounds the result to the nearest integer.\nYou can also pass additional arguments to any of the functions, but you must make sure that you place the arguments within the correct set of brackets.\n\nround(mean(L1.Age), digits = 2)\n\n[1] 37.54\n\n\nIn this example, the argument digits = 2 belongs to the outer function round(); hence it must be placed within the outer set of brackets.\nIn theory, you can nest as many functions as you like, but things can get quite chaotic after more than a couple of functions. You need to make sure that you can trace back which arguments and which brackets belong to which function (see Figure¬†7.3).\n\n\n\n\n\n\nFigure¬†7.3: A schematic representations of a) one function with two arguments, b) two nested functions each with two arguments, and c) three nested functions each with two arguments\n\n\n\n\n\n\n\n\n\nTipTime to think!\n\n\n\nConsider the three lines of code below. Without running them, can you tell which of the three lines of code will output the square root of L1 participant‚Äôs average age to two decimal places?\n\nround(sqrt(mean(L1.Age) digits = 2))\n\nsqrt(round(mean(L1.Age), digits = 2))\n\nround(sqrt(mean(L1.Age)), digits = 2)\n\nThe first line will return an ‚Äúunexpected symbol‚Äù error because it is missing a comma before the argument digits = 2. The second line actually outputs 6.126989, which has more than two decimal places! This is because R interprets the functions from the inside out: first, it calculates the mean value, then it rounds that off to two decimal places, and only then does it compute the square root of that rounded off value. The third line, in contrast, does the rounding operation as the last step. Note that, in the two lines of code that do not produce an error, the brackets around the argument digits = 2 are also located in different places.\nIt is very easy to make bracketing errors when writing code and especially so when nesting functions (see Figure¬†7.3). Watch your commas and brackets (see also Section 5.6)!\n\n\n\n\n7.5.2 Piped Functions\nIf you found all these brackets overwhelming: fear not! There is a second method for combining functions in R, which is often more convenient and almost always easier to decipher. It involves the pipe operator, which in R is |&gt;.1\nThe |&gt; operator passes the output of one function on to the first argument of the next function. This allows us to chain multiple functions together in a much more intuitive way.\n\nL1.Age |&gt; \n mean() |&gt; \n round()\n\n[1] 38\n\n\nIn this example, the object L1.Age is passed on to the first argument of the mean() function. This calculates the mean of L1.Age. Next, this result is passed to the round() function, which rounds the mean value to the nearest integer.\nTo pass additional arguments to any function in the pipeline, we add them within the brackets that belong to that function:\n\nL1.Age |&gt; \n mean() |&gt; \n round(digits = 2)\n\n[1] 37.54\n\n\nLike many of the relational operators we saw in Section 5.5, the R pipe is a combination of two symbols, the computer pipe symbol | and the right angle bracket &gt;. Don‚Äôt worry if you‚Äôre not sure where these two symbols are on your keyboard as RStudio has a handy shortcut for you: Ctrl/Cmd + Shift + M2 (see Figure¬†7.4). I strongly recommend that you write this shortcut on a prominent post-it and learn it asap, as you will need it a lot when you are working in R!\n\n\n\n\n\n\nFigure¬†7.4: Remix of Ren√© Magritte‚Äôs ‚ÄúLa Trahison des images‚Äù with the native R pipe and its RStudio shortcut (Le Foll 2025. Zenodo. https://doi.org/10.5281/zenodo.17440405)\n\n\n\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nQ7.18 Using the R pipe operator, calculate the average mean age of the L2 participants and round off this value to two decimal places. What is the result?\n\n\n\n\n\n\n\n\n\n\n¬†\nQ7.19 Unsurprisingly, in DƒÖbrowska (2019)‚Äòs study, English L1 participants, on average, scored higher in the English vocabulary test than L2 participants. Calculate the difference between L1 and L2 participants‚Äô mean Vocab test results and round off this means difference to two decimal places.\n\n\n\n\n\n\n\n\n\n\nüòá Hover for a hint\n\n\n\n\n¬†\n\n\n\n\n\n\n\n\n\nNoteClick here for a detailed answer to Q7.19\n\n\n\n\n\nThey are lots of ways to tackle this in R. Here is a first approach that involves the pipe operator:\n\n(mean(L1.data$Vocab) - mean(L2.data$Vocab)) |&gt; \n  round(digits = 2)\n\n[1] 16.33\n\n\nNote that this approach requires a set of brackets around the first subtraction operation, otherwise only the second mean value is rounded off to two decimal places. Compare the following lines of code:\n\nmean(L1.data$Vocab) - mean(L2.data$Vocab)\n\n[1] 16.33315\n\n(mean(L1.data$Vocab) - mean(L2.data$Vocab)) |&gt; \n  round(digits = 2)\n\n[1] 16.33\n\nmean(L1.data$Vocab) - round(mean(L2.data$Vocab), digits = 2)\n\n[1] 16.3358\n\n\nAn alternative approach would be to store the difference in means as an R object and, in a second line of code, pass this object to the round() function.\n\nmean.diff.vocab &lt;- mean(L1.data$Vocab) - mean(L2.data$Vocab)\n\nround(mean.diff.vocab, digits = 2)\n\n[1] 16.33\n\n\nOr, you could combine both approaches like this:\n\nmean.diff.vocab &lt;- mean(L1.data$Vocab) - mean(L2.data$Vocab)\nmean.diff.vocab |&gt; \n  round(digits = 2)\n\n[1] 16.33\n\n\nThere is often more than one way to solve problems in R. Choose whichever way you are most comfortable with. As you long as you understand what your code does (see Chapter 15), it doesn‚Äôt matter if it‚Äôs particularly elegant or efficient.\n\n\n\n\n\nCheck your progress üåü\nYou have successfully completed 0 out of 19 questions in this chapter.\nAre you confident that you can‚Ä¶?\n\nInspect a data set in R (Section 7.1)\nRecognise different types of variables (Section 7.2)\nAccess individual columns and data points in R (Section 7.2.4) - (Section 7.3)\nUse built-in R functions and change function arguments (Section 7.4)\nCombine functions in R using both the nesting and the piping methods (Section 7.5.1) - (Section 7.5.2)\n\nYou are now ready to some statistics in R! In Chapter 8, we begin with descriptive statistics.\n\n\n\n\nDƒÖbrowska, Ewa. 2019. Experience, aptitude, and individual differences in linguistic attainment: A comparison of native and nonnative speakers. Language Learning 69(S1). 72‚Äì100. https://doi.org/10.1111/lang.12323.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Va`R`iables and functions</span>"
    ]
  },
  {
    "objectID": "7_VariablesFunctions.html#footnotes",
    "href": "7_VariablesFunctions.html#footnotes",
    "title": "7¬† VaRiables and functions",
    "section": "",
    "text": "This is the native R pipe operator, which was introduced in May 2021 with R version 4.1.0. As a result, you will not find it in code written in earlier versions of R. Previously, piping required an additional R library, the {magrittr} library. The {magrittr} pipe looks like this: %&gt;%. At first sight, they appear to work is in the same way, but there are some important differences. If you are familiar with the {magrittr} pipe and want to understand how it differs from the native R pipe, I recommend this excellent blog post by Isabella Vel√°squez: https://ivelasq.rbind.io/blog/understanding-the-r-pipe/.‚Ü©Ô∏é\nIf, in your version of RStudio, this shortcut produces %&gt;% instead of |&gt;, you have probably not activated the native R pipe option in your RStudio global options (see instructions in Section 4.3.1).‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Va`R`iables and functions</span>"
    ]
  },
  {
    "objectID": "8_DescriptiveStats.html",
    "href": "8_DescriptiveStats.html",
    "title": "8¬† DescRiptive statistics",
    "section": "",
    "text": "Chapter overview\nIn this chapter, you will learn how to:",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Desc`R`iptive statistics</span>"
    ]
  },
  {
    "objectID": "8_DescriptiveStats.html#sec-CentralTendency",
    "href": "8_DescriptiveStats.html#sec-CentralTendency",
    "title": "8¬† DescRiptive statistics",
    "section": "8.1 Measures of central tendency",
    "text": "8.1 Measures of central tendency\nIn Section 7.4, we calculated the mean average age of L1 and L2 participants. Averages are a very useful way to describe the central tendency of a numeric variable - both in science and everyday life. For example, it is useful for me to know if a particular bus journey lasts, on average, 12 minutes or 45 minutes. As it‚Äôs an average value, I am not expecting it to last exactly 12 or 45 minutes, but the average duration is nonetheless helpful to plan my schedule.\nIn science, we use averages to describe the central tendency of numeric variables that are too large for us to be able to examine every single data point. With very small datasets, averages are unnecessary. Imagine that a Breton1 language class in Fiji has five students. Their teacher hardly needs to calculate an average of the students‚Äô vocabulary test results to get an understanding of how her students are doing. She can simply examine all five results!\nNot only are averages of very small datasets unnecessary, they can, in fact, be misleading. Imagine that the five Breton learners got the following results (out of 100) on their vocabulary test:\n\n89, 91, 86, 5, 82\n\nIf we calculate the average result of the class, we get:\n\nmean(c(89, 91, 86, 5, 82))\n\n[1] 70.6\n\n\nThis average grade does not describe very well how any of the students did: Four did much better than that, while one did considerably worse! The results of quantitative studies, however, typically involve much larger datasets so that averages can be a very useful way to describe central tendencies within the data. But it‚Äôs important to understand that, depending on the data, different measures of central tendency make sense. Later on, we will also see that measures of central tendency do not suffice to describe numeric variables: measures of variability (Section 8.3) and good data visualisation (Chapter 10) are also crucial.\n\n8.1.1 Mean\nThe measure of central tendency that we have looked at so far is the arithmetic mean. When people speak of averages, they typically mean mean values.\nIn Section 7.4, we saw that means are calculated by adding up all the values and dividing the sum by the total of values.\n\nsum((c(89, 91, 86, 5, 82))) / 5\n\n[1] 70.6\n\n\nMeans are useful because they are commonly reported and widely understood. Their disadvantage is that they are very susceptible to outliers and skew (which far fewer people actually understand, see Section 8.2). As we saw in the example above, the fact that one ‚Äòoutlier‚Äô learner did very poorly in her Breton vocabulary test led to a much lower average grade than we would expect considering that the other four test-takers did much better than the mean.\nMeans are also frequently misinterpreted as ‚Äúmost likely value‚Äù. This is rarely the case. For example, in this example, 70.6 is not even a score that any of the five students obtained!\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nQ8.1 What was the mean English collocation test score (Colloc) of the L1 participants in DƒÖbrowska (2019)?\n\n\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\nQ8.2 What was the mean English collocation test score (Colloc) of the L2 participants in DƒÖbrowska (2019)?\n\n\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\n\n\n\n\n\n8.1.2 Median\nAnother way to report the central tendency of a set of numeric values like test results is to look for its ‚Äúmiddle value‚Äù. If we sort our five Breton learners‚Äô test results from the lowest to the highest value, we can see that the middle value is 86. This is the median.\n\nsort(c(89, 91, 86, 5, 82))\n\n[1]  5 82 86 89 91\n\n\nFor datasets with an even number of values (e.g.¬†2, 4, 6, 8), we take the mean of the two middle values. Hence, in the following extended dataset with six Breton learners, the median test score is 86.5 because the two middle test results are 86 and 87 and (86¬†+¬†87)¬†/¬†2¬†=¬†86.5.\n\nsort(c(89, 91, 86, 5, 82, 87))\n\n[1]  5 82 86 87 89 91\n\n\nBy now, you will probably not be surprised to learn that there is an R function called median(), which allows us to easily calculate the median value of any set of numbers.\n\nmedian(c(89, 91, 86, 5, 82))\n\n[1] 86\n\nmedian(c(89, 91, 86, 5, 82, 87))\n\n[1] 86.5\n\n\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nQ8.3 What was the median English collocation test score (Colloc) of the L1 participants in DƒÖbrowska (2019)?\n\n\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\nQ8.4 What was the median English collocation test score (Colloc) of the L2 participants in DƒÖbrowska (2019)?\n\n\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\n\n\n\n\n\n8.1.3 Mode\nThe mean and median are measures of central tendency that only work with numeric variables. However, data in the language sciences frequently also include categorical data (see Section 7.2). In the data from DƒÖbrowska (2019), this includes variables such as Gender, NativeLg, OtherLgs, and Occupation. We also need to be able to describe these variables as part of our data analysis. For such categorical variables, the only available measure of central tendency is the mode, which corresponds to the most frequent value in a variable.\nThe table() function outputs how often each unique value occurs in a variable.\n\ntable(L1.data$Gender)\n\n\n F  M \n48 42 \n\n\nFrom this output, we can tell that the mode of the Gender variable in the L1 dataset is F, which stands for ‚Äúfemale‚Äù.\nWhen there are many different unique values (or levels), it makes sense to order them according to their frequency. To do so, we can pipe the output of the table() function into the sort() function (piping was covered in Section 7.5.2). Note that, by default, R sorts by ascending order (decreasing = FALSE). We can change this default to TRUE.\n\ntable(L1.data$Occupation) |&gt; \n  sort(decreasing = TRUE)\n\n\n                  Retired                   Student                Unemployed \n                       14                        14                         4 \n                Housewife            Shop Assistant                   Teacher \n                        3                         3                         3 \n          Admin Assistant            Factory Worker                 Policeman \n                        2                         2                         2 \n        Quantity Surveyor             Admin Officer             Administrator \n                        2                         1                         1 \n              Boilermaker             Care Assisant             Carer/Cleaner \n                        1                         1                         1 \n       Catering Assistant             Civil Servant                   Cleaner \n                        1                         1                         1 \n                    Clerk            Content Editor    Creative Writing Tutor \n                        1                         1                         1 \n Customer Service Advisor              Dental Nurse         Finance Assistant \n                        1                         1                         1 \n   Functions Co-ordinator                 Homemaker           Human Resources \n                        1                         1                         1 \n               IT Support             Manual worker           Nail Technician \n                        1                         1                         1 \nOffice Admin Co-Ordinator         P/T Administrator         Personal Searcher \n                        1                         1                         1 \n      Project Coordinator              Receptionist                    Roofer \n                        1                         1                         1 \n          Sales Assistant     School Crossing Guard    School Crossing Patrol \n                        1                         1                         1 \n          Senior Lecturer                    Singer         Student (college) \n                        1                         1                         1 \n   Student/Support Worker     Supermarket Assistant            Support Worker \n                        1                         1                         1 \n             Train Driver                 Unemploed       University lecturer \n                        1                         1                         1 \n                 Waitress       Warehouse Operative              Web designer \n                        1                         1                         1 \n\n\nWe can see that, among the L1 participants, there were as many ‚ÄúRetired‚Äù participants as there were ‚ÄúStudent‚Äù participants.2 Hence, we have two modes. In general, modal values rarely make good summaries of variables with many different possible values or levels. This is why the mode is not suitable for numeric variables, unless there are only a few possible discrete numeric values (e.g.¬†the values of a five or seven-point Likert scale3).\nCross-tabulations of more than one categorical variable (or numeric variable with just a few unique values) can easily be generated using the table() function. In the following, we cross-tabulate the additional languages that the L1 participants speak with their gender. This allows us to see that most male and female L1 participants did not speak another language other than English. Hence, for both the male and female subsets of L1 participants the mode of the variable OtherLgs is ‚ÄúNone‚Äù.\n\ntable(L1.data$OtherLgs, L1.data$Gender)\n\n         \n           F  M\n  French   1  1\n  German   2  1\n  None    44 40\n  Spanish  1  0\n\n\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nIn comparative studies, it is important to ensure that comparisons are fair and meaningful. For example, it would probably not be very meaningful to compare the linguistic knowledge of a group of undergraduate student learners of English with a group of retired native speakers. In this quiz, you will examine how similar the L1 and the L2 participants in DƒÖbrowska (2019) were in terms of age.\nQ8.5 What was the mean age of the L1 participants in DƒÖbrowska (2019)? Use the round() function to round off the mean value to two decimal places (see Section 7.5.2 for a reminder as to how to combine two functions).\n\n\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\nQ8.6 On average, were the L1 participants in DƒÖbrowska (2019) older or younger than the L2 participants?\n\n\n\n\n\nIt depends whether you base the comparison on mean or median values.\n\n\n\n\nOn average, the L1 participants were older.\n\n\n\n\nIt's impossible to tell based on the available data.\n\n\n\n\nOn average, the L2 participants were older.\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\nQ8.7 Which of the following statements is true about the L1 and L2 participants in DƒÖbrowska (2019)?\n\n\n\n\n\nThe difference between the average ages of the two groups is greater when comparing mean than median ages.\n\n\n\n\nThe difference between the average ages of the two groups is greater when comparing median than mean ages.\n\n\n\n\nThe difference remains the same no matter what type of central tendency measure we use.\n\n\n\n\n\n\n\n\nüòá Hover for a hint",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Desc`R`iptive statistics</span>"
    ]
  },
  {
    "objectID": "8_DescriptiveStats.html#sec-Distributions",
    "href": "8_DescriptiveStats.html#sec-Distributions",
    "title": "8¬† DescRiptive statistics",
    "section": "8.2 Distributions",
    "text": "8.2 Distributions\nData analysis typically begins with the description of individual variables from a dataset. This is referred to as univariate descriptive statistics and is all about describing the distribution of the variables. A distribution is a way to summarise how the values of a variable are dispersed. It tells us things like the variable‚Äôs most frequent values, its range of values, and how the values are clustered or spread out. Examining the shapes and patterns of distributions can help us understand the typical values of the variables of our data, identify outliers, and make informed decisions about how to analyse and visualise our data.\n\n8.2.1 Distributions of categorical variables\nTables can be an effective way to examine the distribution of categorical variables. The table() function outputs the frequency of each level of a categorical variable. By default, the levels are ordered alphabetically.\n\ntable(L1.data$OtherLgs)\n\n\n French  German    None Spanish \n      2       3      84       1 \n\n\nWe saw that we can use the sort() function to change this behaviour.\n\ntable(L1.data$OtherLgs) |&gt; \n    sort(decreasing = TRUE)\n\n\n   None  German  French Spanish \n     84       3       2       1 \n\n\nThe proportions() function allows us to describe the frequency of each level of a categorical variable as a proportion of all data points. This is especially useful if we want to compare the distribution of a categorical variable across different (sub)datasets of different sizes.\n\ntable(L1.data$OtherLgs) |&gt; \n  sort(decreasing = TRUE) |&gt; \n  proportions()\n\n\n      None     German     French    Spanish \n0.93333333 0.03333333 0.02222222 0.01111111 \n\n\nWhen computing proportions, 0 corresponds to 0% and 1 to 100%. If we want to obtain percentages, we therefore need to multiply these numbers by 100. We can therefore see that more than 90% of L1 participants reported not being competent in any language other than English, their native language.\n\nOtherLgs.prop &lt;- \n  table(L1.data$OtherLgs) |&gt; \n  sort(decreasing = TRUE) |&gt; \n  proportions()*100\n\nOtherLgs.prop\n\n\n     None    German    French   Spanish \n93.333333  3.333333  2.222222  1.111111 \n\n\nTo round the values to two decimal places, we can pipe the R object that we created in the previous chunk (OtherLgs.prop) into the round() function.\n\nOtherLgs.prop |&gt; \n  round(digits = 2)\n\n\n   None  German  French Spanish \n  93.33    3.33    2.22    1.11 \n\n\nIn addition to using frequency tables, we can visualise data distributions graphically. Bar plots allow us to easily compare the distribution of categorical variables across different datasets and subsets of data. For example, in Figure¬†8.1, we can see that the distribution of additional languages spoken by the L1 participants is very similar in both the female and the male subset of participants.\n\n\nShow the R code to produce the plot below (but note that data visualisation is covered in Chapter 10).\n# This plot and all other plots in this chapter are generated using the {ggplot2} package from the {tidyverse}. You should have already installed the {tidyverse} in Chapter 6.\n#install.packages(\"tidyverse\")\nlibrary(tidyverse)\n\nL1.data |&gt; \n  mutate(Gender = ifelse(Gender == \"M\", \"Male\", \"Female\")) |&gt;\n  ggplot(aes(x = fct_infreq(OtherLgs), y =..count../sum(..count..))) +\n  geom_bar() +\n  facet_wrap(~ Gender) +\n  theme_bw() +\n  scale_y_continuous(labels = scales::label_percent(),\n                     expand = c(0,0),\n                     limits = c(0,0.5)) +\n  scale_x_discrete(expand = c(0.18,0)) +\n  labs(title = \"Other languages spoken by L1 participants\", \n       x = \"\", \n       y = \"\")\n\n\n\n\n\n\n\n\nFigure¬†8.1: Additional languages spoken by L1 participants in DƒÖbrowska (2019)\n\n\n\n\n\nIn Chapter 10, you will learn how to make plots like Figure¬†8.1 in R using the {ggplot2} package.\n\n\n8.2.2 Distributions of numeric variables\nIn DƒÖbrowska (2019), on average, the L2 participants were younger than the L1 participants.\n\nmean(L1.data$Age) - mean(L2.data$Age)\n\n[1] 4.828027\n\n\nThe difference in mean age was more than four years. But are these two mean values good summaries of the central tendencies of participants‚Äô ages? To check, it is important that we examine the full distribution of participants‚Äô ages. We begin with the distribution of L2 participants‚Äô ages.\nWe first use the table() function to tally L2 participants‚Äô ages.\n\ntable(L2.data$Age)\n\n\n20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 37 38 39 40 41 42 46 47 48 51 \n 1  1  5  3  2  3  2  2  6  3  4  5  2  3  2  2  3  2  5  1  1  1  2  1  1  1 \n52 55 62 \n 1  1  1 \n\n\nAs the above table contains a lot of different values, it‚Äôs easier to visualise these numbers in the form of a bar chart (also called bar plot). The mode (28) has been highlighted in black.\n\n\nShow the R code to produce the plot below (but note that data visualisation is covered in Chapter 10).\nbarplot.mode &lt;- \n  \n  # Take the L2.data data frame and pipe it into the ggplot function:\n  L2.data |&gt;\n\n  # Start a ggplot, mapping Age to the x-axis:\n  ggplot(mapping = aes(x = Age)) + \n\n  # Add a bar plot layer, conditionally fill the bars; bars representing 28 years of age will have a different colour:\n  geom_bar(aes(fill = (Age == 28))) +    \n\n  # Manually control the colours of the bar fill: set the bar representing Age == 28 to \"#0000EE\", and remove the legend:\n  scale_fill_manual(values = c(\"TRUE\" = \"black\"), guide = \"none\") +  \n  \n    # Apply ggplot2's classic theme:\n  theme_classic() +\n\n  # Ensure that there are tick marks for every single whole number and do not extend the limits of y-scale to avoid white space on the plot:\n  scale_y_continuous(name = \"Number of L2 participants\",\n                     breaks = scales::breaks_width(1), \n                     expand = c(0, 0)) +\n\n  # Set the x-axis breaks and remove white space:\n  scale_x_continuous(breaks = scales::breaks_width(1),\n                     expand = c(0, 0)) +\n  \n  # Add label for mode:\n  annotate(\"text\", \n           x = 25, \n           y = 5.8, \n           label = \"mode\", \n           colour = \"black\",\n           family = \"mono\") +\n  \n  # Add curved arrow for mode:\n  annotate(\n    geom = \"curve\",\n    x = 25,\n    y = 5.65, \n    xend = 27.2,\n    yend = 5, \n    curvature = 0.5,\n    arrow = arrow(length = unit(0.2, \"cm\")), \n    colour = \"black\")\n\n# Print the plot\nbarplot.mode\n\n\n\n\n\n\n\n\n\nThanks to this bar chart, it‚Äôs much easier to see that the second most frequent ages after the mode of 28 are 22, 31 and 39. How do these ages compare to the median age?\n\n\nShow R code to generate the plot below.\nbarplot.mode.median &lt;- \n  barplot.mode +\n  geom_bar(aes(fill = ifelse(Age == 31, \"31\", ifelse(Age == 28, \"28\", \"Other\")))) +\n  scale_fill_manual(values = c(\"28\" = \"black\", \"31\" = \"darkred\"), guide = \"none\") +\n  # Add label for median:\n  annotate(\"text\", \n           x = 30.5, \n           y = 5.5, \n           label = \"median\", \n           colour = \"darkred\",\n           family = \"mono\") +\n  \n  # Add curved arrow for median:\n  annotate(\n    geom = \"curve\",\n    x = 30.4,\n    y = 5.35, \n    xend = 30.4,\n    yend = 4.9, \n    curvature = 0.6,\n    arrow = arrow(length = unit(0.2, \"cm\")), \n    colour = \"darkred\")\n\nbarplot.mode.median\n\n\n\n\n\n\n\n\n\nWe can compare the mode (28) and median (31) to the mean (32.72), which, on the following bar chart, is represented as a blue dashed line.\n\n\nShow R code to generate the plot below.\nbarplot.mode.median +\n  geom_vline(aes(xintercept = mean(Age)), \n             color = \"#0000EE\", \n             linetype = \"dashed\",\n             linewidth = 0.8) +\n  \n  # Add label for mean:\n  annotate(\"text\", \n           x = 36, \n           y = 5.3, \n           label = \"mean\", \n           colour = \"#0000EE\",\n           family = \"mono\") +\n  \n  # Add curved arrow for mean:\n  annotate(\n    geom = \"curve\",\n    x = 36,\n    y = 5.15, \n    xend = 33.2,\n    yend = 4.4, \n    curvature = -0.4,\n    arrow = arrow(length = unit(0.2, \"cm\")), \n    colour = \"#0000EE\")\n\n\n\n\n\n\n\n\n\nNext, we can reduce the number of bars by adding together the number of L2 participants aged 20-22, 22-24, 24-26, etc. This is what we call a histogram. Histograms are used to visualise distributions and their bars are called bins because they ‚Äúbin together‚Äù a number of values.\n\n\nShow R code to generate the plot below.\nage.histo &lt;- \n  L2.data |&gt;\n  ggplot(mapping = aes(x = Age)) + \n    geom_vline(aes(xintercept = mean(Age)), \n             color = \"#0000EE\", \n             linetype = \"dashed\",\n             linewidth = 0.8) +\n    geom_vline(aes(xintercept = 28), \n             color = \"black\", \n             linewidth = 0.8) +\n    geom_vline(aes(xintercept = 31), \n             color = \"darkred\", \n             linewidth = 0.8) +  \n  \n  # Add label for mode:\n  annotate(\"text\", \n           x = 24, \n           y = 2.9, \n           label = \"mode\", \n           colour = \"black\",\n           family = \"mono\") +\n  \n  # Add curved arrow for mode:\n  annotate(\n    geom = \"curve\",\n    x = 24,\n    y = 2.6, \n    xend = 27.2,\n    yend = 2, \n    curvature = 0.5,\n    arrow = arrow(length = unit(0.2, \"cm\")), \n    colour = \"black\") +\n  \n  # Add label for mean:\n  annotate(\"text\", \n           x = 36, \n           y = 1.4, \n           label = \"mean\", \n           colour = \"#0000EE\",\n           family = \"mono\") +\n  \n  # Add curved arrow for mean:\n  annotate(\n    geom = \"curve\",\n    x = 36,\n    y = 1.1, \n    xend = 33.2,\n    yend = 0.4, \n    curvature = -0.4,\n    arrow = arrow(length = unit(0.2, \"cm\")), \n    colour = \"#0000EE\") +\n  \n    # Add label for median:\n  annotate(\"text\", \n           x = 25, \n           y = 1.4, \n           label = \"median\", \n           colour = \"darkred\",\n           family = \"mono\") +\n  \n  # Add curved arrow for median:\n  annotate(\n    geom = \"curve\",\n    x = 25,\n    y = 1.1, \n    xend = 30.7,\n    yend = 0.4, \n    curvature = 0.4,\n    arrow = arrow(length = unit(0.2, \"cm\")), \n    colour = \"darkred\") +\n  \n  theme_classic() +\n  scale_y_continuous(name = \"Number of L2 participants\", \n                     breaks = scales::breaks_width(1), \n                     expand = c(0, 0)) +\n  scale_x_continuous(breaks = scales::breaks_width(2), \n                     expand = c(0, 0))\n\nage.histo +\n    geom_histogram(position = \"identity\", \n                 binwidth = 2,\n                 fill = \"black\",\n                 alpha = 0.4)\n\n\n\n\n\n\n\n\n\nIf we reduce the number of bins by having them cover three years instead of two, the histogram looks like this.\n\n\nShow R code to generate the plot below.\nage.histo +\n    geom_histogram(position = \"identity\", \n                 binwidth = 3,\n                 fill = \"black\",\n                 alpha = 0.4) +\n    scale_x_continuous(breaks = scales::breaks_width(3), \n                     expand = c(0, 0))\n\n\n\n\n\n\n\n\n\nAlternatively, we can apply a density function to smooth over the bins of the histogram to generate a density plot of L2 participants‚Äô ages (see purple curve in Figure¬†8.2). Such smoothed curves allow for a better comparison of distribution shapes across different groups and datasets. Note that, in a density plot, the values on the y-axis are no longer counts, but rather density probabilities. We will not use any fancy formulae to work this out mathematically, but you should understand that the total area under the curve (in purple) will always equal to 1, which corresponds to 100% probability. In this dataset, this is because there is a 100% probability that an L2 participant‚Äôs age is between 20 and 62.\n\n\nShow R code to generate the plot below.\n# There is no in-built function in R to calculate the mode of a numeric vector but we can define one ourselves:\nget_mode &lt;- function(x) {\n  ux &lt;- unique(x)\n  ux[which.max(tabulate(match(x, ux)))]\n}\n\nL2.data |&gt;\n  ggplot(mapping = aes(x = Age)) + \n  geom_histogram(aes(x = Age, y = after_stat(density)),\n                 binwidth = 3,\n                 fill = \"black\",\n                 alpha = 0.4) + \n  geom_density(colour = \"purple\",\n               fill = \"purple\",\n               alpha = 0.2,\n               linewidth = 0.8) +\n  geom_vline(aes(xintercept = mean(Age)),\n             color = \"#0000EE\",\n             linetype = \"dashed\",\n             linewidth = 0.8) +\n  geom_vline(aes(xintercept = get_mode(Age)),\n             color = \"black\",\n             linewidth = 0.8) +\n  geom_vline(aes(xintercept = median(Age)),\n             color = \"darkred\",\n             linewidth = 0.8) +  \n  # Add label for mode:\n  annotate(\"text\",\n           x = 25,\n           y = 0.029,\n           label = \"mode\",\n           colour = \"black\",\n           family = \"mono\") +\n\n  # Add curved arrow for mode:\n  annotate(\n    geom = \"curve\",\n    x = 25,\n    y = 0.028,\n    xend = 27.5,\n    yend = 0.025,\n    curvature = 0.6,\n    arrow = arrow(length = unit(0.2, \"cm\")),\n    colour = \"black\") +\n\n  # Add label for mean:\n  annotate(\"text\",\n           x = 36,\n           y = 0.014,\n           label = \"mean\",\n           colour = \"#0000EE\",\n           family = \"mono\") +\n\n  # Add curved arrow for mean:\n  annotate(\n    geom = \"curve\",\n    x = 36,\n    y = 0.011,\n    xend = 33.2,\n    yend = 0.004,\n    curvature = -0.4,\n    arrow = arrow(length = unit(0.2, \"cm\")),\n    colour = \"#0000EE\") +\n  \n  # Add label for median:\n  annotate(\"text\",\n           x = 25.5,\n           y = 0.01,\n           label = \"median\",\n           colour = \"darkred\",\n           family = \"mono\") +\n\n  # Add curved arrow for median:\n  annotate(\n    geom = \"curve\",\n    x = 25.5,\n    y = 0.008,\n    xend = 30.8,\n    yend = 0.004,\n    curvature = 0.4,\n    arrow = arrow(length = unit(0.2, \"cm\")),\n    colour = \"darkred\") +  \n\n  theme_classic() +\n  scale_y_continuous(name = \"Density\",\n                     breaks = scales::breaks_width(0.01),\n                     expand = c(0, 0)) +\n  scale_x_continuous(breaks = scales::breaks_width(3),\n                     expand = c(0, 0))\n\n\n\n\n\n\n\n\nFigure¬†8.2: Density plot showing the distribution of L2 participants‚Äô ages\n\n\n\n\n\nIf we wanted to work out the probability of an L2 participant being between 42 and 62 years old, we would have to calculate the area under the curve between these two points on the x-axis. Even without doing any maths, you can see that this area is considerably smaller than between the ages of 22 and 42. This means that, in this dataset, participants are considerably more likely to be between 22 and 42 than between 42 and 62 years old.4\nThe density plot of L2 participants‚Äô ages features a characteristic bell-shaped curve, which indicates that the distribution resembles a normal distribution. However, it also features a long tail towards the older years. We are therefore dealing with a skewed distribution. Skewness is a measure of asymmetry in a distribution. Skewed distributions occur when one tail end of the bell is longer than the other. Here, the asymmetry is due to the fact that DƒÖbrowska (2019)‚Äôs L2 data includes quite a few participants who were older than 40 at the time of the study, whereas there were none who were younger than 20. As the tail is to the right of the plot, this is a right skewed (or positive) distribution.\nThe median is usually better than the mean for describing the central tendency of a skewed distribution because it is less susceptible to the outlier(s) contained in the tail of a skewed distribution (see Section 8.1.2). Figure¬†8.2 confirms that the median is a better approximation of L2 participants‚Äô ages than the mean.\n\n\n8.2.3 Normal (or Gaussian) distributions\nIn a perfectly normally distributed variable, the mean and the median are exactly the same. They are both found at the centre of the distribution and the bell shape of the distribution is perfectly symmetrical. Hence, the skewness of a normal distribution is near zero.\nPerfectly normal distributions, however, are very rarely found in real life! Here is what the normal distribution of 10,000 participants‚Äô age might look like in real life (using numbers randomly generated from a perfectly normal distribution thanks to the R function rnorm()).\n\n\nShow R code to generate the plot below.\n# The {truncnorm} package contains density, probability, quantile and random number generation functions for the truncated normal distribution:\n#install.packages(\"truncnorm\")\nlibrary(truncnorm)\n\nset.seed(42)\nnormal.age.sd8 &lt;- round(rtruncnorm(mean = 35, sd = 8, n = 10000, a = 10, b = 100))\n#get_mode(normal.age.sd8) \n\nggplot(mapping = aes(x = normal.age.sd8)) + \n    geom_vline(aes(xintercept = mean(normal.age.sd8)),\n             color = \"#0000EE\",\n             linetype = \"dashed\",\n             linewidth = 0.8) +\n    geom_vline(aes(xintercept = median(normal.age.sd8)),\n             color = \"darkred\",\n             #linetype = \"dotted\",\n             linewidth = 0.6) +\n    # geom_vline(aes(xintercept = get_mode(normal.age.sd8)),\n    #          color = \"black\",\n    #          linewidth = 0.8) +  \n  \n  # Add label for mean:\n  annotate(\"text\",\n           x = 30.6,\n           y = 0.014,\n           label = \"mean\",\n           colour = \"#0000EE\",\n           family = \"mono\") +\n\n  # Add curved arrow for mean:\n  annotate(\n    geom = \"curve\",\n    x = 30.5,\n    y = 0.013,\n    xend = 34.7,\n    yend = 0.008,\n    curvature = 0.5,\n    arrow = arrow(length = unit(0.2, \"cm\")),\n    colour = \"#0000EE\") +  \n  \n  # Add label for median:\n  annotate(\"text\",\n           x = 38,\n           y = 0.007,\n           label = \"median\",\n           colour = \"darkred\",\n           family = \"mono\") +\n\n  # Add curved arrow for median:\n  annotate(\n    geom = \"curve\",\n    x = 38,\n    y = 0.006,\n    xend = 35.2,\n    yend = 0.004,\n    curvature = -0.5,\n    arrow = arrow(length = unit(0.2, \"cm\")),\n    colour = \"darkred\") +\n  theme_classic() +\n  scale_y_continuous(name = \"Density\",\n                     breaks = scales::breaks_width(0.01),\n                     expand = c(0, 0)) +\n  scale_x_continuous(name = \"Age\",\n                     breaks = scales::breaks_width(2),\n                     expand = c(0, 0)) +\n  geom_histogram(aes(x = normal.age.sd8, y = after_stat(density)),\n                 binwidth = 2,\n                 fill = \"black\",\n                 alpha = 0.4) + \n  geom_density(colour = \"purple\",\n               linewidth = 0.8,\n               fill = \"purple\",\n               alpha = 0.3)\n\n\n\n\n\n\n\n\nFigure¬†8.3: A normal distribution of the age of a fictitious group of participants\n\n\n\n\n\nIn Figure¬†8.3, the mean (34.9235) and the median (35) age of our 10,000 fictitious learners are very close to each other. So close that, on the plot, the lines overlap. The skew is near zero, hence the density curve forms near-symmetrical bell shape. This means that the purple area under the curve to the left of the central tendency is pretty much the same as the area to the right of the line. In other words, there are as many people whose age is below the central tendency (50%) as there are people whose age is above the central tendency (50%). These are the characteristics of a normal distribution.\n\n\n\nUnderstanding and being able to recognise the characteristics of a normal distribution is important as many statistical tests assume that the variables entered in these tests are (approximately) normally distributed (see Chapter 11).\n\n\n\n\n8.2.4 Non-normal (or non-parametric) distributions\nWe saw that the distribution of L2 participants‚Äô ages in DƒÖbrowska (2019) was close to a normal distribution but with a right skew towards older years. This meant that the mean age was higher than median age.\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nDoes the distribution of L1 participants‚Äô ages follow a similar shape? Are we dealing with a distribution that is normal, left or right skewed, or something entirely different?\nQ8.8 What were the modal ages of the L1 and L2 participants in DƒÖbrowska (2019)?\n\n\n\n\n\n17 (for L1 participants) and 28 (L2 participants)\n\n\n\n\n29 (for L1 participants) and 28 (L2 participants)\n\n\n\n\n28 (for L1 participants) and 29 (L2 participants)\n\n\n\n\n32 (for L1 participants) and 31 (L2 participants)\n\n\n\n\n\n\n\n\nüòá Hover for a hint\n\n\n\n\n¬†\nQ8.9 Which of the following statements is true?\n\n\n\n\n\nThe difference between the mean and median age is greater in the L1 group than in the L2 group.\n\n\n\n\nThe mean and median age of the L1 group are almost the same.\n\n\n\n\nThe difference between mean and median age is smaller in the L1 group than in the L2 group.\n\n\n\n\n\n\n\n\n¬†\nQ8.10 Which measure of central tendency best describes L1 participants‚Äô ages?\n\n\n\n\n\nThe mean.\n\n\n\n\nThe median.\n\n\n\n\nThe mode.\n\n\n\n\nNone of them.\n\n\n\n\n\n\n\n\n¬†\n\n\n\nFigure¬†8.4 shows the distribution of L1 participants‚Äô ages both as a histogram (in grey) and a density plot (in purple). Both of these visualisations make quite clear that this distribution of ages is not at all normal! It is non-normal or non-parametric. This is because it does not consist of one (more or less) symmetrical bell shape. Instead, we can see several small bell shapes.\n\n\nShow R code to generate the plot below.\nL1.data |&gt;\n  ggplot(mapping = aes(x = Age)) + \n  geom_histogram(aes(x = Age, y = after_stat(density)),\n                 binwidth = 2,\n                 fill = \"black\",\n                 alpha = 0.4) + \n  geom_density(colour = \"purple\",\n               fill = \"purple\",\n               alpha = 0.2,\n               linewidth = 0.8) +\n  geom_vline(aes(xintercept = mean(Age)),\n             color = \"#0000EE\",\n             linetype = \"dashed\",\n             linewidth = 0.8) +\n  geom_vline(aes(xintercept = get_mode(Age)),\n             color = \"black\",\n             linewidth = 0.8) +\n  geom_vline(aes(xintercept = median(Age)),\n             color = \"darkred\",\n             linewidth = 0.8) + \n  \n  # Add label for mode:\n  annotate(\"text\",\n           x = 20.5,\n           y = 0.064,\n           label = \"mode\",\n           colour = \"black\",\n           family = \"mono\") +\n\n  # Add curved arrow for mode:\n  annotate(\n    geom = \"curve\",\n    x = 20.5,\n    y = 0.062,\n    xend = 18,\n    yend = 0.058,\n    curvature = -0.4,\n    arrow = arrow(length = unit(0.2, \"cm\")),\n    colour = \"black\") +\n\n  # Add label for mean:\n  annotate(\"text\",\n           x = 40,\n           y = 0.03,\n           label = \"mean\",\n           colour = \"#0000EE\",\n           family = \"mono\") +\n\n  # Add curved arrow for mean:\n  annotate(\n    geom = \"curve\",\n    x = 40,\n    y = 0.028,\n    xend = 37.8,\n    yend = 0.025,\n    curvature = -0.4,\n    arrow = arrow(length = unit(0.2, \"cm\")),\n    colour = \"#0000EE\") +\n  \n  # Add label for median:\n  annotate(\"text\",\n           x = 28,\n           y = 0.01,\n           label = \"median\",\n           colour = \"darkred\",\n           family = \"mono\") +\n\n  # Add curved arrow for median:\n  annotate(\n    geom = \"curve\",\n    x = 28,\n    y = 0.008,\n    xend = 31.7,\n    yend = 0.004,\n    curvature = 0.4,\n    arrow = arrow(length = unit(0.2, \"cm\")),\n    colour = \"darkred\") +  \n\n  theme_classic() +\n  scale_y_continuous(name = \"Density\",\n                     breaks = scales::breaks_width(0.01),\n                     expand = c(0, 0)) +\n  scale_x_continuous(breaks = scales::breaks_width(2),\n                     expand = c(0, 0))\n\n\n\n\n\n\n\n\nFigure¬†8.4: Density plot showing the distribution of L1 participants‚Äô ages\n\n\n\n\n\nThe histogram also shows that the most frequent age (the mode) corresponds to the lowest age in the dataset (17) and that both the median and mean are far removed from most participants‚Äô ages. This distribution of L1 participants‚Äô ages points to the limits of measures of central tendency. They are useful to describe approximately normally distributed variables, but are far less informative when it comes to other types of distributions.\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nQ8.11 What can we reasonably deduce from the distribution of L1 participants‚Äô ages visualised in Figure¬†8.4?\n\n\n\n\n\nPeople in their 40s are most likely to be willing to participate in linguistic studies.\n\n\n\n\nThe researcher excluded 34 year-olds from this study.\n\n\n\n\nFor this study, the researcher specifically targeted 17 and 60 year-olds.\n\n\n\n\nIt was easier to recruit adults in their 50s and 60s to participate in this study.\n\n\n\n\nIt was easier to recruit teenagers to participate in this study.\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\nQ8.12 What are the pros of reporting the median rather than the mean to describe the central tendency of a variable?\n\n\n\n\n\nThe median is the most widely used measure of central tendency.\n\n\n\n\nThe median is less susceptible to skew.\n\n\n\n\nThe median is ideal for very small datasets.\n\n\n\n\nThe median has the highest probability of being the true central value.\n\n\n\n\nThe median also works well with nominal variables.\n\n\n\n\nThe median is less susceptible to outliers.\n\n\n\n\nAs the middle value, the median is fairly easy to interpret.\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\nQ8.13 What are the cons of reporting the median rather than the mean to describe the central tendency of a variable?\n\n\n\n\n\nThe median can be misleading if it is not a value in the dataset.\n\n\n\n\nThe median does not take all values into account.\n\n\n\n\nThe median is often less meaningful with small sample sizes.\n\n\n\n\nThe median is more susceptible to skew.\n\n\n\n\nThe median only works with odd numbers of values.\n\n\n\n\nThe median is more susceptible to outliers.\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Desc`R`iptive statistics</span>"
    ]
  },
  {
    "objectID": "8_DescriptiveStats.html#sec-Variability",
    "href": "8_DescriptiveStats.html#sec-Variability",
    "title": "8¬† DescRiptive statistics",
    "section": "8.3 Measures of variability",
    "text": "8.3 Measures of variability\nMeasures of central tendency should never be reported alone. As we saw with L1 participants‚Äô age in Section 8.2.4, measures of central tendency are not always very informative and can even be misleading. But, even when they are informative, they only tell us part of the story: the average value of a dataset, but not the spread or variability of the data. For example, a mean age of 25 might suggest a group of young adults, but without a measure of variability, we can‚Äôt tell if the group is relatively homogeneous (e.g.¬†all students in their twenties) or heterogeneous (e.g.¬†with some participants in their teens and others in their thirties or older). Therefore, it is essential to report measures of central tendency in conjunction with measures of variability, such as the range, interquartile range, or standard deviation, to provide a more complete picture of the data.\nConsider the three distributions of ages presented in Figure¬†8.5. As you can tell from their shapes, these are three normal distributions. They each have exactly the same mean and median of 35; yet they evidently correspond to very different groups of people!\n\n\nShow R code to generate the plot below.\n# The {truncnorm} package contains density, probability, quantile and random number generation functions for the truncated normal distribution. You will need to install it before you can load it.\n\n#install.packages(\"truncnorm\")\nlibrary(truncnorm)\n\nset.seed(42)\nnormal.age.A &lt;- rtruncnorm(mean = 35, sd = 1, n = 10000, a = 10, b = 64)\nnormal.age.B &lt;- rtruncnorm(mean = 35, sd = 5, n = 10000, a = 10, b = 64)\nnormal.age.C &lt;- rtruncnorm(mean = 35, sd = 10, n = 10000, a = 10, b = 64)\n\nnormal.density &lt;- function(ages) {\n  ggplot(mapping = aes(x = ages)) + \n      geom_vline(aes(xintercept = mean(ages)),\n               color = \"#0000EE\",\n               linetype = \"dashed\",\n               linewidth = 0.8) +\n    theme_classic() +\n    scale_y_continuous(name = \"Density\",\n                       #breaks = scales::breaks_width(0.01),\n                       expand = c(0, 0)) +\n    scale_x_continuous(name = \"Age\",\n                       breaks = scales::breaks_width(2),\n                       limits = c(10,66),\n                       expand = c(0, 0)) +\n    geom_histogram(aes(x = ages, y = after_stat(density)),\n                   binwidth = 1,\n                   fill = \"black\",\n                   alpha = 0.4) + \n    geom_density(colour = \"purple\",\n                 linewidth = 0.8,\n                 fill = \"purple\",\n                 alpha = 0.3)\n}\n\n# The three plots are printed as one figure using the {patchwork} library. You will need to install this library before you can load it and use it.\n\n#install.packages(\"patchwork\")\nlibrary(patchwork)\n\nnormal.density(normal.age.A) / normal.density(normal.age.B) / normal.density(normal.age.C) +\n    # Add captions A, B, C\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\n\n\nFigure¬†8.5: Three normal distributions of ages with a mean/median of 35 years.\n\n\n\n\n\nWhereas Group A only includes adults aged 32 to 39, the Group C includes children as young as 10 as well as adults well into their 50s and early 60s - even though they both have the same mean/median on 35. This is why both measures of central tendency and variability are important when describing numeric variables! Measures of variability help us to understand how far each data point is from the central tendency. Hence, for Group A in Figure¬†8.5, we can say that all data points are pretty close to the mean/median of 35. In Group B, participants‚Äô ages are, on average, more ‚Äòspread out‚Äô to the left and right of the central tendency. And this is even more notable in Group C.\n\n8.3.1 Range\nThe most basic measure of variability is one that you will already be familiar with: range. It is easily calculated by subtracting the highest value of a variable from its lowest value. For example, in DƒÖbrowska (2019), the range of results obtained by the L1 participants in the English grammar comprehension test is:\n\nmax(L1.data$GrammarR) - min(L1.data$GrammarR)\n\n[1] 22\n\n\nBy contrast, the range of results in this same test among the L2 participants is:\n\nmax(L2.data$GrammarR) - min(L2.data$GrammarR)\n\n[1] 40\n\n\nIn practice, the range is usually reported by explicitly mentioning a variable‚Äôs lowest and highest values as this is usually much more informative than the range itself. Here is how DƒÖbrowska (2019) reports the age of the participants in the published article:\n\nThe L1 participants were all born and raised in the United Kingdom and were selected to ensure a range of ages, occupations, and educational backgrounds. The age range was from 17 to 65 years [‚Ä¶]. The nonnative participants ranged in age from 20 to 62 years [‚Ä¶] (DƒÖbrowska 2019: 6).\n\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nQ8.14 Complete the description of the GrammarR variable in L1.data and L2.data below.\nCopy and paste the following paragraph into a text processor (e.g.¬†LibreOffice Writer or Microsoft Word) and fill in the six blanks using figures that you calculated in R. If necessary, round off values to two decimal places.\n\nOn average, English native speakers performed only marginally better in the English grammatical comprehension test (median = ______) than English L2 learners (median = ______). However, L1 participants‚Äô grammatical comprehension test results ranged from ______to ______, whereas L2 participants‚Äô results ranged from ______to ______.\n\n¬†\n\n\n\n\n\n\n\n\n\nNoteClick here for the solution to Q8.14\n\n\n\n\n\nYour paragraph should read as follows:\n\nOn average, English native speakers performed only marginally better in the English grammatical comprehension test (median = 76) than English L2 learners (median = 75). L1 participants‚Äô grammatical comprehension test results ranged from 58 to 80. In this same test, L2 participants‚Äô results ranged 40 to 80.\n\nThe following lines of R code can be used to obtain these numbers.\n\nmedian(L1.data$GrammarR)\n\n[1] 76\n\nmedian(L2.data$GrammarR)\n\n[1] 75\n\nmin(L1.data$GrammarR)\n\n[1] 58\n\nmax(L1.data$GrammarR)\n\n[1] 80\n\nmin(L2.data$GrammarR)\n\n[1] 40\n\nmax(L2.data$GrammarR)\n\n[1] 80\n\n\n\n\n\n\n\n8.3.2 Interquartile range\nWe saw that the median is a measure of central tendency that represents the middle value. This means that 50% of the data falls below the median and 50% falls above the median. Going back to the test results of our six learners of Breton in Fiji, this means that half of the class scored below 86.5 and the other half above 86.5.\n\nmedian(c(5, 82, 86, 87, 89, 91))\n\n[1] 86.5\n\n\nWe can further subdivide the distribution into chunks of 25% of the data, or quartiles (see Figure¬†8.6).\n\nThe first quartile (Q1) is the value below which 25% of the data falls. In other words, the first quartile corresponds to a value that lies above one-quarter of the values in the data set.\nThe second quartile (Q2) is the median and, as we know, half of the data (25% + 25% = 50%) are below this value, the other half are above.\nThe third quartile (Q3) is the value below which 75% of the data falls. In other words, it is also the value above which the upper 25% of the data are.\nThe interquartile range (IQR) is the range between the second and the third quartile: it therefore covers the middle 50% of the data. This is illustrated below with a growing number of imaginary Breton learners.\n\n\n\n\n\n\n\nFigure¬†8.6: Animation showing the interquartile range of five different sets of values (Le Foll 2024. Zenodo. https://doi.org/10.5281/zenodo.17440319)\n\n\n\nThe easiest way to examine a variable‚Äôs IQR in R is to use the handy summary() function which, when applied to a numeric variable, returns a number of useful descriptive statistics including its first and third quartiles.5\n\nsummary(L1.data$GrammarR)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  58.00   71.25   76.00   74.42   79.00   80.00 \n\n\nFrom the output of the summary() function, we can easily calculate the IQR, which we know is equal to the range between the first and the third quantile.\n\n79 - 71.25\n\n[1] 7.75\n\n\nAlternatively, we can compute the IQR directly using the IQR() function.\n\nIQR(L1.data$GrammarR)\n\n[1] 7.75\n\n\nThe reason that the summary() function is probably more useful than IQR() is that, like the full range, the interquantile range is not usually reported as the difference between Q3 and Q1. This is because it is more informative to consider the first quartile (Q1), the median (Q2), and the third quartile (Q3) together to grasp both the central tendency of a set of numbers and the amount of variability there is around this central tendency.\nIn practice, quartiles are rarely reported as numbers. Instead, they are usually visualised as boxplots. Boxplots present a visual summary of a numeric variable‚Äôs central tendency and variability around this central tendency. On a boxplot, the box represents the IQR. Its dividing line is the median. The whiskers and any outlier points represent the rest of the distribution (see Figure¬†8.7). In other words, the lower whisker roughly covers the lower 25% of the data and the upper whisker the top 25% of the data. Boxplots are most often displayed vertically and are used to visually compare the main characteristics of distributions of numeric values across different groups (e.g.¬†grammar comprehension across different language proficiency groups).\n\n\n\n\n\n\nFigure¬†8.7: Animation showing the making of a boxplot (Le Foll 2024. Zenodo. https://doi.org/10.5281/zenodo.17440384)\n\n\n\nRemember that, in a perfectly normal distribution, the mean and median are equal. When a variable follows a normal distribution, its box is divided into two equal halves and the whiskers are of equal length (see Figure¬†8.8). This symmetry comes from the fact that the values are equally distributed to the left and right of the median/mean. For the same reason, the bells of the normal distributions in Figure¬†8.5 were all (almost) symmetrical, although they had very different heights and widths.\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nQ8.15 Examine the boxplots displayed in Figure¬†8.8.\n\n\n\n\n\n\n\n\nFigure¬†8.8: Three boxplots\n\n\n\n\n\n\n\n\n\n\nAges in Group 1 are normally distributed.\n\n\n\n\nAges in Group 1 are not normally distributed.\n\n\n\n\nAges in Group 2 are normally distributed.\n\n\n\n\nAges in Group 2 are not normally distributed.\n\n\n\n\nAges in Group 3 are normally distributed.\n\n\n\n\nAges in Group 3 are not normally distributed\n\n\n\n\nOn average, participants were older in Group 1 and younger in Group 3.\n\n\n\n\nIn all three groups, the median age is approximately the same.\n\n\n\n\nIn all three groups, the IQR is approximately the same.\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a first hint.\n\n\n\n\n¬†\nQ8.16 The boxplots in Figure¬†8.8 are based on the same data as the three density plots in Figure¬†8.5. Compare the two figures. Which distribution corresponds to which boxplot?\n\n\n\n\n\nDistribution A is visualised in boxplot 3.\n\n\n\n\nDistribution C is visualised in boxplot 1.\n\n\n\n\nDistribution B is visualised in boxplot 1.\n\n\n\n\nDistribution C is visualised in boxplot 3.\n\n\n\n\nDistributions A, B and C are visualised in boxplot 2.\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\nQ8.17 Examine the following distribution of scores on the grammatical comprehension test administered as part of DƒÖbrowska (2019).\n\n\n\n\n\n\n\n\nFigure¬†8.9: Density plot of participants‚Äô scores on the English comprehension grammar test\n\n\n\n\n\n¬†\nAre the scores visualised in Figure¬†8.9 normally distributed?\n\n\n\n\n\nYes, they are approximately normally distributed.\n\n\n\n\nNo, they are far from normally distributed.\n\n\n\n\nYes, they are approximately normally distributed, but with a slight positive skew.\n\n\n\n\nIt's impossible to tell from the plot alone.\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\nQ8.18 Compare the following outputs of the summary() function.\n\nsummary(L1.data$GrammarR)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  58.00   71.25   76.00   74.42   79.00   80.00 \n\nsummary(L2.data$GrammarR)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  40.00   58.50   75.00   67.76   78.00   80.00 \n\n\nBased on the outputs of the summary() function, what does Figure¬†8.9 display?\n\n\n\n\n\nThe distribution of GrammarR scores among L1 participants.\n\n\n\n\nThe distribution of GrammarR scores among L2 participants.\n\n\n\n\nThe distribution of GrammarR scores among both L1 and L2 participants.\n\n\n\n\nNone of the above.\n\n\n\n\n\n\n\n\n¬†\nQ8.19 Compare the following boxplots which summarise the distribution of scores on the grammatical comprehension test (GrammarR) administered as part of DƒÖbrowska (2019).\n\n\n\n\n\n\n\n\nFigure¬†8.10: Boxplots showing L1 and L2 participants‚Äô English grammar comprehension test scores\n\n\n\n\n\nWhy do the two boxplots look so different?\n\n\n\n\n\nBecause the two groups were not of equal size (there were more L1 than L2 participants).\n\n\n\n\nBecause the range of scores was much larger among L2 participants than among L1 participants.\n\n\n\n\nBecause the median L2 score is much lower than the median L1 score.\n\n\n\n\nBecause more than a quarter of L2 participants scored below 60, whereas only one L1 participant scored below 60.\n\n\n\n\nBecause proportionally more L2 participants scored below the L2 median score than L1 participants did below the L1 median.\n\n\n\n\nBecause the IQR of scores was much larger among L2 participants than among L1 participants.\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\nü¶â Hover over the owl for a second hint.\n\n\n\n\n\n\n\n\n\n8.3.3 Standard deviation\nIn the language sciences and in many other disciplines, standard deviation is the most common reported measure of variability. Whereas the interquartile range (IQR) is a measure of variability around the median, standard deviation (SD) measures variability around the mean. In other words, if you report a mean value as a measure of central tendency, you should report the standard deviation along side it. However, if you report the median, than it makes more sense to report the IQR in the form of a boxplot (see Section 8.3.2).\n\nIn a nutshell, the standard deviation tells us how far away, on average, each data point is from the mean.\n\n¬†Considering the test scores of our five Breton learners, we already know that the standard deviation is likely to be large because the mean (70.6) is quite far away from all five data points.\n\n5, 82, 86, 89, 91\n\nTo calculate how far exactly, we first measure how far each point is from the mean, e.g.¬†for the first data point we calculate 5 - 70.6, for the second 82 - 70.6, etc.\n\nBreton.scores &lt;- c(5, 82, 86, 89, 91)\n\nBreton.scores - mean(Breton.scores)\n\n[1] -65.6  11.4  15.4  18.4  20.4\n\n\nAs you can see, some of the differences between the data points and the mean value are negative, whilst others are positive. For standard deviation, we are not interested in whether data points are above or below the mean, but rather in how far removed they are from the mean. To remove any negative sign, we therefore square all these distances. The squaring operation (^2) also has the effect making large differences even larger.\n\n(Breton.scores - mean(Breton.scores))^2\n\n[1] 4303.36  129.96  237.16  338.56  416.16\n\n\nRemember that standard deviation is a measure of how different, on average, a set of numbers are from one another, with respect to the mean. We have just calculated the sum of the squared differences from the mean and we now need to calculate the average of these squared differences. To calculate the mean squared difference, we sum the differences and divide them by the number of data points.\n\nsum((Breton.scores - mean(Breton.scores))^2) / 5\n\n[1] 1085.04\n\n\nThis is the variance. The problem with the variance is that it is not in the original scale of our variable, but rather in squared units, i.e.¬†here, in squared test scores, which is rather difficult to interpret! This is why we more commonly report the standard deviation, which is the square root of the variance. The square root function in R is sqrt().\n\nsqrt(sum((Breton.scores - mean(Breton.scores))^2) / 5)\n\n[1] 32.93995\n\n\nFrom the above result, we can deduce that, on average, learners‚Äô test scores are 32 points away from the group mean of 70.6 points.\nOf course, there is a base R function to calculate the standard deviation. It is called sd(). However, if we use the sd() function to calculate the standard deviation of our five Breton learners‚Äô test scores, we get a slightly different result.\n\nsd(Breton.scores)\n\n[1] 36.82798\n\n\nThis is because, in practice, we almost always divide the sum of squares not by the total number of data points (N ), but by the total number minus one (N-1). This is the difference between the population standard deviation and the sample standard deviation. The population standard deviation is used when we have access to the entire population (e.g.¬†all L2 English users worldwide!), which is rare in real-world scenarios. In most cases, we work with samples (e.g.¬†as in DƒÖbrowska 2019, a sample of 67 L2 English users). Dividing by N-1 gives us a more accurate estimate of the population‚Äôs standard deviation based on our sample. It helps to reduce the bias in our estimate, making it a more reliable measure of variability around the mean.\nIn R, the sd() function calculates the sample standard deviation.\n\nsqrt(sum((Breton.scores - mean(Breton.scores))^2) / 4)\n\n[1] 36.82798\n\n\nWith a normal distribution, the standard deviation informs us about the width of the bell around the central tendency. In Figure¬†8.5 we saw that three normal distributions, all with a median/mean of 35 could have very different bell shapes. This is because they have very different standard deviations around that central tendency. Let us compare the distribution shapes of these three distributions in detail.\nDistribution A (Figure¬†8.11) is a normal distribution with a mean of 35 years (xÃÖ = 35) and a standard deviation of one year (sd = 1).\n\n\n\n\n\n\n\n\nFigure¬†8.11: Density plot of Distribution A\n\n\n\n\n\nDistribution B (Figure¬†8.12) is a normal distribution with a mean of 35 years (xÃÖ = 35) and a standard deviation of 5 years (sd = 5).\n\n\n\n\n\n\n\n\nFigure¬†8.12: Density plot of Distribution B\n\n\n\n\n\nDistribution C (Figure¬†8.13) is a normal distribution with a mean of 35 years (xÃÖ = 35) and a standard deviation of 10 years (sd = 10).\n\n\n\n\n\n\n\n\nFigure¬†8.13: Density plot of Distribution C\n\n\n\n\n\nThe standard deviation provides a single metric of the variability around the mean. This means that knowing the mean and standard deviation of a numeric variable is not enough to tell whether a distribution is (approximately) normal or skewed. Like the range and the IQR, a large standard deviation value indicates greater variability within a variable, but tells us nothing more. For instance, comparing the following two SDs tells us that there is more variability around the mean in L2 participants‚Äô grammar comprehension test scores than in that of the L1 participants, but nothing more about the distribution of the test scores in either group.\n\nsd(L1.data$GrammarR) |&gt;\n  round(digits = 2)\n\n[1] 5.01\n\nsd(L2.data$GrammarR) |&gt; \n  round(digits = 2)\n\n[1] 13.48\n\n\nIn this respect, boxplots are more informative (compare the above SDs with Figure¬†8.10). To evaluate the full shape of a numeric variable‚Äôs distribution, however, there is no alternative to plotting it as a histogram or density plot.\nIn sum, remember that, when describing variables, it is important to report both an appropriate measure of central tendency and an appropriate measure of variability. In addition, it is good practice to visualise the full distribution of a variable‚Äôs values in the form of a table, histogram, or density plot (see Chapter 11 on data visualisation). This is because any combination of a single measure of central tendency and a single measure of variability can correspond to an array of different distribution shapes.\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nQ8.20 What is the standard deviation of L1 participants‚Äô age in DƒÖbrowska (2019)? Calculate the sample standard deviation to two decimal places.\n\n\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\nQ8.21 Compare the standard deviation of the Age variable in the L1 and L2 datasets. What can you conclude on the basis of this comparison?\n\n\n\n\n\nAge is not normally distributed in both the L1 and the L2 data.\n\n\n\n\nThere is greater variability around the mean age in the L1 data than in the L2 data.\n\n\n\n\nThere is greater variability around the mean age in the L2 data than in the L1 data.\n\n\n\n\nL2 participants are more likely to be older than L1 participants.\n\n\n\n\nThere is almost twice as much variability in L2 participants' ages than in L1's.\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\n\n\n\n\n\n\n\n\n\nNoteFurther reading\n\n\n\nAs a follow-up, I highly recommend reading this short and highly accessible article by Fahd Alhazmi (2020), who provides a wonderful visual guide to understanding standard deviation: https://medium.com/data-science/a-visual-interpretation-of-the-standard-deviation-30f4676c291c.\n\n\n\n\nCheck your progress üåü\nYou have successfully completed 0 out of 21 questions in this chapter.\nAre you confident that you can‚Ä¶?\n\nUse and interpret different measures of central tendency (Section 8.1)\nCalculate the mode, mean, median of a numeric variable in R (Section 8.1.1 - Section 8.1.2)\nInterpret histograms and density plots (Section 8.2)\nRecognise the characteristics of a normal distribution (Section 8.2.3)\nInterpret and calculate the interquartile range in R (Section 8.3.2)\nInterpret boxplots (Section 8.3.2)\nInterpret and calculate the standard deviation (Section 8.3.3)\n\nIn Chapter 10, we will cover the basics of data visualisation and learn how to create a range of informative and elegant plots (including histograms and density plots) using the popular R package ggplot2. But, first, we need to learn about data wrangling (Chapter 9) to prepare our data for data visualisation and multivariable analyses. Are you ready? :nerd-face:\n\n\n\n\nAlhazmi, Fahd. 2020. A visual interpretation of the standard deviation. Medium. https://towardsdatascience.com/a-visual-interpretation-of-the-standard-deviation-30f4676c291c.\n\n\nDƒÖbrowska, Ewa. 2019. Experience, aptitude, and individual differences in linguistic attainment: A comparison of native and nonnative speakers. Language Learning 69(S1). 72‚Äì100. https://doi.org/10.1111/lang.12323.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Desc`R`iptive statistics</span>"
    ]
  },
  {
    "objectID": "8_DescriptiveStats.html#footnotes",
    "href": "8_DescriptiveStats.html#footnotes",
    "title": "8¬† DescRiptive statistics",
    "section": "",
    "text": "Breton is the Celtic language of Brittany (now in North-West France). With around 216,000 active speakers (Wikipedia, 26/08/2024), Breton is classified as ‚Äòseverely endangered‚Äô in the UNESCO‚Äôs Atlas of the World‚Äôs Languages in Danger. It would presumably be quite a feat to put together a class of five Breton learners in Fiji, an island country far removed from Brittany in the South Pacific Ocean with fewer than one million inhabitants (Wikipedia, 26/08/2024)!‚Ü©Ô∏é\nWe also see that these data needs cleaning before we can do any serious data analysis. There are also a few typos (e.g.¬†Unemploed) and synonyms (School Crossing Guard and School Crossing Patrol) that we will need to standardise. This process is part of data wrangling and we will cover how to do this in a reproducible way in R in Chapter 9.‚Ü©Ô∏é\nA Likert scale is a type of rating scale used to measure attitudes, opinions, or feelings. It typically consists of a series of statements or questions with a range of possible responses, often on a scale from ‚Äústrongly disagree‚Äù to ‚Äústrongly agree‚Äù. For example, in a study on language attitudes, participants might be asked to rate their agreement with the statement ‚ÄúI think it‚Äôs important to speak standard English in formal situations‚Äù on a scale from ‚Äú1 (strongly disagree)‚Äù to ‚Äú5 (strongly agree)‚Äù. The resulting variable will therefore consist of numbers ranging between 1 and 5. Note also that, strictly speaking, Likert scales are not numeric variables, but rather ordinal variables (see Section 7.2). The numbers refer to different categories that describe an order of responses, rather than a quantity.‚Ü©Ô∏é\nOf course, there is an R function to help you do the maths! The ecdf() function allows us to calculate the area under the curve between the ages of 42 and 62.\n\necdf(L2.data$Age)(62) - ecdf(L2.data$Age)(42)\n\n[1] 0.119403\n\n\nIn other words, there is a 11.94 % probability of any L2 participant in this study being aged between 42 and 62 (corresponding to the light purple area in plot A). Compare this to the probability of a participant being between 22 and 42 years old.\n\necdf(L2.data$Age)(42) - ecdf(L2.data$Age)(22)\n\n[1] 0.7761194\n\n\nThis is, indeed, a much higher probability (ca. 78 %), as depicted by the much larger area highlighted in plot B.\n\n\n\n\n\n\n\n\n\n‚Ü©Ô∏é\nQuartiles can also be computed using the quantile() function, which takes two arguments: the variable and a value between 0 and 1 corresponding to our quantile of interest. We are interested in the first and third quartiles, therefore in the values below which lie one quarter (0.25) and three-quarters (0.75) of all the data.\nTo compute the first quantile (Q1), we therefore enter:\n\nquantile(L1.data$GrammarR, 0.25)\n\n  25% \n71.25 \n\n\nFor the third quantile (Q3), we need:\n\nquantile(L1.data$GrammarR, 0.75)\n\n75% \n 79 \n\n\n‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Desc`R`iptive statistics</span>"
    ]
  },
  {
    "objectID": "9_DataWrangling.html",
    "href": "9_DataWrangling.html",
    "title": "9¬† Data wRangling",
    "section": "",
    "text": "Chapter overview\nIn this chapter, you will learn how to:",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Data w`R`angling</span>"
    ]
  },
  {
    "objectID": "9_DataWrangling.html#sec-tidyverse",
    "href": "9_DataWrangling.html#sec-tidyverse",
    "title": "9¬† Data wRangling",
    "section": "9.1 Welcome to the tidyverse! ü™ê",
    "text": "9.1 Welcome to the tidyverse! ü™ê\nThis chapter explains how to examine, clean, and manipulate data mostly using functions from the {tidyverse}: a collection of useful R packages increasingly used for all kinds of data analysis projects. Tidyverse functions are designed to work with tidy data (see Figure¬†9.1) and, as a result, they are often easier to combine.\n\n\n\n\n\n\nFigure¬†9.1: Tidy data illustration from the Openscapes blog Tidy Data for reproducibility, efficiency, and collaboration by Horst & Lowndes (2020).\n\n\n\nLearning to manipulate data and conduct data analysis in R ‚Äúthe tidyverse-way‚Äù can help make your workflows more efficient.\n\nIf you ensure that your data are tidy, you‚Äôll spend less time fighting with the tools and more time working on your analysis. (Wickham, Vaughan & Girlich)",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Data w`R`angling</span>"
    ]
  },
  {
    "objectID": "9_DataWrangling.html#sec-Conflicts",
    "href": "9_DataWrangling.html#sec-Conflicts",
    "title": "9¬† Data wRangling",
    "section": "9.2 Base R vs.¬†tidyverse functions",
    "text": "9.2 Base R vs.¬†tidyverse functions\nNovice R users may find it confusing that many operations can be performed using either a base R function or a tidyverse one. For example, in Chapter 6, we saw that both the base R function read.csv() and the tidyverse function read_csv() can be used to import CSV files. The functions have slightly different arguments and default values, which can be annoying, even though they are fundamentally designed to perform the same task. But don‚Äôt fret over this too much: it‚Äôs fine for you to use whichever function you find most convenient and intuitive and it‚Äôs also absolutely fine to combine base R and tidyverse functions!\nYou will no doubt have noticed that the functions read.csv() and read_csv() have very similar but not exactly identical names. This is helpful to differentiate between the two functions. Unfortunately, some function names are found in several packages, which can lead to confusion and errors! For example, you may have noticed that when you load the tidyverse library the first time in a project, a message similar to Figure¬†9.2 is printed in the Console.\n\n\n\n\n\n\nFigure¬†9.2: Screenshot of the R Console after having loaded the {tidyverse} library\n\n\n\nFirst, the error message reproduced in Figure¬†9.2 confirms that loading the {tidyverse} package has led to the successful loading of a total of nine packages and that these are now ready to use. Crucially, the message also warns us about conflicts between some {tidyverse} packages and base R packages. These conflicts are due to the fact that two functions from the {dplyr} package have exactly the same name as functions from the base R {stats} package. The warning informs us that, by default, the {dplyr} functions will be applied.\nTo force R to use a function from a specific package, we can use the package::function() syntax. Hence, to force R to use the base R {stats} filter() function rather than the tidyverse one, we would use stats::filter(). On the contrary, if we want to be absolutely certain that the tidyverse one is used, we can use dplyr::filter().\n\n\n\n\n\n\nFigure¬†9.3: A galaxy of tidyverse-related hex stickers (artwork by @allison_horst).\n\n\n\nIn this chapter, we will explore functions from {dplyr}, {stringr}, and {tidyr}. The popular {ggplot2} tidyverse library for data visualisation following the ‚ÄúGrammar of Graphics‚Äù approach will be introduced in Chapter 10. Make sure that you have loaded the tidyverse packages before proceeding with the rest of this chapter.\n\nlibrary(tidyverse)",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Data w`R`angling</span>"
    ]
  },
  {
    "objectID": "9_DataWrangling.html#sec-Sanity",
    "href": "9_DataWrangling.html#sec-Sanity",
    "title": "9¬† Data wRangling",
    "section": "9.3 Checking data sanity",
    "text": "9.3 Checking data sanity\nBefore beginning any data analysis, it is important to always check the sanity of our data. In the following, we will use tables and descriptive statistics to do this. In Chapter 10, we will learn how to use data visualisation to check for outliers and other issues that may affect our analyses.\n\n\n\n\n\n\nWarningPrerequisites\n\n\n\n\n\nIn this chapter and the following chapters, all examples, tasks, and quiz questions are based on data from:\n\nDƒÖbrowska, Ewa. 2019. Experience, Aptitude, and Individual Differences in Linguistic Attainment: A Comparison of Native and Nonnative Speakers. Language Learning 69(S1). 72‚Äì100. https://doi.org/10.1111/lang.12323.\n\nYou will only be able to reproduce the analyses and answer the quiz questions from this chapter if you have created an RProject and successfully imported the two datasets from DƒÖbrowska (2019) into your local R environment (see Figure¬†7.1). Detailed instructions to do so can be found from Section 6.3 to Section 6.5.\nAlternatively, you can download Dabrowska2019.zip from the textbook‚Äôs GitHub repository. To launch the project correctly, first unzip the file and then double-click on the Dabrowska2019.Rproj file.\n\n\n\nBefore we get started, make sure that both the L1 and the L2 datasets are correctly loaded by checking the structure of the R objects using the str() function.\n\nlibrary(here)\n\nL1.data &lt;- read.csv(file = here(\"data\", \"L1_data.csv\"))\nstr(L1.data)\n\nL2.data &lt;- read.csv(file = here(\"data\", \"L2_data.csv\"))\nstr(L2.data)\n\n\n9.3.1 Numeric variables\nIn Section 8.3.2, we used the summary() function to obtain some useful descriptive statistics on a single numeric variable, namely the range, mean, median, and interquartile range (IQR).\n\nsummary(L1.data$GrammarR)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  58.00   71.25   76.00   74.42   79.00   80.00 \n\n\nTo check the sanity of a dataset, we can use this same function on an entire data table (provided that the data are in the tidy format, see Section 9.1). Thus, the command summary(L1.data)1 outputs summary statistics on all the variables of the L1 dataset - in other words, on all the columns of the data frame L1.data.\n\nsummary(L1.data)\n\n\n\n Participant             Age           Gender           Occupation       \n Length:90          Min.   :17.00   Length:90          Length:90         \n Class :character   1st Qu.:25.00   Class :character   Class :character  \n Mode  :character   Median :32.00   Mode  :character   Mode  :character  \n                    Mean   :37.54                                        \n                    3rd Qu.:55.00                                        \n                    Max.   :65.00                                        \n  OccupGroup          OtherLgs          Education             EduYrs     \n Length:90          Length:90          Length:90          Min.   :10.00  \n Class :character   Class :character   Class :character   1st Qu.:12.00  \n Mode  :character   Mode  :character   Mode  :character   Median :13.00  \n                                                          Mean   :13.71  \n                                                          3rd Qu.:14.00  \n                                                          Max.   :21.00  \n    ReadEng1        ReadEng2        ReadEng3        ReadEng      \n Min.   :0.000   Min.   :0.000   Min.   :0.000   Min.   : 0.000  \n 1st Qu.:1.000   1st Qu.:1.000   1st Qu.:2.000   1st Qu.: 5.000  \n Median :2.000   Median :2.000   Median :2.000   Median : 7.000  \n Mean   :2.522   Mean   :2.433   Mean   :2.233   Mean   : 7.189  \n 3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.: 9.750  \n Max.   :5.000   Max.   :5.000   Max.   :4.000   Max.   :14.000  \n\n\nFor the numeric variables in the dataset, the summary() function provides us with many useful descriptive statistics to check the sanity of the data. For example, we can check whether the minimum values include improbably low values (e.g.¬†a five-year-old participant in a written language exam) or outright impossible ones (e.g.¬†a minus 18-year old participant!). Equally, if we know that the maximum number of points that could be obtained in the English grammar test is 100, a maximum value of more than 100 would be highly suspicious and warrant further investigation.\nAs far as we can see from the output of summary(L1.data) above, the numeric variables in DƒÖbrowska (2019)‚Äôs L1 dataset do not appear to feature any obvious problematic values.\n\n\n9.3.2 Categorical variables as factors\nHaving examined the numeric variables, we now turn to the non-numeric, categorical ones (see Section 7.2). For these variables, the descriptive statistics returned by summary(L1.data) are not as insightful. They only tell us that they each include 90 values, which corresponds to the 90 participants in the L1 dataset. As we can see from the output of the str() function, these categorical variables are stored in R as character string vectors (abbreviated in the str() output to ‚Äúchr‚Äù).\n\nstr(L1.data$Gender)\n\n chr [1:90] \"M\" \"M\" \"M\" \"F\" \"F\" \"F\" \"F\" \"M\" \"M\" \"F\" \"F\" \"M\" \"M\" \"F\" \"M\" \"F\" ...\n\n\nCharacter string vectors are a useful R object type for text but, in R, categorical variables are best stored as factors. Factors are a more efficient way to store character values because each unique character value is stored only once. The data itself are stored as a vector of integers. Let‚Äôs look at an example.\nFirst, we convert the categorical variable Gender from L1.data that is currently stored as a character string vector to a factor vector called L1.Gender.fct.\n\nL1.Gender.fct &lt;- factor(L1.data$Gender)\n\nWhen we now inspect its structure using str(), we can see that L1.Gender.fct is a factor with two levels ‚ÄúF‚Äù and ‚ÄúM‚Äù. The values themselves, however, are no longer listed as ‚ÄúM‚Äù ‚ÄúM‚Äù ‚ÄúM‚Äù ‚ÄúF‚Äù ‚ÄúF‚Äù‚Ä¶, but rather as integers: 2 2 2 1 1 1‚Ä¶.\n\nstr(L1.Gender.fct)\n\n Factor w/ 2 levels \"F\",\"M\": 2 2 2 1 1 1 1 2 2 1 ...\n\n\nBy default, the levels of a factor are ordered alphabetically, hence in L1.Gender.fct, 1 corresponds to ‚ÄúF‚Äù and 2 to ‚ÄúM‚Äù.\nThe summary output of factor vectors are far more insightful than of character variables (and look rather like the output of the table() function that we used in Section 8.1.3).\n\nsummary(L1.Gender.fct)\n\n F  M \n48 42 \n\n\nThe tidyverse package {forcats} has a lot of very useful functions to manipulate factors. They all start with fct_.\n\n\n\n\n\nHex sticker of the {forcats} package\n\n\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nQ9.1 Type ?fct_ in an R script or directly in the Console and then press the tab key (‚Üπ or ‚á• on your keyboard). A list of all loaded functions that start with fct_ should pop up. Which of these is not listed?\n\n\n\n\n\nfct_count\n\n\n\n\nfct_mutate\n\n\n\n\nfct_na_level_to_value\n\n\n\n\nfct_rev\n\n\n\n\nfct_reorder\n\n\n\n\n\n\n\n\n¬†\nQ9.2 In the factor object L1.Gender.fct (which we created above), the first level is ‚ÄúF‚Äù because it comes first in the alphabet. Which of these commands will make ‚ÄúM‚Äù the first level instead? Check out the help files of the following {forcats} functions to understand what they do and try them out.\n\n\n\n\n\nfct_match(L1.Gender.fct, c(\"M\", \"F\"))\n\n\n\n\nfct_rev(L1.Gender.fct)\n\n\n\n\nfct_relevel(L1.Gender.fct, \"M\")\n\n\n\n\nfct_reorder(L1.Gender.fct, c(\"M\", \"F\"))\n\n\n\n\nfct_recode(L1.Gender.fct, first = \"M\", second = \"F\")\n\n\n\n\n\n\n\n\nüòá Hover for a hint",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Data w`R`angling</span>"
    ]
  },
  {
    "objectID": "9_DataWrangling.html#sec-Preprocessing",
    "href": "9_DataWrangling.html#sec-Preprocessing",
    "title": "9¬† Data wRangling",
    "section": "9.4 Pre-processing data",
    "text": "9.4 Pre-processing data\n\n9.4.1 Using mutate() to add and replace columns\nIn the previous section, we stored the factor representing L1 participants‚Äô gender as a separate R object called L1.Gender.fct. If, instead, we want to add this factor as an additional column to our dataset, we can use the mutate() function from {dplyr}.\n\n\n\n\n\nHex sticker of the {dplyr} package\n\n\n\nL1.data &lt;- L1.data |&gt; \n  mutate(Gender.fct = factor(L1.data$Gender))\n\nThe mutate() function allows us to add new columns to a dataset. By default, it also keeps all the existing ones (to control which columns are retained, check the help file and read about the ‚Äú.keep =‚Äù argument).\n\n\n\n\n\n\nFigure¬†9.4: Artwork explaining the dplyr::mutate() function by @allison_horst.\n\n\n\nWe can use the colnames() function to check that the new column has been correctly appended to the table. Alternatively, you can use the View() function to display the table in full in a new RStudio tab. In both cases, you should see that the new column is now the last column in the table (column number 32).\n\ncolnames(L1.data)\n\n [1] \"Participant\" \"Age\"         \"Gender\"      \"Occupation\"  \"OccupGroup\" \n [6] \"OtherLgs\"    \"Education\"   \"EduYrs\"      \"ReadEng1\"    \"ReadEng2\"   \n[11] \"ReadEng3\"    \"ReadEng\"     \"Active\"      \"ObjCl\"       \"ObjRel\"     \n[16] \"Passive\"     \"Postmod\"     \"Q.has\"       \"Q.is\"        \"Locative\"   \n[21] \"SubCl\"       \"SubRel\"      \"GrammarR\"    \"Grammar\"     \"VocabR\"     \n[26] \"Vocab\"       \"CollocR\"     \"Colloc\"      \"Blocks\"      \"ART\"        \n[31] \"LgAnalysis\"  \"Gender.fct\" \n\n\nWatch out: if you add a new column to a table using an existing column name, mutate() will overwrite the entire content of the existing column with the new values! In the following code chunk, we are therefore overwriting the character vector Gender with a factor vector also called Gender. We should only do this if we are certain that we won‚Äôt need to compare the original values with the new ones!\n\nL1.data &lt;- L1.data |&gt; \n  mutate(Gender = factor(L1.data$Gender))\n\n\n\n9.4.2 Using across() to transform multiple columns\nIn addition to Gender, there are quite a few more character vectors in L1.data that represent categorical variables and that would therefore be better stored as factors. We could use mutate() and factor() to convert them one by one like we did for Gender above, but that would require several lines of code in which we could easily make a silly error or two. Instead, we can use a series of neat tidyverse functions to convert all character vectors to factor vectors in one go.\n\nL1.data.fct &lt;- L1.data |&gt; \n  mutate(across(where(is.character), factor))\n\nAbove, we use mutate() to convert across() the entire dataset all columns where() there are character vectors to factor() vectors (using the is.character() function to determine which columns contain character vectors).\n\n\n\n\n\n\nFigure¬†9.5: Artwork explaining the across() function by @allison_horst.\n\n\n\nWe can check that the correct variables have been converted by comparing the output of summary(L1.data) (partially printed in Section 9.3.1) with the output of summary(L1.data.fct) (partially printed below).\n\nsummary(L1.data.fct)\n\n\n\n  Participant      Age        Gender          Occupation OccupGroup\n 1      : 1   Min.   :17.00   F:48   Retired       :14   C  :22    \n 100    : 1   1st Qu.:25.00   M:42   Student       :14   I  :23    \n 101    : 1   Median :32.00          Unemployed    : 4   M  :20    \n 104    : 1   Mean   :37.54          Housewife     : 3   PS :24    \n 106    : 1   3rd Qu.:55.00          Shop Assistant: 3   PS : 1    \n 107    : 1   Max.   :65.00          Teacher       : 3             \n (Other):84                          (Other)       :49             \n    OtherLgs                                  Education      EduYrs     \n French : 2   student                              : 8   Min.   :10.00  \n German : 3   A level                              : 5   1st Qu.:12.00  \n None   :84   BA                                   : 5   Median :13.00  \n Spanish: 1   GCSEs                                : 5   Mean   :13.71  \n              NVQ                                  : 4   3rd Qu.:14.00  \n              Northern Counties School Leaving Exam: 3   Max.   :21.00  \n              (Other)                              :60                  \n    ReadEng1        ReadEng2        ReadEng3        ReadEng      \n Min.   :0.000   Min.   :0.000   Min.   :0.000   Min.   : 0.000  \n 1st Qu.:1.000   1st Qu.:1.000   1st Qu.:2.000   1st Qu.: 5.000  \n Median :2.000   Median :2.000   Median :2.000   Median : 7.000  \n Mean   :2.522   Mean   :2.433   Mean   :2.233   Mean   : 7.189  \n 3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.: 9.750  \n Max.   :5.000   Max.   :5.000   Max.   :4.000   Max.   :14.000  \n                                                                 \n\n\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nIn this task, you will do some data wrangling on the L2 dataset from DƒÖbrowska (2019).\nQ9.3 Which of these columns from L2.data represent categorical variables and therefore ought to be converted to factors?\n\n\n\n\n\nOccupGroup\n\n\n\n\nEdNative\n\n\n\n\nUseEngC\n\n\n\n\nNativeLg\n\n\n\n\nArrival\n\n\n\n\nFirstExp\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\nQ9.4 Convert all character vectors of L2.data to factors and save the new table as L2.data.fct. Use the str() function to check that your conversion has worked as planned. How many different factor levels are there in the categorical variable Occupation?\n\n\n\n\n\n48\n\n\n\n\n44\n\n\n\n\n4\n\n\n\n\n67\n\n\n\n\n45\n\n\n\n\n27\n\n\n\n\n\n\n\n\n\n\nClick here to view R code to help you answer c.\nL2.data.fct &lt;- L2.data |&gt; \n  mutate(across(where(is.character), factor))\n\nstr(L2.data.fct)\n\n\nQ9.5 Use the summary() and str() functions to inspect the sanity of L2 dataset now that you have converted all the character vectors to factors. Have you noticed that there are three factor levels in the Gender variable of the L2 dataset whereas there are only two in the L1 dataset? What is the most likely reason for this?\n\n\n\n\n\nBecause these six participants declined to answer the question about their gender.\n\n\n\n\nBecause these six participants identify as non-binary.\n\n\n\n\nBecause these six participants were under 18 at the time of data collection.\n\n\n\n\nBecause sometimes \"female\" was recorded as lower-case \"f\" rather than upper-case \"F\".\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Data w`R`angling</span>"
    ]
  },
  {
    "objectID": "9_DataWrangling.html#data-cleaning",
    "href": "9_DataWrangling.html#data-cleaning",
    "title": "9¬† Data wRangling",
    "section": "9.5 Data cleaning üßº",
    "text": "9.5 Data cleaning üßº\nBy closely examining the data, we noticed that the values of the categorical variables were not always entered in a consistent way, which may lead to incorrect analyses. For example, in the L2 dataset, most female participants‚Äô gender is recorded as F except for six participants, where it is f. As R is a case-sensitive language, these two factor levels are treated as two different levels of the Gender variable. This means that any future analyses on the effect of Gender on language learning will compare participants across these three groups.\n\nsummary(L2.data.fct$Gender)\n\n f  F  M \n 6 40 21 \n\n\n\n\n\n\n\n\nImportant\n\n\n\nTo ensure that our analyses are reproducible (see Section 14.2) from the beginning to the end, it is crucial that we document all of our corrections in a script. This ensures that if we need to go back on any data pre-processing decision that we made or if we need to make any additional corrections, we can do so without having to re-do our entire analyses. In addition, it means that our corrections and other data pre-processing steps are transparent and can be inspected and challenged by our peers.\n\n\n\n9.5.1 Using {stringr} functions\nTo convert all of the lower-case ‚Äúf‚Äù in the Gender variable to upper-case ‚ÄúF‚Äù, we can combine the mutate() with the str_to_upper() function. This ensures that all values in the new Gender.corrected column are in capital letters.\n\nL2.data.cleaned &lt;- L2.data.fct |&gt; \n  mutate(Gender.corrected = str_to_upper(Gender))\n\nWe should check that our correction has gone to plan by comparing the original Gender variable with the new Gender.corrected. To this end, we display them side by side using the select() function from {dplyr}.\n\nL2.data.cleaned |&gt; \n  select(Gender, Gender.corrected)\n\n\n\n  Gender Gender.corrected\n1      F                F\n2      f                F\n3      F                F\n4      F                F\n5      M                M\n6      F                F\n\n\nLike mutate() and select(), str_to_upper() also comes from a tidyverse package2. All functions that begin with str_ come from the {stringr} package, which features lots of useful functions to manipulate character string vectors. These include:\n\n\n\n\n\nHex sticker of the {stringr} package\n\n\n\nstr_to_upper() converts to string upper case.\nstr_to_lower() converts to string lower case.\nstr_to_title() converts to string title case (i.e.¬†only the first letter of each word is capitalised).\nstr_to_sentence() converts string to sentence case (i.e.¬†only the first letter of each sentence is capitalised).\n\nFor more useful functions to manipulate character strings, check out the {stringr} cheatsheet: https://github.com/rstudio/cheatsheets/blob/main/strings.pdf.\nNote that in the code chunk above, we did not save the output to a new R object. We merely printed the output in the Console. Once we have checked that our data wrangling operation went well, we can overwrite the original Gender variable with the cleaned version by using the original variable name as the name of the new column.\n\nL2.data.cleaned &lt;- L2.data.fct |&gt; \n  mutate(Gender = str_to_upper(Gender))\n\nUsing summary() or class(), we can see that manipulating the Gender variable with a function from {stringr} has resulted in the factor variable being converted back to a character variable.\n\nsummary(L2.data.cleaned$Gender)\n\n   Length     Class      Mode \n       67 character character \n\nclass(L2.data.cleaned$Gender)\n\n[1] \"character\"\n\n\nWe therefore need to add a line of code to reconvert it to a factor. We can do this within a single mutate() command.\n\nL2.data.cleaned &lt;- L2.data.fct |&gt; \n  mutate(Gender = str_to_upper(Gender),\n         Gender = factor(Gender))\n\nclass(L2.data.cleaned$Gender)\n\n[1] \"factor\"\n\n\nNow the summary() function provides a tally of male and female participants that corresponds to the values reported in DƒÖbrowska (2019: 5).\n\nsummary(L2.data.cleaned$Gender)\n\n F  M \n46 21 \n\n\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nThis task focuses on the OccupGroup variable, which is found in both the L1 and L2 datasets.\nOccupGroup is a categorical variable that groups participants‚Äô professional occupations into different categories. In the L2 dataset, there are four occupational categories.\n\nL2.data.fct |&gt; \n  count(OccupGroup)\n\n  OccupGroup  n\n1          C 10\n2          I  3\n3          M 21\n4         PS 33\n\n\nDƒÖbrowska (2019: 6) explains that these abbreviations correspond to:\n\nC: Clerical positions\nI: Occupationally inactive (i.e.¬†unemployed, retired, or homemakers)\nM: Manual jobs\nPS: Professional-level jobs or studying for a degree\n\nQ9.6 Examine the OccupGroup variable in the L1 dataset (L1.data). What do you notice? Why are L1 participants grouped into five rather than four occupational categories?\n\n\n\n\n\nBecause this study has more L1 participants than L2 participants.\n\n\n\n\nBecause one L1 participant had an occupation that did not fit any of the other four categories.\n\n\n\n\nBecause an extra space character was accidentally added after one \"PS\" value.\n\n\n\n\nBecause the original data file was saved in a format incompatible with R.\n\n\n\n\n\n\n\n\n\n\nClick here to help you answer a.\nsummary(L1.data.fct$OccupGroup)\n##   C   I   M  PS PS  \n##  22  23  20  24   1\n\nL1.data.fct |&gt; \n  count(OccupGroup)\n##   OccupGroup  n\n## 1          C 22\n## 2          I 23\n## 3          M 20\n## 4         PS 24\n## 5        PS   1\n\n\n¬†\nQ9.7 Which {stringr} function removes trailing spaces from character strings? Find the appropriate function on the {stringr} cheatsheet.\n\n\n\n\n\nstr_flatten()\n\n\n\n\nstr_ends()\n\n\n\n\nstr_trim()\n\n\n\n\nstr_glue()\n\n\n\n\nstr_squish()\n\n\n\n\nstr_extract()\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n\n\nShow R code to use the function and check that it worked as expected.\nL1.data.cleaned &lt;- L1.data.fct |&gt; \n  mutate(OccupGroup = str_trim(OccupGroup))\n\nL1.data.cleaned |&gt; \n  count(OccupGroup)\n\n\n¬†\nQ9.8 Following the removal of trailing whitespaces, what percentage of L1 participants have a professional-level jobs/are studying for a degree?\n\n\n\n\n\n20%\n\n\n\n\n22.22%\n\n\n\n\n24.44%\n\n\n\n\n25%\n\n\n\n\n26%\n\n\n\n\n28%\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n\n\nShow R code to answer question c.\nL1.data.cleaned |&gt; \n  count(OccupGroup) |&gt; \n  mutate(percent = n / sum(n),\n         percent = percent*100, \n         percent = round(percent, digits = 2)\n         )\n\n\n\n\n\nSo far, we have looked at rather simple data cleaning cases. Let‚Äôs now turn to a slightly more complex one: In the L2 dataset, the variable NativeLg contains character string values that correspond to the L2 participants‚Äô native language. Using the base R function unique(), we can see that there are a total of 22 unique values in this variable. However using sort() to order these 22 values alphabetically, we can easily see that there are, in fact, fewer unique native languages in this dataset due to different spellings and the inconsistent use of upper-case letters.\n\nL2.data$NativeLg |&gt; \n  unique() |&gt; \n  sort()\n\n [1] \"Cantonese\"           \"Cantonese/Hokkein\"   \"chinese\"            \n [4] \"Chinese\"             \"french\"              \"German\"             \n [7] \"greek\"               \"Italian\"             \"Lithuanian\"         \n[10] \"Lithunanina\"         \"Lituanian\"           \"Mandarin\"           \n[13] \"Mandarin Chinese\"    \"Mandarin/ Cantonese\" \"mandarin/malaysian\" \n[16] \"Mandarine Chinese\"   \"polish\"              \"Polish\"             \n[19] \"Polish/Russian\"      \"russian\"             \"Russian\"            \n[22] \"Spanish\"            \n\n\nIf we convert all NativeLg values to title case, we can reduce the number of unique languages to 19.\n\nL2.data$NativeLg |&gt;\n  str_to_title() |&gt; \n  unique() |&gt; \n  sort()\n\n [1] \"Cantonese\"           \"Cantonese/Hokkein\"   \"Chinese\"            \n [4] \"French\"              \"German\"              \"Greek\"              \n [7] \"Italian\"             \"Lithuanian\"          \"Lithunanina\"        \n[10] \"Lituanian\"           \"Mandarin\"            \"Mandarin Chinese\"   \n[13] \"Mandarin/ Cantonese\" \"Mandarin/Malaysian\"  \"Mandarine Chinese\"  \n[16] \"Polish\"              \"Polish/Russian\"      \"Russian\"            \n[19] \"Spanish\"            \n\n\nSecond, to facilitate further analyses, we may decide to only retain the first word/language from each entry as this will further reduce the number of different levels in this categorical variable. To abbreviate ‚ÄúMandarin Chinese‚Äù to ‚ÄúMandarin‚Äù, we can use the word() function from the {stringr} package.\nBelow is an extract of the help page for the word() function (accessed with the command ?word). Can you work out how to extract the first word of a character string?\n\n\n\n\nword {stringr}\nR Documentation\n\n\n\nExtract words from a sentence\nDescription\nExtract words from a sentence\nUsage\nword(string, start = 1L, end = start, sep = fixed(\" \"))\nArguments\n\n\n\n\n\n\n\nstring\nInput vector. Either a character vector, or something coercible to one.\n\n\nstart, end\nPair of integer vectors giving range of words (inclusive) to extract. If negative, counts backwards from the last word.\nThe default value select the first word.\n\n\nsep\nSeparator between words. Defaults to single space.\n\n\n\n\nThe help file tells us that ‚ÄúThe default value select the first word‚Äù. In our case, this means that we can simply use the word() function with no specified argument as this will automatically retain only the first word of every entry.\n\nL2.data$NativeLg |&gt;\n  str_to_title() |&gt; \n  word() |&gt; \n  unique() |&gt; \n  sort()\n\n [1] \"Cantonese\"          \"Cantonese/Hokkein\"  \"Chinese\"           \n [4] \"French\"             \"German\"             \"Greek\"             \n [7] \"Italian\"            \"Lithuanian\"         \"Lithunanina\"       \n[10] \"Lituanian\"          \"Mandarin\"           \"Mandarin/\"         \n[13] \"Mandarin/Malaysian\" \"Mandarine\"          \"Polish\"            \n[16] \"Polish/Russian\"     \"Russian\"            \"Spanish\"           \n\n\nAlternatively, we can choose to specify the ‚Äústart‚Äù argument as a reminder of what we did and to better document our code. The output is exactly the same.\n\nL2.data$NativeLg |&gt;\n  str_to_title() |&gt; \n  word(start = 1) |&gt; \n  unique() |&gt; \n  sort()\n\n [1] \"Cantonese\"          \"Cantonese/Hokkein\"  \"Chinese\"           \n [4] \"French\"             \"German\"             \"Greek\"             \n [7] \"Italian\"            \"Lithuanian\"         \"Lithunanina\"       \n[10] \"Lituanian\"          \"Mandarin\"           \"Mandarin/\"         \n[13] \"Mandarin/Malaysian\" \"Mandarine\"          \"Polish\"            \n[16] \"Polish/Russian\"     \"Russian\"            \"Spanish\"           \n\n\nAs you can tell from the output above, the word() function uses white space to identify word boundaries. In this dataset, however, some of the participants‚Äô native languages are separated by forward slashes (/) rather than or in addition to spaces. The ‚ÄúUsage‚Äù section of the help file for the word() function (see ?word and above) also confirms that the default word separator symbol is a space and shows us the syntax for changing the default separator. Below we change it to a forward slash.\n\nL2.data$NativeLg |&gt;\n  str_to_title() |&gt; \n  word(start = 1, sep = fixed(\"/\")) |&gt; \n  unique() |&gt; \n  sort()\n\n [1] \"Cantonese\"         \"Chinese\"           \"French\"           \n [4] \"German\"            \"Greek\"             \"Italian\"          \n [7] \"Lithuanian\"        \"Lithunanina\"       \"Lituanian\"        \n[10] \"Mandarin\"          \"Mandarin Chinese\"  \"Mandarine Chinese\"\n[13] \"Polish\"            \"Russian\"           \"Spanish\"          \n\n\nNow we can combine these two word extraction methods using the pipe operator (|&gt;) so that ‚ÄúCantonese/Hokkein‚Äù is abbreviated to ‚ÄúCantonese‚Äù and ‚ÄúMandarin/ Cantonese‚Äù to ‚ÄúMandarin‚Äù.\n\nL2.data$NativeLg |&gt;\n  str_to_title() |&gt; \n  word(start = 1) |&gt; # Extracts the first word before the first space\n  word(start = 1, sep = fixed(\"/\")) |&gt; # Extracts the first word before the first forward slash\n  unique() |&gt; \n  sort()\n\n [1] \"Cantonese\"   \"Chinese\"     \"French\"      \"German\"      \"Greek\"      \n [6] \"Italian\"     \"Lithuanian\"  \"Lithunanina\" \"Lituanian\"   \"Mandarin\"   \n[11] \"Mandarine\"   \"Polish\"      \"Russian\"     \"Spanish\"    \n\n\n\n\n\n\n\n\nNoteGoing further: Using regular expressions (regex) :nerd-face:\n\n\n\n\n\nMany functions of the {stringr} package involve regular expressions (short: regex). The second page of the {stringr} cheatsheet provides a nice overview of how regular expressions can be used to manipulate character strings in R.\nUsing the str_extract() function together with the regex \\\\w+, it is possible to extract the first word of each NativeLg value with just one line of code:\n\nL2.data$NativeLg |&gt;\n  str_to_title() |&gt; \n  str_extract(\"\\\\w+\") |&gt; \n  unique() |&gt; \n  sort()\n\n [1] \"Cantonese\"   \"Chinese\"     \"French\"      \"German\"      \"Greek\"      \n [6] \"Italian\"     \"Lithuanian\"  \"Lithunanina\" \"Lituanian\"   \"Mandarin\"   \n[11] \"Mandarine\"   \"Polish\"      \"Russian\"     \"Spanish\"    \n\n\nRegular expressions provide incredibly powerful and versatile ways to work with text in all kinds of programming languages. When conducting corpus linguistics research, they also allow us to conduct complex corpus queries.\nEach programming language/software has a slightly different flavour of regex but the basic principles are the same across all languages/software and are well worth learning. To get started, I highly recommend this beautifully designed interactive regex tutorial for beginners: https://regexlearn.com/learn/regex101. Have fun! :nerd-face:\n\n\n\n\n\n9.5.2 Using case_when()\nWe have now reduced the number of levels in the NativeLg variable to just 14 unique languages. But we still have some typos to correct, e.g.¬†‚ÄúLithunanina‚Äù and ‚ÄúLituanian‚Äù.\nWe can correct these on a case-by-case basis using case_when(). This is a very useful tidyverse function from the {dplyr} package that is easy to use once you have gotten used to its syntax. Figure¬†9.6 illustrates the syntax with a toy example dataset about the dangerousness of dragons (df). In this annotated line of code in Figure¬†9.6, mutate() is used to add a new column called danger whose values depend on the type of dragon that we are dealing with. The first argument of case_when() determines that, when the dragon type is equal to ‚Äúkraken‚Äù, then the danger value is set to ‚Äúextreme‚Äù, otherwise the danger value is set to ‚Äúhigh‚Äù. You can see the outcome in the appended danger column.\n\n\n\n\n\n\nFigure¬†9.6: Artwork explaining the case_when() function by @allison_horst).\n\n\n\nApplying case_when() to fix the typos in the NativeLg variable in L2.data, we determine that:\n\nif the shortened NativeLg value is ‚ÄúMandarine‚Äù, we replace it with ‚ÄúMandarin‚Äù, and\nif the shortened NativeLg value corresponds to either ‚ÄúLithunanina‚Äù or ‚ÄúLituanian‚Äù, we replace it with ‚ÄúLithuanian‚Äù.\n\nUsing mutate(), we save this cleaned-up version of the NativeLg variable as a new column in our L2.data table, which we call NativeLg.cleaned.\n\nL2.data &lt;- L2.data |&gt;\n  mutate(\n    NativeLg.cleaned = str_to_title(NativeLg) |&gt; \n      word(start = 1) |&gt; \n      word(start = 1, sep = fixed(\"/\")),\n    NativeLg.cleaned = case_when(\n      NativeLg.cleaned == \"Mandarine\" ~ \"Mandarin\",\n      NativeLg.cleaned %in% c(\"Lithunanina\", \"Lituanian\") ~ \"Lithuanian\",\n      TRUE ~ NativeLg.cleaned)\n    )\n\nWhenever we do any data wrangling, it is crucial that we take the time to carefully check that we have not made any mistakes in the process. To this end, we display the original NativeLg and the new NativeLg.cleaned variables side by side using the select() function.\n\nL2.data |&gt; \n  select(NativeLg, NativeLg.cleaned)\n\n\n\n    NativeLg NativeLg.cleaned\n1 Lithuanian       Lithuanian\n2     polish           Polish\n3     Polish           Polish\n4    Italian          Italian\n5  Lituanian       Lithuanian\n6     Polish           Polish\n\n\nAs you can see, only the first six rows of the table are printed above. Run the code yourself to check all the other rows.\n\n\n\n\n\n\nNoteUsing base R functions instead\n\n\n\n\n\nThis chapter focuses on {tidyverse} functions, however all of the above data wrangling and cleaning operations can equally be achieved using base R functions. For example, the mutate() code chunk above could be replaced by the following lines of base R code.\n\n1L2.data$NativeLg.cleaned.base &lt;- gsub(\"([a-zA-Z]+).*\", \"\\\\1\", L2.data$NativeLg)\n2L2.data$NativeLg.cleaned.base &lt;- tools::toTitleCase(L2.data$NativeLg.cleaned.base)\n3L2.data$NativeLg.cleaned.base[L2.data$NativeLg.cleaned.base == \"Mandarine\"] &lt;- \"Mandarin\"\n4L2.data$NativeLg.cleaned.base[L2.data$NativeLg.cleaned.base %in% c(\"Lithunanina\", \"Lituanian\")] &lt;- \"Lithuanian\"\n\n\n1\n\nWith the first line, we extract the first string of letters before any space or slash in NativeLg and save this to a new variable called NativeLg.cleaned.base.\n\n2\n\nThis line converts all the values of the new variable to title case using a base R function from the {tools} package. The {tools} package comes with R so you don‚Äôt need to install it separately but, if you haven‚Äôt loaded it earlier in your R session, you need to call the function with the prefix tools:: so that R knows where to find the toTitleCase() function.\n\n3\n\nThe third line corrects a typo with a direct replacement.\n\n4\n\nThis line replaces two typos with a single correction.\n\n\n\n\nIf we now compare the variable created with the tidyverse code (NativeLg.cleaned) vs.¬†the one created using base R functions only (NativeLg.cleaned.base), we can see that they are exactly the same.\n\nL2.data |&gt; \n  select(NativeLg.cleaned, NativeLg.cleaned.base) \n\n\n\n  NativeLg.cleaned NativeLg.cleaned.base\n1       Lithuanian            Lithuanian\n2           Polish                Polish\n3           Polish                Polish\n4          Italian               Italian\n5       Lithuanian            Lithuanian\n6           Polish                Polish\n\n\nAn undeniable advantage of sticking to base R functions is that your code is more portable as it does not require the installation of any additional packages, keeping dependencies on external packages to the minimum. However, base R lacks the consistency of the tidyverse framework, which can make certain data transformation tasks considerably more tricky and code less readable (and therefore less transparent) to yourself and others.\nAs we don‚Äôt need two versions of the cleaned NativLg variable, we will now remove the NativeLg.cleaned.base column from L2.data. To do so, we use the select() function combined with the - operator to ‚Äúunselect‚Äù the column we no longer need.\n\nL2.data &lt;- L2.data |&gt; \n  select(- NativeLg.cleaned.base)\n\n\n\n\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nFor some analyses, it may be useful to group together participants whose native languages come from the same family of languages. For example, French, Spanish and Italian L1 speakers, may be considered as a one group of participants whose native language is a Romance language.\nUse mutate() and case_when() to add a new variable to L2.data that corresponds to the L2 participant‚Äôs native language family. Call this new variable NativeLgFamily. Use the following language family categories:\n\nBaltic\nChinese\nGermanic\nHellenic\nRomance\nSlavic\n\nIf you‚Äôre not sure which language family a language belongs to, look it up on Wikipedia (e.g.¬†the Wikipedia page on the German language informs us in a text box at the top of the article that German is a Germanic language).\nQ9.9 Which language family is the second most represented among L2 participants‚Äô native languages in DƒÖbrowska (2019)?\n\n\n\n\n\nBaltic\n\n\n\n\nChinese\n\n\n\n\nGermanic\n\n\n\n\nHellenic\n\n\n\n\nRomance\n\n\n\n\nSlavic\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\nQ9.10 How many L2 participants are native speakers of a language that belongs to the family of Romance languages?\n\n\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\nQ9.11 What percentage of L2 participants have a Slavic native language? Round your answer to the nearest percent.\n\n\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\nQ9.12 If you check the output of colnames(L2.data) or View(L2.data), you will see that the new variable that you created is now the last column in the table. Consult the help file of the {dplyr} function relocate() to work out how to place this column immediately after NativeLg.\n\n\n\n\nL2.data &lt;- L2.data |&gt; relocate(f, .after = NativeLg)\nL2.data &lt;- L2.data |&gt; relocate(NativeLgFamily, after = NativeLg)\nL2.data &lt;- L2.data |&gt; relocate(NativeLgFamily, .before = NativeLg)\nL2.data &lt;- L2.data |&gt; relocate(NativeLgFamily, .after = NativeLg)\nL2.data &lt;- L2.data |&gt; relocate(NativeLgFamily, .after = NULL)\n\n\n\n\n\n\n\n¬†\n\n\n\n\n\n\n\n\n\nNoteClick here for solutions to Q9.9‚ÄîQ9.12.\n\n\n\n\n\nAs is often the case, there are several ways to solve these Your turn! tasks. Here is one solution based on what we have covered so far in this chapter.\nQ9.9 Note that the following code will only work if you followed the instructions in the section above to create the NativeLg.cleaned variable as it relies on this variable to create the new NativeLgFamily variable.\n\nL2.data &lt;- L2.data |&gt; \n  mutate(NativeLgFamily = case_when(\n    NativeLg.cleaned == \"Lithuanian\" ~ \"Baltic\",\n    NativeLg.cleaned %in% c(\"Cantonese\", \"Mandarin\", \"Chinese\") ~ \"Chinese\",\n    NativeLg.cleaned == \"German\" ~ \"Germanic\",\n    NativeLg.cleaned == \"Greek\" ~ \"Hellenic\",\n    NativeLg.cleaned %in% c(\"French\", \"Italian\", \"Spanish\") ~ \"Romance\",\n    NativeLg.cleaned %in% c(\"Polish\", \"Russian\") ~ \"Slavic\"))\n\nAs always, it is important to check that things have gone to plan.\n\nL2.data |&gt; \n  select(NativeLg.cleaned, NativeLgFamily)\n\n\n\n  NativeLg.cleaned NativeLgFamily\n1       Lithuanian         Baltic\n2           Polish         Slavic\n3           Polish         Slavic\n4          Italian        Romance\n5       Lithuanian         Baltic\n6           Polish         Slavic\n\n\nQ9.10 We can display the distribution of language families using either the base R table() function or the {tidyverse} count() function.\n\ntable(L2.data$NativeLgFamily)\n\n\n  Baltic  Chinese Germanic Hellenic  Romance   Slavic \n       5       15        1        1        6       39 \n\nL2.data |&gt; \n  count(NativeLgFamily)\n\n  NativeLgFamily  n\n1         Baltic  5\n2        Chinese 15\n3       Germanic  1\n4       Hellenic  1\n5        Romance  6\n6         Slavic 39\n\n\nQ9.11 We can add a column to show the distribution in percentages by adding a new ‚Äúpercent‚Äù column to the count() table using mutate():\n\n1L2.data |&gt;\n2  count(NativeLgFamily) |&gt;\n3  mutate(percent = n / sum(n),\n4         percent = percent*100,\n5         percent = round(percent, digits = 0)\n         ) |&gt; \n6  arrange(desc(n))\n\n\n1\n\nWe start with the dataset that contains the new NativeLgFamily variable.\n\n2\n\nWe pipe it into the count() function. As shown above, this function produces a frequency table with counts stored in the variable n.\n\n3\n\nWe divide the number of participant with each native language (n) by the total number of participants (sum(n)). We obtain proportions ranging from 0 to 1.\n\n4\n\nWe multiply these by 100 to get percentages.\n\n5\n\nWe round the percentages to two decimal places.\n\n6\n\nWe reorder the table so that the most represented group is at the top. To do so, we pipe our table into the dplyr::arrange(). By default, arrange() orders values in ascending order (from smallest to largest); hence, we add the desc() function to sort the table in descending order of frequency.\n\n\n\n\n  NativeLgFamily  n percent\n1         Slavic 39      58\n2        Chinese 15      22\n3        Romance  6       9\n4         Baltic  5       7\n5       Germanic  1       1\n6       Hellenic  1       1\n\n\nü™ê Note that this a {tidyverse} approach to working out percentages, see Section 8.1.3 for a base R approach.\nQ9.12 At the time of writing, the help file of the relocate() function still featured examples using the {magrittr} pipe (%&gt;%) rather than the native R pipe (|&gt;) (see Section 7.5.2), but the syntax remains the same. The first argument is the data which we are piping into the function, the second argument is the column that we want to move. Then, we need to specify where to with either the ‚Äú.after‚Äù or the ‚Äú.before‚Äù argument.\n\nL2.data &lt;- L2.data |&gt; \n  relocate(NativeLgFamily, .after = NativeLg)\n\n\n\n\n\n\n\nFigure¬†9.7: Artwork explaining the dplyr::relocate() function by @allison_horst.\n\n\n\nNote that the help file specifies that both ‚Äú.after‚Äù and ‚Äú.before‚Äù begin with a dot. If you leave the dot out, the function will not work as expected! Can you see what‚Äôs happened here?\n\nL2.data |&gt; \n  relocate(NativeLgFamily, after = NativeLg) |&gt; \n  str()\n\n\n\n'data.frame':   67 obs. of  6 variables:\n $ Participant   : int  220 244 46 221 222 230 247 237 243 213 ...\n $ Gender        : chr  \"F\" \"f\" \"F\" \"F\" ...\n $ Occupation    : chr  \"Student\" \"student\" \"Cleaner\" \"Student\" ...\n $ OccupGroup    : chr  \"PS\" \"PS\" \"M\" \"PS\" ...\n $ NativeLg      : chr  \"Lithuanian\" \"polish\" \"Polish\" \"Italian\" ...\n $ NativeLgFamily: chr  \"Baltic\" \"Slavic\" \"Slavic\" \"Romance\" ...\n\n\nThe relocate() function has moved NativeLgFamily to the first column (the function‚Äôs default position) and has also moved NativeLg to the second position, but it has renamed the column after.\nThis is a reminder to always check whether your data wrangling operations have gone as planned. Just because you didn‚Äôt get an error message doesn‚Äôt mean that your code did what you wanted! ‚ö†Ô∏èÔ∏è",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Data w`R`angling</span>"
    ]
  },
  {
    "objectID": "9_DataWrangling.html#sec-CombiningDatasets",
    "href": "9_DataWrangling.html#sec-CombiningDatasets",
    "title": "9¬† Data wRangling",
    "section": "9.6 Combining datasets",
    "text": "9.6 Combining datasets\nSo far, we have analysed the L1 and L2 datasets individually. In the following chapters, however, we will conduct comparative analyses, comparing the performance of the L1 and L2 participants in the various language-related tests conduced as part of DƒÖbrowska (2019). To this end, we need to create a combined table that includes the data of all participants from DƒÖbrowska (2019).\nRemember that both tables, L1.data and L2.data, are in a tidy data format. This means that:\n\neach row represents an observation (i.e.¬†here, a participant),\neach cell represents a measurement, and\neach variable forms a column.\n\nTo combine the two datasets, therefore, we need to combine the rows of the two tables. However, we cannot simply add the rows of the L2.data table to the bottom of L1.data table because, as shown below, the two tables do not have the same number of columns and the shared columns are not in the same position! We therefore need to ensure that, when the two datasets are combined, the shared columns are aligned.\n\ncolnames(L1.data)\n\n [1] \"Participant\" \"Age\"         \"Gender\"      \"Occupation\"  \"OccupGroup\" \n [6] \"OtherLgs\"    \"Education\"   \"EduYrs\"      \"ReadEng1\"    \"ReadEng2\"   \n[11] \"ReadEng3\"    \"ReadEng\"     \"Active\"      \"ObjCl\"       \"ObjRel\"     \n[16] \"Passive\"     \"Postmod\"     \"Q.has\"       \"Q.is\"        \"Locative\"   \n[21] \"SubCl\"       \"SubRel\"      \"GrammarR\"    \"Grammar\"     \"VocabR\"     \n[26] \"Vocab\"       \"CollocR\"     \"Colloc\"      \"Blocks\"      \"ART\"        \n[31] \"LgAnalysis\"  \"Gender.fct\" \n\n\n\ncolnames(L2.data)\n\n [1] \"Participant\"      \"Gender\"           \"Occupation\"       \"OccupGroup\"      \n [5] \"NativeLg\"         \"NativeLgFamily\"   \"OtherLgs\"         \"EdNative\"        \n [9] \"EdUK\"             \"Age\"              \"EduYrsNat\"        \"EduYrsEng\"       \n[13] \"EduTotal\"         \"FirstExp\"         \"Arrival\"          \"LoR\"             \n[17] \"EngWork\"          \"EngPrivate\"       \"ReadEng1\"         \"ReadOth1\"        \n[21] \"ReadEng2\"         \"ReadOth2\"         \"ReadEng3\"         \"ReadOth3\"        \n[25] \"ReadEng\"          \"ReadOth\"          \"Active\"           \"ObjCl\"           \n[29] \"ObjRel\"           \"Passive\"          \"Postmod\"          \"Q.has\"           \n[33] \"Q.is\"             \"Locative\"         \"SubCl\"            \"SubRel\"          \n[37] \"GrammarR\"         \"Grammar\"          \"VocabR\"           \"Vocab\"           \n[41] \"CollocR\"          \"Colloc\"           \"Blocks\"           \"ART\"             \n[45] \"LgAnalysis\"       \"UseEngC\"          \"NativeLg.cleaned\"\n\n\nNote, also, that participants‚Äô total number of years in education is stored in the EduYrs column in L1.data, whereas the corresponding column in L2.data is called EduTotal. Hence, we first use the {dplyr} function rename() to rename EduYrs in L1.data as EduTotal before we merge the two tables.\n\nL1.data &lt;- L1.data |&gt; \n  rename(EduTotal = EduYrs)\n\nThe {dplyr} package boasts an array of useful functions to combine tables (see Figure¬†9.8). For our purposes, bind_rows() appears to be the perfect function.3\n\n\n\n\n\n\nFigure¬†9.8: Extract of the data transformation with {dplyr} cheatsheet (CC BY SA Posit Software, PBC)\n\n\n\nHowever, when we try to combine L1.data and L2.data using bind_rows(), we get an error message‚Ä¶ üò¢ Does this error remind you of Q10 from Section 7.2.3 by any chance?\n\ncombined.data &lt;- bind_rows(L1.data, L2.data)\n\nError in `bind_rows()`:\n! Can't combine `..1$Participant` &lt;character&gt; and `..2$Participant` &lt;integer&gt;.\nWhat this error message tells us is that the bind_rows() function cannot combine the two Participant columns because in L1.data it is a string character vector, whereas in L2.data it is an integer vector. However, to avoid data loss, bind_rows() can only match columns of the same data type!\nWe must therefore first convert the Participant variable in L2.data to a character vector.\n\nL2.data &lt;- L2.data |&gt; \n  mutate(Participant = as.character(Participant))\n\nNow, we can combine the two data frames using bind_rows().\n\ncombined.data &lt;- bind_rows(L1.data, L2.data)\n\nThe problem is that now that we have merged our two datasets into one, it‚Äôs not obvious which rows correspond to L1 participants and which to L2 participants! There are various ways to solve this, but here‚Äôs a simple three-step solution that relies exclusively on functions that you are already familiar with.\nStep 1: We add a new column to L1.data called Group and fill this column with the value ‚ÄúL1‚Äù for all rows.\n\nL1.data &lt;- L1.data |&gt; \n  mutate(Group = \"L1\")\n\nStep 2: We add a new column to L2.data also called Group and fill this column with the value ‚ÄúL2‚Äù for all rows.\n\nL2.data &lt;- L2.data |&gt; \n  mutate(Group = \"L2\")\n\nStep 3: We use bind_rows() as above to combine the two datasets that now both include the extra Group column.4\n\ncombined.data &lt;- bind_rows(L1.data, L2.data)\n\nVerification step: The combined.data table now includes the column Group, which we can use to easily identify the observations that belong to L1 and L2 participants. As expected, our combined dataset includes 90 participants from the L1 group and 67 from the L2 group:\n\ncombined.data |&gt; \n  count(Group)\n\n  Group  n\n1    L1 90\n2    L2 67\n\n\nOur combined dataset contains all the columns that appear in either L1.data or L2.data. Check that this is the case by examining the structure of the new dataset with str(combined.data).\nYou will have noticed that, in some columns, there are lots of NA (‚ÄúNot Available‚Äù) values. These represent missing data. R has inserted these NA values in the columns that only appear in one of the two datasets. For example, the L1 dataset does not include an Arrival variable (indicating the age when participants first arrived in an English-speaking country), presumably because they were all born in an English-speaking country! We only have this information for the L2 participants and this explains the 90 NA values in the Arrival column of the combined dataset.\n\ncombined.data$Arrival\n\n  [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n [26] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n [51] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n [76] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA 17 18 19 19 19 19 20 22 22 23\n[101] 24 25 26 26 28 28 30 44 45 49 17 23 23 24 19 22 23 26 16 18 24 24 25 29 30\n[126] 32 33 22 25 30 44 27 18 20 21 33 38 47 16 19 20 24 25 28 33 20 22 25 28 26\n[151] 26 16 24 32 20 16 20\n\n\nWe can also check this by cross-tabulating the Group and the Arrival variables.\n\ncombined.data |&gt; \n  count(Group, Arrival)\n\n\n\n  Group Arrival  n\n1    L1      NA 90\n2    L2      16  4\n3    L2      17  2\n4    L2      18  3\n5    L2      19  6\n6    L2      20  6\n\n\nRun View(combined.data) to inspect the combined dataset and check in which other columns there are NA values.\n\n\n\n\n\n\nNoteWhat if my data are not yet in tidy format? :face-with-raised-eyebrow:\n\n\n\n\n\nCombining the two datasets from DƒÖbrowska (2019) was relatively easy because the data was already in tidy format. But, fear not: if you need to first convert your data to tidy format, the {tidyr} package has got you covered! :smiling-face-with-sunglasses:\nThe pivot_longer() and pivot_wider() functions allow you to easily convert tables from ‚Äúlong‚Äù to ‚Äúwide‚Äù format and vice versa (see Figure¬†9.9).\n\n\n\n\n\n\nFigure¬†9.9: Extract of the data tidying with tidyr cheatsheet (CC BY SA Posit Software, PBC)\n\n\n\n\nRemember to carefully check the output of any data manipulation that you do before moving on to doing any analyses! To this end, the View() function is particularly helpful.\n\n\n\n\n\n\n\nHex sticker of the {tidyr} package\n\n\n\n\n\n\n\n\nImportantUsing AI tools for coding ‚ö†Ô∏èÔ∏è\n\n\n\nNote that older textbooks/tutorials, and especially AI tools such as ChatGPT that have been trained on older web data, will frequently suggest superseded (i.e.¬†outdated) functions for data manipulation such as spread(), gather(), select_all(), and mutate_if(). If you use superseded functions, your code will still work, but R will print a warning in the Console and usually suggest a modern alternative.\nAI tools may also suggest using functions that are deprecated. As with superseded functions, you will get a warning message with a recommended alternative. In this case, however, you must follow the advice of the warning, as writing new code with deprecated functions is really asking for trouble! Deprecated functions are scheduled for removal, which means that your code will eventually no longer run on up-to-date R versions.\n\n\n\n\n\n\nFigure¬†9.10: The four main stages of the lifecycle of R packages, functions, function arguments: experimental developments can become stable and stable can eventually become deprecated or superseded (image by Henry and Wickham 2023 for Posit Software, PBC, https://lifecycle.r-lib.org/articles/stages.html).\n\n\n\nIn sum, to ensure the future compatibility of your code, do not ignore warnings about deprecated functions and, in general, never ever blindly trust the output of AI tools!",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Data w`R`angling</span>"
    ]
  },
  {
    "objectID": "9_DataWrangling.html#sec-filter",
    "href": "9_DataWrangling.html#sec-filter",
    "title": "9¬† Data wRangling",
    "section": "9.7 A pre-processing pipeline",
    "text": "9.7 A pre-processing pipeline\nSo far in this chapter, we have learnt how to pre-process data for future statistical analyses and data visualisation. In the process, we have learnt about lots of different functions, mostly from the tidyverse environment (see Section 9.1 ü™ê). Now it‚Äôs time to put everything together and save our pre-processed combined dataset for future use.\nBut, first, let‚Äôs recap all of the data wrangling operations that we performed in this chapter and combine them into one code chunk. Before running this code, we first reload the original data from DƒÖbrowska (2019) to overwrite any changes that were made during this chapter. This will ensure that we all have exactly the same version of the dataset for the following chapters. Detailed instructions to download and load the original data can be found in Chapter 6.\n\nlibrary(here)\n\nL1.data &lt;- read.csv(file = here(\"data\", \"L1_data.csv\"))\nL2.data &lt;- read.csv(file = here(\"data\", \"L2_data.csv\"))\n\nThen, run the following lines of code to create a new R object called combined.data that contains the wrangled data.\n\nL2.data &lt;- L2.data |&gt; \n  mutate(Participant = as.character(Participant)) |&gt; \n  mutate(Group = \"L2\")  \n\nL1.data &lt;- L1.data |&gt; \n  mutate(Group = \"L1\") |&gt; \n  rename(EduTotal = EduYrs)\n\ncombined.data &lt;- bind_rows(L1.data, L2.data) |&gt;\n  mutate(across(where(is.character), str_to_title)) |&gt;\n  mutate(across(where(is.character), str_trim)) |&gt;\n  mutate(OccupGroup = str_to_upper(OccupGroup)) |&gt; \n  mutate(\n    NativeLg = word(NativeLg, start = 1),\n    NativeLg = word(NativeLg, start = 1, sep = fixed(\"/\")),\n    NativeLg = case_when(\n      NativeLg == \"Mandarine\" ~ \"Mandarin\",\n      NativeLg %in% c(\"Lithunanina\", \"Lithunanina\", \"Lituanian\") ~ \"Lithuanian\",\n      TRUE ~ NativeLg)) |&gt; \n  mutate(NativeLgFamily = case_when(\n    NativeLg == \"Lithuanian\" ~ \"Baltic\",\n    NativeLg %in% c(\"Cantonese\", \"Mandarin\", \"Chinese\") ~ \"Chinese\",\n    NativeLg == \"German\" ~ \"Germanic\",\n    NativeLg == \"Greek\" ~ \"Hellenic\",\n    NativeLg %in% c(\"French\", \"Italian\", \"Spanish\") ~ \"Romance\",\n    NativeLg %in% c(\"Polish\", \"Russian\") ~ \"Slavic\")) |&gt; \n  mutate(across(where(is.character), factor))\n\nDon‚Äôt forgot To check the result by examining the output of View(combined.data) and str(combined.data).\n\nsummary(combined.data)\n\n\n\n  Participant       Age        Gender             Occupation OccupGroup\n 1      :  1   Min.   :17.00   F:94   Student          :27   C :32     \n 100    :  1   1st Qu.:25.00   M:63   Retired          :15   I :26     \n 101    :  1   Median :31.00          Product Operative: 5   M :41     \n 104    :  1   Mean   :35.48          Teacher          : 5   PS:58     \n 106    :  1   3rd Qu.:42.00          Cleaner          : 4             \n 107    :  1   Max.   :65.00          Unemployed       : 4             \n (Other):151                          (Other)          :97             \n              OtherLgs    Education     EduTotal        ReadEng1    \n None             :98   Student: 8   Min.   : 8.50   Min.   :0.000  \n No               :11   A Level: 5   1st Qu.:13.00   1st Qu.:1.000  \n English          : 8   Ba     : 5   Median :14.00   Median :3.000  \n German           : 6   Gcses  : 5   Mean   :14.62   Mean   :2.599  \n English At School: 3   Nvq    : 4   3rd Qu.:17.00   3rd Qu.:4.000  \n English, German  : 2   (Other):63   Max.   :24.00   Max.   :5.000  \n (Other)          :29   NA's   :67                                  \n    ReadEng2        ReadEng3        ReadEng      \n Min.   :0.000   Min.   :0.000   Min.   : 0.000  \n 1st Qu.:1.000   1st Qu.:1.000   1st Qu.: 5.000  \n Median :2.000   Median :2.000   Median : 7.000  \n Mean   :2.465   Mean   :2.019   Mean   : 7.083  \n 3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:10.000  \n Max.   :5.000   Max.   :4.000   Max.   :14.000  \n                                                 \n\n\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nQ9.13 The following operations describe the steps performed by the data wrangling code chunk above. In which order are the operations performed?\n\n\n\n\n\n\nConvert all the values of one variable to upper case.\n\n\n\n‚áÖ\n\n\n\nAdd a new variable based on another variable.\n\n\n\n‚áÖ\n\n\n\nAdd two new variables that each have all the same values.\n\n\n\n‚áÖ\n\n\n\nMerge data from the two datasets into one.\n\n\n\n‚áÖ\n\n\n\nConvert one variable to a character variable.\n\n\n\n‚áÖ\n\n\n\nRemove whitespace at the start and end of all values in all character variables.\n\n\n\n‚áÖ\n\n\n\nConvert the values of all character variables to title case.\n\n\n\n‚áÖ\n\n\n\nConvert all character string vectors to factors.\n\n\n\n‚áÖ\n\n\n\nShorten and correct typos in the values of one variable.\n\n\n\n‚áÖ\n\n\n\nChange the name of one variable\n\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\nQ9.14 In the combined dataset, how many participants have a clerical occupation?\n\n\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\nQ9.15 Of the participants who have a clerical occupation, how many were over 50 years old at the time of the data collection?\n\n\n\n\n\nnone\n\n\n\n\n1\n\n\n\n\n2\n\n\n\n\n6\n\n\n\n\nall of them\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipClick here to see the R code necessary to answer Q9.15\n\n\n\n\n\nThere are various ways to find the answer to Q5. Sticking to a function that we have looked at so far, you could cross-tabulate Age and OccupGroup using the count() function.\n\ncombined.data |&gt; \n  count(OccupGroup, Age)\n\n\n\n   OccupGroup Age n\n1           C  20 1\n2           C  25 6\n3           C  27 2\n4           C  28 3\n5           C  29 4\n6           C  30 3\n7           C  32 3\n8           C  37 1\n9           C  38 1\n10          C  39 1\n11          C  41 1\n12          C  51 2\n13          C  52 1\n14          C  53 1\n15          C  57 1\n16          C  60 1\n\n\nAnd then add up the frequencies listed in the rows that correspond to participants with clerical jobs who are 50.\n\n2 + 1 + 1 + 1 +1 \n\n[1] 6\n\n\nBut, of course, this is method is rather error-prone! Instead, we can use dplyr::filter() (see Figure¬†9.11) to filter the combined dataset according to our two criteria of interest and then count the number of rows (i.e.¬†participants) remaining in the dataset once the filter has been applied.\n\ncombined.data |&gt;\n  filter(OccupGroup == \"C\" & Age &gt; 50) |&gt; \n  nrow()\n\n[1] 6\n\n\n\n\n\n\n\n\nFigure¬†9.11: Artwork explaining the dplyr::filter() function by @allison_horst.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Data w`R`angling</span>"
    ]
  },
  {
    "objectID": "9_DataWrangling.html#sec-ExportingRObjects",
    "href": "9_DataWrangling.html#sec-ExportingRObjects",
    "title": "9¬† Data wRangling",
    "section": "9.8 Saving and exporting R objects",
    "text": "9.8 Saving and exporting R objects\nAs a final step, we want to save the R object combined.data to a local file on our computer so that, when we continue our analyses in a new R session, we can immediately start working with the wrangled dataset. We can either save the wrangled dataset as an R object (.rds) or export it as a DSV file (e.g.¬†.csv, see Section 2.5.1). The pros and cons of the two solutions are summarised in Table¬†9.1.\n\n\n\nTable¬†9.1: Pros and cons of saving DSV and R data files.\n\n\n\n\n\n\n\n\nDSV files (e.g.¬†.csv, .tsv, .tab) | R data files (.rds) | :================================================================================================+:========================================================================================================================================+ :check-mark-button: Highly portable (i.e., can be opened in all standard spreadsheet software and text editors). | :cross-mark: Specific to R and cannot be opened in standard spreadsheet software or text editors.\n\n\n:cross-mark: Inefficient for very large datasets. | :check-mark-button: Efficient memory usage for more compact data storage and faster loading times in R.\n\n\n:check-mark-button: Universal, language-independent format and therefore suitable for long-term archiving. | :cross-mark: No guarantee that older .rds files will be compatible with newer versions of R and therefore unsuitable for long-term archiving.\n\n\n:cross-mark: Loss of metadata. | :check-mark-button: Preserve R data structures (e.g., factor variables remain stored as factors).\n\n\n\n\n\n\nWe will save both a .csv and an .rds version of the wrangled data but in the following chapters, we will use the .rds file.\nWe will save both files to a subfolder of our project ‚Äúdata‚Äù folder called ‚Äúprocessed‚Äù. If we try to save the file to this subfolder before it has been created at this location we get an error message.\n\nsaveRDS(combined.data, file = here(\"data\", \"processed\", \"combined_L1_L2_data.rds\"))\n\nError in gzfile(file, mode) : cannot open the connection\nWe first need to create the ‚Äúprocessed‚Äù subfolder before we can save to this location! There are two ways of doing this:\n\nEither in the Files pane of RStudio or in a File Navigator/Explorer window, navigate to the ‚Äúdata‚Äù folder and, from there, click on the ‚ÄúCreate a new folder‚Äù icon to create a new subfolder called ‚Äúprocessed‚Äù.\nAlternatively, we can use the dir.create() function to create the subfolder from R itself. If the folder already exists at this location, we will get a warning.\n\n\ndir.create(file.path(here(\"data\", \"processed\")))\n\nNow that the subfolder exists, we can save combined.data as an .rds file. We will work with this file in the following chapters.\n\nsaveRDS(combined.data, file = here(\"data\", \"processed\", \"combined_L1_L2_data.rds\"))\n\nIf you want to share your wrangled dataset with a colleague who does not (yet? :winking-face:) use R, you can use the tidyverse function write_csv().5 Your colleague will be able to open this file in any standard spreadsheet programme or text editor (but do warn them about the dangers of opening .csv file in spreadsheets, see Section 2.6!).\n\nwrite_csv(combined.data, file = here(\"data\", \"processed\", \"combined_L1_L2_data.csv\"))",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Data w`R`angling</span>"
    ]
  },
  {
    "objectID": "9_DataWrangling.html#check-your-progress",
    "href": "9_DataWrangling.html#check-your-progress",
    "title": "9¬† Data wRangling",
    "section": "Check your progress üåü",
    "text": "Check your progress üåü\nYou have successfully completed 0 out of 15 questions in this chapter.\nThat was a lot of data wrangling, but we are now ready to proceed with some comparative analyses of L1 and L2 English speakers‚Äô language skills!\nAre you confident that you can‚Ä¶?\n\nDefine tidy data (Section 9.1)\nCheck the sanity of a dataset (Section 9.3)\nConvert character vectors representing categorical data to factors (Section 9.3.2)\nAdd and replace columns in a table (Section 9.4.1)\nTransform several columns at once (Section 9.4.2)\nUse {stringr} functions to manipulate text values (Section 9.5.1)\nInterpret R package cheatsheets\nGain insights from the help file of R functions\nUse tidyverse functions to pre-process data in an readable and reproducible way (Section 9.4) and (Section 9.7)\nSave R objects as .rds and .csv files on your computer (Section 9.8)\n\nIn Chapter 10 we continue to explore the tidyverse as we learn how to use the popular tidyverse package {ggplot2} to visualise the pre-processed data from DƒÖbrowska (2019). Are you ready to get creative? :artist-palette:\n\n\n\n\nDƒÖbrowska, Ewa. 2019. Experience, aptitude, and individual differences in linguistic attainment: A comparison of native and nonnative speakers. Language Learning 69(S1). 72‚Äì100. https://doi.org/10.1111/lang.12323.\n\n\nHorst, Allison & Julie Lowndes. 2020. Openscapes - Tidy data for efficiency, reproducibility, and collaboration. https://openscapes.org/blog/2020-10-12-tidy-data/.\n\n\nWickham, Hadley, Davis Vaughan & Maximilian Girlich. Tidy messy data. https://tidyr.tidyverse.org/.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Data w`R`angling</span>"
    ]
  },
  {
    "objectID": "9_DataWrangling.html#footnotes",
    "href": "9_DataWrangling.html#footnotes",
    "title": "9¬† Data wRangling",
    "section": "",
    "text": "Note that, throughout this chapter, long code output is shortened to save space. When you run this command on your own computer, however, you will see that the output is much longer than what is reprinted in this chapter. You will likely need to scroll up in your Console window to view it all.‚Ü©Ô∏é\nThe equivalent base R function is toupper().‚Ü©Ô∏é\nWe could also use the full_join() function since we want to retain all rows and all columns from both datasets.‚Ü©Ô∏é\nAlternatively, you may have gathered from the cheatsheet (Figure¬†9.8) that the bind_rows() function has an optional ‚Äú.id‚Äù argument that can be used to create an additional column to disambiguate between the two combined datasets. In this case, we do not need to add a Group column to both datasets prior to combining them.\n\ncombined.data &lt;- bind_rows(L1 = L1.data, \n                           L2 = L2.data, \n                           .id = \"Group\")\n\n‚Ü©Ô∏é\nAs usual, there is a base R alternative: write.csv() will work just as well but, for larger datasets, it is considerably slower than write_csv(). For finer differences, check out the functions‚Äô respective help files.‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Data w`R`angling</span>"
    ]
  },
  {
    "objectID": "10_Dataviz.html",
    "href": "10_Dataviz.html",
    "title": "10¬† The GrammaR of Graphics",
    "section": "",
    "text": "Chapter overview\nOne of the advantages of working in R is that it allows us to create highly customised graphs for effective data visualisation. The Grammar of Graphics (Wilkinson 2005) is a theoretical framework that defines a structured approach to building and understanding statistical graphs. We will focus on using the tidyverse package {ggplot2} (Wickham 2016) to create effective data visualisations. The {ggplot2} is an implementation of the Grammar of Graphics (GG) syntax in R.\nThis chapter is divided into two parts: the first explains the syntax of the Grammar of Graphics (Wilkinson 2005) and how the {ggplot2} package works, while the second part focuses on the semantics of statistical graphics and provides an introduction to the many different types of data visualisations that can be created using the package.\nIn this chapter, you will learn how to:",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>The Gramma`R` of Graphics</span>"
    ]
  },
  {
    "objectID": "10_Dataviz.html#sec-Syntax",
    "href": "10_Dataviz.html#sec-Syntax",
    "title": "10¬† The GrammaR of Graphics",
    "section": "10.1 The syntax of graphics",
    "text": "10.1 The syntax of graphics\nThe syntax of the Grammar of Graphics (Wilkinson 2005) is made up of layers (Figure¬†10.2), which allow us to create highly effective and efficient data visualisations, while giving us lots of flexibility and control.\n\n\n\n\n\n\nFigure¬†10.2: The syntax of the Grammar of Graphics as visualised in the QCBS R Workshop Series (CC-BY-NC-SA)\n\n\n\nThe data layer and the aesthetics layer are compulsory as you cannot build a graph that does not map some data onto some visual aspect (= aesthetic) of a graph. The remaining layers are optional, but some are very important. In the following, we will explain how the geometries, facet, scales, coordinates, and theme layers are used to build and customise graphs using the {ggplot2} library in R.\n\n\n\n\n\nHex sticker of the {ggplot2} package\n\n\n\n10.1.1 Aesthetics\nAs explained in the documentation, the ggplot() function1 has two compulsory arguments. First, we must select the data that we want to visualise. Second, we must specify which variable(s) from the data should be mapped onto which visual property or aesthetics (short: aes) of the plot.\n\n\n\n\nggplot {ggplot2}\nR Documentation\n\n\n\nDescription\nggplot() initializes a ggplot object. It can be used to declare the input data frame for a graphic and to specify the set of plot aesthetics intended to be common throughout all subsequent layers unless specifically overridden.\nUsage\nggplot(data = NULL, mapping = aes(), ...)\n\nFor example, to create a bar plot visualising the distribution of participants‚Äô occupational groups in the combined dataset from DƒÖbrowska (2019) (Dabrowska.data), we need to map the OccupGroup variable from Dabrowska.data onto our plot‚Äôs x-axis (x).\n\nggplot(data = Dabrowska.data,\n       mapping = aes(x = OccupGroup))\n\n\n\n\n\n\n\nFigure¬†10.3: Attempt to plot the distribution of participants‚Äô occupational groups in DƒÖbrowska (2019)\n\n\n\n\n\nAs you can see from Figure¬†10.3, however, running this code returns an empty plot: All we get is a grid background and a nicely labelled x-axis, but no data‚Ä¶ Why might that be? ¬†:thinking-face:\n\n\n10.1.2 Geometries\nThe reason we are not seeing any data is that we have not yet specified with which kind of geometry (short: geom) we would like to plot the data. The {ggplot2} library features more than 30 different geom functions! They all begin with the prefix geom_. To create a bar plot showing participants‚Äô occupational groups, we need to add a geom_bar() layer to our empty ggplot object (see Figure¬†10.4).\n\nggplot(data = Dabrowska.data,\n       mapping = aes(x = OccupGroup)) +\n  geom_bar()\n\n\n\n\n\n\n\nFigure¬†10.4: Distribution of participants‚Äô occupational groups (C = clerical position, I = inactive (i.e.¬†unemployed, retired, or homemakers), M = manual jobs, PS = professional-level job or studying for a degree)\n\n\n\n\n\nNote that we use the + operator to add layers to ggplot objects. If we try to use the pipe operator (|&gt;) within the ggplot() function, we will get an error message.\n\nggplot(data = Dabrowska.data,\n       mapping = aes(x = OccupGroup)) |&gt; \n  geom_bar()\n\nError in `geom_bar()`: \n! `mapping` must be created by `aes()`. \n‚Ñπ Did you use `%&gt;%` or `|&gt;` instead of `+`? \nRun `rlang::last_trace()` to see where the error occurred.\n\n\n10.1.3 Statistics and labels\nWe now have a simple bar plot that represents the distribution of participants‚Äô occupational groups in DƒÖbrowska (2019). By default, the axis labels are simply the names of the variables that are mapped onto the plot‚Äôs aesthetics. That‚Äôs why, in Figure¬†10.4, our x-axis is labelled ‚ÄúOccupGroup‚Äù.\nWhat about the y-axis? We did not specify a y-aesthetic within the mapping argument of our ggplot() object, yet the y-axis is labelled ‚Äúcount‚Äù. This is because geom_bar() automatically computes a ‚Äúcount‚Äù statistic that gets mapped to the y-aesthetic.\nIf we want to change these axis labels, we can do so by adding a labs() layer to our plot (see Figure¬†10.5).\n\nggplot(data = Dabrowska.data,\n       mapping = aes(x = OccupGroup)) +\n  geom_bar() +\n  labs(x = \"Occupational group\",\n       y = \"Number of participants\")\n\n\n\n\n\n\n\nFigure¬†10.5: Distribution of participants‚Äô occupational groups (C = clerical position, I = inactive (i.e.¬†unemployed, retired, or homemakers), M = manual jobs, PS = professional-level job or studying for a degree)\n\n\n\n\n\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nQ10.1 Which of the following labels can be added or modified using the labs() function?\n\n\n\n\n\ny-axis label\n\n\n\n\nx-axis label\n\n\n\n\nplot title\n\n\n\n\nplot caption\n\n\n\n\nplot alt-text\n\n\n\n\nplot subtitle\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\n\n\n\n\n\n\n\n\n\nNoteWhat is alt-text and why is it important?\n\n\n\n\n\nAlternative text, or alt-text, is a concise description of an image used to make its informational content accessible to people with visual impairments. Using a screen reader programme, blind and low-vision readers can have the alt-text associated with an image read out to them.\nA good alt-text aims to convey the main message and insights of the graph, allowing someone who cannot see it to understand the information being presented (WAI) (2022). In the context of online publications, alt-text is also useful in regions with low bandwidth as images may take a very long time to load. By including alt-text, we can therefore make our work more accessible and inclusive, enabling more people to engage with and understand our data and analyses.\nSection 14.8 explains how to add alt-text to plots and figures in Quarto documents. Whilst many so-called ‚ÄúAI‚Äù tools will now automatically generate alt-text for us, it is best to write alt-text ourselves. This is because auto-generated alt-text often does not focus on the visual information that we want to convey, misses out on important aspects, and/or overwhelms the user with redundant information.\nBlind and low-vision readers may also want to check out the {BrailleR} package (Godfrey et al. 2025), which converts plots generated in R into a textual form that can be interpreted by blind and low-vision R users who cannot access the graphs without printing the image to a tactile embosser, or who need the extra text to support any tactile images that they do create.\n\n\n\n\n\n10.1.4 Data\nInstead of using the data argument of the ggplot() function as we did above, we can pipe the data into the function‚Äôs first argument (see Section 7.5.2). Compare these two methods and their outputs.\n\n\nUsing the data argument of ggplot()\n\nggplot(data = Dabrowska.data,\n       mapping = aes(x = OccupGroup))      +\n  geom_bar() +\n  labs(x = \"Occupational group\",\n       y = \"Number of participants\")\n\n\n\n\n\n\n\n\n\n\n\nPiping the data into ggplot()\n\nDabrowska.data |&gt; \n  ggplot(mapping = aes(x = OccupGroup)) +\n  geom_bar() +\n  labs(x = \"Occupational group\",\n       y = \"Number of participants\")\n\n\n\n\n\n\n\n\n\n\nThe outputs are exactly the same! Piping the dataset into the ggplot() function, however, allows us to easily wrangle the data that we want to visualise ‚Äòon the fly‚Äô, without transforming the data object itself. For example, we can use the tidyverse filter() function (see Section 9.7) to examine the distribution of occupational groups among L2 participants only (see Figure¬†10.6).\n\nDabrowska.data |&gt; \n  filter(Group == \"L2\") |&gt; \n  ggplot(mapping = aes(x = OccupGroup)) +\n  geom_bar() +\n  labs(x = \"Occupational group\",\n       y = \"Number of participants\",\n       title = \"Occupational groups of L2 participants\")\n\n\n\n\n\n\n\nFigure¬†10.6\n\n\n\n\n\nWe can also combine several filter() conditions using the & (AND) and | (OR) operators. For example, we may want to visualise the distribution of the occupational groups of participants who are L2 speakers of English and whose first language is Polish (see Figure¬†10.7).\n\nDabrowska.data |&gt; \n  filter(Group == \"L2\" & NativeLg == \"Polish\") |&gt; \n  ggplot(mapping = aes(x = OccupGroup)) +\n  geom_bar() +\n  labs(x = \"Occupational group\",\n       y = \"Number of participants\",\n       title = \"Occupational groups of Polish L2 participants\")\n\n\n\n\n\n\n\nFigure¬†10.7\n\n\n\n\n\n\n\n10.1.5 Facets\nIf we want to compare two subsets of the data, we can add a facet layer to subdivide the plot into several plots each representing a subset of the data. In the following, we use the facet_wrap() function to subdivide our bar plot by the Group variable (~ Group). This allows us to easily compare the distribution of occupations across L1 and the L2 participants (see Figure¬†10.8).\n\nDabrowska.data |&gt; \n  ggplot(mapping = aes(x = OccupGroup)) +\n  geom_bar() +\n  facet_wrap(~ Group) +\n  labs(x = \"Occupational group\",\n       y = \"Number of participants\",\n       title = \"Occupational groups of participants\")\n\n\n\n\n\n\n\nFigure¬†10.8\n\n\n\n\n\n\n\n\n\n\n\nWarningError message: ‚Äúseq.default(from, to, by) : invalid‚Äù\n\n\n\n\n\nIf you get the error message\nError in seq.default(from, to, by) : invalid '(to - from)/by' \nwhen trying to run a chunk of code that generates a plot, this is most likely due to your Plots pane in RStudio not being large enough to accommodate the plot. If you increase its size and rerun the chunk, your plot should appear in the Plots pane as expected.\nIf you have a small screen, you can also click on the ‚Äú:magnifying-glass-tilted-right: Zoom‚Äù button at the top of the Plots pane to view your plot in a separate RStudio window, which you can resize according to your needs.\n\n\n\nTo compare the distributions of occupations of the male and female L2 participants, we can combine a filter() operation to select only the L2 participants, with a facet_wrap() layer (see Figure¬†10.9).\n\nDabrowska.data |&gt; \n  filter(Group == \"L2\") |&gt; \n  ggplot(mapping = aes(x = OccupGroup)) +\n  geom_bar() +\n  facet_wrap(~ Gender) +\n  labs(x = \"Occupational group\",\n       y = \"Participants\",\n       title = \"Occupational groups of L2 participants\")\n\n\n\n\n\n\n\nFigure¬†10.9\n\n\n\n\n\nTo explore potential gender differences in occupational groups across both L1 and L2 groups, we can combine the two variables within the facet_wrap() function (see Figure¬†10.10).\n\nDabrowska.data |&gt; \n  ggplot(mapping = aes(x = OccupGroup)) +\n  geom_bar() +\n  facet_wrap(~ Group + Gender) +\n  labs(x = \"Occupational group\",\n       y = \"Participants\")\n\n\n\n\n\n\n\nFigure¬†10.10\n\n\n\n\n\n\n\n10.1.6 Scales\nScale layers allow us to map data values to the visual values of an aesthetic. For example, to make our facetted plot in Figure¬†10.10 easier to read, we could add some colour using a fill aesthetic to fill each bar with a colour that corresponds to the participants‚Äô gender. To do so, we map each unique value of the variable Gender (‚ÄúF‚Äù and ‚ÄúM‚Äù) onto a colour that is then used to fill the corresponding bars of our bar plot. Adding the fill aesthetic automatically generates a legend (see Figure¬†10.11).\n\nDabrowska.data |&gt; \n  ggplot(mapping = aes(x = OccupGroup, \n                       fill = Gender)) +\n  geom_bar() +\n  facet_wrap(~ Group + Gender) +\n  labs(x = \"Occupational group\",\n       y = \"Participants\")\n\n\n\n\n\n\n\nFigure¬†10.11\n\n\n\n\n\nAs we did not specify any fill colours for Figure¬†10.11, {ggplot2} used default colours taken from the {scales} package of the tidyverse environment. This is because, in the Grammar of Graphics, colour palettes are governed by scales. To specify a different set of colours, we therefore need to specify a scale layer.\nOne way to do this is to use scale_fill_manual() to manually pick our own colours, either using R colour codes (such as purple) or hexadecimal colour codes (such as #34027d). Note that both types of colour codes must be enclosed in quotation marks.\n\nDabrowska.data |&gt; \n  ggplot(mapping = aes(x = OccupGroup, \n                       fill = Gender)) +\n  geom_bar() +\n  facet_wrap(~ Group + Gender) +\n  labs(x = \"Occupational group\",\n       y = \"Participants\") +\n  scale_fill_manual(values = c(\"purple\", \"#34027d\"))\n\n\n\n\n\n\n\nFigure¬†10.12: A facetted bar plot with hand-picked colours\n\n\n\n\n\nAlthough it makes the plot easier to interpret, the colour aesthetic (here fill) is not strictly necessary to understand the data represented in Figure¬†10.12. After all, the two gender subgroups are already distinguished by the facet_wrap() layer. That‚Äôs not necessarily a bad thing, but you must consider whether such redundant elements facilitate the interpretation of the data visualised or not.\nIn some cases, colour is used as the only way of identifying subgroups in the data, for example in a stacked bar plot (see Figure¬†10.13). In such cases, it is important to consider how the plot will be perceived by different people (see note on colour blindness below).\n\nDabrowska.data |&gt; \n  ggplot(mapping = aes(x = OccupGroup, \n                       fill = Gender)) +\n  geom_bar() +\n  facet_wrap(~ Group) +\n  labs(x = \"Occupational group\",\n       y = \"Participants\") +\n  scale_fill_manual(values = c(\"purple\", \"#34027d\"))\n\n\n\n\n\n\n\nFigure¬†10.13: A stacked bar plot with hand-picked colours\n\n\n\n\n\nScale layers can be used to control the axes of your plots. Play around with the ‚Äúexpand‚Äù and ‚Äúlimits‚Äù arguments of the scale_y_continuous() function to understand how they work.\n\nDabrowska.data |&gt; \n  ggplot(mapping = aes(x = OccupGroup)) +\n  geom_bar() +\n  labs(x = \"Occupational group\",\n       y = \"Participants\") +\n  scale_y_continuous(expand = c(0,0),\n                     limits = c(0,60))  \n\n\n\n\n\n\n\nFigure¬†10.14: A bar plot with a y-axis that starts at zero.\n\n\n\n\n\n\n\n\n\n\n\nNoteA note on colours and colour blindness üåà\n\n\n\n\n\nColour blindness is a condition that results in a decreased ability to see colours and perceive differences in colour. There are different types of colour blindness but, in general, it is best to avoid red-green contrasts. To ensure that your data visualisations are accessible to as many people as possible, you may want to use the {colorBlindness} package (Ou 2021) to simulate the appearance of a set of colours for people with different forms of colour blindness.\n\n#install.packages(\"colorBlindness\")\nlibrary(colorBlindness)\n\ncolorBlindness::displayAllColors(scales::hue_pal()(6))\n\n\n\n\n\n\n\n\nUsing the {colorBlindness} package, we can immediately see that the default {scales} discrete palette that {ggplot2} used in Figure¬†10.11 is not accessible to colour blind people (deuteranope and protanope), nor is it distinguishable when printed in grey-scale (desaturate). In contrast, our hand-picked colours from Figure¬†10.12 fare much better.\n\ncolorBlindness::displayAllColors(c(\"pink\", \"#34027d\"))\n\n\n\n\n\n\n\n\nBut you need not manually pick colours, as many people have developed and shared R packages that feature attractive, ready-to-use colour-blind friendly palettes. The {viridis} package (Garnier et al. 2023), for example, includes eight such palettes (‚Äúmagma‚Äù, ‚Äúinferno‚Äù, ‚Äúplasma‚Äù, ‚Äúcividis‚Äù, ‚Äúrocket‚Äù, ‚Äúturbo‚Äù) that also reproduce well in grey-scale. And, as it is included in the {ggplot2} installation, you don‚Äôt even need to install the {viridis} package separately!\n\ncolorBlindness::displayAllColors(viridis::viridis(6))\n\n\n\n\n\n\n\n\nChoosing an appropriate palette is not the only way to make your visualisations accessible to colour-blind readers. Another way is to provide redundant mappings to other aesthetics such as size, line type, shape, or pattern. Finally, it is important to remember that colour blindness is by no means the only type of visual impairment you should consider when creating visualisations. Worldwide, far more people are affected by blindness and low vision. Section 14.8 explains how to add alternative texts (alt-text) to plots and images. Many people with visual impairments rely on screen readers that use these alternative texts to provide audio descriptions of images and plots. These alternative texts can also improve the experience of users facing internet connection issues resulting in images that do not load properly or quickly enough.\n\n\n\nSome academic publishers still require grey-scale plots, in which case you will want to use the scale layer scale_fill_grey(). Alternatively, the colour palettes of the {viridis} package (see information box on colour blindness) render well in grey, too. The {viridis} function for a discrete colour scale (as needed for a categorical variable such as Gender) can be called up using the scale_fill_viridis_d() function. With the ‚Äúoption‚Äù argument, you can switch between eight different viridis palettes (‚Äúmagma‚Äù, ‚Äúinferno‚Äù, ‚Äúplasma‚Äù, ‚Äúcividis‚Äù, ‚ÄúüöÄ‚Äù, ‚Äúturbo‚Äù).\n\nscale_fill_grey()scale_fill_viridis_dscale_fill_viridis_d(option = ‚Äúturbo‚Äù)\n\n\n\nDabrowska.data |&gt; \n  ggplot(mapping = aes(x = OccupGroup, fill = Gender)) +\n  geom_bar() +\n  facet_wrap(~ Group) +\n  labs(x = \"Occupational group\",\n       y = \"Participants\") +\n  scale_y_continuous(expand = c(0,0),\n                     limits = c(0,35)) +\n  scale_fill_grey()\n\n\n\n\n\n\n\nFigure¬†10.15\n\n\n\n\n\n\n\n\nDabrowska.data |&gt; \n  ggplot(mapping = aes(x = OccupGroup, fill = Gender)) +\n  geom_bar() +\n  facet_wrap(~ Group) +\n  labs(x = \"Occupational group\",\n       y = \"Participants\") +\n  scale_y_continuous(expand = c(0,0),\n                     limits = c(0,35)) +\n  scale_fill_viridis_d(option = \"viridis\")\n\n\n\n\n\n\n\nFigure¬†10.16\n\n\n\n\n\n\n\n\nDabrowska.data |&gt; \n  ggplot(mapping = aes(x = OccupGroup, fill = Gender)) +\n  geom_bar() +\n  facet_wrap(~ Group) +\n  labs(x = \"Occupational group\",\n       y = \"Participants\") +\n  scale_y_continuous(expand = c(0,0),\n                     limits = c(0,35)) +\n  scale_fill_viridis_d(option = \"turbo\")\n\n\n\n\n\n\n\nFigure¬†10.17\n\n\n\n\n\n\n\n\nIf you like colours, check out the {paletteer} package (Hvitfeldt 2021), which provides a neat interface to access a very large collection of R colour packages, some of which are very fun! The advantage is that you only need to install one package (install.packages(\"paletteer\")) to have a huge range of palettes at your disposal. Below is a small selection of some personal favourites.\n\nbeyonce::X11BridgetRileyjanelleFridaKahlokissspeakNow\n\n\n\nDabrowska.data |&gt; \n  ggplot(mapping = aes(x = OccupGroup, fill = OccupGroup)) +\n  geom_bar() +\n  labs(x = \"Occupational group\",\n       y = \"Participants\") +\n  scale_y_continuous(expand = c(0,0),\n                     limits = c(0,60)) +\n  paletteer::scale_fill_paletteer_d(\"beyonce::X11\")\n\n\n\n\n\n\n\nFigure¬†10.18\n\n\n\n\n\n\n\n\nDabrowska.data |&gt; \n  ggplot(mapping = aes(x = OccupGroup, fill = OccupGroup)) +\n  geom_bar() +\n  labs(x = \"Occupational group\",\n       y = \"Participants\") +\n  scale_y_continuous(expand = c(0,0),\n                     limits = c(0,60)) + \n  paletteer::scale_fill_paletteer_d(\"lisa::BridgetRiley\", direction = -1)\n\n\n\n\n\n\n\nFigure¬†10.19\n\n\n\n\n\n\n\n\nDabrowska.data |&gt; \n  ggplot(mapping = aes(x = OccupGroup, fill = OccupGroup)) +\n  geom_bar() +\n  labs(x = \"Occupational group\",\n       y = \"Participants\") +\n  scale_y_continuous(expand = c(0,0),\n                     limits = c(0,60)) +\n  paletteer::scale_fill_paletteer_d(\"rockthemes::janelle\")\n\n\n\n\n\n\n\nFigure¬†10.20\n\n\n\n\n\n\n\n\nDabrowska.data |&gt; \n  ggplot(mapping = aes(x = OccupGroup, fill = OccupGroup)) +\n  geom_bar() +\n  labs(x = \"Occupational group\",\n       y = \"Participants\") +\n  scale_y_continuous(expand = c(0,0),\n                     limits = c(0,60)) +\n  paletteer::scale_fill_paletteer_d(\"lisa::FridaKahlo\", direction = -1)\n\n\n\n\n\n\n\nFigure¬†10.21\n\n\n\n\n\n\n\n\nDabrowska.data |&gt; \n  ggplot(mapping = aes(x = OccupGroup, fill = OccupGroup)) +\n  geom_bar() +\n  labs(x = \"Occupational group\",\n       y = \"Participants\") +\n  scale_y_continuous(expand = c(0,0),\n                     limits = c(0,60)) +\n  paletteer::scale_fill_paletteer_d(\"ltc::kiss\")\n\n\n\n\n\n\n\nFigure¬†10.22\n\n\n\n\n\n\n\n\nDabrowska.data |&gt; \n  ggplot(mapping = aes(x = OccupGroup, fill = OccupGroup)) +\n  geom_bar() +\n  labs(x = \"Occupational group\",\n       y = \"Participants\") +\n  scale_y_continuous(expand = c(0,0),\n                     limits = c(0,60)) +\n  paletteer::scale_fill_paletteer_d(\"tayloRswift::speakNow\")\n\n\n\n\n\n\n\nFigure¬†10.23\n\n\n\n\n\n\n\n\n\n\n10.1.7 Themes\nThe {ggplot2} framework also allows for the addition of an optional theme() layer to further customise the look of plots. The default {ggplot2} theme is theme_grey(). Here are some of the pre-built themes that come with the {ggplot2} library for you to compare. As with colour palettes, you can install additional libraries that will give you access to literally hundreds of ready-made themes for you to explore.\n\ntheme_bw()theme_dark()theme_light()theme_minimal()theme_void()\n\n\n\nggplot(data = Dabrowska.data,\n       mapping = aes(x = OccupGroup)) +\n  geom_bar() +\n  labs(x = \"Occupational group\",\n       y = \"Number of participants\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = Dabrowska.data,\n       mapping = aes(x = OccupGroup)) +\n  geom_bar() +\n  labs(x = \"Occupational group\",\n       y = \"Number of participants\") +\n  theme_dark()\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = Dabrowska.data,\n       mapping = aes(x = OccupGroup)) +\n  geom_bar() +\n  labs(x = \"Occupational group\",\n       y = \"Number of participants\") +\n  theme_light()\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = Dabrowska.data,\n       mapping = aes(x = OccupGroup)) +\n  geom_bar() +\n  labs(x = \"Occupational group\",\n       y = \"Number of participants\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = Dabrowska.data,\n       mapping = aes(x = OccupGroup)) +\n  geom_bar() +\n  labs(x = \"Occupational group\",\n       y = \"Number of participants\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\nPretty much all aspects of plot themes can be customised. To demonstrate this, Figure¬†10.25 displays a bar plot with some highly customised aesthetics. I will let you judge how meaningful these custom choices are and whether they genuinely help the reader to interpret the data‚Ä¶ :face-with-raised-eyebrow:\n\n\nSee {ggplot2} code to generate plot.\nggplot(data = Dabrowska.data,\n       mapping = aes(x = OccupGroup, fill = Gender)) +\n  geom_bar() +\n  labs(x = \"Occupational group\",\n       y = \"Number of participants\",\n        title = \"An example of an extravagantly customised ggplot...\") +\n    theme(\n      panel.background = element_rect(fill = \"#FFC080\", color = NA),\n      panel.grid.major = element_line(color = \"gold\", linewidth = 1.5),\n      panel.grid.minor = element_line(color = \"grey20\", linewidth = 0.5),\n      axis.title.x = element_text(face = \"bold\", size = 12, color = \"brown\", angle = 10),\n      axis.title.y = element_text(size = 25, color = \"green\", family = \"Courier New\"),\n      axis.text.x = element_text(face = \"italic\", size = 12, color = \"cyan\"),\n      axis.text.y = element_text(size = 14, color = \"grey\"),\n      plot.title = element_text(face = \"bold\", size = 10, color = \"purple\", family = \"Comic Sans MS\")\n      )\n\n\n\n\n\n\n\n\nFigure¬†10.24\n\n\n\n\n\n\n\n10.1.8 Coordinates\nBy default, the coordinate system that is used in ggplot objects is the Cartesian coordinate system, which has a horizontal axis (x) and a vertical axis (y) that are perpendicular to each other. To change this default Cartesian coordinate system, we need to add a coordinate layer.\nFor example, if we want to display the full names of the four occupational groups used in DƒÖbrowska (2019), we can change the labels of the categories using mutate() and fct_recode() before pipping the data into ggplot() (see Section 10.1.4) and then flip the x and y axes using the coordinate layer coord_flip(). As shown in Figure¬†10.26, this makes long labels much easier to read.\n\nDabrowska.data |&gt; \n  mutate(OccupGroup = fct_recode(OccupGroup,\n                                 `Professionally inactive` = \"I\",\n                                 `Clerical profession` = \"C\",\n                                 `Manual profession` = \"M\",\n                                 `Professional-level job/\\nstudent` = \"PS\")) |&gt; \n  mutate(Gender = fct_rev(Gender)) |&gt; \n  ggplot(mapping = aes(x = OccupGroup, fill = Gender)) +\n  geom_bar() +\n  labs(x = NULL,\n       y = \"Participants\") +\n  scale_fill_viridis_d() +\n  coord_flip() +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 12))\n\n\n\n\n\n\n\nFigure¬†10.26\n\n\n\n\n\nThe vast majority of statistical graphs use the Cartesian coordinate system. Pie charts and other circular plots, however, use the polar coordinate system (coord_polar), whereby quantities are mapped onto angles rather than distances. In general, humans are much better at judging lengths than angles or areas (Cleveland & McGill 1987), which is why circular graphs such as pie charts are typically not recommended forms of good data visualisations (see, e.g. Few). That said, they can be produced using the {ggplot2} library by adding the coordinate layer coord_polar(\"y\") and modifying a few parameters.\n\nDabrowska.data |&gt; \n  mutate(OccupGroup = fct_recode(OccupGroup,\n                                 `Professionally inactive` = \"I\",\n                                 `Clerical profession` = \"C\",\n                                 `Manual profession` = \"M\",\n                                 `Professional-level job/\\nstudent` = \"PS\")) |&gt; \n  ggplot(mapping = aes(x = \"\", fill = OccupGroup)) +\n  geom_bar(width = 1) +\n  labs(fill = \"Occupational group\") +\n  scale_fill_viridis_d(direction = -1) +\n  coord_polar(\"y\") +\n  theme_void()\n\n\n\n\n\n\n\nFigure¬†10.27",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>The Gramma`R` of Graphics</span>"
    ]
  },
  {
    "objectID": "10_Dataviz.html#sec-Semantics",
    "href": "10_Dataviz.html#sec-Semantics",
    "title": "10¬† The GrammaR of Graphics",
    "section": "10.2 The semantics of graphics",
    "text": "10.2 The semantics of graphics\nSo far, we have seen how the syntax of the Grammar of Graphics can be used to build statistical graphs layer by layer. We now turn to the semantics of graphics. As linguists are well placed to know, semantics is the study of meaning. In the Grammar of Graphics, the semantics of graphics is defined as ‚Äúthe meanings of the representative symbols and arrangements we use to display information‚Äù (Wilkinson 2005: 20). In what follows, we will see how thinking about the semantics of graphics can help us to think about how the different components of a graph interact to convey insightful visual information from raw data. This will help us to make informed choices when choosing the geometries, scales, facets, and themes of our data visualisations.\nBut, first, let‚Äôs think about why we visualise data. Data visualisation is about more than just communicating the results of our analyses to others at the publication stage. In fact, good data visualisation can help us make informed decisions throughout the research process from the data wrangling stage to the evaluation of complex statistical models. Here are some reasons for visualising data. Can you think of others? :thinking-face:\n\n\nFor yourself\n\nTo explore your data\nTo detect data processing errors and outliers\nTo check assumptions of statistical tests or models (see Section 11.6 and Section 12.4)\nTo examine variation across different subsets of the data\nTo better interpret the results of statistical tests (see Chapter 11) and models (see Chapter 12 and 13)\n\n\n\n\nFor others\n\nTo communicate the results of your analyses more effectively\nTo communicate about your data (in more detail)\nTo communicate complex information more efficiently\nTo attract the reader‚Äôs attention\nTo allow the reader to reach their own conclusions\n\n\n\n\n\n\n\n\n\nFigure¬†10.28: Using the {ggplot2} package for data exploration (artwork by @allison_horst)\n\n\n\nDepending on the type of data that we want to visualise and why, we can choose different types of plots. A great resource to choose a graphic that is suitable for your data is the R Graph Gallery.\nIn the following, we will first look at how we can plot categorical variables and discrete numeric variables, before we move on to visualising continuous numeric variables and combinations of different types of variables (see Section 7.2).\n\n10.2.1 Bar plots\nAs we saw in Section 10.1, bar plots (also called bar charts) are a great way to visualise categorical variables. We also saw that, when using horizontal writing systems, it is often easier to interpret a bar plot if its coordinates are flipped so that longer labels can be read more readily.\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nQ10.2 Study Figure¬†10.29 and think about which {ggplot2} functions were used to generate it. :thinking-face:\nThen, click on the ‚ÄúShow R code‚Äù button just above the figure to compare your intuitions with the actual code. Note that there may well be more than one solution, so do try out your version and see if you can spot any differences!\n\n\n\n\n\nShow R code.\nDabrowska.data |&gt; \n  filter(Group == \"L2\") |&gt; \n  mutate(NativeLg = fct_rev(fct_infreq(NativeLg))) |&gt; \n  ggplot(aes(x = NativeLg, \n           fill = NativeLgFamily)) +\n  geom_bar() +\n  coord_flip() +\n  scale_fill_viridis_d(option = \"F\") +\n  scale_y_continuous(limits = c(0, 40)) +\n  theme_minimal() +\n  labs(x = NULL, \n       y = NULL, \n       fill = \"Language family\",\n       title = \"Native languages of L2 participants\") +\n  theme_minimal(base_size = 16)\n\n\n\n\n\n\n\n\nFigure¬†10.29\n\n\n\n\n\nTo create Figure¬†10.29, we first reordered the factor levels of the NativeLg variable using two functions from the {forcats} package (see Section 9.3.2): fct_infreq() is first used to order the factors according to their frequency (by default, they are sorted alphabetically), and then fct_rev() is used to reverse that order. The latter step is needed because the coord_flip() functions reverses everything. You can check the order of a factor‚Äôs level using the function levels(). Note that, if two levels have the same number of occurrences, they are ordered alphabetically (as seen in Figure¬†10.29).\n\nlevels(Dabrowska.data$NativeLg)\n\n [1] \"Cantonese\"  \"Chinese\"    \"French\"     \"German\"     \"Greek\"     \n [6] \"Italian\"    \"Lithuanian\" \"Mandarin\"   \"Polish\"     \"Russian\"   \n[11] \"Spanish\"   \n\nlevels(fct_infreq(Dabrowska.data$NativeLg))\n\n [1] \"Polish\"     \"Mandarin\"   \"Lithuanian\" \"Cantonese\"  \"Chinese\"   \n [6] \"Spanish\"    \"French\"     \"Russian\"    \"German\"     \"Greek\"     \n[11] \"Italian\"   \n\nlevels(fct_rev(fct_infreq(Dabrowska.data$NativeLg)))\n\n [1] \"Italian\"    \"Greek\"      \"German\"     \"Russian\"    \"French\"    \n [6] \"Spanish\"    \"Chinese\"    \"Cantonese\"  \"Lithuanian\" \"Mandarin\"  \n[11] \"Polish\"    \n\n\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nQ10.3 Compare the two plots below. Which one makes it easier to see which occupational group has the fewest participants and why?\n\n\nBar plot\n\n\nShow R code for the bar plot\nDabrowska.data |&gt; \n  mutate(OccupGroup = fct_recode(OccupGroup,\n                                 `Professionally inactive` = \"I\",\n                                 `Clerical profession` = \"C\",\n                                 `Manual profession` = \"M\",\n                                 `Professional-level job/\\nstudent` = \"PS\")) |&gt; \n  filter(Gender == \"M\") |&gt; \n  ggplot(mapping = aes(x = OccupGroup, \n                       fill = OccupGroup)) +\n  geom_bar() +\n  labs(x = NULL,\n       y = \"Participants\") +\n  scale_fill_viridis_d() +\n  coord_flip() +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 15),\n        legend.position = \"none\")\n\n\n\n\n\n\n\n\nFigure¬†10.30\n\n\n\n\n\n\n\n\nPie chart\n\n\nShow R code for the pie chart\nDabrowska.data |&gt; \n  mutate(OccupGroup = fct_recode(OccupGroup,\n                                 `Professionally inactive` = \"I\",\n                                 `Clerical profession` = \"C\",\n                                 `Manual profession` = \"M\",\n                                 `Professional-level job/\\nstudent` = \"PS\")) |&gt; \n  filter(Gender == \"M\") |&gt; \n  ggplot(mapping = aes(x = \"\", fill = OccupGroup)) +\n    geom_bar(width = 1) +\n    scale_fill_viridis_d(direction = -1) +\n    coord_polar(\"y\") +\n    theme_void(base_size = 20) # This increases the font size.\n\n\n\n\n\n\n\n\nFigure¬†10.31\n\n\n\n\n\n\n\n\n\n\n\n\nThe pie chart because the colours are easier to distinguish when they are close to each other.\n\n\n\n\nThe bar plot because humans dislike round objects and prefer objects with sharp angles.\n\n\n\n\nThe bar plot because humans are better at distinguishing small differences in lengths than in angles.\n\n\n\n\nThe bar plot because the labels are immediately next to the corresponding bars.\n\n\n\n\nThe pie chart as most humans learn to read clocks early on in life.\n\n\n\n\n\n\n\n\nü¶â Hover over the owl for a first hint.\n\n\n\n\nüê≠ Click on the mouse for a second hint.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nUsing the {ggplot2} library, create a bar plot that shows the distribution of occupational groups (OccupGroup) among male L1 and L2 participants in DƒÖbrowska (2019)‚Äôs study.\nQ10.4 Drawing on the information provided by your bar plot, how many male participants reported having manual jobs?\n\n\n\n\n\nunder 10\n\n\n\n\nexactly 12\n\n\n\n\nexactly 20\n\n\n\n\nexactly 30\n\n\n\n\na little more than 40\n\n\n\n\n\n\n\n\n\n\nShow sample code to answer Q10.4\nDabrowska.data |&gt; \n  filter(Gender == \"M\") |&gt; \n  ggplot(mapping = aes(x = OccupGroup)) +\n  geom_bar() +\n  labs(x = \"Occupational group\", \n       y = \"Male participants\") +\n  theme_minimal()\n\n\nQ10.5 Is the legend in the bar plot that you have created necessary?\n\n\n\n\n\nNo, it only adds clutter.\n\n\n\n\nYes, it makes the plot easier to interpret.\n\n\n\n\nNot really, but it makes the plot look more professional.\n\n\n\n\nNot really, but it's impossible to remove the legend using the {ggplot2} library.\n\n\n\n\n\n\n\n\n\n\nSee code.\nDabrowska.data |&gt; \n  filter(Gender == \"M\") |&gt; \n  ggplot(mapping = aes(x = OccupGroup, \n                       fill = OccupGroup)) +\n  geom_bar() +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\nQ10.6 Create a pie chart that shows the distribution of occupational groups among male participants (as shown in Figure¬†10.27). Which line of code is essential to create a pie chart using {ggplot2}?\n\n\n\n\n\ncoord_polar(\"y\")\n\n\n\n\ntheme_void()\n\n\n\n\ncoord(\"polar\")\n\n\n\n\ngeom_piechart()\n\n\n\n\ngeom_circle()\n\n\n\n\n\n\n\n\n\n\nShow code to create pie chart below.\nDabrowska.data |&gt; \n  filter(Gender == \"M\") |&gt; \n  ggplot(mapping = aes(x = \"\", \n                       fill = OccupGroup)) +\n    geom_bar(width = 1) +\n    coord_polar(\"y\") +\n    theme_void()\n\n\n\n\n\n\n\n\n\nQ10.7 Transform the pie chart that you just created in c. to make it look like Figure¬†10.32. To achieve this, wrangle the data before piping it into the data argument of the ggplot() function. Which {tidyverse} function can you use to rename the labels?\n\n\n\n\n\nfct_relevel()\n\n\n\n\nrename()\n\n\n\n\nlabs()\n\n\n\n\nfct_recode()\n\n\n\n\nfct_mutate()\n\n\n\n\n\n\n\n\n\n\nShow sample code to answer Q10.7\nDabrowska.data |&gt; \n  mutate(`OccupGroup` = fct_recode(OccupGroup,\n                                 `Professionally inactive` = \"I\",\n                                 `Clerical profession` = \"C\",\n                                 `Manual profession` = \"M\",\n                                 `Professional-level job` = \"PS\")) |&gt; \n  filter(Gender == \"M\") |&gt; \n  ggplot(mapping = aes(x = \"\", \n                       fill = OccupGroup)) +\n  geom_bar(width = 1) +\n  coord_polar(\"y\") +\n  theme_void() +\n  labs(fill = \"Occupational group\") # This last line of code changes the title of the legend, which is the label for the variable associated with the `fill` aestetics. \n\n\n\n\n\n\n\n\nFigure¬†10.32\n\n\n\n\n\n¬†\n\n\n\n\n\n10.2.2 Histograms\nIn Section 8.2.2, we visually examined the distribution of participants‚Äô ages in a bar plot. This was possible because the age variable in DƒÖbrowska (2019) was recorded as a discrete numeric variable (i.e.¬†either as 18 or 19, but not 18.4 years of age).\n\nggplot(data = Dabrowska.data,\n       mapping = aes(Age)) +\n  geom_bar() +\n  scale_x_continuous() +\n  theme_minimal()\n\n\n\n\n\n\n\nFigure¬†10.33\n\n\n\n\n\nBar plots are best suited for categorical data and should only be used to visualise discrete numeric variables that have a fairly limited number of possible values. As we can see from the output of the unique() function, this is not the case for the Age variable in Dabrowska.data, as it includes 40 different age values, ranging from 17 to 65.\n\nunique(Dabrowska.data$Age) |&gt; \n  sort()\n\n [1] 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 37 38 39 40 41 42\n[26] 44 46 47 48 51 52 53 55 57 58 59 60 61 62 65\n\n\nThe distribution of participants‚Äô ages is therefore better visualised as a histogram or density plot. To visualise participants‚Äô age as histogram (@fig-AgeHistogram) rather than as a bar plot (@fig-AgeBarPlot), change the plot geometry (see Section 10.1.2) from geom_bar() to geom_histogram().\n\nggplot(data = Dabrowska.data,\n       mapping = aes(x = Age)) +\n  geom_histogram() +\n  labs(x = \"Age (in years)\",\n       y = \"Number of participants\") +\n  theme_minimal()\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\nFigure¬†10.34\n\n\n\n\n\nWhen generating this histogram, a message appears in the R console that informs us that, by default, the geom_histogram() function subdivided the Age values into 30 bins. This means that the age range from 17 to 65 has been subdivided into 30 groups of equal size. Given that there is a range of 48 in the Age values in this dataset, this is not a great way to subdivide the values of this variable.\nAs indicated in the message, to change this behaviour, we can adjust the value of the ‚Äúbinwidth‚Äù argument. This argument determines how many years go in each bin. So if we choose to have two years in each subdivision of the Age variable, we will end up with 24 bins. Logically, if we decide to group four years in each subdivision of the Age variable, we will end up with just 12 bins.\nCompare the three histograms below. In your opinion, which binwidth provides the most effective way to visualise the distribution of participants‚Äô ages? :thinking-face:\n\ndefault bin widthbinwidth = 2binwidth = 4\n\n\n\nggplot(data = Dabrowska.data,\n       mapping = aes(x = Age)) +\n  geom_histogram() +\n  labs(x = \"Age (in years)\",\n       y = \"Number of participants\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = Dabrowska.data,\n       mapping = aes(x = Age)) +\n  geom_histogram(binwidth = 2) +\n  labs(x = \"Age (in years)\",\n       y = \"Number of participants\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = Dabrowska.data,\n       mapping = aes(x = Age)) +\n  geom_histogram(binwidth = 4) +\n  labs(x = \"Age (in years)\",\n       y = \"Number of participants\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nThe distribution of two continuous variables can also be compared by superimposing two histograms as in @fig-OverlappingHisto. This requires the addition of a fill aesthetics (see Section 10.1.6) and the argument position = \"identity\". For both distributions to be visible, it is also necessary to add some transparency to the fill colour of the bars. This is achieved using the alpha argument of the geom_histogram() function. An alpha value of 0 corresponds to full transparency (e.g.¬†no colour), whilst an alpha value of 1 corresponds to complete opacity.\n\nggplot(data = Dabrowska.data,\n       mapping = aes(x = Age,\n                     fill = Group)) + \n  geom_histogram(binwidth = 5,\n                 position = \"identity\",\n                 alpha = 0.5) +\n  scale_fill_viridis_d() +\n  labs(x = \"Age (in years)\",\n       y = \"Number of participants\") +\n  theme_minimal()\n\n\n\n\n\n\n\nFigure¬†10.35\n\n\n\n\n\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nQ10.8 How many different scores did participants obtain on the English grammar test (as stored in the Grammar variable).\n\n\n\n\n\n\n\n\n\n\n\n\nShow sample code to help you answer Q10.8.\nunique(Dabrowska.data$Grammar) |&gt; \n  length()\n\n\nQ10.9 What are the lowest Grammar scores among L1 and L2 participants?\n\n\n\n\n\n45 points among L1 and zero points among L2 participants\n\n\n\n\n58 points among L1 and 40 points among L2 participants\n\n\n\n\n17 points among L1 and 20 points among L2 participants\n\n\n\n\n8.89 points among L1 and -13 points among L2 participants\n\n\n\n\n\n\n\n\n\n\nShow sample code to help you answer Q10.9.\nDabrowska.data |&gt; \n  group_by(Group) |&gt; \n  summarise(lowest = min(Grammar))\n\n\nQ10.10 Create a histogram of participants‚Äô Grammar scores. Which geometrical parameters need to be used to obtain exactly the same histogram as below?\n\n\n\n\n\ngeom_histogram()\n\n\n\n\ngeom_histogram(binwidth = 2)\n\n\n\n\ngeom_histogram(binwidth = 4)\n\n\n\n\ngeom_histogram(binwidth = 6)\n\n\n\n\ngeom_histogram(binwidth = 8)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow answer to Q10.10\nggplot(data = Dabrowska.data,\n       mapping = aes(x = Grammar)) +\n  geom_histogram(binwidth = 6) +\n  labs(x = \"Scores on English grammar test\",\n       y = \"Number of participants\") +\n  theme_bw()\n\n\nQ10.11 Without trying out the code for yourself, which script was used to generate Plot 1 below?\n\n\n\n\n\nScript A\n\n\n\n\nScript B\n\n\n\n\nScript C\n\n\n\n\nScript D\n\n\n\n\n\n\n\n\nPlot 1\n\n\n\n\n\n\n\n\n\n\n\nScript A\n\nggplot(data = Dabrowska.data,\n       mapping = aes(x = Grammar,\n                     fill = Group)) +\n  geom_histogram(binwidth = 6,\n                 alpha = 0.6) +\n  labs(x = \"Scores on English grammar test\",\n       y = \"Number of participants\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\nScript B\n\nggplot(data = Dabrowska.data,\n       mapping = aes(x = Grammar,\n                     fill = Group)) +\n  geom_bar(alpha = 0.6) +\n  labs(x = \"Scores on English grammar test\",\n       y = \"Number of participants\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\nScript C\n\nggplot(data = Dabrowska.data,\n       mapping = aes(x = Grammar,\n                     colour = Group)) +\n  geom_histogram(binwidth = 6) +\n  labs(x = \"Scores on English grammar test\",\n       y = \"Number of participants\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\nScript D\n\nggplot(data = Dabrowska.data,\n       mapping = aes(x = Grammar,\n                     fill = Group)) +\n  geom_histogram(binwidth = 6,\n                 alpha = 0.6) +\n  facet_wrap(~ Group) +\n  labs(x = \"Scores on English grammar test\",\n       y = \"Number of participants\") +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\nQ10.12 Without trying out the code for yourself, which script was used to generate Plot 2 below?\n\n\n\n\nScript A\n\n\n\n\nScript B\n\n\n\n\nScript C\n\n\n\n\nScript D\n\n\n\n\n\n\n\nPlot 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.2.3 Density plots\nAn alternative to displaying the data in discrete bins is to apply a density function to smooth over the bins of the histogram. This is what we call a density plot. Figure¬†10.36 is a density plot of participants‚Äô grammar test scores.\nCreate density plots in R using {ggplot2} is very simple. Because, yes, you‚Äôve guessed it: there‚Äôs a geom_ function for density plots and it‚Äôs called‚Ä¶ geom_density()! üòÉ\n\nggplot(data = Dabrowska.data,\n       mapping = aes(x = Grammar)) +\n  geom_density(fill = \"purple\") +\n  labs(x = \"Scores on English grammar test\") +\n  theme_minimal()\n\n\n\n\n\n\n\nFigure¬†10.36: Distribution of English vocabulary test results\n\n\n\n\n\nDensity plots are particularly useful to examine distribution shapes visually. Looking at Figure¬†10.36, we can immediately see that the values of the Grammar variable are not normally distributed (see Section 8.2.2).\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nQ10.13 Which line of code needs to be added to the code used to generate Figure¬†10.36 to produce a two-panel density plot like Figure¬†10.37?\n\n\n\n\n\nfacets(~ \"Group\")\n\n\n\n\nfct_wrap(\"Group\")\n\n\n\n\nfacet_wrap(Group)\n\n\n\n\nfacet_wrap(~ Group)\n\n\n\n\ngeom_grid(Group, facet = 2)\n\n\n\n\n\n\n\n\n\n\nShow sample code to help you answer Q10.13.\nggplot(data = Dabrowska.data,\n       mapping = aes(x = Grammar)) +\n  geom_density(fill = \"purple\") +\n  facet_wrap(~ Group) +\n  labs(x = \"Scores on English grammar test\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nFigure¬†10.37\n\n\n\n\n\nQ10.14 Create four density plots to visualise the distribution of participants‚Äô ART (Author Recognition Test), Blocks (non-verbal IQ test), Colloc (English collocation test), and Vocab (English vocabularly test) scores.\nWhich variable‚Äôs distribution is closest to a normal distribution?\n\n\n\n\n\nART\n\n\n\n\nBlocks\n\n\n\n\nColloc\n\n\n\n\nVocab\n\n\n\n\nART and Vocab\n\n\n\n\n\n\n\n\n\n\nShow sample code to help you answer Q10.13.\nDabrowska.data |&gt; \n  select(Vocab, Colloc, ART, Blocks) |&gt;  \n  tidyr::gather() |&gt;  # This function from tidyr converts a selection of variables into two variables: a key and a value. The key contains the names of the original variable and the value the data. This means we can then use the facet_wrap function from ggplot2\n  ggplot(aes(value)) +\n    theme_bw() +\n    facet_wrap(~ key, scales = \"free\", ncol = 2) +\n    scale_x_continuous(expand=c(0,0)) +\n    geom_density(fill = \"purple\")\n\n\n\n\n\n\n\n\n\n\n\nNoteGoing further: Setting properties within geoms\n\n\n\n\n\nYou can also change the attributes of any geometry layer by specifying them as arguments within their geom_ function.\nThe help file of each geom_ function provides a list of the aesthetics arguments that each function has (see below for relevant extract). If we do not specify any of the optional aesthetics of the geom_ functions, sensible default values will be used. For instance, the line colour of density plots will be black, unless otherwise specified with the argument ‚Äúcolour‚Äù.\n\n?geom_density\n\n\n[‚Ä¶] Aesthetics\ngeom_density() understands the following aesthetics (required aesthetics are in bold):\n\nx\ny\nalpha ‚Üí NA\ncolour ‚Üí via theme()\nfill ‚Üí via theme()\ngroup ‚Üí inferred\nlinetype ‚Üí via theme()\nlinewidth ‚Üí via theme()\nweight ‚Üí 1\n\nLearn more about setting these aesthetics in vignette(\"ggplot2-specs\"). [‚Ä¶]\n\nFigure¬†10.38 is an example of a density plot with some highly customised aesthetics. It goes without saying that, just because you can customise many aspects of a geom_ layer, it doesn‚Äôt necessarily mean that it‚Äôs a good idea to do so! :upside-down-face:\n\n\nShow code to generate plot below.\nDabrowska.data |&gt; \n  filter(Group == \"L2\") |&gt; \n  ggplot(mapping = aes(x = Blocks)) +\n    geom_density(colour = \"purple\", # Sets the colour of the outline of the density plot\n                linewidth = 5, # Sets the width of the outline\n                linetype = \"dotdash\", # Sets the line type of the outline\n                fill = \"pink\", # Sets the colour of the area under the density plot\n                alpha = 0.6 # Sets the transparency level of the fill colour (with 0 being fully transparent and 1 being completely opaque)\n    ) +\n    labs(x = \"Blocks test results\",\n         title = \"L2 participants' non-verbal IQ scores\")\n\n\n\n\n\n\n\n\nFigure¬†10.38\n\n\n\n\n\nIt can, however, be very useful to help identify different elements within a complex plot as in Figure¬†10.39.\n\n\nShow code to generate plot below.\n# We first create an R object that contains the mean Block scores for both L1 and L2 participants.\nmean.blocks &lt;- Dabrowska.data |&gt;\n  group_by(Group) |&gt;\n  summarise(mean = mean(Blocks))\n\n# Next we plot density plots and also add a geom_vline layer to plot the mean values as vertical lines. We use the linetype argument to make the lines dotted.\nDabrowska.data |&gt;\n  ggplot(mapping = aes(x = Blocks, \n                       fill = Group,\n                       colour = Group)) + \n  geom_density(alpha = 0.6, \n               position = \"identity\") +\n  geom_vline(data = mean.blocks, \n             aes(xintercept = mean, \n                 colour = Group), \n             linetype = \"dashed\",\n             linewidth = 0.8) +\n  scale_colour_viridis_d(guide = NULL) +\n  scale_fill_viridis_d() +\n  theme_minimal() +\n  labs(x = \"Non-verbal IQ test (Blocks) test scores\",\n       fill = NULL,\n       title = \"On average, non-native English speakers have higher IQ scores\\nthan native speakers.\",\n       caption = \"The dotted lines represent the means of each group.\")\n\n\n\n\n\n\n\n\nFigure¬†10.39\n\n\n\n\n\nTo create Figure¬†10.39, we first calculated the mean Blocks scores for both L1 and L2 participants and stored these values in a new object called mean.blocks (see code link above). These values are then called within the geom_vline() function, which draws vertical lines. In other words, in addition to setting aes() mappings within the main ggplot() function at the start of our plot code, we can also add additional data mappings within a specific geom_ function. With all these options, it‚Äôs no exaggeration to say that, if you really set your mind to, pretty much anything is possible with {ggplot2}! :upside-down-face:\n\n\n\n\n\n10.2.4 Boxplots\nIn Section 8.3.2, we saw that boxplots are a great way to visualise both the central tendency (median) of a numeric variable and the spread around this central tendency (IQR). There is an in-built function to create boxplots in {ggplot2}. No prizes will be awarded for guessing that the necessary geom_ function is called‚Ä¶ geom_density()! :grinning-squinting-face:\nWhilst it‚Äôs possible to plot just a single boxplot, that rarely makes sense. In fact, the x-axis in Figure¬†10.40 is entirely nonsensical! The distribution of Grammar scores across the entire dataset is much better visualised as a histogram or density plot (see Section 10.2.3) than as a single boxplot.\n\nDabrowska.data |&gt;\n  ggplot(mapping = aes(y = Grammar)) +\n  geom_boxplot() +\n  theme_minimal() + \n  labs(y = \"Grammar scores\")\n\n\n\n\n\n\n\nFigure¬†10.40\n\n\n\n\n\nIf, however, we want to compare the Grammar scores of two or more different groups of participants, a boxplot makes a lot more sense (see Figure¬†10.41). To achieve this, we add a second argument within the aes() function, which maps the values of the Group variable (which are either ‚ÄúL1‚Äù or ‚ÄúL2‚Äù) to the plot‚Äôs x-axis.\n\nDabrowska.data |&gt;\n  ggplot(mapping = aes(y = Grammar, \n                       x = Group)) +\n  geom_boxplot() +\n  theme_minimal() +\n  labs(y = \"Grammar scores\")\n\n\n\n\n\n\n\nFigure¬†10.41\n\n\n\n\n\nThe meaning conveyed by Figure¬†10.41 is clear: there is hardly any difference between the average (median) grammar comprehension test scores of L1 and L2 participants in DƒÖbrowska (2019)‚Äôs dataset. Indeed, we can see that the thicker, middle lines within each boxplot are almost at the same level. However, the two boxplots have very different shapes and overall lengths: the scores of the 50% of L2 participants who scored below the median are much more spread out than those of the L1 participants who obtained below-average scores. This makes intuitive sense: native English speakers living in the UK who volunteer for such a study are likely to all have a fairly high to very high understanding of English grammar. By contrast, the L2 speakers are much more varied: some are highly proficient in English, while others are not. This range of proficiency could due to all sorts of reasons.\nWhat are some of the possible reasons that you can think of? :thinking-face: Make a note of them as we will explore these hypotheses further in Section 10.2.5.\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nQ10.15 Create a boxplot to compare how participants in different occupational groups (OccupGroup) performed on the English grammar test. Which part of the code used to produce Figure¬†10.41 do you need to modify to achieve this?\n\n\n\n\n\ngeom_boxplot()\n\n\n\n\nx = OccupGroup\n\n\n\n\ny = Grammar\n\n\n\n\nDabrowska.data\n\n\n\n\n\n\n\n\n\n\nShow sample code to answer Q10.15.\nDabrowska.data |&gt;\n  ggplot(mapping = aes(y = Grammar, \n                       x = OccupGroup)) +\n  geom_boxplot() +\n  theme_minimal() +\n  labs(y = \"Grammar scores\")\n\n\nQ10.16 The code below was used to create Figure¬†10.42, except that the arguments of the aes() function have been deleted. Which data mappings were specified inside the aes() function to produce Figure¬†10.42?\n\nDabrowska.data |&gt;\n  mutate(Group = fct_recode(Group,\n                            `L1 participants` = \"L1\",\n                            `L2 participants` = \"L2\")) |&gt; \n  ggplot(mapping = aes(‚ñà ‚ñà ‚ñà ‚ñà ‚ñà ‚ñà ‚ñà ‚ñà ‚ñà ‚ñà ‚ñà ‚ñà)) +\n    geom_boxplot(alpha = 0.8) +\n    scale_fill_viridis_d(option = \"viridis\") +\n    scale_y_continuous(breaks = seq(0, 100, 10)) +\n    labs(y = \"Grammar scores\", \n         x = NULL,\n         fill = \"Occupational group\",\n         title = \"English grammar comprehension test results\",\n         subtitle = \"among L1 and L2 participants with different occupations\") +\n    theme_bw() +\n    theme(element_text(size = 12),\n          legend.position = \"bottom\", # We move the legend to the bottom of the plot. \n          legend.box.background = element_rect()) # We add a frame around the legend.\n\n\n\n\n\n\n\n\n\nFigure¬†10.42\n\n\n\n\n\n\n\n\n\n\ny = OccupGroup\n\n\n\n\ncolour = OccupGroup\n\n\n\n\nfacet = OccupGroup\n\n\n\n\nx = Group\n\n\n\n\nlegend = OccupGroup\n\n\n\n\nx = OccupGroup\n\n\n\n\ny = Grammar\n\n\n\n\nfill = OccupGroup\n\n\n\n\nfacet = Group\n\n\n\n\n\n\n\n\nü¶â Hover over the owl for a first hint.\n\n\n\n\nüê≠ Click on the mouse for a second hint.\n\n\n\n\n¬†\n\n\n\n\n\n\n\n\n\nNoteGoing further: Dot plots and violin plots\n\n\n\n\n\nThe {ggplot2} library offers many more geom_ functions for you to explore. Here are two more types of graphs that are currently only rarely used in the language sciences, but which can be very effective ways to visualise the distribution of a numeric variable across different levels of a categorical variable.\n\nDot plots\nIn a dot plot, each data point (corresponding, here, to a single participant) is represented by a single dot. The size of each dot corresponds to the chosen bin width. This makes dot plots a combination of a boxplot (see Section 8.3.2) and a histogram (see Section 10.2.2).\n\n\n\nggplot(data = Dabrowska.data,\n    mapping = aes(y = Colloc, \n                     x = Group)) +\n    geom_dotplot(binaxis = \"y\", \n              stackdir = \"center\",\n              binwidth = 3) +\n    labs(y = \"Scores on English collocation test\",\n         x = NULL) +\n    theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nViolin plots\nThe help file of the geom_violin() function describes violin plots as follows:\n\nA violin plot is a compact display of a continuous distribution. It is a blend of geom_boxplot() and geom_density(): a violin plot is a mirrored density plot displayed in the same way as a boxplot.\n\n\n\n\nggplot(data = Dabrowska.data,\n       mapping = aes(y = Colloc, \n                       x = Group)) +\n    geom_violin() +\n    labs(y = \"Scores on English collocation test\",\n         x = NULL) +\n    theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBy themselves, violin plots are rather abstract representations of variable distributions. However, in combination with boxplots, they can be an effective way to visualise and compare data distributions. Note that, here, the order of the layers is important because, if we first draw the boxplots and then the violin plots, the violin plots will mask the boxplots completely.\n\n\n\nDabrowska.data |&gt; \n  ggplot(mapping = aes(y = Colloc, \n                       x = Group)) +\n    geom_violin(width = 1, \n                colour = \"grey\", \n                fill = \"grey\") +\n    geom_boxplot(width = 0.08, \n                 alpha = 0.2, \n                 outliers = FALSE) +\n    labs(y = \"Scores on English collocation test\",\n         x = NULL) +\n    theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.2.5 Scatter plots\nScatter plots are ideal to examine the relationship between two numeric variables. They are best suited to continuous numeric variables.\nIn the following, we will build a scatter plot to explore the following hypothesis:\n\nIn the data from DƒÖbrowska (2019), English grammar comprehension scores are more strongly associated with the level of formal education among L2 speakers than among L1 speakers.\n\nTo explore this hypothesis, we map the total number of years that participants spent in formal education (EduTotal) onto the x-axis and their Grammar scores onto the y-axis. In addition, we use the facet_wrap() function to split the data into two panels: one for the L1 participants and the other for the L2 group.\n\nDabrowska.data |&gt; \n  ggplot(mapping = aes(x = EduTotal, \n                     y = Grammar)) +\n  facet_wrap(~ Group) +\n  geom_point() +\n  labs(x = \"Number of years in formal education\",\n       y = \"Grammar comprehension test scores\",\n       title = \"Exploring hypothesis 1\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nAt first glance, it would seem that our data do not support our initial hypothesis: among the L2 participants, there is no obvious trend suggesting that those who scored lowest on the grammar test were the ones who spent fewer years in formal education.-\nIn Figure¬†10.43, we add a regression line (in blue) per panel to our facetted scatter plot using the geom_smooth(method = \"lm\") function. This allows us to visualise the correlation between participants‚Äô grammar scores and the number of years they spent in formal education.\n\nDabrowska.data |&gt; \n  ggplot(mapping = aes(x = EduTotal, \n                       y = Grammar)) +\n  facet_wrap(~ Group) +\n  geom_point() +\n  geom_smooth(method = \"lm\",\n              se = FALSE) +\n  labs(x = \"Number of years in formal education\",\n       y = \"Grammar comprehension test scores\",\n       title = \"Exploring hypothesis 1\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure¬†10.43\n\n\n\n\n\nRegression lines in scatter plots are interpreted as follows:\n\nIf the regression line goes up, there is a positive correlation between the two numeric variables. ‚ÜóÔ∏è\nIf the line goes down, there is a negative correlation. ‚ÜòÔ∏è\nThe steeper the line, the stronger the correlation. :flexed-biceps:\nIf the line is flat (or nearly flat), there is no (linear) correlation between the two variables. :right-arrow:\nBe aware that even very strong correlations do not necessarily imply (direct) causation. :cross-mark:\n\nThe regression lines added by the geom_smooth(method = \"lm\") function are lines of best fit: the better the fit, the closer the points are to the line. If few points are on or close to the line, it means that the regression line is not a good approximation of the relationship between the two variables. This is clearly the case in Figure¬†10.43 - especially in the L2 panel (more on this in Section 11.5). Our data visualisation therefore do not support our hypothesis that, in the DƒÖbrowska (2019) data, grammar scores are more strongly associated with the level of formal education among the L2 speakers than among the L1 speakers. If anything, our data show the opposite pattern! Our line of fit is both closer to the data points and steeper in the L1 panel than in the L2 panel.\nSo far, all of our data visualisations have only displayed the characteristics of the collected data. In other words, they display descriptive statistics (see Chapter 8) that do not allow us to make inferences about other participants who were not tested as part of DƒÖbrowska (2019)‚Äôs study. Tests of statistical significance, including of correlations, are introduced in Chapter 11.\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nIn this quiz, you will explore another hypothesis:\n\nL2 speakers‚Äô grammar comprehension scores (Grammar) are positively correlated with their length of residence in the UK (LoR): L2 speakers who have lived in the UK for longer have a better understanding than those who arrived more recently.\n\nUsing {ggplot2}, create a scatter plot that allows you to explore this hypothesis.\nQ10.17 What do you need to do before piping the data into the ggplot() function?\n\n\n\n\n\nselect(Grammar, LoR, Group, L2)\n\n\n\n\nselect(LoR, L2)\n\n\n\n\nfilter(Group == \"L2\")\n\n\n\n\nfilter(Group != \"L2\")\n\n\n\n\nselect(Group, \"L2\")\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n\n\nShow sample code to answer Q10.17.\nDabrowska.data |&gt; \n  filter(Group == \"L2\") |&gt; \n  ggplot(mapping = aes(x = LoR, \n                       y = Grammar)) +\n  geom_point() +\n  labs(x = \"Length of residence in the UK (years)\",\n       y = \"Grammar comprehension test scores\") +\n  theme_bw()\n\n\nQ10.18 On your scatter plot, which data points are clearly outliers?\n\n\n\n\n\nThe L2 speaker who has lived in the UK for more than 40 years.\n\n\n\n\nThe 90 speakers with missing values.\n\n\n\n\nThe L2 speaker who scored 0 on the grammar test.\n\n\n\n\nAll the L2 speakers who have lived in the UK for 15 years or more.\n\n\n\n\nThe three L2 speakers who scored 100 on the grammar test.\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\nIdentifying outliers is important to check and understand our data. If we spot an outlier corresponding to a participant having lived in a country for 400 years, we can immediately tell that something has gone wrong during data collection and/or pre-processing. In this case, however, common sense tells us that it is prefectly possible to have lived 40+ years in a country. Still, compared to the rest of the data, the outlier is striking and we should check that it is not due to an error.\nQ10.19 Using data wrangling functions from the {tidyverse} (see Chapter 9), check whether the outlier participant is old enough to have lived in the UK for longer than 40 years. How old were they when they first came to the UK?\n\n\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n\n\nShow sample code to answer Q10.19.\nDabrowska.data |&gt; \n  filter(LoR &gt; 40) |&gt; \n  select(Age, LoR)\n\n62-42\n\n\nQ10.20 In order to better visualise a potential correlation between L2 participants‚Äô grammar comprehension test results and how long they‚Äôve lived in the UK, we will exclude the outlier and re-draw the scatter plot. Which function can you use to exclude the identified outlier before piping the data into the ggplot() function?\n\n\n\n\n\nfilter(LoR &lt; 40)\n\n\n\n\nfilter(LoR ~ 40)\n\n\n\n\nfilter(LoR == \"&lt; 40\")\n\n\n\n\nfilter(LoR != 40)\n\n\n\n\nfilter(LoR &gt;= 40)\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n\n\nShow sample code to answer Q10.20.\nDabrowska.data |&gt; \n  filter(Group == \"L2\") |&gt; \n  filter(LoR &lt; 40) |&gt; \n  ggplot(mapping = aes(x = LoR, \n                       y = Grammar)) +\n  geom_point() +\n  geom_smooth(method = \"lm\",\n              se = FALSE) +\n  labs(x = \"Length of residence in the UK (years)\",\n       y = \"Grammar comprehension test scores\") +\n  theme_bw()\n\n\nQ10.21 Having removed the outlier point from the scatter plot, now add a regression line to visualise the correlation between the two variables. Which of the following options best fits the gap in the sentence below?\n\nIn this group of L2 speakers, there is _____________________ correlation between participants‚Äô grammar test scores and their length of residence in the UK.\n\n\n\n\n\n\na positive\n\n\n\n\na negative\n\n\n\n\na perfect\n\n\n\n\nno\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n\n\nShow sample code to answer Q10.21.\nDabrowska.data |&gt; \n  filter(Group == \"L2\") |&gt; \n  filter(LoR &lt; 40) |&gt; \n  ggplot(mapping = aes(x = LoR, \n                       y = Grammar)) +\n  geom_point() +\n  geom_smooth(method = \"lm\",\n              se = FALSE) +\n  labs(x = \"Length of residence in the UK\",\n       y = \"Grammar comprehension test scores\") +\n  theme_bw()\n\n\n\n\n\n\n\n10.2.6 Word clouds\nThe {ggplot2} library does not include an in-built function to create word clouds. However, members of the R community are continuously creating and sharing new functions and packages to improve and extend data visualisation options in R. One such community member, Erwan Le Pennec, created the {ggwordcloud} package, which adds the geom_text_wordcloud() function to the {ggplot2} environment.\n\n\n\n\n\nHex sticker of the {ggwordcloud} package\n\n\n\n#install.packages(\"ggwordcloud\")\nlibrary(ggwordcloud)\n\nWe will use the geom_text_wordcloud() function to create a word cloud representing L2 participants‚Äô native languages. To this end, we first need to create a table that tallies how often each native language was mentioned. We also include a column for the language family (see Task 9.3 in Section 9.5.2).\n\nNativeLg_freq &lt;- Dabrowska.data |&gt; \n  filter(Group == \"L2\") |&gt; \n  count(NativeLg, NativeLgFamily)\n\nNativeLg_freq\n\n     NativeLg NativeLgFamily  n\n1   Cantonese        Chinese  4\n2     Chinese        Chinese  3\n3      French        Romance  2\n4      German       Germanic  1\n5       Greek       Hellenic  1\n6     Italian        Romance  1\n7  Lithuanian         Baltic  5\n8    Mandarin        Chinese  8\n9      Polish         Slavic 37\n10    Russian         Slavic  2\n11    Spanish        Romance  3\n\n\nNext, we enter this new dataset, NativeLg_freq, in a ggplot() function and map each native language to a text label aesthetics, the number of participants to have this native language to the size of the words, and each native language family to a colour.\n\nggplot(data = NativeLg_freq,\n       aes(label = NativeLg,\n           size = n,\n           colour = NativeLgFamily)) +\n  geom_text_wordcloud() +\n  scale_size_area(max_size = 30) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWord clouds are widely used in the media and corporate world, but they are much rarer in academic research. Although word clouds can help to visualise the most prominent words in a set of words, they mostly serve decorative purposes. Data visualisation experts warn against them for two main reasons:\n\nLonger words appear larger simply due to their length.\nThe human eye is not good at discerning small differences in font sizes.\n\n\n\n\n\n\n\nNoteGoing further: Using geom_text()\n\n\n\n\n\nWe have seen that, on a scatter plot, each point represents an observation, in our case, a participant. For instance, in Figure¬†10.44, each point represents an L2 participant and displays their age and grammar comprehension test score. In addition, we can colour the points according to their native language family.\n\n\nSee R code.\nDabrowska.data |&gt; \n  filter(Group == \"L2\") |&gt; \n  ggplot(aes(x = Age, \n           y = Grammar,\n           colour = NativeLgFamily)) +\n  geom_point() +\n  labs(x = \"Age\", \n       y = \"Grammar comprehension test score\", \n       color = \"Language family\") +\n  theme_bw(base_size = 14)\n\n\n\n\n\n\n\n\nFigure¬†10.44\n\n\n\n\n\nInstead of plotting points on the scatter plot, we can plot text. Thus, instead of using geom_point(), we can use geom_text(). For example, we can display the native language of each participant by mapping NativeLg to the label aesthetics.\n\n\nSee R code.\nDabrowska.data |&gt; \n  filter(Group == \"L2\") |&gt; \n  ggplot(aes(x = Age, \n           y = Grammar,\n           colour = NativeLgFamily,\n           label = NativeLg)) +\n  geom_text() +\n  scale_x_continuous(limits = c(15,65)) +\n  labs(x = \"Age\", \n       y = \"Grammar comprehension test score\", \n       color = \"Language family\") +\n  theme_bw(base_size = 14)\n\n\n\n\n\n\n\n\nFigure¬†10.45\n\n\n\n\n\nClearly, there are too many L2 participants for Figure¬†10.45 to be legible. For now, we will therefore focus on female L2 participants only. Moreover, we can see that there is dense cluster of L2 participants who are young and scored very high on the grammar comprehension test. In this exploratory analysis, we will therefore focus on female participants who obtained fewer than 90 points on the test.\n\n\nSee R code.\nDabrowska.data |&gt; \n  filter(Group == \"L2\") |&gt; \n  filter(Grammar &lt; 90 & Gender == \"F\") |&gt; \nggplot(aes(x = Age, \n           y = Grammar,\n           colour = NativeLgFamily,\n           label = NativeLg)) +\n  geom_text() +\n  scale_x_continuous(limits = c(15,55)) +\n  labs(x = \"Age\", \n       y = \"Grammar comprehension test score\", \n       color = \"Language family\",\n       caption = \"(Subgroup analysis of female L2 participants with grammar scores below 90.)\") +\n  theme_bw(base_size = 14)\n\n\n\n\n\n\n\n\nFigure¬†10.46\n\n\n\n\n\nNote that we had to expand the limits of the x-axis in order for labels of the youngest and oldest participants to be displayed in full on Figure¬†10.46. Still, there are a few labels that overlap nonetheless. If this bothers you, you‚Äôll be pleased to hear that there is a handy {ggplot2} extension called {ggrepel}, which was designed to avoid text labels overlapping.\n\n\n\nSee R code.\n#install.packages(\"ggrepel\")\nlibrary(ggrepel)\n\nDabrowska.data |&gt; \n  filter(Group == \"L2\") |&gt; \n  filter(Grammar &lt; 90 & Gender == \"F\") |&gt; \nggplot(aes(x = Age, \n           y = Grammar,\n           colour = NativeLgFamily,\n           label = NativeLg)) +\n  geom_text_repel() +\n  scale_x_continuous(limits = c(15,55)) +\n  labs(x = \"Age\", \n       y = \"Grammar comprehension test score\", \n       color = \"Language family\",\n       caption = \"(Subgroup analysis of female L2 participants with grammar scores below 90.)\") +\n  theme_bw(base_size = 14)\n\n\n\n\n\n\n\n\nFigure¬†10.47\n\n\n\n\n\n\n\n\n\n\n\n\nHex sticker of the {ggrepel} package featuring a red pin attaching a label\n\n\n\n\n10.2.7 Complex plots\nPart of the magic of the Grammar of Graphics is that we can easily combine different geometries and aesthetics to create highly informative graphs. If you just read about the geom_text() function (see Figure¬†10.47), then you have already seen how this can work.\nFigure¬†10.48 is another example. In this plot, we combine geom_boxplot() with geom_point() to see both the defining characteristics of the distributions of grammar test scores among L1 and L2 participants, as well as the exact score of each individual participant.\nBy default, the geom_boxplot() function plots dots for any outliers. Since we are now also plotting all the data points using the geom_point() function, we need to switch off this option, otherwise the outliers will be plotted twice.\n\nDabrowska.data |&gt;\n  ggplot(mapping = aes(y = Grammar, \n                       x = Group)) +\n  geom_boxplot(outliers = FALSE) +\n  geom_point(alpha = 0.6,\n             size = 2) +\n  labs(x = NULL,\n       y = \"Grammar comprehension test score\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure¬†10.48\n\n\n\n\n\nWhen there are too many points overlapping, even adding some transparency to the points with the alpha option may not suffice to tell the points apart. An alternative is to add a little bit of randomness to the position of the dots using the geom_jitter() function to ensure that there are fewer overlaps.\n\nDabrowska.data |&gt;\n  ggplot(mapping = aes(y = Grammar, \n                       x = Group)) +\n  geom_boxplot(outliers = FALSE) +\n  geom_jitter(alpha = 0.6,\n              size = 2,\n              width = 0.1) +\n  labs(x = NULL,\n       y = \"Grammar comprehension test score\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure¬†10.49\n\n\n\n\n\nWe can also map the colour of the jittered points to another categorical variable from our dataset, e.g.¬†Gender.\n\nDabrowska.data |&gt;\n  ggplot(mapping = aes(y = Grammar, \n                       x = Group, \n                       colour = Gender)) +\n  geom_boxplot(outliers = FALSE,\n               colour = \"black\") +\n  geom_jitter(alpha = 0.6,\n              size = 2,\n              width = 0.1) +\n  labs(x = NULL,\n       y = \"Grammar comprehension test score\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure¬†10.50\n\n\n\n\n\nFigure¬†10.50 allows us to see that the lowest grammar comprehension test scores among L1 participants come from male participants, whereas the lowest scores in the L2 group come from female participants. Can you think of why this might be? :thinking-face: Make a note of your hypothesis as you will have a chance to explore it further in Section 10.2.8.\nWe can, in theory, visualise even more of our data by adding a shape aesthetics to the geom_jitter() (see Figure¬†10.50). However, increased plot complexity does not necessarily result in more meaningful or effective statistical graphics. How effective is Figure¬†10.51 in your opinion?\n\nDabrowska.data |&gt;\n  ggplot(mapping = aes(y = Grammar, \n                       x = Group, \n                       colour = OccupGroup)) +\n  geom_boxplot(colour = \"black\",\n               outliers = FALSE) +\n  geom_jitter(mapping = aes(shape = Gender),\n              size = 3,\n              alpha = 0.7, \n              width = 0.1) +\n  labs(x = NULL,\n       y = \"Grammar comprehension test score\",\n       colour = \"Occupational\\ngroup\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure¬†10.51\n\n\n\n\n\nAs the saying goes, less is often more. In other words, simpler plots are often more effective than complex ones. With {ggplot2} and its many extensions, the possibilities are almost endless, but that doesn‚Äôt mean that simple bar charts, histograms, and scatter plots should be abandoned! When designing effective data visualisation to communicate your research, think carefully about your audience and the message that you want to convey.\n\n\n10.2.8 Interactive plots\nIn Section 10.2.7, we saw that we can combine geometries and aesthetics, but that there are some limits as to what is meaningful and effective. In particular, it is difficult to visualise categorical variables with many different levels on a static graph.\nThe good news is that now that you know how to create a plot using {ggplot2}, you are 10 seconds away from making it interactive! Interactive plots are particularly useful for data sanity checks and data exploration.\n\n\n\n\n\nHex sticker of the {plotly} package\n\n\nThe {plotly} package provides access to a popular javascript visualisation toolkit within R. You will first need to install the package and load it.\n\n#install.packages(\"plotly\")\nlibrary(plotly)\n\nThen, all you need to do is create and save a ggplot object, and call that object with the ggplotly() function. In RStudio, your interactive plot will be displayed in the Viewer pane. Hover your mouse over the data points of Figure¬†10.52 to explore the data interactively. You will see that, in addition to the variables mapped onto the plot itself, the variable mapped onto the label aesthetics (OccupGroup) also appears when you hover the mouse over any single data point.\n\nmyplot &lt;- Dabrowska.data |&gt; \n  ggplot(mapping = aes(y = Grammar, \n                       x = Group, \n                       colour = Gender,\n                       label = Occupation)) +\n  geom_jitter(alpha = 0.8, \n              width = 0.1) +\n  geom_boxplot(alpha = 0, \n               colour = \"black\",\n               outliers = FALSE) +\n  labs(x = NULL,\n       y = \"Grammar comprehension test score\") +\n  theme_bw()\n\nggplotly(myplot)\n\n\n\n\n\n\n\n\n\nFigure¬†10.52\n\n\n\n\nIf you want to display more than one additional variable on hover, you can do using the text mapping. Here you can customise which variables are displayed and how. As the interactive plot is coded in HTML, you need to use the ‚Äú‚Äù tag to add a new line.\n\nmyplot2 &lt;- Dabrowska.data |&gt; \n  ggplot(mapping = aes(y = Grammar, \n                       x = Group, \n                       colour = Gender,\n                       text = paste(\"Age:\", Age, \"&lt;/br&gt;Years in formal education:\", EduTotal, \"&lt;/br&gt;Job:\", Occupation))) +\n  geom_jitter(alpha = 0.8, \n              width = 0.1) +\n  geom_boxplot(alpha = 0, \n               colour = \"black\",\n               outliers = FALSE) +\n  labs(x = NULL,\n       y = \"Grammar comprehension test score\") +\n  theme_bw()\n\nggplotly(myplot2)",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>The Gramma`R` of Graphics</span>"
    ]
  },
  {
    "objectID": "10_Dataviz.html#sec-ExploringPlots",
    "href": "10_Dataviz.html#sec-ExploringPlots",
    "title": "10¬† The GrammaR of Graphics",
    "section": "10.3 Exporting plots üöÄ",
    "text": "10.3 Exporting plots üöÄ\nYou may want to share the plots that you have created in R with others or insert them in a research paper or presentation. {ggplot2} provides a convenient way to export plots in various formats using the ggsave() function. You can specify the file name, format, and other options such as the width, height, and resolution of the image.\nIf you run the ggsave() function without specifying which plot should be saved, it will save the last plot that was displayed. The image file type is determined by the extension of the file name that you provide, e.g.¬†a file name ending in .png will export your graph to a PNG file. You can specify its dimensions in centimetres (cm), millimetres (mm), inches (in), and pixel (px). Working out appropriate dimensions can be tricky and typically requires a bit of experimenting until you get it right.\n\nDabrowska.data |&gt; \n  ggplot(mapping = aes(x = Blocks,\n                       fill = Group)) +\n  geom_density(alpha = 0.7) +\n  scale_fill_viridis_d() +\n  scale_x_continuous(expand = c(0,0),\n                     limits = c(0,30)) +\n  labs(x = \"Non-verbal IQ (Blocks test)\") +\n  theme_minimal()\n\nggsave(\"BlocksDensityPlot.png\",\n       width = 10,\n       height = 8,\n       units = \"cm\")\n\nBy default, plots will be exported to the project‚Äôs working directory. You can save it elsewhere by specifying a different file path (see Section 3.3). For example, this function will save the last plot displayed as an SVG file in the project‚Äôs subfolder ‚Äúfigures‚Äù. Note that, if the subfolder to which you want to save your graph does not yet exist, R will return an error. So make sure that the subfolder exists at the path that you specified before attempting to save anything in it!\n\nggsave(\"figures/BlocksDensityPlot.svg\",\n       width = 1500,\n       height = 1000,\n       units = \"px\")\n\n\nCheck your progress üåü\nWell done! You have successfully completed this chapter on data visualisation using the Grammar of Graphics approach. You have answered 0 out of 21 questions correctly.\nAre you confident that you can‚Ä¶?\n\nUse the syntax of the Grammar of Graphics to build and customise your graphs layer-by-layer (using data, aesthetics, geometries, facetting, scales, coordinates, and theme layers) (Section 10.1)\nCreate and interpret bar plots, histograms, density plots, boxplots, scatter plots, and word clouds (Section 10.2)\nCreate and interpret complex plots and interactive plots for data exploration (Section 10.2.7) and (Section 10.2.8)\nExport and share your plots in various formats (Section 10.3)\n\nIf so, you are ready to move out on inferential statistics in Chapter 11!\n\n\n\n\n\n\nNoteCharting your way to success üó∫Ô∏è\n\n\n\nIf you have completed this chapter, you can now produce some of the most widely used plots in the language sciences: Congratulations!\nBar plots, boxplots, scatter plots, density plots and histograms may well be all that you need for now. But you should know that the possibilities with R, the {ggplot2} library, and its many extensions are pretty much limitless: The R Graph Gallery lists some 50 different types of charts with over 400 examples with code!\nAnother great resource is The R Graphics Cookbook by Winston Chang. To go beyond the basics and to find out more information about the theoretical underpinnings of the {ggplot2} package, I recommend ggplot2: Elegant Graphics for Data Analysis.\nThere are also great R packages to produce more specialised types of graphs frequently used in linguistics such as vowel charts and dialectal maps (see list of next-step resources).\n\n\n\n\n\n\nCleveland, William S. & Robert McGill. 1987. Graphical perception: The visual decoding of quantitative information on graphical displays of data. Journal of the Royal Statistical Society: Series A (General) 150(3). 192‚Äì210. https://doi.org/10.2307/2981473.\n\n\nDƒÖbrowska, Ewa. 2019. Experience, aptitude, and individual differences in linguistic attainment: A comparison of native and nonnative speakers. Language Learning 69(S1). 72‚Äì100. https://doi.org/10.1111/lang.12323.\n\n\nFew, Stephen. Save the pies for dessert. August 2007. http://www.perceptualedge.com/articles/08-21-07.pdf.\n\n\nGarnier, Simon, Noam Ross, BoB Rudis, Antoine Filipovic-Pierucci, Tal Galili, Timelyportfolio, Alan O‚ÄôCallaghan, et al. 2023. Sjmgarnier/viridis: CRAN release v0.6.3. Zenodo. https://doi.org/10.5281/ZENODO.4679423.\n\n\nGodfrey, A. Jonathan R., Debra Warren, Deepayan Sarkar, Gabriel Becker, James Thompson, Paul Murrell, Timothy Bilton & Volker Sorge. 2025. BrailleR: Improved access for blind users. https://github.com/ajrgodfrey/BrailleR.\n\n\nHvitfeldt, Emil. 2021. Paletteer: Comprehensive collection of color palettes. https://github.com/EmilHvitfeldt/paletteer.\n\n\nOu, Jianhong. 2021. colorBlindness: Safe color set for color blindness. https://CRAN.R-project.org/package=colorBlindness.\n\n\nSouth Carolina, University of. Alternative text. Digital Accessibility. https://sc.edu/about/offices_and_divisions/digital-accessibility/toolbox/best_practices/alternative_text/.\n\n\n(WAI), W3C Web Accessibility Initiative. 2022. Images. Strategies, standards, resources to make the Web accessible to people with disabilities. https://www.w3.org/WAI/tutorials/images/.\n\n\nWickham, Hadley. 2016. ggplot2: Elegant graphics for data analysis. New York: Springer. https://ggplot2.tidyverse.org.\n\n\nWilkinson, Leland. 2005. The Grammar of Graphics (Statistics and Computing). New York: Springer. https://doi.org/10.1007/0-387-28695-0.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>The Gramma`R` of Graphics</span>"
    ]
  },
  {
    "objectID": "10_Dataviz.html#footnotes",
    "href": "10_Dataviz.html#footnotes",
    "title": "10¬† The GrammaR of Graphics",
    "section": "",
    "text": "Note that, while the library is called {ggplot2}, its main function is called ggplot().‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>The Gramma`R` of Graphics</span>"
    ]
  },
  {
    "objectID": "11_Inferential.html",
    "href": "11_Inferential.html",
    "title": "11¬† InfeRential statistics",
    "section": "",
    "text": "Chapter overview\nThis chapter provides an introduction to:\nDescribing and visualising sample data belongs to the realm of descriptive statistics. Attempting to generalise trends from our observed data to a larger population takes us to inferential statistics. Whereas descriptive statistics is about summarising and describing a specific dataset (our sample), with inferential statistics, we draw on our sample data to make educated guesses about a larger population that we have not directly studied. This helps us to determine whether the patterns that we observed thanks to descriptive statistics and data visualisations are likely to reflect broader trends that apply to the larger population or, instead, can more likely be attributed to random variation in the sample data.",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Infe`R`ential statistics</span>"
    ]
  },
  {
    "objectID": "11_Inferential.html#sec-Sampling",
    "href": "11_Inferential.html#sec-Sampling",
    "title": "11¬† InfeRential statistics",
    "section": "11.1 From the sample to the population",
    "text": "11.1 From the sample to the population\nSo far, we have described the data collected by DƒÖbrowska (2019). In Chapter 8, we described these data using descriptive statistics thanks to measures of central tendencies (e.g.¬†means) and measures of variability around the central tendency (e.g.¬†standard deviations). In Chapter 10, we described the data visually using different kinds of statistical plots. These descriptive analyses enabled us to spot some interesting patterns (and there are many more for you to explore!). For instance, we noticed that:\n\nOn average, L2 participants scored lower than the L1 participants on the English grammar, vocabulary and collocation comprehension tests. This was to be expected, but our visualisations also revealed that many L2 participants scored at least as well and sometimes even better than average L1 participants.\nOn average, L2 participants obtained higher non-verbal IQ scores (as measured by the Blocks test) than L1 participants but, here, too, there was a lot of overlap between the two distributions.\nFor both L1 and L2 participants, there was a positive correlation between the number of years they were in formal education and their English grammar comprehension test scores: the longer they were in education, the better they performed on the test.\n\nIn this chapter, we ask whether these observations are likely to be generalisable beyond DƒÖbrowska (2019)‚Äôs sample of 90 English native speakers and 67 non-native English speakers to a broader population. In the context of this study, we will define the full population as all adult English native and non-native speakers living in the UK.\nIn the language and education sciences we rarely have access to the entire population for which we would ideally like to generalise our findings. For example, we can hardly go and test all English speakers living in the UK. Instead, we have to make due with a sample of L1 and L2 English speakers from the UK. Since our studies attempt to infer information about entire populations based only sample data, the quality of our samples is crucial: no sophisticated statistical procedure can produce any meaningful inferential statistics from a biased or otherwise flawed sample!\n\n\n\n\n\n\nNoteSampling methods\n\n\n\nThere are different ways to draw samples from a population. The most common methods are summarised below.\n\n\n\n\n\n\n\n\n\n\nMethod\nWhat it means\nTypical use in linguistics\nMain advantages\nMain limitations\n\n\n\n\nRandom sampling\nEvery member of the target population has an equal chance of being selected.\nRarely feasible for human populations, but can sometimes be approximated with census lists. Feasible in other contexts, e.g.¬†when the target population are all words featured in a specific dictionary.\nMinimises systematic bias. The results can be generalised using inferential statistics methods.\nRequires a complete sampling frame (e.g.¬†a list of all English speakers living in the UK) and a perfect world in which everyone sampled consents to participating in study.\n\n\nStratified sampling\n(a subtype of random sampling)\nThe population is divided into strata (e.g.¬†by dialectal region, age group, education level) and a random sample is drawn from each stratum.\nUsed when researchers need balanced data from distinct sub‚Äëpopulations (e.g.¬†speakers from several dialectal areas) and their intersections (e.g.¬†a good balance of genders across each dialectal area).\nGuarantees coverage of all relevant sub‚Äëpopulations; reduces sampling error within the strata.\nAs above, requires reliable, exhaustive lists for each stratum and consent for all selected; more complex to organise.\n\n\nCluster sampling\n(a subtype of random sampling)\nWhole clusters (e.g.¬†villages, schools, neighbourhoods) are randomly selected, then all members of the chosen clusters are studied.\nData collection in schools and fieldwork in remote regions where a full speaker list is impractical.\nEfficient when clusters are naturally defined; reduces costs and organisational burden.\nIncreases sampling error if clusters are internally homogeneous; may miss variation outside selected clusters.\n\n\nRepresentative\n(quota) sampling\nThe sample is designed so as to match the population on key characteristics (e.g.¬†age, gender, region, education).\nResearchers recruit speakers until the sample mirrors known demographics from census data or other sources.\nOften considered to provide a ‚Äúgood enough‚Äù picture when true random sampling is impossible.\nOften difficult to implement because we rarely known enough about the characteristics of the full population; vulnerable to self-selection bias.\n\n\nConvenience sampling\nParticipants are chosen because they are reachable and willing to participate.\nMost common in experimental linguistics, online, and classroom‚Äëbased surveys.\nQuicker, cheaper, and simpler.\nOver‚Äërepresents certain groups; suffers from self-selection bias.\n\n\n\nIn practice, it is quite common for combination of these methods to be used. For example, when researchers run linguistics experiments via commercial platforms such as Qualtrics or Amazon MechanicalTurk, they can select their participants based on some demographic data to obtain a more representative sample. But the sample nonetheless remains a convenience sample as only people who sign up to earn money on these platforms can, by definition, be recruited on these platforms. As you can imagine, these click-workers are hardly representative of the full population, even if they are carefully sampled for age, gender, or socio-economic status.\n\n\nProvided we have a sufficiently representative sample, inferential statistics can help us to answer questions such as: Across all adult English speakers in the UK‚Ä¶\n\ndo L1 speakers, on average, achieve higher scores than L2 speakers on English grammar and vocabulary comprehension tests?\ndo L2 English speakers, on average, perform better on the non‚Äëverbal IQ (Blocks) test than L1 speakers?\nis there a positive linear relationship between the number of years speakers were in formal education and their performance in an English grammar comprehension test?\ndoes the strength of this education‚Äëgrammar comprehension relationship differ between L1 and L2 speakers, or across different age cohorts?\n\nThough by no means the only framework available to us, the most common approach to answering such questions is null hypothesis significance testing (NHST) within the framework of frequentist statistical philosophy. What‚Äôs philosophy got to with statistics, you may ask? It turns out that, contrary to popular belief, statistics is anything but an exact science. By definition, statistical inference involves making inferences about the unknown. Therefore, there are different ways to approach these questions.\nA promising alternative framework that is gaining traction in many disciplines ‚Äî including in the language sciences ‚Äî is Bayesian statistics (see Next-step resources for recommended readings). In this textbook, however, we will focus on the basic principles of statistical inference within the frequentist framework ‚Äî not because it is easier than Bayesian statistics, but rather because it remains the most widely used framework to date. Hence, even if you decide not to use frequentist statistics for your own research, you will certainly need to understand its principles in order to correctly interpret the results of published studies.\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nQ11.1 Which of the following statements best describes random sampling?\n\n\n\n\n\nThe sample is built so that its composition (age, gender, socioeconomic status, etc.) matches known census figures.\n\n\n\n\nEvery member of the target population is guaranteed to take part in the study.\n\n\n\n\nParticipants are selected because they are easy to reach (e.g. university students).\n\n\n\n\nEach individual in the population has the same known probability of being chosen (e.g. 0.001%).\n\n\n\n\n\n\n\n\nQ11.2 True or false: In a convenience sample, the researcher can ensure that the sample is representative of the entire population if they are careful to match the sample to census demographics (age, gender, region, etc.).\n\n\n\n\n\nTrue\n\n\n\n\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n11.1.1 Null hypothesis significance testing (NHST)\nLet‚Äôs begin by considering the following, intriguing research question:\n\nAcross all adult English speakers in the UK, do L2 English speakers, on average, perform better on the non‚Äëverbal IQ (Blocks) test than L1 speakers?\n\nIn the sample collected by DƒÖbrowska (2019), we can see that L2 speakers, on average, performed better than L1 speakers on the Blocks test. For the two groups, the mean Blocks test scores were:\n\nDabrowska.data |&gt; \n  group_by(Group) |&gt;\n  summarise(mean = mean(Blocks),\n            SD = sd(Blocks))\n\n# A tibble: 2 √ó 3\n  Group  mean    SD\n  &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 L1     13.8  5.56\n2 L2     17.5  4.70\n\n\nHowever, we are also aware that there is a lot of variation around these average values. The standard deviations (SD) around these mean values inform us that there is more variability in the L1 group than in the L2 group. Figure¬†11.1 visualises this variability (see Section 8.3.2 on how to interpret boxplots). On this boxplot, diamonds represent the mean values.\n\n\nSee code to generate plot.\ngroup_means &lt;- Dabrowska.data |&gt; \n  group_by(Group) |&gt; \n  summarise(mean_blocks = mean(Blocks))\n\nggplot(data = Dabrowska.data, \n       aes(x = Group, \n           y = Blocks)) +\n  geom_boxplot(width = 0.5) +\n  stat_summary(\n    fun = mean,                     # what to plot\n    geom = \"point\",                 # as a point\n    shape = 18,                     # in a diamond shape\n    size = 4,                       # a little larger than the default\n    colour = \"purple\") +\n  geom_line(\n    data = group_means,                   \n    aes(x = as.numeric(Group), \n        y = mean_blocks),\n    colour = \"purple\",\n    linewidth = 1,\n    linetype = \"dashed\") +\n  geom_text(data = group_means,\n    aes(x = as.numeric(Group), \n        y = mean_blocks,\n        label = sprintf(\"%.2f\", mean_blocks)), # print the means to two decimal points\n    vjust = -1.4,\n    colour = \"purple\") +\n  labs(x = NULL,\n    y = \"Non-verbal IQ (Blocks) test\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nFigure¬†11.1: Comparison of non-verbal IQ (blocks) test scores between groups L1 and L2\n\n\n\n\n\nFollowing the NHST framework, we can quantify how likely it is that this observed difference in means ‚Äî which is visualised as the dotted line in Figure¬†11.1 ‚Äî could have occurred due to chance, i.e.¬†due to naturally occurring variation, only.\nWe start with the assumption that the patterns observed in our sample occurred by chance alone. This initial assumption is known as the null hypothesis (H0). It suggests that any patterns observed in our data arose by mere coincidence rather than due to any real effect or underlying relationship. To answer our research question, we formulate the following null hypothesis:\n\nH0: On average, adult English L1 and L2 speakers in the UK perform equally well on the non-verbal IQ (Blocks) test.\n\nWe can also formulate an alternative hypothesis. This is the hypothesis that we will adopt if we have enough evidence to reject the null hypothesis:\n\nH1: On average, adult English L2 speakers in the UK perform differently on the non-verbal IQ (Blocks) test than L1 speakers.\n\nWe can conduct a statistical significance test to test the likelihood of the null hypothesis given our observed data. Such tests allow us to estimate the probability of observing the patterns that we have noticed (or more extreme ones) in a sample size similar to ours, assuming that there is no real pattern or relationship in the full population. In other words, these tests help us evaluate whether our findings are likely to be a random occurrence within our sample or indicative of an actual trend that is likely to be found in the entire population.\nIt is important to understand that these tests do not prove anything. They rely on probabilities and, as such, can only inform us as to how likely our observations (or more extreme ones) are, assuming that the null hypothesis is true. Thus, statistical significance tests can only provide information about whether we can reasonably reject or fail to reject a null hypothesis based on our sample data.\n\n\n\n\n\n\nWarningCommon misconception\n\n\n\nContrary to what is sometimes claimed, significance tests do not provide any information about the likelihood of either the null or the alternative hypothesis being true or false. In fact, what they provide can be conceptualised as the opposite: they help us to estimate the likelihood of our data given the null hypothesis.\n\nAlways remember that the null hypothesis is an assumption ‚Äî its truth cannot be known. (Winter 2020: 171)\n\nBy definition, inferential statistics is about attempting to infer unknown information about a population from a - often very small - sample of that population. As such, we cannot use statistics to prove or disprove a hypothesis that makes a claim about a population to which we do not have full access. Statistics may be a powerful science, but it‚Äôs not magic! üßô‚Äç‚ôÄÔ∏è",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Infe`R`ential statistics</span>"
    ]
  },
  {
    "objectID": "11_Inferential.html#sec-ttest",
    "href": "11_Inferential.html#sec-ttest",
    "title": "11¬† InfeRential statistics",
    "section": "11.2 Using t-tests to compare two groups",
    "text": "11.2 Using t-tests to compare two groups\nThe null hypothesis that we formulated concerns the average non-verbal IQ test scores of two groups of individuals (English native speakers and non-native English speakers):\n\nH0: On average, adult English L1 and L2 speakers in the UK perform equally well on the non-verbal IQ (Blocks) test.\n\nProvided that certain assumptions are met (see Section 11.6), we can test null hypotheses involving the comparison of two mean values using a t-test. The t-test takes three things into account:\n\nThe magnitude of the difference between the two mean values (i.e.¬†how big is the difference?)\nThe amount of variability around the two means (i.e.¬†how much variation is there around the means?)\nThe sample size (i.e.¬†how many data points ‚Äî in this case, participants ‚Äî are there?)\n\nOur descriptive analysis showed that, on average, the L2 participants scored 3.62 points higher than the L1 participants in the sample. But we know that mean values alone are not sufficient to describe data and, as we saw in Figure¬†11.1, there was a lot of variability around these mean values. Moreover, we know that our sample is relatively small: it only has 90 L1 speakers and 67 L2 speakers (though many experimental linguistics study have fewer data points). The less data we have, the more likely we are to observe extreme values and this aspect is also taken into account by the significance tests such as the t-test.1\nThe R function t.test() takes a formula as its first argument and the data at its second argument. In R, formulas rely on the tilde symbol (~) to indicate that the variable to the left of the tilde is dependent on the variables to the right of the tilde. By specifying the formula as Blocks ~ Group, we are therefore testing whether the mean results of the Blocks test are dependent on whether the participants are L1 or L2 speakers of English (Group). In other words, we apply the t-test to test our null hypothesis, which can be reformulated as:\n\nH0: On average, the results of the non-verbal IQ (Blocks) test are not dependent on whether the test-takers are L1 or L2 speakers of English.\n\n\nt.test(formula = Blocks ~ Group, \n       data = Dabrowska.data)\n\n\n    Welch Two Sample t-test\n\ndata:  Blocks by Group\nt = -4.4084, df = 152.46, p-value = 1.956e-05\nalternative hypothesis: true difference in means between group L1 and group L2 is not equal to 0\n95 percent confidence interval:\n -5.239791 -1.996693\nsample estimates:\nmean in group L1 mean in group L2 \n        13.84444         17.46269 \n\n\nThe output of the t.test() function is a bit overwhelming at first, so let‚Äôs focus on the most relevant aspects:\n\nThe first line of the output informs us that we ran a Welch Two Sample t-test. We ran a two-sample test because we are comparing the means of two independent groups: L1 vs.¬†L2 speakers. And it is a Welch t-test as opposed to a Student‚Äôs t-test because we are not assuming that the standard deviation of the two groups‚Äô test scores in the full population is equal.2\nNext, the t-statistic is reported as t = -4.4084. In this case, it is a negative value which means that the mean score of the first group (here L1) is lower than that of the second group (here L2). Note that, by default, the groups are ordered alphabetically. The larger the absolute value of the t-statistic, the greater the difference between the group means. At the same time, however, the more variability there is in the data, the lower the absolute value of the t-statistic.\ndf = 152.46 corresponds to the degrees of freedom. These are automatically calculated by the t.test() function based on the number of data points in our sample and the number of constraints in our test.\nThe p-value is reported as 1.956e-05. This is scientific notation for: 1.956 multiplied by 10 to the power of minus 5 (1.956¬†*¬†10^-5), which equals 0.00001956.3 This means that our test estimates that there is a very, very small probability ‚Äî namely 0.001956% ‚Äî of observing a difference in mean scores on the Blocks test as large as we observed (3.62 points) or an even larger one under the null hypothesis, i.e.¬†under the assumption that there is no real-world difference between L1 and L2 speakers‚Äô performance on this test.\nThe output also includes a 95% confidence interval (CI) of the difference between the means. It ranges from -5.239791 to -1.996693. In our sample, we observed a mean difference between L1 participants‚Äô and L2 participants‚Äô Blocks test scores of -3.62 points. If we were to repeat this experiment a 100 times with a 100 different samples of L1 and L2 speakers, we can be confident that, in 95 out of 100 repetitions, the confidence interval that we compute would include the true average difference across the entire population. In other words, the average difference between L1 and L2 speakers could be quite a bit larger than in Dabrowska‚Äôs sample or quite a bit lower, but is very unlikely to be zero (which would correspond to the null hypothesis of no difference). Given the same observed difference, the larger our sample, the smaller our confidence interval.\nAt the very bottom of the output, we can read the sample estimates for the L1 and the L2 groups. These are the mean Blocks test scores that we had already calculated using descriptive statistics (see Section 11.1.1). They simply serve as a reminder that we are testing the statistical significance of the difference between these two means under the null hypothesis of no difference.\n\n\n\n\n\n\n\nNoteHow to report\n\n\n\nTo summarise these results, we can write that we conducted a Welch two-sample t-test to compare the mean Blocks score of L1 and L2 English speakers. On average, L2 speakers performed significantly better (M¬†=¬†17.46, SD¬†=¬†4.70) than L1 speakers (M¬†=¬†13.84, SD¬†=¬†5.56), t(152.46)¬†=¬†-4.4084, p¬†&lt;¬†0.001.\n\n\n\n\n\n\n\n\nWarningCommon misconception\n\n\n\nUnfortunately, confidence intervals (CI) are a bit of a misnomer, which frequently leads to misunderstandings. Contrary to what so-called ‚ÄúAI‚Äù tools (see Chapter 15) and even some statistics textbooks may claim, confidence intervals do not tell us that we can be 95% confident that the true difference across the entire population lies within the 95% confidence interval. Bodo Winter (2020: 165) clarifies this common misconception as follows:\n\n[T]he actual population parameter of interest [i.e.¬†in the case of a t-test, the difference in mean values] may or may not be inside the confidence interval ‚Äì you will actually never know for sure. However, if you imagine an infinite series of experiments and compute a confidence interval each time, 95% of the time this interval would contain the true population parameter.\n\nIn other words, a 95% confidence interval does not tell us how confident we can be about any specific value, but rather that, in the long-run, if the study were to be repeated many times, 95% of the time, the 95% confidence interval would contain the true value.\n\n\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nConsider the following research question:\n\nAcross all adult English speakers in the UK, do L1 speakers, on average, achieve higher scores than L2 speakers on the English receptive vocabulary test?\n\nQ11.3 Which null hypothesis could you formulate for this research question?\n\n\n\n\n\nAcross the entire adult UK population, there is no statistically significant difference between the mean scores of L1 speakers and those of L2 speakers on the English receptive vocabulary test.\n\n\n\n\nAcross the entire adult UK population, the mean scores of both L1 and L2 speakers on the English receptive vocabulary test are null.\n\n\n\n\nAcross the entire adult UK population, there is no difference in the mean scores of L1 vs. L2 speakers on the English receptive vocabulary test.\n\n\n\n\n\n\n\n\nQ11.4 Run a t-test on the DƒÖbrowska (2019) data to test the null hypothesis that you selected in Q11.3 above. What is the value of the t-statistic?\n\n\n\n\n\n23.240497\n\n\n\n\n133.83\n\n\n\n\n9.425801\n\n\n\n\n4.6768\n\n\n\n\n0.000007032\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n\n\nShow sample code to answer Q11.4.\n# It is always a good idea to visualise the data before running a statistical test:\nDabrowska.data |&gt; \n  ggplot(mapping = aes(y = Vocab,\n                       x = Group)) +\n  geom_boxplot(alpha = 0.7) +\n  theme_bw()\n\n# Running the t-test:\nt.test(formula = Vocab ~ Group, \n       data = Dabrowska.data)\n\n\nQ11.5 What is the p-value associated with the t-test you ran above?\n\n\n\n\n\n7.032 to the power of minus six\n\n\n\n\n0.000007032\n\n\n\n\n0.0000007032\n\n\n\n\n7.032 multiplied by minus six\n\n\n\n\n7.03206\n\n\n\n\n\n\n\n\nQ11.6 The p-value is very, very small. What does this mean? Select all that apply.\n\n\n\n\n\nIf there were no association between English native-speaker status and receptive English vocabulary scores, it is very unlikely that we would observe this difference in mean values by chance alone.\n\n\n\n\nThe difference in the mean receptive English vocabulary scores of English native and non-native speakers is highly relevant.\n\n\n\n\nThe true difference in the mean receptive English vocabulary scores of English native and non-native speakers is not equal to zero.\n\n\n\n\nThe true difference (across the full population) in the mean receptive English vocabulary scores of English native speakers is only very slightly higher than that of non-native speakers.\n\n\n\n\nGiven the sample size, the results of this t-test are highly precise.\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Infe`R`ential statistics</span>"
    ]
  },
  {
    "objectID": "11_Inferential.html#sec-PValues",
    "href": "11_Inferential.html#sec-PValues",
    "title": "11¬† InfeRential statistics",
    "section": "11.3 Statistical significance and p-values",
    "text": "11.3 Statistical significance and p-values\nWhen used correctly, the p‚Äëvalue is a very useful metric that can help us to determine whether an observed statistic, such as a difference in means in a t-test, is likely to be due to chance variation in the sample rather than indicative of a true effect in the population. A common way4 to use p‚Äëvalues is to define ‚Äî prior to conducting the analysis ‚Äî a significance level threshold (also called alpha or Œ±-level), which corresponds to the risk that we are willing to accept of mistakenly concluding that there is an effect when, in fact, there is none (this is called a false positive result). In the language and social sciences, the significance level is typically set to 0.05. This means that, if the null hypothesis is true, we accept a 5% risk of obtaining a p-value that suggests that we should reject the null hypothesis when, in fact, we shouldn‚Äôt.5 The significance level should be chosen before looking at the data and be clearly mentioned in the methods section of every study that uses statistical significance testing.\nIn the NHST framework, when the calculated p‚Äëvalue is smaller than our chosen significance level (Œ±), we reject the null hypothesis in favour of the alternative hypothesis and say that the result is statistically significant. When the p‚Äëvalue is larger than Œ±, we fail to reject the null hypothesis and say that the result is not statistically significant. However, the latter does not prove that the null hypothesis is true. It only tells us that the data that we have do not provide enough evidence against the null hypothesis. Note that, following this school of statistics, the actual p-value is irrelevant: it is either below or above the Œ±-level threshold. We do not compare p-values and it does not make sense to claim that one result is more or less statistically significant than the other.\n\nOnce p &lt; Œ±, a result is claimed to be ‚Äòstatistically significant‚Äô, which is just the same as saying that the data are sufficiently incompatible with the null hypothesis. If the researcher obtained a significant result for a t-test, the researcher may act as if there actually was a group difference in the population. (Winter 2020: 168)\n\nContrary to what some researchers seem to believe, in and of themselves, p-values are not the holy grail! They can only meaningfully be interpreted together with other important contextual information such as the context in which the data were collected, the magnitude of the observed effect (the effect size), and the variability around the estimated effect (e.g.¬†as shown in data visualisation) (see Figure¬†11.2).\n\n\n\n\n\n\nFigure¬†11.2: Don‚Äôt let your p-values sing solo! (artwork by Allison Horst (CC BY 4.0)\n\n\n\nThe problem with p-values is that they are a composite metric that is dependent on three aspects:\n\nThe size of the observed effect (the larger the effect, the smaller the p-value)\nThe variability within the data (the less variability, the smaller the p-value)\nThe sample size (the larger the sample size, the smaller the p-value)\n\nNote that the size (or magnitude) of the observed effect is only one of three factors that influence the p-value! It is therefore incorrect to claim that an effect (e.g.¬†a difference in means) is particularly large based on a particularly small p-value. It is equally incorrect to claim that a p-value that falls below the chosen significance level points to a (statistically) relevant result. To evaluate the relevance of a result, we need contextual information that goes far beyond the results of a single statistical test. In statistics, ‚Äúsignificance‚Äù and ‚Äúsignificant‚Äù are terms that have nothing to do with either the relevance or importance of results.",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Infe`R`ential statistics</span>"
    ]
  },
  {
    "objectID": "11_Inferential.html#sec-EffectSize",
    "href": "11_Inferential.html#sec-EffectSize",
    "title": "11¬† InfeRential statistics",
    "section": "11.4 Effect sizes and confidence intervals",
    "text": "11.4 Effect sizes and confidence intervals\nIn Section 11.2, we saw that the larger the absolute value of the t-statistic, the greater the difference between the group means. At the same time, the more variability there is in the data, the lower the absolute value of the t-statistic. This makes the t-statistic a measure of effect size. However, it is an unstandardised measure, which means that t-statistic values cannot be compared across different studies.\n\n\n\n\n\nHex sticker of the {effectsize} package\n\n\nBy contrast, Cohen‚Äôs d is a standardised effect size measure. As such, it can be used to compare the magnitude of the difference in mean values across different variables, samples, and studies. Cohen‚Äôs d (the d stands for difference) can be calculated by dividing the difference between two means (the raw strength of an effect) by the standard deviation of both groups together (the overall variability of the data). But fear not: we don‚Äôt need to do the maths ourselves as the formula is implemented in several R packages. In the following, we will use cohens_d() from the {effectsize} package (Ben-Shachar, L√ºdecke & Makowski 2020) which, like the t.test() function, also takes a formula as its first argument.\n\ninstall.packages(\"effectsize\")\nlibrary(effectsize)\n\nRecall the difference that we observed between L1 and L2 English speakers‚Äô non-verbal IQ (Blocks) test results in Figure¬†11.1. With the cohens_d() function, we can now answer the question: How large is this effect?\n\ncohens_d(Blocks ~ Group, \n         data = Dabrowska.data)\n\nCohen's d |         95% CI\n--------------------------\n-0.69     | [-1.02, -0.37]\n\n- Estimated using pooled SD.\n\n\nThe output shows that Cohen‚Äôs d is -0.69. As with the t-statistic, the minus sign tells us that the L1 group performed worse than the L2 participants. The absolute value, here 0.69, corresponds to the strength of the effect. According to Cohen‚Äôs (1988) own rule of thumb, the absolute values can be interpreted as follows:\n\nCohen‚Äôs d¬†=¬†0.2 - 0.5 ‚Üí Small effect size\nCohen‚Äôs d¬†=¬†0.5 - 0.8 ‚Üí Medium effect size\nCohen‚Äôs d¬†&gt;¬†0.8 ‚Üí Large effect size\n\nHowever, Cohen (1988: 25) himself cautioned that:\n\nThe terms ‚Äúsmall,‚Äù ‚Äúmedium,‚Äù and ‚Äúlarge‚Äù are relative, not only to each other, but to the area of behavioral science or even more particularly to the specific content and research method being employed in any given investigation [‚Ä¶].\n\nHence, it is important that linguists and education researchers base their interpretation of standardised effect sizes on prior research relevant to their field of research (see, e.g. Plonsky & Oswald (2014) for L2 research).\nThe output of the cohens_d() function above also includes a 95% confidence interval (CI) around Cohen‚Äôs¬†d. It turns out that there is a direct relationship between the confidence interval around an effect size and the statistical significance of a null hypothesis significance test: if an effect is statistically significant in a two-sided6 independent t-test with a significance (Œ±) level of 0.05, the 95% confidence interval (CI) for the mean difference between the two groups will not include zero. The t-test that we conducted on the results of the Blocks test across the L1 and L2 groups produced a p-value of 0.00001956 which is less than 0.05 and was therefore statistically significant at the Œ±-level of 0.05. But we didn‚Äôt really need To check the p-value because we can see that the effect is statistically significant at the Œ±-level of 0.05 by looking at the 95% CI around Cohen‚Äôs d: the lower bound is -1.02 and the upper bound -0.37. In other words, the CI does not straddle zero.\nNow, let‚Äôs consider a new research question and a new null hypothesis:\n\nH0: On average, the results of the non-verbal IQ (Blocks) test are not dependent on the gender of the test-takers.\n\nRecall that, in this dataset, Gender is a binary variable. The descriptive statistics suggest that male participants perform slightly better than female participants on the non-verbal IQ test, but that there is quite a bit of variability in the data:\n\nDabrowska.data |&gt; \n  group_by(Gender) |&gt; \n  summarise(mean = mean(Blocks),\n            SD = sd(Blocks))\n\n# A tibble: 2 √ó 3\n  Gender  mean    SD\n  &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 F       15.2  5.30\n2 M       15.7  5.81\n\n\nWe now compute a standardised effect size for this gender gap: Cohen‚Äôs d.\n\ncohens_d(Blocks ~ Gender, \n       data = Dabrowska.data)\n\nCohen's d |        95% CI\n-------------------------\n-0.10     | [-0.42, 0.22]\n\n- Estimated using pooled SD.\n\n\nHere, the cohens_d() function compares the scores of the female participants with those of the male participants because ‚Äòfemale‚Äô comes first alphabetically. Hence, the negative Cohen‚Äôs d value means that, on average, males perform better than females. However, we can see that the effect size (-0.10) is very small.\nNow turning to the 95% confidence interval (CI) also output by the function, we can see that, while the lower confidence bound corresponds to a negative effect size, the upper bound is positive, which means that the confidence interval contains the possibility of an effect size of zero, corresponding to no effect at all. Hence, we must conclude that this difference in scores between female and male participants is not statistically significant at an Œ±-level of 0.05. We can confirm this by performing a t-test. It returns a p-value that is greater than 0.05:\n\nt.test(formula = Blocks ~ Gender, \n       data = Dabrowska.data)\n\n\n    Welch Two Sample t-test\n\ndata:  Blocks by Gender\nt = -0.59558, df = 124.58, p-value = 0.5525\nalternative hypothesis: true difference in means between group F and group M is not equal to 0\n95 percent confidence interval:\n -2.352092  1.263946\nsample estimates:\nmean in group F mean in group M \n       15.17021        15.71429 \n\n\n\n\n\n\n\n\nNoteHow to report\n\n\n\nTo summarise these results, we can write that, while male participants performed marginally better (M¬†=¬†15.71, SD¬†=¬†5.81) than female participants (M¬†=¬†15.17, SD¬†=¬†5.30), this difference is very small (Cohen‚Äôs d = -0.10; 95% CI [-042, 0.22]) and is not statistically significant at an Œ±-level of 0.05: t(124.58)¬†=¬†0.5956, p¬†=¬†0.5525.\n\n\nBy default, the cohens_d() function computes a 95% confidence interval, but, if we had chosen a lower Œ±-level of, say, 0.01, we can change this default:\n\ncohens_d(Blocks ~ Gender, \n       data = Dabrowska.data,\n       ci = 0.99)\n\nCohen's d |        99% CI\n-------------------------\n-0.10     | [-0.52, 0.32]\n\n- Estimated using pooled SD.\n\n\nAs you can see, this increases the size of the interval and hence makes it harder to obtain a statistically significant result. This is because lowering the Œ±-level means that we are less willing to risk reporting a false positive result, i.e.¬†reporting a difference based on our data where no real difference exists.\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nCompute three Cohen‚Äôs d values to capture the magnitude of the difference between L1 and L2 speakers:\n\nEnglish grammar (Grammar) test scores\nEnglish receptive vocabulary (Vocab) test scores, and\nEnglish collocation (Colloc) test scores.\n\nAs these are three standardised effect sizes, we can compare them.\nQ11.7 Comparing the three Cohen‚Äôs d that you computed, which statement(s) is/are true?\n\n\n\n\n\nBy far the largest difference between L1 and L2 speakers is in the English collocation test.\n\n\n\n\nThe effect sizes across the three English tests are practically equal.\n\n\n\n\nThe largest difference between L1 and L2 speakers is in the receptive English vocabulary test.\n\n\n\n\nThe smallest effect size is observed in the collocation test.\n\n\n\n\nThe difference in all three effect sizes is statistically significant.\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n\n\nShow sample code to answer Q11.7.\ncohens_d(Grammar ~ Group, \n       data = Dabrowska.data)\n\ncohens_d(Vocab ~ Group, \n       data = Dabrowska.data)\n\ncohens_d(Colloc ~ Group, \n       data = Dabrowska.data)\n\n\nQ11.8 Now compute 99% confidence intervals (CI) around the same three Cohen‚Äôs d values. Which statement(s) is/are true?\n\n\n\n\n\nFor all three effect sizes, the 99% CIs are wider than the 95% CI.\n\n\n\n\nWe can be 99% confident that all three differences in means are real.\n\n\n\n\nThe most relevant difference can be found in the collocation test data.\n\n\n\n\nAll three differences in means are statistically confident at the Œ±-level of 0.01.\n\n\n\n\nAll three differences in means are statistically significant at the Œ±-level of 0.01.\n\n\n\n\nIn 99% of cases, L1 speakers perform better than L2 speakers on all three tests.\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n\n\nShow code to answer Q11.8.\ncohens_d(Grammar ~ Group, \n       data = Dabrowska.data,\n       ci = 0.99)\n\ncohens_d(Vocab ~ Group, \n       data = Dabrowska.data,\n       ci = 0.99)\n\ncohens_d(Colloc ~ Group, \n       data = Dabrowska.data,\n       ci = 0.99)\n\n\nQ11.9 Consider the code and its outputs below. We are now comparing the English grammar comprehension test scores of male and female native speakers of Romance languages only. Hence, we are now looking at a very small sample size comprising just five female and one male participants.\n\nDabrowska.Romance &lt;- Dabrowska.data |&gt; \n  filter(NativeLgFamily == \"Romance\")\n\ntable(Dabrowska.Romance$Gender)\n\n\nF M \n5 1 \n\ncohens_d(Grammar ~ Gender,\n         data = Dabrowska.Romance)\n\nCohen's d |        95% CI\n-------------------------\n-1.04     | [-3.24, 1.27]\n\n- Estimated using pooled SD.\n\n\nWhich statement(s) is/are true about the standardised effect size computed for the difference in the English comprehension grammar scores of male and female Romance L1 speakers in this very small dataset (n = 6)?\n\n\n\n\n\nAccording to Cohen's rule of thumb, the effect size is very small.\n\n\n\n\nAccording to Cohen's rule of thumb, the effect size is very large.\n\n\n\n\nThe 95% CI around Cohen's d includes zero, which means that there is no practical difference between the performance of male and female participants on the grammar test.\n\n\n\n\nWe cannot reject the null hypothesis that there is no difference between male and female performance on the grammar test.\n\n\n\n\nThe difference in mean grammar test scores across the two genders is not statistically significant at the Œ±-level of 0.05.\n\n\n\n\nThe effect size is negative, which means that the single male participant performed better on the grammar test than the five female participants.\n\n\n\n\nAll six participants did equally well at the Œ±-level of 0.05.\n\n\n\n\n\n\n\n\nü¶â Hover over the owl for a first hint.\n\n\n\n\nüê≠ Click on the mouse for a second hint.",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Infe`R`ential statistics</span>"
    ]
  },
  {
    "objectID": "11_Inferential.html#sec-Correlations",
    "href": "11_Inferential.html#sec-Correlations",
    "title": "11¬† InfeRential statistics",
    "section": "11.5 Correlation tests",
    "text": "11.5 Correlation tests\nSo far, we have looked at just one type of statistical significance test, the t-test, which we used to compare two mean values. This kind of t-test is used to test the association between a numeric variable (e.g.¬†test scores ranging from 0 to 100) and a binary categorical variable (e.g.¬†L1 vs.¬†L2 status).\nIn this section, we return to correlations ‚Äî a concept that we encountered in Section 10.2.5 when we generated and interpreted scatter plots. Recall that correlations capture the strength of the association between two numeric variables (e.g.¬†age and grammar test scores).\nAt the beginning of the chapter, we summarised the following observations based on our descriptive analyses of the DƒÖbrowska (2019) data:\n\nFor both L1 and L2 participants, there is a positive correlation between the number of years they were in formal education and their English grammar comprehension test scores: the longer they were in formal education, the better they did on the test (see Figure¬†11.3).\n\n\nDabrowska.data |&gt; \n  ggplot(mapping = aes(x = EduTotal, \n                       y = Grammar)) +\n  facet_wrap(~ Group) +\n  geom_point() +\n  geom_smooth(method = \"lm\",\n              se = FALSE) +\n  labs(x = \"Number of years in formal education\",\n       y = \"English grammar comprehension test scores\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure¬†11.3: Relationship between years in formal education and English grammar comprehension test scores for groups L1 and L2\n\n\n\n\n\nHow strong are the correlations visualised by the blue regression lines on Figure¬†11.3? And how likely is it that we might observe such correlations or stronger ones by chance alone? The first question is about the size of the effect, whilst the second is about its statistical significance.\nWe can answer both questions using the cor.test() function. This function also takes a formula as its first argument: the two numeric variables whose correlation we want to estimate come after the tilde (~) and the two variables are combined using the plus (+) operator:\n\ncor.test(formula = ~ EduTotal + Grammar,\n         data = L1.data)\n\n\n    Pearson's product-moment correlation\n\ndata:  EduTotal and Grammar\nt = 3.5821, df = 88, p-value = 0.0005581\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.1615747 0.5250334\nsample estimates:\n      cor \n0.3567294 \n\n\nAs with the t.test() (see Section 11.2), the output of the cor.test() function includes a t-statistic (t), the number of degrees of freedom (df; which here corresponds to the number of data points minus two), and a p-value. Immediately, we can see that the p-value is &lt;¬†0.05, which means that, for the L1 population, we can reject the null hypothesis that there is no correlation between the total number of years that participants spent in education and their English grammar comprehension test scores. But how strong is the correlation? To find out, we turn to the sample estimate, Pearson‚Äôs r: cor = 0.36. Like Cohen‚Äôs d, Pearson‚Äôs r is also a standardised effect size. It can range between -1 and +1.\n\nA correlation coefficient of 1 means that there is a perfect positive linear relationship between the two variables.\n\nExample: The relationship between the number of questions that a person correctly answered in a test and the percentage of questions that they got right.\n\n\nShow R code to simulate data with a perfect positive correlation.\n# First, let's imagine that 100 learners took a test with 10 questions. We simulate the number of questions that they answered correctly using the `sample()` function that generates random numbers, specifying that learners can get between zero and 10 questions right:\ncorrect.answers &lt;- sample(0:10, 100, replace = TRUE)\n\n# Next, we convert the number of correctly answered questions into the percentage of questions that they answered correctly:\naccuracy &lt;- correct.answers / 10 * 100\n\n# Finally, we compute the correlation coefficient between the number of  and the percentage of correctly answered questions:\ncor(correct.answers, accuracy)\n\n# The correlation coefficient (Pearson's r) equals 1 because the two variables are perfectly correlated with each other. If we know one variable, there is a simple mathematical formula that allows us to obtain the exact value of the other!\n\n\n\nA correlation coefficient of 0 means that there is no linear relationship between the two variables. Note that, in the real world, we will never find correlations of exactly zero, but rather very close to zero.\n\nExample: The relationship between two completely randomly generated strings of numbers.\n\n\nShow R code to simulate random data with a near-zero correlation\n# First, we set a seed to ensure that the outcome of our randomly generated number series are always exactly the same:\nset.seed(42)\n\n# Next, we generate two series of a thousand randomly generated numbers ranging between 0 and 100:\nx &lt;- sample(0:100, 1000, replace = TRUE)\ny &lt;- sample(0:100, 1000, replace = TRUE)\n\n# We can now compute the correlation coefficient. As expected, we find that it is very close to zero but not exactly zero:\ncor(x, y)\n\n# However, if we run a correlation test on our two randomly generated variables x and y, we find that a) the p-value is &gt;¬†0.05 and b) our 95% confidence interval includes 0. \ncor.test(x, y)\n\n# We can therefore conclude there is not enough evidence in our sample data to reject the null hypothesis of no correlation in the full population. This makes sense because we are testing the correlation of two independently, randomly generated strings of numbers that shouldn't have anything to do with each other!\n\n\n\nA correlation coefficient of -1 means that there is a perfect negative linear relationship between the two variables.\n\nExample: The relationship between the number of errors a person makes in a test and the percentage of questions that they got right.\n\n\nShow R code to simulate data with a perfectly negative correlation\n# First, let's imagine that 100 learners took a test with 20 questions. We simulate the number of errors that they each made:\nerrors &lt;- sample(0:20, 100, replace = TRUE)\n\n# Next, we convert the number of errors into the percentage of questions that they correctly answered:\naccuracy &lt;- ((20 - errors)/20) * 100\n\n# Finally, we compute the correlation coefficient between the number of errors and the percentage of correctly answered questions:\ncor(errors, accuracy)\n\n\n\n\nGoing back to the output of the cor.test() function above, we have a correlation coefficient of 0.36, which is positive, meaning that we are looking at a positive correlation. We already knew this from the direction of the regression line in the L1 panel of Figure¬†11.3. The question is now: is 0.36 a weak, medium or strong positive correlation? Again, it depends‚Ä¶ For some research questions in the language sciences, 0.36 may be considered a medium-sized correlation but, for others, it may be considered a small correlation‚Ä¶ As always, numbers alone do not suffice to draw conclusions: we need contextual information about research domain (see Plonsky & Oswald 2014).\nThe output of the cor.test() function also returned a 95% confidence interval around the correlation coefficient: [0.16, 0.53]. It does not straddle zero which is why our p-value was &lt;¬†0.05. That said, the lower bound of the interval corresponds to a very small correlation, suggesting that the correlation in the full L1 population may be considerably smaller than what we observed in our data (or quite a bit larger as demonstrated by the upper bound). In other words, there is quite a bit of uncertainty around the strength of this correlation coefficient because there is a lot variability in the data and we do not have a particularly large sample size.\n\n\n\n\n\n\nNoteHow to report\n\n\n\nTo summarise these results, we can write that, in this dataset, there is a positive and statistically significant correlation between the number of years that L1 participants reported spending in formal education and their receptive English vocabulary test scores, r¬†=¬†0.36, 95% CI [0.16, 0.53], df¬†=¬†88, p¬†=¬†0.0005581.\n\n\nHow about L2 speakers? From the L2 panel of Figure¬†11.3, we can see that the Grammar scores of L2 participants are, on average, much further away from the regression line than in the L1 panel, suggesting that it summarises the data far less well. Indeed, when we run the correlation test on the L2 data, we not only find that the correlation coefficient is much smaller (0.13), the 95% confidence interval around this coefficient [-0.11, 0.36] includes zero:\n\ncor.test(formula = ~ EduTotal + Grammar,\n         data = L2.data)\n\n\n    Pearson's product-moment correlation\n\ndata:  EduTotal and Grammar\nt = 1.0936, df = 65, p-value = 0.2782\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.1093254  0.3629045\nsample estimates:\n      cor \n0.1344131 \n\n\nAlong with a p-value of &gt;¬†0.05, this suggests that we do not have enough evidence to reject the null hypothesis of no correlation between years in formal education and English grammar comprehension in the L2 population.\n\n\n\n\n\n\nNoteHow to report\n\n\n\nTo summarise these results, we can write that, in this dataset, there is a small positive, but non-significant correlation between the number of years that L2 participants reported spending in formal education and their receptive English vocabulary test scores, r¬†=¬†0.13, 95% CI [-0.11, 0.36], df¬†=¬†65, p¬†=¬†0.2782.\n\n\nConfidence intervals around correlation coefficients can be difficult to interpret as numbers. The good news is that they can easily be visualised using the {ggplot2} library. In Section 10.2.5, we used the argument se = FALSE inside the geom_smooth() function. If, instead, we set it to TRUE, 95% confidence intervals will be displayed as grey bands around the regression lines. To change the Œ±-level, you will need to change the default value of the level argument.\n\nDabrowska.data |&gt; \n  ggplot(mapping = aes(x = EduTotal, \n                       y = Grammar)) +\n  facet_wrap(~ Group) +\n  geom_point() +\n  geom_smooth(method = \"lm\",\n              se = TRUE,\n              level = 0.95) +\n  labs(x = \"Number of years in formal education\",\n       y = \"English grammar comprehension test scores\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure¬†11.4: Relationship between years in formal education and English grammar comprehension test scores for groups L1 and L2 with 95% confidence bands\n\n\n\n\n\nThe interpretation of the grey bands is as follows: if it‚Äôs possible to draw a horizontal (flat) line that stays within the 95 % confidence band, it is very likely that there is no statistically significant correlation between the two numeric variables displayed on the plot at the Œ±-level of 0.05.\nAs illustrated in Figure¬†11.5, it is impossible to draw such a line in the L1 panel, which is why we reject the null hypothesis of no correlation between years spent in formal education and grammar comprehension test scores for L1 speakers. We conclude that this correlation is significantly different from zero. By contrast, it is perfectly possible to draw such a horizontal line in the L2 panel, which is why we must conclude that, at the Œ±-level of 0.05, we do not have enough evidence to reject the null hypothesis of no correlation in the L2 population. The observed correlation is not statistically significantly different from zero.\n\n\n\n\n\n\n\n\nFigure¬†11.5: The dotted lines are compatible with the null hypothesis of no correlation.\n\n\n\n\n\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nGenerate a facetted scatter plot (similar to Figure¬†11.4) visualising the correlation between age (Age) and English receptive vocabulary test scores (Vocab) for L1 and L2 participants in two separate panels. Add a grey band visualising 95% confidence intervals around the regression lines.\nQ11.10 Looking at your plot only, which of these statements can you confidently say is/are true?\n\n\n\n\n\nEven though one correlation is positive and the other negative, in absolute values, the correlations are equally strong.\n\n\n\n\nFor both the L1 and L2 groups, the correlations are positive.\n\n\n\n\nThere is more variability around the correlation estimate of the L1 than the L2 group.\n\n\n\n\nThere is more variability around the correlation estimate of the L2 than the L1 group.\n\n\n\n\nIn the L1 group, the correlation is positive, whereas in the L2 group it is negative.\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n\n\nShow code to generate the plot needed to answer Q11.10.\nDabrowska.data |&gt; \n   ggplot(mapping = aes(x = Age, \n                        y = Vocab)) +\n     geom_point() +\n     facet_wrap(~ Group) +\n     geom_smooth(method = \"lm\", \n                 se = TRUE) +\n  theme_bw()\n\n\nNow use the cor.test() function to find out how strong the correlations are in both the L1 and the L2 groups. Use the default Œ±-level of 0.05.\nQ11.11 Based on the outputs of the cor.test() function, which of these statements can you confidently say is/are true? Note that, in this question and the next one, values are reported to two decimal places.\n\n\n\n\n\nThe correlation of Age and Vocab scores in the L1 sample is 0.37.\n\n\n\n\nIn absolute values, the correlation between Age and Vocab scores is stronger in the L1 than in the L2 sample.\n\n\n\n\nThe correlation of Age and Vocab scores in the L2 population is -0.20.\n\n\n\n\nBased on the data available to us and an Œ±-level of 0.05, we cannot reject the null hypothesis of no correlation between Age and Vocab scores for the L2 population.\n\n\n\n\nAssuming that the null hypothesis of no correlation between Age and Vocab scores in the L1 population is true, there is only a 0.03% chance of observing a correlation of 0.37 or larger in a sample of this size.\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n\n\nShow code to answer Q11.11.\ncor.test(formula = ~ Age + Vocab,\n         data = L1.data)\n\ncor.test(formula = ~ Age + Vocab,\n         data = L2.data)\n\n\nConsider the code and its output below. Figure¬†11.6 visualises the correlation between L2 speakers‚Äô English receptive vocabulary test scores (Vocab) and their age of arrival in the UK (Arrival).\n\nL2.data |&gt; \n   ggplot(mapping = aes(x = Arrival, \n                        y = Vocab)) +\n     geom_point() +\n     geom_smooth(method = \"lm\", \n                 se = TRUE) +\n  theme_bw()\n\n\n\n\n\n\n\nFigure¬†11.6: Correlation between L2 speakers‚Äô English vocabulary test scores and their age of arrival\n\n\n\n\n\nQ11.12 Run a correlation test on the correlation visualised in Figure¬†11.6. How strong is the correlation (to two decimal places)?\n\n\n\n\n\n\n\n\n\n\nü¶â Hover over the owl for a hint.\n\n\n\n\n\n\nShow code to answer Q11.12.\ncor.test(formula = ~ Vocab + Arrival,\n         data = L2.data)\n\n\nQ11.13 How likely are we to observe such a strong correlation or an even stronger one in a sample of this size, if there is actually no correlation in the full L2 population?\n\n\n\n\n\nabout 1%\n\n\n\n\nabout 0.01%\n\n\n\n\nabout 10%\n\n\n\n\nIt's impossible to tell because the correlation is not statistically significant.\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Infe`R`ential statistics</span>"
    ]
  },
  {
    "objectID": "11_Inferential.html#sec-Assumptions",
    "href": "11_Inferential.html#sec-Assumptions",
    "title": "11¬† InfeRential statistics",
    "section": "11.6 Assumptions of statistical tests",
    "text": "11.6 Assumptions of statistical tests\nA crucial aspect that we have glossed over so far in this chapter is that the results of statistical tests, such as t-tests and correlation tests, can only be considered accurate if the test‚Äôs underlying assumptions are met.\n\n11.6.1 Randomisation\nThe only assumption that we have mentioned so far is that of random sampling (Section 11.1). In practice, this assumption is rarely met in the language sciences. For instance, the L1 and L2 participants recruited for DƒÖbrowska (2019) were not randomly drawn from the entire adult UK population. DƒÖbrowska (2019: 5-6) reports that the participants ‚Äúwere recruited through personal contacts, church and social clubs, and advertisements in local newspapers‚Äù. Such transparency over the sampling procedure is crucial to interpreting the results of a study.\n\nWhen the assumption of random sampling is not met, inferences to the population become difficult. In this case, researchers should describe the sample and population in sufficient detail to justify that the sample was at least representative of the intended population (Wilkinson and APA Task Force on Statistical Inference, 1999). If such a justification is not made, readers are left to their own interpretation as to the generalizability of the results. (Nimon 2012: 1)\n\n\n\n11.6.2 Independence\nAnother crucial assumption for both tests covered in this chapter is that the data points be independent of each other. We assumed that this is the case with the DƒÖbrowska (2019) data because it only has one observation per participant (although interdependencies may occur at other levels, e.g.¬†when several participants come from the same school, work place, or neighbourhood). If, however, we had multiple observations per participant because they completed the tests twice, say once in the morning and once in the evening, and then entered both the morning and the evening data into a single statistical test, our data would violate the assumption of independence and the results of the test would be inaccurate. When our data violate the assumption of independence, we cannot use statistical tests like t-tests. Instead, we must turn to statistical methods that allow us to model these interdependencies in the data. In the language sciences, this is most commonly achieved using mixed-effects models (see e.g. Winter 2020: Ch. 14-15).\n\n\n11.6.3 Normality\nFor many inferential statistical tests commonly reported in the language sciences, it is also assumed that the population data are normally distributed (see Section 8.2.3). This assumption is often quite reasonable, because many real-world quantities are normally distributed. This is why we typically say that this assumption can be relaxed if we have more than 30 observations per group.\nHowever, there are some things in the world that are inherently non-normally distributed. For instance, word frequencies in a text or a collection of texts (i.e.¬†a corpus) are never normally distributed: a handful of words occur extremely frequently (in written English typically: the, of, a, in, to, etc.), some words are fairly frequent, but the vast majority are very infrequent. Reaction times is another example of a kind of variable that hardly ever meets the criterion of normality. One way to deal with highly skewed distributions like word frequencies and reaction times is to apply transformations to these variables before attempting to do any inferential statistics (see Winter 2020: Chapter 5).\nOf course, we cannot check if the population data meet the assumption of normality because we do not have the entire population data (and if we did, we wouldn‚Äôt need inferential statistics!) so the best we can do is check if our data are normally distributed. This is best achieved visually. For instance, before conducting a t-test comparing the mean difference in Grammar scores in the L1 and L2 groups using the t.test() function, we should first visualise the two distributions of Grammar scores to check that they are approximately normally distributed. Figure¬†11.7 shows that this is clearly not the case!\n\n\nShow code to generate plot.\nDabrowska.data |&gt; \n  ggplot(mapping = aes(x = Grammar,\n                       fill = Group)) +\n  geom_density(alpha = 0.7) +\n  scale_fill_viridis_d() +\n  labs(x = \"English grammar comprehension test scores\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure¬†11.7: Density plot comparing English grammar comprehension test scores between groups L1 and L2\n\n\n\n\n\nWe are dealing here with non-normal or non-parametric data, hence we need a non-parametric version of the t-test: the Wilcoxon test (also known as the Mann-Whitney test or the U-test):\n\nwilcox.test(Grammar ~ Group, \n            data = Dabrowska.data)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  Grammar by Group\nW = 3640, p-value = 0.026\nalternative hypothesis: true location shift is not equal to 0\n\n\nThe output of the wilcox.test() function tells us that there is a 2.6% chance of obtaining our observed difference in median Grammar scores in an L1 and L2 sample of this size or an even larger difference under the assumption of the null hypothesis (of no difference across the two groups). Hence, if we had previously defined our Œ±-level as 0.05, we reject the null hypothesis. If, however, we had chosen a stricter Œ±-level of 0.01, we fail to reject the null hypothesis.\nThe Wilcoxon test does not make any assumptions about the distributions of the population data, which means that it can be used for all kinds of continuous variables including word frequencies. However, there is a drawback: it is usually less powerful than the standard t-test. Using it, we are more likely to report a false negative (i.e.¬†to fail to report a real difference in two means), which is why a transformation may be wiser (for more about transformations, see Winter 2020: Chapter 5).\n\n\n11.6.4 Linearity and outliers\nAll statistical significance tests are sensitive to outliers. Hence, it is important to identify any outliers (e.g.¬†by plotting your data as part of preliminary data exploration) and to carefully consider the extent to which they may influence the results of your analysis.\nAnother important assumption of Pearson‚Äôs correlation coefficient (r) is that the continuous variables are linearly related. Again, this is best perceived visually: if you plot the two variables against one another in a scatter plot (see Section 10.2.5), the points should fall roughly along a single, straight line. When the true association is non-linear (e.g.¬†curvilinear), Pearson‚Äôs r will underestimate the strength of that association as it only captures the linear component of an association (Tabachnick & Fidell 2014: 117).\nIn 1973, the statistician Frank Anscombe put together a small dataset with four pairs of variables (x and y) with the aim of illustrating the necessity to visualise data and not rely solely on statistics. The dataset is included in base R so we can access it by calling its name without downloading or installing anything:\n\nanscombe\n\n   x1 x2 x3 x4    y1   y2    y3    y4\n1  10 10 10  8  8.04 9.14  7.46  6.58\n2   8  8  8  8  6.95 8.14  6.77  5.76\n3  13 13 13  8  7.58 8.74 12.74  7.71\n4   9  9  9  8  8.81 8.77  7.11  8.84\n5  11 11 11  8  8.33 9.26  7.81  8.47\n6  14 14 14  8  9.96 8.10  8.84  7.04\n7   6  6  6  8  7.24 6.13  6.08  5.25\n8   4  4  4 19  4.26 3.10  5.39 12.50\n9  12 12 12  8 10.84 9.13  8.15  5.56\n10  7  7  7  8  4.82 7.26  6.42  7.91\n11  5  5  5  8  5.68 4.74  5.73  6.89\n\n\nKnown as the Anscombe Quartet, the dataset‚Äôs four pairs of variables have exactly the same correlation coefficient (when rounded to two decimal places):\n\ncor(anscombe$x1, anscombe$y1) |&gt; \n  round(2)\n\n[1] 0.82\n\ncor(anscombe$x2, anscombe$y2) |&gt; \n  round(2)\n\n[1] 0.82\n\ncor(anscombe$x3, anscombe$y3) |&gt; \n  round(2)\n\n[1] 0.82\n\ncor(anscombe$x4, anscombe$y4) |&gt; \n  round(2)\n\n[1] 0.82\n\n\nHowever, when we visualise the data in scatter plots, it turns out that the relationships between each pair are completely different!\n\n\nSee {ggplot2} code to generate plots.\nPair1 &lt;- ggplot(data = anscombe, \n       mapping = aes(x = x1, y = y1)) +\n  geom_point(colour = \"blue\",\n             size = 2,\n             alpha = 0.8) +\n  labs(title=\"Pair 1\") +\n  geom_smooth(method = \"lm\", \n              colour = \"blue\",\n              se = FALSE) +\n  theme_bw()\n\nPair2 &lt;- ggplot(data = anscombe, \n       mapping = aes(x = x2, y = y2)) +\n  geom_point(colour = \"orange\",\n             size = 2,\n             alpha = 0.8) +\n  labs(title=\"Pair 2\") +\n  geom_smooth(method = \"lm\",\n              colour = \"orange\",\n              se = FALSE) +\n  theme_bw()\n\nPair3 &lt;- ggplot(data = anscombe, \n       mapping = aes(x = x3, y = y3)) +\n  geom_point(colour = \"darkgreen\",\n             size = 2,\n             alpha = 0.8) +\n  labs(title=\"Pair 3\") +\n  geom_smooth(method = \"lm\",\n              colour = \"darkgreen\",\n              se = FALSE) +\n  theme_bw()\n\nPair4 &lt;- ggplot(data = anscombe, \n       mapping = aes(x = x4, y = y4)) +\n  geom_point(colour = \"darkred\",\n             size = 2,\n             alpha = 0.8) +\n  labs(title=\"Pair 4\") +\n  geom_smooth(method = \"lm\", \n              colour = \"darkred\",\n              se = FALSE) +\n  theme_bw()\n\n#install.packages(\"patchwork\")\nlibrary(patchwork)\nPair1 + Pair2 + Pair3 + Pair4\n\n\n\n\n\n\n\n\nFigure¬†11.8: Four plots showing the relationship between pairs of variables\n\n\n\n\n\nFrom Figure¬†11.8, we can immediately see that the relationship between the two variables in Pair 2 is non-linear, which makes the yellow linear regression line nonsensical. The data visualisations for Pairs 3 and 4 illustrate how a single outlier can either create an illusion of a correlation that does not exist (Pair 4) or overestimate one that does exist but is probably considerably weaker than Pearson‚Äôs r would lead us to believe (Pair 3).\nThe assumption of linearity is relevant to many widely used statistical methods ‚Äì notably linear regression models (see Chapter 12 and Chapter 13) ‚Äì which rely on Pearson‚Äôs correlation coefficients. In practice, researchers usually assume linearity unless there is a strong theoretical reason to expect a non‚Äëlinear pattern (Cohen‚ÄØet‚ÄØal.‚ÄØ2003). However, this assumption should always be checked. The most straightforward way to do this is to visualise the data.\n\n\n\n\n\n\nNoteNon-parametric correlations\n\n\n\nFor an in-depth explanation of non-parametric correlation coefficients (Spearman‚Äôs œÅ and Kendall‚Äôs œÑ) and statistical significance tests with examples from the language sciences, I recommend reading Levshina (2015: Chapter¬†6).\n\n\n\n\n11.6.5 Homogeneity of variance and homoscedasticity\nWhen conducting a Student‚Äôs t-test, the variances of the samples should be constant, or homogeneous. This is referred to as the assumption of homogeneity of variance. It means that the variances of the groups entered in a test should be roughly the same.\nAgain, this assumption is best examined visually. For example, we can generate a boxplot to check that the variance of the two groups that we want to compare with a t-test are roughly equal (see Figure¬†11.9).\n\n\nSee code to generate plot.\nggplot(Dabrowska.data,\n       aes(x = Group, y = Grammar)) +\n  geom_boxplot() +\n  theme_bw()\n\n\n\n\n\n\n\n\nFigure¬†11.9: Comparison of grammar scores between L1 and L2 groups\n\n\n\n\n\nAs we already saw in Section 11.6.3, the Grammar scores of the L2 group are not normally distributed; hence, we can also see from Figure¬†11.9 that there is much more variance in the L2 data than there is in the L1. In other words, we have heterogeneity of variance. One way to deal with this is to conduct a non-parametric test instead: Wilcox‚Äôs test does not assume homogeneity of variance. However, if your data meet the criterion of normality and only fails to meet that homogeneity of variance, you can still conduct a parametric t-test because the standard t.test() function in R includes Welch‚Äôs adjustment to correct for unequal variances (Field, Miles & Field 2012: 373).\nA related assumption is made in correlation tests and linear regression and is called the assumption of homoscedasticity (see also Section 12.4.3). To check the assumption of homoscedasticity for a correlation, we can visualise the variance (or variability) around the correlation (the linear regression line) with the help of a scatter plot. In Figure¬†11.10, we can see that this variance is much larger for higher Vocab scores than for low to medium scores.\n\n\nSee code to generate plot.\nDabrowska.data |&gt; \n  filter(Group == \"L1\") |&gt; \n  ggplot(aes(x = Vocab, y = EduTotal)) +\n  geom_point() +\n  geom_smooth(method = \"lm\",\n              se = FALSE) +\n  theme_bw()\n\n\n\n\n\n\n\n\nFigure¬†11.10: The relationship between vocabulary and total education years\n\n\n\n\n\nIn other words, these data do not meet the assumption of homoscedasticity. In this case, we should therefore use a non-parametric correlation statistic, such as Spearman‚Äôs œÅ (‚Äòrho‚Äô) and Kendall‚Äôs œÑ (‚Äòtau‚Äô). As we are looking at a relatively small dataset here (only L1 speakers) and some speakers performed equally well on the Vocab test (in other words, we have some ties in the ranks), Kendall‚Äôs œÑ is recommended:\n\ncor.test(formula = ~ Vocab + EduTotal,\n         data = L1.data,\n         method = \"kendall\")\n\n\n    Kendall's rank correlation tau\n\ndata:  Vocab and EduTotal\nz = 3.5406, p-value = 0.0003992\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n      tau \n0.2761618 \n\n\nIt returns a correlation coefficient œÑ of 0.28 and a p-value of &lt;¬†0.05, which allows us to reject the null hypothesis of no association between Vocab and EduTotal scores among L1 English speakers.\n\n\n\n\n\n\nCautionOptional (fun) task ü¶ñ\n\n\n\n\n\nInspired by the Anscombe Quartet, Matejka & Fitzmaurice (2017) created a set of 12 pairs of variables that have the same descriptive statistics as the data that produce a scatter plot representing a tyrannosaurus (see Figure¬†11.11).\n\n\n\n\n\n\n\n\nFigure¬†11.11: Scatter plot depicting a tyrannosaurus (originally created by Alberto Cairo)\n\n\n\n\n\n1. Install and load the datasauRus package to access the R data object datasaurus_dozen:\n\ninstall.packages(\"datasauRus\")\nlibrary(datasauRus)\n\ndatasaurus_dozen\n\n2. Run the following code to compare the descriptive statistics of all three datasets within the datasaurus_dozen. The fourth set, dino, is the one visualised in Figure¬†11.11. Note that there is a very small, negative correlation between all pairs of variables of -0.0656.\n\ndatasaurus_dozen |&gt; \n  group_by(dataset) |&gt; \n  summarise(\n    mean_x = mean(x), \n    mean_y = mean(y), \n    std_dev_x = sd(x), \n    std_dev_y = sd(y), \n    corr_x_y = cor(x, y))\n\n# A tibble: 13 √ó 6\n   dataset    mean_x mean_y std_dev_x std_dev_y corr_x_y\n   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 away         54.3   47.8      16.8      26.9  -0.0641\n 2 bullseye     54.3   47.8      16.8      26.9  -0.0686\n 3 circle       54.3   47.8      16.8      26.9  -0.0683\n 4 dino         54.3   47.8      16.8      26.9  -0.0645\n 5 dots         54.3   47.8      16.8      26.9  -0.0603\n 6 h_lines      54.3   47.8      16.8      26.9  -0.0617\n 7 high_lines   54.3   47.8      16.8      26.9  -0.0685\n 8 slant_down   54.3   47.8      16.8      26.9  -0.0690\n 9 slant_up     54.3   47.8      16.8      26.9  -0.0686\n10 star         54.3   47.8      16.8      26.9  -0.0630\n11 v_lines      54.3   47.8      16.8      26.9  -0.0694\n12 wide_lines   54.3   47.8      16.8      26.9  -0.0666\n13 x_shape      54.3   47.8      16.8      26.9  -0.0656\n\n\n3. Run the following code to visualise the relationships between the other 12 pairs of variables.\n\nggplot(datasaurus_dozen, \n       aes(x = x, y = y, colour = dataset)) + \n  geom_point() + \n  facet_wrap(~ dataset, ncol = 3) +\n  theme_bw() + \n  theme(legend.position = \"none\") + \n  labs(x = NULL,\n       y = NULL)\n\n4. Add a geom_ layer (see Section 10.1.2) to the following {ggplot2} code to add a blue linear regression line in all 13 panels.\n\n\nView code to complete Part 4 of the task.\nggplot(datasaurus_dozen, \n       aes(x = x, y = y, colour = dataset)) + \n  geom_point() + \n  facet_wrap(~ dataset, ncol = 3) +\n  geom_smooth(method = \"lm\",\n              se = FALSE,\n              colour = \"blue\") +\n  theme_bw() + \n  theme(legend.position = \"none\") + \n  labs(x = NULL,\n       y = NULL)  \n\n\nClearly, if we had only used correlation statistics to describe the relationships between these 13 pairs of variables, we would have missed some literally dinosaur-sized patterns! :exploding-head:\n\n\n\n\n\n\n\n\nHex sticker of the {datasauRus} package\n\n\n![Remember that the best way to check test assumptions is to visualise your data! (artwork by Allison Horst (CC BY 4.0)(images/AHorst_NotNormal.png){#fig-NotNormal fig-alt=‚ÄúCartoon of a normal distribution looking skeptically at an excited looking bimodal / negatively skewed distribution. The first says to the second,‚Äùyou‚Äôre not normal.‚Äù‚Äù width=‚Äú448‚Äù}",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Infe`R`ential statistics</span>"
    ]
  },
  {
    "objectID": "11_Inferential.html#sec-pHacking",
    "href": "11_Inferential.html#sec-pHacking",
    "title": "11¬† InfeRential statistics",
    "section": "11.7 Multiple testing problem",
    "text": "11.7 Multiple testing problem\nIn Section 11.3 we saw that conducting a statistical test with an Œ±-level of 0.05 means that we accept a 5% risk of falsely rejecting the null hypothesis when it is actually true. Falsely rejecting the null hypothesis leads to a false positive result, also referred to as making a Type I error. It is crucial to understand that if we perform multiple tests on the same data, we dramatically increase the risk of reporting such false positive results. This is known as the multiple testing or multiple comparisons problem.\nImagine that we want to test 20 independent null hypotheses on a single dataset using Œ±¬†=¬†0.05 as our significance threshold. Even if all null hypotheses are actually true, the probability of obtaining at least one significant result, i.e.¬†at least one p-value &lt;¬† 0.05, by chance is equal to 64%, as shown below:\n\n1 - (1 - 0.05)^20\n\n[1] 0.6415141\n\n\nThus, having conducted 20 independent tests, we have a 64% chance of finding at least one statistically significant result purely by chance. As the number of tests increases, this probability approaches certainty (see also Baayen 2008: 106-107). With 100 tests, we reach 99%!\n\n1 - (1 - 0.05)^100\n\n[1] 0.9940795\n\n\nTo avoid reporting false positive results, it is therefore recommended that we correct our Œ±-level to account for the number of tests performed on the data. The simplest (but also most conservative) approach to do so is called the Bonferroni correction. It consists in adjusting our chosen Œ±-level by dividing it by the number of tests that we are conducting on the data. Hence, if we want to use 0.05 as our significance level and conduct 20 independent tests on the same data, we divide 0.05 by 20:\n\n0.05 / 20\n\n[1] 0.0025\n\n\nThis means that we would now only report the outcome of statistical test as ‚Äústatistically significant‚Äù if its p-value is¬†&lt;¬†0.0025. Other correction methods for multiple testing exist. Holm‚Äôs method, for example, is a popular, slightly less conservative alternative to the Bonferroni correction. The base R function p.adjust() can be used to automatically adjust p-values using different methods including Bonferroni‚Äôs and Holm‚Äôs.\nThe following line of code creates an R object that contains seven p-values. These are the p-values that we obtained from the seven independent statistical tests that we performed on the DƒÖbrowska (2019) data as part of this chapter:\n\np_values &lt;- c(1.956e-05, 0.000007032, 0.5525, 0.0005581, 0.2782, 0.026, 0.0003992)\n\nAt our chosen Œ±-level of 0.05, five of these p-values were statistically significant, leading us to reject the corresponding five null hypotheses. However, if we apply Holm‚Äôs correction using the p.adjust() function, we find that we can only reject four of these null hypotheses.\n\np_values.adjusted &lt;- p.adjust(p_values, method = \"holm\")\n\np_values.adjusted\n\n[1] 1.1736e-04 4.9224e-05 5.5640e-01 2.2324e-03 5.5640e-01 7.8000e-02 1.9960e-03\n\n\nIt can be difficult to interpret numbers displayed in scientific notation, so here they are in standard notation:\n\nformat(p_values.adjusted, scientific = FALSE)\n\n[1] \"0.000117360\" \"0.000049224\" \"0.556400000\" \"0.002232400\" \"0.556400000\"\n[6] \"0.078000000\" \"0.001996000\"\n\n\nTo find out which of these corrected p-values are below 0.05, we can use the &lt; operator like this:\n\np_values.adjusted &lt; 0.05\n\n[1]  TRUE  TRUE FALSE  TRUE FALSE FALSE  TRUE\n\n\nIt is crucial to understand that every p-value represents a probabilistic finding, not a definitive statement about reality. A p-value of 0.03 does not mean that there is a 97% chance that the alternative hypothesis is true. Rather it indicates that, if the null hypothesis were true and we were to run this experiment many times, we would observe data at least as extreme as in our sample only 3% of the time. Given the probabilistic nature of statistical inference and the multiple testing problem, the replication of findings across independent studies is essential for building scientific knowledge (see Section 14.2). No single study, regardless of its sample size or p-value, can ever provide us with definitive evidence.\nBecause of the multiple testing problem, only reporting statistically significant results and failing to report the number of tests conducted to find them is considered a Questionable Research Practice (QRP). It is one way to p-hack results, along with not correcting for multiple comparisons. A great resource for definitions and links to further literature on good research practices is the FORRT Glossary ‚Äî a community-sourced glossary of Open Scholarship terms (Parsons et al. 2022). Below is the FORRT Glossary‚Äôs entry for QRPs.\n\nQuestionable Research Practices or Questionable Reporting Practices (QRPs)\nAlso available in: Arabic | German |\nDefinition: A range of activities that intentionally or unintentionally distort data in favour of a researcher‚Äôs own hypotheses - or omissions in reporting such practices - including; selective inclusion of data, hypothesising after the results are known (HARKing), and p-hacking. Popularized by John et al.¬†(2012).\nRelated terms: Creative use of outliers, Fabrication, File-drawer, Garden of forking paths, HARKing, Nonpublication of data, p-hacking, p-value fishing, Partial publication of data, Post-hoc storytelling, Preregistration, Questionable Measurement Practices (QMP), Researcher degrees of freedom, Reverse p-hacking, Salami slicing\nReference: Banks et al.¬†(2016); Fiedler and Schwartz (2016); Hardwicke et al.¬†(2014); John et al.¬†(2012); Neuroskeptic (2012); Sijtsma (2016); Simonsohn et al.¬†(2011)\nDrafted and Reviewed by: Mahmoud Elsherif, Tamara Kalandadze, William Ngiam, Sam Parsons, Mariella Paul, Eike Mark Rinke, Timo Roettger, Fl√°vio Azevedo\n\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nThe dataset from DƒÖbrowska (2019) includes the results of numerous additional tests that we have not yet examined.\nIn this task, we imagine that a friend of yours has decided to test whether, on average, male and female speakers performed equally well on four additional English grammar tests. The four variables that he selected correspond to participants‚Äô scores on English tests assessing participants‚Äô mastery of:\n\nthe active voice (Active)\nthe passive voice (Passive)\npost-modified subjects (Postmod)\nsubject relatives (SubRel)\n\nYour friend conducted the following four t-tests to find out whether he could reject the the null hypothesis of no difference in the average performance of male and female English speakers in these four grammar tests. He chose 0.05 as his significance threshold and, on the basis of these four tests, concluded that he could reject the null hypothesis in two out of four tests: the one concerning the active voice and the one about subject relatives.\n\nt.test(formula = Active ~ Gender, \n       data = Dabrowska.data)\n\nt.test(formula = Passive ~ Gender, \n       data = Dabrowska.data)\n\nt.test(formula = Postmod ~ Gender, \n       data = Dabrowska.data)\n\nt.test(formula = SubRel ~ Gender, \n       data = Dabrowska.data)\n\nQ11.14 Your friend tells you that, based on the results of the two significant t-tests, he has decided to write a term paper focussing on how men have a better understanding of the active voice and subject relative constructions than women. You warn him that this approach reminds you of a questionable research practice (QRP) that you read about in the FORRT glossary. Which one?\n\n\n\n\n\nCreative use of outliers\n\n\n\n\nSalami slicing\n\n\n\n\nOptional stopping\n\n\n\n\nHARKing\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\nQ11.15 You explain to your friend that, if he wants to conduct four separate tests on the same dataset, he needs to correct the p-values for multiple testing. Which reason(s) can you use to explain this to your friend?\n\n\n\n\n\nRunning multiple tests on the same dataset increases your chances of obtaining a false positive result.\n\n\n\n\nThe sample size gets split up when you conduct multiple tests, which results in less accurate results.\n\n\n\n\nEven though you chose an Œ±-level of 0.05 for each test, your overall risk of erroneously rejecting the null hypothesis in at least one of these four tests is actually much higher than 5%.\n\n\n\n\nRunning multiple tests on the same dataset makes each individual test weaker and less reliable.\n\n\n\n\nMultiple tests on the same dataset make it harder to detect real differences when they actually exist.\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\nQ11.16 How high is your friend‚Äôs risk of erroneously rejecting the null hypothesis in at least one of his four tests?\n\n\n\n\n\n5%\n\n\n\n\n7%\n\n\n\n\n19%\n\n\n\n\n20%\n\n\n\n\nalmost 100%\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n\n\nShow sample code to answer Q11.16\n1 - (1 - 0.05)^4 \n\n\nQ11.17 Use Holm‚Äôs correction to correct the four p-values that your friend obtained to account for the fact that he conducted four tests on the same dataset. After correction, which null hypotheses can be rejected at Œ±-level¬†=¬†0.05?\n\n\n\n\n\nAll four of them.\n\n\n\n\nOnly the one concerning the passive voice.\n\n\n\n\nOnly the one concerning the active voice.\n\n\n\n\nThe one concerning the active voice and the one concerning subject relatives.\n\n\n\n\nNone of them.\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n\n\nShow sample code to answer Q11.17\n# Running the four t-tests and saving the output as four R objects to the local environment:\nactive.ttest &lt;- t.test(formula = Active ~ Gender, \n       data = Dabrowska.data)\n\npassive.ttest &lt;- t.test(formula = Passive ~ Gender, \n       data = Dabrowska.data)\n\npostmod.ttest &lt;- t.test(formula = Postmod ~ Gender, \n       data = Dabrowska.data)\n\nsubjrel.ttest &lt;- t.test(formula = SubRel ~ Gender, \n       data = Dabrowska.data)\n\n# Extracting just the p-values from the test outputs\np.values &lt;- c(active.ttest$p.value, passive.ttest$p.value, postmod.ttest$p.value, subjrel.ttest$p.value)\n\n# The original p-values:\np.values\n\n# Applying Holm's correction to the p-values\np_values.adjusted &lt;- p.adjust(p.values, method = \"holm\")\n\n# The adjusted p-values in scientific notation:\np_values.adjusted\n\n# Now only the first p-value is below 0.05:\np_values.adjusted &lt; 0.05\n\n\n¬†\n\n\n\n\n\n\n\n\n\nNoteIn search of the truth‚Ä¶\n\n\n\nInferential statistics, even if used correctly, when all test assumptions are met and p-values corrected for multiple comparisons, tell us nothing about the validity or reliability of our results. Fancy statistics cannot fix inaccurate measurements or inconsistent annotations! In her study, Ewa DƒÖbrowska largely relied on tests that had been tested for validity and reliability in previous studies, which is why we trust that these test instruments mostly do measure what they claim to measure (which makes them valid) and that the results that they produce are consistent across participants (which makes them reliable). However, if this is not the case, we need to be extremely careful about the claims that we are making based on such data!\nRecall how we observed that, for both L1 and L2 participants, there was a positive correlation between the number years they were in formal education and their English grammar comprehension test scores: the longer they were in formal education, the higher their test scores. Even if we show that, post-adjustment for multiple testing, this correlation is statistically significant at a conventional level of significance, this result could well be an artefact of our measuring instrument: perhaps the participants who spent less time in formal education simply have less practice completing academic-style tests. Maybe, as a result, they did not fully understand the instructions or were unable to complete the test within the given time. Not being good at completing this test, however, may not be reflective of how well they can actually understand complex grammatical structures in English because understanding language typically doesn‚Äôt happen in artificial test contexts!",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Infe`R`ential statistics</span>"
    ]
  },
  {
    "objectID": "11_Inferential.html#whats-next",
    "href": "11_Inferential.html#whats-next",
    "title": "11¬† InfeRential statistics",
    "section": "11.8 What‚Äôs next?",
    "text": "11.8 What‚Äôs next?\nThis chapter has given you the keys to understanding the basics of inferential statistics following the frequentist null hypothesis significance testing (NHST) framework. We looked at how we can use t-tests and correlation tests to infer information about a population based on a random sample from the population. We introduced p-values, standardised effect sizes, and confidence intervals. These are complex concepts that take time to understand. Frequentist inferential statistics can be very powerful and useful, but it isn‚Äôt intuitive. Most people will need to read this chapter and other resources (see recommended readings) several times to really get to grips with these concepts.\nIn the following two chapters, we will move from single statistical tests to multiple linear regression models. In Chapter 12, we will first see how t-tests and correlation tests can be understood and computed as simple linear regression models before exploring the potential of multiple linear regression models in Chapter 13. The latter allow us to quantify and test the effects of several variables in a single model. Multiple linear regression naturally handles multiple predictors simultaneously, thus providing more nuanced insights into the contributions of each variable and their potential interactions.\n\n\n\n\n\n\nTipRecommended readings üìö\n\n\n\n\n√áetinkaya-Rundel, Mine & Johanna Hardin. 2021. Foundations of inference. In Introduction to Modern Statistics 2e. Open Access online book: https://openintro-ims.netlify.app/foundations-of-inference\nJan√©, Matthew B., Qinyu Xiao, Siu Kit Yeung, Flavio Azevedo, Mattan S. Ben-Shachar, Aaron R Caldwell, Denis Cousineau, et al.¬†2024. Guide to effect sizes and confidence intervals. An Open Educational Resource: https://doi.org/10.17605/OSF.IO/D8C4G.\nLakens, Dani√´l. 2022. Improving Your Statistical Inferences. https://doi.org/10.5281/ZENODO.6409077. An Open Educational Resource: https://lakens.github.io/statistical_inferences/\n\n\n\n\nCheck your progress üåü\nWell done! You have successfully completed this chapter introducing the complex topic of inferential statistics. You have answered 0 out of 17 questions correctly.\nAre you confident that you can‚Ä¶?\n\nDifferentiate between different methods of sampling (Section 11.1)\nFormulate null hypotheses and alternative hypotheses (Section 11.1.1)\nTest null hypotheses using t-tests in R (Section 11.2)\nExplain what statistical significance and p-values mean (Section 11.3)\nCalculate Cohen‚Äôs d in R (Section 11.4)\nCalculate, test, and interpret correlations in R (Section 11.5)\nCheck that the main underlying assumptions of the most common statistical tests are met (Section 11.6)\nCorrect p-values for multiple comparisons and explain why this is important (Section 11.7)\n\n\n\n\n\nBaayen, R. Harald. 2008. Analyzing linguistic data: A practical introduction to statistics using R. Cambridge University Press.\n\n\nBen-Shachar, Mattan, Daniel L√ºdecke & Dominique Makowski. 2020. Effectsize: Estimation of effect size indices and standardized parameters. Journal of Open Source Software 5(56). 2815. https://doi.org/10.21105/joss.02815.\n\n\nCohen, Jacob. 1988. Statistical power analysis for the behavioral sciences. 2. ed., reprint. New York, NY: Psychology Press.\n\n\nDƒÖbrowska, Ewa. 2019. Experience, aptitude, and individual differences in linguistic attainment: A comparison of native and nonnative speakers. Language Learning 69(S1). 72‚Äì100. https://doi.org/10.1111/lang.12323.\n\n\nField, Andy P., Jeremy Miles & Zo√´ Field. 2012. Discovering statistics using r. Sage.\n\n\nLakens, Dani√´l. 2022. Improving your statistical inferences. Zenodo. https://doi.org/10.5281/ZENODO.6409077.\n\n\nLevshina, Natalia. 2015. How to do linguistics with R: Data exploration and statistical analysis. John Benjamins.\n\n\nMatejka, Justin & George Fitzmaurice. 2017. Same stats, different graphs: Generating datasets with varied appearance and identical statistics through simulated annealing. In, 12901294. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3025453.3025912.\n\n\nNimon, Kim F. 2012. Statistical assumptions of substantive analyses across the general linear model: A mini-review. Frontiers in Psychology 3. https://doi.org/10.3389/fpsyg.2012.00322.\n\n\nParsons, Sam, Fl√°vio Azevedo, Mahmoud M. Elsherif, Samuel Guay, Owen N. Shahim, Gisela H. Govaart, Emma Norris, et al. 2022. A community-sourced glossary of open scholarship terms. Nature Human Behaviour. Nature 6(3). 312‚Äì318. https://doi.org/10.1038/s41562-021-01269-4.\n\n\nPlonsky, Luke & Frederick L. Oswald. 2014. How big is ‚Äúbig‚Äù? Interpreting effect sizes in L2 research. Language Learning 64(4). 878‚Äì912. https://doi.org/10.1111/lang.12079.\n\n\nTabachnick, Barbara G. & Linda S. Fidell. 2014. Using multivariate statistics (Always Learning). Pearson new international edition, sixth edition. Pearson.\n\n\nWinter, Bodo. 2020. Statistics for linguists: An introduction using R. Routledge. https://doi.org/10.4324/9781315165547.",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Infe`R`ential statistics</span>"
    ]
  },
  {
    "objectID": "11_Inferential.html#footnotes",
    "href": "11_Inferential.html#footnotes",
    "title": "11¬† InfeRential statistics",
    "section": "",
    "text": "If you are not immediately convinced by this statement, imagine that a friend flips a coin three times and gets heads all three times. On the basis of these observed data, he claims that the coin is biased towards heads. How likely are you to believe his claim that the coin is unfair? What about if he flips the coin 10 times and it lands on head all 10 times? This seems far more unlikely. In fact, the probability of getting three heads in a row with a fair coin is 0.125%, which means that it will happen about 1 in 8 times, whereas the probability of getting 10 heads in a row is just 0.001%, which is a one-in-a-thousand occurrence!‚Ü©Ô∏é\nWhich, based on the results of our descriptive statistics, is a very reasonable assumption to make about the full population. If, however, you wanted to conduct a Student‚Äôs t-test that treats the variance of both groups as equal, then you would need to change the default value of the var.equal argument of the t.test() function to TRUE (for details see ?t.test).‚Ü©Ô∏é\nIn scientific notation, ‚ÄúE‚Äù stands for ‚Äúexponent‚Äù, which refers to the number of times a number needs to be multiplied by 10 or, if it is followed by a minus sign, multiplied by minus 10. This notation is used as a shorthand way of writing very large or very small numbers. One way to convert values from scientific notation to standard notation in R is to use the format() function like this:\n\nformat(1.956e-05, scientific = FALSE)\n\n[1] \"0.00001956\"\n\n\n‚Ü©Ô∏é\nAgain, it is important to note that this is by no means the only way to interpret p-values. As with many things in statistics, how you interpret p-values depends on the statistical philosophy that you subscribe to. The approach described in this textbook corresponds to the statistical hypothesis testing approach developed by Neyman & Pearson (1933). ‚ÄúIn a Neyman-Pearson framework, the goal of statistical tests is to guide the behavior of researchers with respect to these two hypotheses. Based on the results of a statistical test, and without ever knowing whether the hypothesis is true or not, researchers choose to tentatively act as if the null hypothesis or the alternative hypothesis is true.‚Äù (Lakens 2022: Section 1.1)‚Ü©Ô∏é\nIt is worth noting that, just because 0.05 is (currently) the most widely used threshold, doesn‚Äôt mean that you have to use 0.05, too. If you are not comfortable accepting a 5% risk (which, statistically speaking, will happen one in 20 times, after all), you can define a lower threshold, e.g.¬†0.01, corresponding to a 1% risk. However, depending on how large your observed effect is and how much data you have, you may find that, with a lower significance‚Äëlevel, you fail to reject the null hypothesis even when there is a true effect in the population so it‚Äôs a difficult balance to strike. To find out more, I recommend reading about statistical power, e.g.¬†in Lakens (2022: Chapter 2).‚Ü©Ô∏é\nAll of the statistical tests performed in this chapter are two-sided. For a discussion of one-sided vs.¬†two-sided tests, see Lakens (2022): Section 5.10).‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Infe`R`ential statistics</span>"
    ]
  },
  {
    "objectID": "12_SimpleLinearRegression.html",
    "href": "12_SimpleLinearRegression.html",
    "title": "12¬† IntRoduction to statistical modelling",
    "section": "",
    "text": "From tests to models\nThis chapter will take you from statistical tests to statistical modelling. By the end of this chapter, you will be able to:",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Int`R`oduction to statistical modelling</span>"
    ]
  },
  {
    "objectID": "12_SimpleLinearRegression.html#sec-Correlationsregression",
    "href": "12_SimpleLinearRegression.html#sec-Correlationsregression",
    "title": "12¬† IntRoduction to statistical modelling",
    "section": "12.1 Correlations as regression over a numeric variable",
    "text": "12.1 Correlations as regression over a numeric variable\nThis section explains how the principle of correlation, which we covered in Section 11.5, is integral to linear regression modelling. We begin with a ‚Äòtoy‚Äô example to familiarise ourselves with the concept of statistical modelling. It is important to take the time to genuinely understand how simple linear regression works before moving on to more complex, real-world research questions.\n\n12.1.1 A perfect prediction\nIn the following, we will fit a simple linear regression model to predict the percentage of correct answers that a student obtained in a multiple-choice test based on the number of questions that they correctly answered in this same test. Clearly, this is a purely hypothetical example because, if we know the number of questions that a student correctly answered and how many questions there were in the test, we can easily calculate the percentage of questions that the student answered correctly.\nIf a test has 10 questions and a student answered 8 of them correctly, we can calculate the percentage of questions that they successfully answered like this:\n\n8 / 10 * 100\n\n[1] 80\n\n\nAnd we can simplify this operation to a single multiplication:\n\n8 * 10\n\n[1] 80\n\n\nWe will fit our first simple linear regression model on a simulated dataset. It consists of 100 test results expressed:\n\nas the number of correctly answered questions (N.correct), and\nas a percentage (Accuracy).\n\nThe dataset contains 100 rows corresponding to the results of 100 test-takers. Table¬†12.1 displays the first six rows of this simulated dataset (test.results).\n\n\n\nTable¬†12.1\n\n\n\nShow code to simulate the dataset.\n# First, we set a seed to ensure that the outcome of our randomly generated number series are always exactly the same:\nset.seed(42)\n\n# Second, we simulate the number of questions that 100 learners answered correctly using the `sample()` function that generates random numbers, specifying that learners got between 1 and 10 questions right:\nN.correct &lt;- sample(1:10, 100, replace = TRUE)\n\n # Third, we convert the number of correctly answered questions into the percentage of questions that these learners answered correctly:\nAccuracy &lt;- N.correct / 10 * 100\n\n# Finally, we put these two variables together in a dataframe:\ntest.results &lt;- data.frame(N.correct, Accuracy)\n\n\n\n\n\n\n\n\n\n\n\n\nN.correct\nAccuracy\n\n\n\n\n1\n10\n\n\n5\n50\n\n\n1\n10\n\n\n9\n90\n\n\n10\n100\n\n\n4\n40\n\n\n\n\n\n\n\nWe can now plot the two variables of our simulated dataset as a scatter plot to visualise the correlation between the number of questions learners answered correctly (N.correct) and the percentage of questions they answered accurately (Accuracy). As expected, these two variables are perfectly correlated: every data point sits exactly on the regression line.\n\n\nShow code to generate the scatter plot.\nlibrary(tidyverse)\nggplot(data = test.results,\n       aes(x = N.correct, y = Accuracy)) +\n  geom_smooth(method = \"lm\",\n              se = FALSE) +\n  geom_point(alpha = 0.5,\n             size = 2) +\n  labs(x = \"Number of correct answers\",\n       y = \"% of questions correctly answered\") +\n  scale_x_continuous(limits = c(0, 10.5), expand = c(0,0), breaks = c(0,2,4,6,8,10)) +\n  scale_y_continuous(limits = c(0, 105), expand = c(0,0), breaks = c(0, 20, 40, 60, 80, 100)) +\n  theme_bw()\n\n\n\n\n\n\n\n\nFigure¬†12.1: The correlation between learners‚Äô test results expressed as the number of correct answers and the percentage of questions answered correctly\n\n\n\n\n\nThis is confirmed by a correlation test (see below), which shows:\n\na correlation coefficient (Pearson‚Äôs r) of exactly 1,\nan extremely small p-value (the smallest that R can display: &lt; 2.2e-16), and\nthe narrowest 95% confidence interval possible [1, 1].\n\n\ncor.test(~ N.correct + Accuracy,\n         data = test.results)\n\n\n    Pearson's product-moment correlation\n\ndata:  N.correct and Accuracy\nt = 156587349, df = 98, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 1 1\nsample estimates:\ncor \n  1 \n\n\nNow, we use the base R function lm() to fit a simple linear model to predict the percentage of questions that learners correctly answered based on the number of correct answers that they gave. Like the significance test functions that we saw in Chapter 11, lm() takes a formula as its first argument. We want to predict the proportion of questions that learners correctly answered as a percentage (Accuracy) so this variable comes before the tilde (~). In statistical modelling, the variable that we want to predict is referred to as the outcome variable1. We want to use the number of correct answers that the learners gave to make our prediction, so we place the variable N.correct after the tilde. Variables used to predict the outcome are called predictors2.\n\ntest.model &lt;- lm(formula = Accuracy ~ N.correct,\n                 data = test.results)\n\nWe have saved our model to our local environment as an R object called test.model. We can now use the summary() function to examine the model:\n\nsummary(test.model)\n\n\nCall:\nlm(formula = Accuracy ~ N.correct, data = test.results)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-3.648e-14 -1.351e-15 -4.300e-16  7.280e-16  7.057e-14 \n\nCoefficients:\n             Estimate Std. Error   t value Pr(&gt;|t|)    \n(Intercept) 9.681e-15  1.781e-15 5.436e+00    4e-07 ***\nN.correct   1.000e+01  2.890e-16 3.461e+16   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.319e-15 on 98 degrees of freedom\nMultiple R-squared:      1, Adjusted R-squared:      1 \nF-statistic: 1.198e+33 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nIn addition to this model summary, the code above outputs a warning about an ‚Äúessentially perfect fit‚Äù in the Console. We will come to this warning at the end of this section, but for now let‚Äôs start by looking at the coefficients of our model summary. Every model has an intercept coefficient estimate ((Intercept)), which is the value that the model predicts for the outcome variable when the predictor variables are zero. In other words, it is where the regression line would cross the y-axis if the line were extended to go all the way to zero as in Figure¬†12.2.\n\n\nShow code to generate plot.\nggplot(data = test.results,\n       aes(x = N.correct, y = Accuracy)) +\n  geom_abline(slope = 10, \n              intercept = c(0,0),\n              linetype = \"dotted\",\n              colour = \"blue\") +       \n  geom_smooth(method = \"lm\",\n              se = FALSE) +\n  geom_point(alpha = 0.5,\n             size = 2) +\n  labs(x = \"Number of correct answers\",\n       y = \"% of questions correctly answered\") +\n  scale_x_continuous(limits = c(0, 10.5), expand = c(0,0), breaks = c(0,2,4,6,8,10)) +\n  scale_y_continuous(limits = c(0, 105), expand = c(0,0), breaks = c(0, 20, 40, 60, 80, 100)) +\n  theme_bw()\n\n\n\n\n\n\n\n\nFigure¬†12.2: The correlation between learners‚Äô test results expressed as the number of correct answers and the percentage of questions answered correctly\n\n\n\n\n\nIn our case, we only have one predictor, so the intercept coefficient that our model estimates (5.151e-15) corresponds to the predicted percentage of questions answered correctly (outcome) when the number of correct answers (predictor) is equal to zero. Instinctively, we know that this value should be zero because 0 points in a test¬†=¬†0% correct in the test. And, indeed, our model predicts an intercept extremely close to zero (5.151e-15). Using the format() function, we can convert this coefficient estimate from scientific notation to standard notation:\n\nformat(5.151e-15, scientific = FALSE)\n\n[1] \"0.000000000000005151\"\n\n\nThe second, N.correct coefficient estimate in our model summary is displayed as 1.000e+01 which is equal to:\n\nformat(1.000e+01, scientific = FALSE)\n\n[1] \"10\"\n\n\nThis coefficient estimate means that, for every correct answer, our prediction for the percentage of questions answered correctly increases by 10%. For example, if a student answered 9 questions correctly, our model predicts that the percentage of correctly answered question is equal to the intercept coefficient of (nearly) zero plus 9 multiplied by 10, which is equal to 90%:\n\n-5.151e-15 + 9 * 10\n\n[1] 90\n\n\nIn our model summary, the coefficient estimate for N.correct is associated with a p-value of &lt;2e-16. It corresponds to the p-value of our cor.test() on the same data, and is actually the smallest value that R will display. In other words, it is extremely small. This p-value indicates that we can safely reject the null hypothesis that this coefficient is zero under the assumptions of a linear regression model. In other words, we can reject the null hypothesis that the number of correct answers is not a useful predictor to predict the percentage of correctly answered questions under the assumption of a linear regression model (more on these assumptions in Section 11.6).\nThe penultimate line of the model summary is also important. It features two R-squared (R2) values. Like the absolute values of correlation coefficients, these can range between 0 and 1. R2¬†=¬†0 means that the model accounts for 0% of the variance in the outcome variable. R2¬†=¬†1 means that the model accounts for 100% of the variance, i.e.¬†that it can perfectly predict the values of the outcome variable from the predictor variable(s). As our simulated test.results dataset is based on a perfect correlation, our model is able to perfectly predict the outcome variable, hence both our R2 values equal 1. In this chapter and Chapter 13, however, we will focus on the adjusted R-squared value because it accounts for the fact that the more predictors we include in our model, the easier it is to predict the outcome variable.\nFinally, you may have also noticed that the lm() function returned a warning message when we fitted this practice model:\nWarning message:\nIn summary.lm(test.model) :\n  essentially perfect fit: summary may be unreliable\nWith this message, the authors of the lm() function are warning us that our model can make almost perfect predictions. Given that in real-life research this is extremely unlikely, an ‚Äúessentially perfect‚Äù prediction is usually a sign that we may have made an error of some kind. Here, however, we can safely ignore this warning because we know that the number of correct answers can, indeed, be converted to percentages of correctly answered questions with perfect accuracy (hence our R2 of 1 or 100%).\n\n\n12.1.2 A real-life prediction\nIn the remaining sections of the chapter, we fit simple regression models to real data from:\n\nDƒÖbrowska, Ewa. 2019. Experience, Aptitude, and Individual Differences in Linguistic Attainment: A Comparison of Native and Nonnative Speakers. Language Learning 69(S1). 72‚Äì100. https://doi.org/10.1111/lang.12323.\n\n\n\n\n\n\n\nWarningPrerequisites\n\n\n\n\n\nOur starting point for this chapter is the wrangled combined dataset that we created and saved in Chapter 9. Follow the instructions in Section 9.7 to create this R object.\nAlternatively, you can download Dabrowska2019.zip from the textbook‚Äôs GitHub repository. To launch the project correctly, first unzip the file and then double-click on the Dabrowska2019.Rproj file.\n\nlibrary(here)\n\nDabrowska.data &lt;- readRDS(file = here(\"data\", \"processed\", \"combined_L1_L2_data.rds\"))\n\nBefore you get started, check that you have correctly imported the data by examining the output of View(Dabrowska.data) and str(Dabrowska.data). In addition, run the following lines of code to load the {tidyverse} packages and create ‚Äúclean‚Äù versions of the L1 and L2 datasets as separate R objects:\n\nlibrary(tidyverse)\n\nL1.data &lt;- Dabrowska.data |&gt; \n  filter(Group == \"L1\")\n\nL2.data &lt;- Dabrowska.data |&gt; \n  filter(Group == \"L2\")\n\nOnce you are satisfied that the data are correctly loaded, you are ready to start modelling! üöÄ\n\n\n\nRecall that, in Section 11.5, we saw that there is a positive correlation between the total number of years that English L1 speakers spent in formal education (EduTotal) and their English grammar comprehension test scores (Grammar). Figure¬†12.3 suggests that this positive correlation also holds for the association between the number of years participants spent in formal education and their receptive vocabulary test scores (Vocab).\n\n\nSee code to generate plot.\nL1.data |&gt; \n  ggplot(mapping = aes(x = EduTotal, \n                       y = Vocab)) +\n  geom_point() +\n  geom_smooth(method = \"lm\",\n              se = FALSE) + \n  theme_bw()\n\n\n\n\n\n\n\n\nFigure¬†12.3: Relationship between years spent in formal education and vocabulary test scores among L1 speakers\n\n\n\n\n\nOn average, the longer L1 speakers were in formal education, the better they performed on the vocabulary test. The correlation is larger than for Grammar scores, and it is statistically significant at an Œ±-level of 0.05 (r = 0.43, 95% CI [0.24, 0.58]):\n\ncor.test(formula = ~ Vocab + EduTotal,\n         data = L1.data)\n\n\n    Pearson's product-moment correlation\n\ndata:  Vocab and EduTotal\nt = 4.4281, df = 88, p-value = 2.721e-05\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2410910 0.5824698\nsample estimates:\n      cor \n0.4268695 \n\n\nThe straight (i.e.¬†linear) regression line going through our scatter plot in Figure¬†12.3, does not go through all the data points like it did in Figure¬†12.1. This is because, if we know how long an L1 participant was in formal education, we cannot perfectly predict how well they will perform on the Vocab test, even though we do know that, on average, having spent longer in formal education correlates with Vocab test scores. Indeed, Figure¬†12.3 clearly shows that some of the individuals who attended formal education for the least amount of time scored very low, whilst others scored very high in the Vocab test. This is not surprising, as we can expect that many other factors will play a role in L1 vocabulary knowledge.\nWe now fit a simple linear regression model using the lm() function (which stands for linear model) to the L1 data from DƒÖbrowska (2019) with the aim of predicting Vocab scores (our outcome variable) based on the number of years that they spent in formal education (EduTotal; our predictor variable):\n\nmodel1 &lt;- lm(formula = Vocab ~ EduTotal,\n             data = L1.data)\n\nWe examine the model using the summary() function:\n\nsummary(model1)\n\n\nCall:\nlm(formula = Vocab ~ EduTotal, data = L1.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-57.725 -10.716   0.526  12.417  36.033 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  20.5159    11.1519   1.840   0.0692 .  \nEduTotal      3.5460     0.8008   4.428 2.72e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.51 on 88 degrees of freedom\nMultiple R-squared:  0.1822,    Adjusted R-squared:  0.1729 \nF-statistic: 19.61 on 1 and 88 DF,  p-value: 2.721e-05\n\n\nThis time, we‚Äôll begin our interpretation of the model summary from the top:\n\nThe first line is a reminder of the model formula and of the data on which we fitted our linear model (lm).\nThe second paragraph provides information about the residuals. They represent the difference between participants‚Äô actual Vocab scores (the observed values) and the model‚Äôs predictions (the predicted values). Hence, they correspond to the variance in Vocab scores that is left unaccounted for by the model. The closer to zero the residuals are, the better the model fits the data. However, no real-life model is perfect so we expect there to be some ‚Äúleft-over‚Äù variance.\n\nMin, 1Q, Median, 3Q, and Max are the minimum, first quartile, median, third quartile, and maximum residuals, respectively. These are the descriptive statistics of a distribution that are typically displayed in a boxplot (see Section 8.3.2). These statistics give us a sense of the spread of the residuals. Whilst these values can be informative, it‚Äôs best to plot residuals to get a sense of their distribution (see Section 12.4.3). Ideally, the residuals should be normally distributed and centred around zero (see Section 12.4).\n\nThe intercept coefficient estimate of 20.516 is the model‚Äôs prediction for the vocabulary score of participants who spent zero years (!) in formal education (EduTotal = 0). It is the model‚Äôs baseline or reference Vocab score.\nThe EduTotal coefficient estimate of 3.546 means that, for each year of formal education, our model predicts that Vocab test scores will increase by an additional 3.546 units on top of the baseline score of 20.516, provided that all other variables remain the same. Hence, if an individual spent 14 years in formal education, their predicted Vocab score is:\n\n20.5159 + 14*3.5460\n\n[1] 70.1599\n\n\nThe p-value associated with the EduTotal coefficient is very small (2.72e-05).\n\nformat(2.72e-05, scientific = FALSE)\n\n[1] \"0.0000272\"\n\n\nIt suggests that the predictor EduTotal makes a statistically significant contribution to our model predicting Vocab scores among L1 speakers.\nThe adjusted R-squared (R¬≤) is 0.1729, which means that our model accounts for ca. 17% of the total variance in Vocab scores. That may not seem a lot, but it is worth recalling that our model only includes one predictor variable. We can reasonably assume that other predictors will help us to predict L1 speakers‚Äô vocabulary knowledge more accurately (e.g.¬†their age, how much they read, perhaps their profession, etc.).\n\n\n\n\n\n\n\nNoteIt‚Äôs all linear regression!\n\n\n\nDid you notice that the t-value and the p-value associated with the EduTotal coefficient in our linear regression model are exactly the same as those that we obtained earlier using the cor.test() function? This is because linear regression with a single numeric predictor is ‚Äì under the hood ‚Äì exactly the same as a Pearson‚Äôs correlation test!\nMoreover, we said that R¬≤ stands for the squared coefficient of correlation, which is why, if we take the square root of our unadjusted R¬≤ coefficient, we get 0.43, which corresponds to the correlation coefficient (Pearson‚Äôs r) that we obtained from the cor.test() function:\n\nsqrt(0.1822)\n\n[1] 0.4268489\n\n\nHaving gone through Chapter 11, you may now ask yourself: why do we need correlation tests if simple linear regression does the same thing? :face-with-monocle: Honestly, we don‚Äôt. Chapter 11 introduced correlation tests and t-tests because they are widely used in the language sciences, so it is important that you understand them, but the truth is: you can achieve much more by taking a modelling rather than a testing approach to analysing data. Read on to find out more!\n\n\n\n\n12.1.3 Predicted values and residuals\nFigure¬†12.4 visualises the Vocab scores that our first model (model1) predicts as a function of the number of years that L1 speakers have spent in formal education. We can access our model‚Äôs predictions by applying the predict() function to our model object (model1). By definition, regression models predict perfect linear associations. As shown in Figure¬†12.4, here, our model predicts a perfect, positive linear association between our predictor variable (EduTotal) and our outcome variable (Vocab).\n\n\nSee code to generate plot.\nggplot(L1.data, \n       aes(x = EduTotal, \n           y = predict(model1))) + \n  geom_abline(slope = 3.55, \n              intercept = 20.51,\n              linetype = \"dotted\",\n              colour = \"blue\") +   \n  geom_count() +\n  scale_size_area(guide = NULL) +\n  labs(y='Predicted Vocab scores', \n       x='Years in formal education') +\n  theme_bw()\n\n\n\n\n\n\n\n\nFigure¬†12.4: Relationship between years spent in formal education and predicted vocabulary test scores\n\n\n\n\n\nYou may have noticed that there are fewer dots on Figure¬†12.4 than data points in the dataset that we used to fit the model (N¬†=¬†90). This is because, if more than one L1 participant reported having been in formal education for the same duration of time, the model predicts the same Vocab score for all these people and this means that the dots overlap on the plot of predicted values. To ensure that we keep in mind that a single dot may represent more than one participant, in Figure¬†12.4, the geom_count() function is used to map the size of each dot onto the number of participants that it represents.\nThis is all very well, but as we learnt from Figure¬†12.3, in reality, our predictor variable EduTotal does not correlate perfectly with Vocab scores ‚Äî far from it! Let us now compare the predicted values of our model with the observed values from the data. How well does our model fit the data? To help us answer this question, Figure¬†12.5 visualises the relationship between L1 participants‚Äô actual Vocab scores (x-axis) and the scores that the model predicted for these participants (y-axis).\n\n\nShow code to generate plot.\npred_vals   &lt;- predict(model1) # Vector of predictions\nxmin_pred   &lt;- min(pred_vals)  # Lowest predicted score\n\nhighlight_df &lt;- data.frame(\n  x = c(xmin_pred, pred_vals[68]),\n  y = c(L1.data$Vocab[which.min(pred_vals)],\n      91))\n\nggplot(L1.data, \n       aes(x = pred_vals, \n           y = Vocab)) + \n  geom_point() +\n  # 45¬∞ reference line (the \"perfect‚Äëprediction\" line):\n  geom_abline(intercept = 0, \n              slope = 1,\n              linetype = \"dotted\",\n              colour = \"blue\") +\n  annotate(\"segment\",\n           x = xmin_pred, \n           y = 29, \n           yend = xmin_pred,\n           colour = \"blue\",\n           arrow = arrow(length = unit(0.25, \"cm\"))) +\n  annotate(\"segment\",\n           x = pred_vals[68], \n           y = 91, \n           yend = pred_vals[68],\n           colour = \"blue\",\n           arrow = arrow(length = unit(0.25, \"cm\"))) +  \n  geom_point(data = highlight_df,\n             aes(x = x, y = y),\n             colour = \"blue\") +  \n  scale_y_continuous(limits = c(0, 100), \n                     expand = c(0,0)) +\n  scale_x_continuous(limits = c(0, 100), \n                     expand = c(0,0)) +\n  labs(x = \"Predicted Vocab scores\", \n       y = \"Actual Vocab scores\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nFigure¬†12.5: Relationship between actual and predicted Vocab test scores\n\n\n\n\n\nIn Figure¬†12.5, the dotted blue line represents where the points would lie, if model1 perfectly predicted Vocab scores based only on the number of years that participants spent in formal education. The dots that fall on or very close to the dotted line stand for L1 participants for whom our model accurately predicts their Vocab score based solely on the number of years they spent in formal education. The further the dots are from the dotted line, the worse the predictions for these speakers. The distances between each point and the dotted line in Figure¬†12.5 correspond to the model‚Äôs residuals. Residuals are therefore the ‚Äòleft-over‚Äô variability in the data that our model cannot account for.\nResiduals can be positive or negative, depending on whether the dots on the predicted vs.¬†actual values plot are above or below the dotted line of perfect fit. However, what matters more are their absolute values. The larger the residuals, the worse the model fit. Returning to the question, ‚ÄòHow good is our model fit?‚Äô, we can conclude from Figure¬†12.5 that it‚Äôs not great. But we already knew that from the model‚Äôs fairly low R¬≤ value and, crucially, we also know that many other factors are likely to account for some of the remaining variation in vocabulary scores.\n\n\n\n\n\n\nWarningAssociation does not imply causation.\n\n\n\nYou may be familiar with the Latin phrase:\n\nCum hoc ergo propter hoc (‚Äòwith this, therefore because of this‚Äô)\n\nor its English equivalent:\n\nCorrelation does not imply causation.\n\nThey both refer to the human tendency to assume that, if two things are regularly associated, one probably causes the other. However, this is a logical fallacy. For example, even if we had observed a very high correlation between the number of years participants were in formal education and their receptive vocabulary test scores (and had therefore reported an excellent model fit with very small residuals), this would by no means imply that, on average, spending more time in formal education leads to greater vocabulary knowledge. A correlation merely describes an association; it tells us nothing about any causal relationship.\nIt is often tempting to interpret the results of statistical tests and models causally, but this is the result of a well-known human cognitive bias (see, e.g, Matute et al. 2015; Kaufman & Kaufman 2018). In the language sciences and across the social sciences more generally, there are usually far too many potential factors at play to warrant any causal interpretation. What we are modelling throughout this chapter and Chapter 13 are statistical associations. We cannot draw any conclusions about what caused what from these models. In the context of statistical modelling, we can extend the famous saying to: Association does not imply causation.\nThere are three main reasons for this:\n\nThere may be one or more confounding variables that we have not accounted for in our model. For instance, we can predict with a fair degree of accuracy how much vocabulary an infant understands based on their weight. However, the weight of young children is, of course, highly correlated with their age and, by extension, the amount of language exposure they‚Äôve had so far in their life. Clearly, it is not their weight that is causing them to understand more words. Unfortunately, it‚Äôs not always that obvious. Going back to model1, it is very likely that some of the younger participants in the DƒÖbrowska (2019) dataset were still in formal education at the time of data collection; this includes all those who reported being students. In other words, for at least some of the participants, the EduTotal variable confounds age and years spent in formal education.\nEven if there is a causal effect, it may not be in the direction that we assume. For example, the vocabulary knowledge of adult learners of a foreign language is probably a fairly good predictor of how many foreign language classes these adult learners have attended. However, this does not mean that vocabulary knowledge causes adults to attend classes. If anything, it is more likely to be the other way around. Whilst it seems rather obvious in this example, in linguistics and education, complex interdependencies between predictors are very common. Consider reading ability: the more a child reads, the better they become at reading. Therefore, we might conclude that spending more time reading leads to better reading skills. However, for some children, poor reading skills may actually cause a lack of motivation and interest in reading in the first place. Clearly, determining causal relationships is tricky!\nFinally, it is important to consider the possibility that even a statistically significant association could be entirely spurious. This happens more often than you might think, and it is more likely to happen with smaller datasets. Moreover, the more we test associations, the more likely we are to find statistically significant ones (see Section 11.7 on the multiple testing problem and Type 1 errors). When associations are very strong, we may be tempted to think that they are real, but this may not be the case. To raise awareness of this risk, Tyler Vigen conducted correlation tests on millions of combinations of randomly selected variables and created a website featuring thousands of examples of very large and highly significant correlations that are all entirely spurious (e.g. Figure¬†12.6).\n\n\n\n\n\n\nFigure¬†12.6: Find this spurious correlation (CC BY 4.0) and thousands more by Tyler Vigen at https://tylervigen.com/spurious-correlations.\n\n\n\n\n\n\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nIn this task, you will seek answers to the research question: To what extent can the results of the non-verbal IQ Blocks test be used to predict Vocab scores among L1 and L2 speakers of English based on the data from DƒÖbrowska (2019)? As we will be answering this question within a linear regression framework, what we are really asking is: To what extent are these variables linearly associated with each other?\nQ12.1 Fit a linear model to predict Vocab scores among L1 participants based on their Blocks test scores. What is the adjusted R2 value of this model?\n\n\n\n\n\n0.0009\n\n\n\n\n0.006\n\n\n\n\n0.007\n\n\n\n\n0.07\n\n\n\n\n0.08\n\n\n\n\n0.7\n\n\n\n\n0.9\n\n\n\n\n\n\n\n\n\n\nShow sample code to answer Q12.1.\ntaskmodel1 &lt;- lm(formula = Vocab ~ Blocks,\n                 data = L1.data)\n\nsummary(taskmodel1) # Adjusted R-squared:  0.07232 \n\n\nQ12.2 According to your model, which of these values is the predicted Vocab score of an L1 speaker with a Blocks score of 20?\n\n\n\n\n\nAbout 75\n\n\n\n\nAbout 21\n\n\n\n\nAbout 55\n\n\n\n\nAbout 1\n\n\n\n\nJust over 0\n\n\n\n\nAbout -54\n\n\n\n\n\n\n\n\nü¶â Hover over the owl for a first hint.\n\n\n\n\nüê≠ Click on the mouse for a second hint.\n\n\n\n\n\n\nShow code to answer Q12.2\n# a) Taking a numerical approach, you can add up the model coefficients printed in the model summary. As always, start with the intercept coefficient and add to it the coefficient corresponding to an increase in one point on the Blocks test multiplied by the number of points that this participant achieved (20):\nsummary(taskmodel1)\n\n54.5614 + 1.0527*20\n\n# Alternatively, we can take the coefficient estimate values directly from the model object to get an even more precise prediction like this:\ntaskmodel1$coefficients[1] + (taskmodel1$coefficients[2] * 20)\n\n# b) Taking a graphical approach, we need to plot the model's predicted Vocab scores against all Blocks test scores in the dataset in order to find out how what the model's prediction is for a Blocks test score of 20. The ggplot code below adds an arrow and a dotted line to help you read the predicted Vocab score from the plot:\nggplot(L1.data, \n       aes(x = Blocks, \n           y = predict(taskmodel1))) + \n  geom_point() +\n  annotate(\"segment\",\n           x = 20, \n           y = 50, \n           yend = 75,\n           colour = \"blue\",\n           arrow = arrow(length = unit(0.25, \"cm\"))) +\n  annotate(\"segment\",\n           x = 0, \n           xend = 19.5, \n           y = 75.6,\n           colour = \"blue\",\n           linetype = \"dotted\") +\n  labs(y='Predicted Vocab scores', \n       x='Blocks test scores') +\n  theme_minimal()\n\n\nQ12.3 Now fit a new linear model to predict Vocab scores among L2 participants based on their Blocks test scores. Comparing the adjusted R2 value of this new L2 model to your previous L1 model, which of these statements is/are true?\n\n\n\n\n\nBlocks test scores are a much more useful predictor of Vocab scores for L1 than L2 participants.\n\n\n\n\nBlocks test scores are a much useful predictor of Vocab scores for L2 than L1 participants.\n\n\n\n\nHigher Blocks test scores lead to statistically significantly higher Vocab scores among L1 participants.\n\n\n\n\nBlocks test scores allow us to almost perfectly predict Vocab scores among L1 participants.\n\n\n\n\nBlocks test scores allow us to almost perfectly predict Vocab scores among L2 participants.\n\n\n\n\n\n\n\n\nü¶â Hover over the owl for a first hint.\n\n\n\n\nüê≠ Click on the mouse for a second hint.\n\n\n\n\n\n\nShow sample code to answer Q12.3.\ntaskmodel2 &lt;- lm(formula = Vocab ~ Blocks,\n                 data = L2.data)\n\nsummary(taskmodel2) # Adjusted R-squared:  0.007177 \nsummary(taskmodel1) # Adjusted R-squared:  0.07232 \n\n\nQ12.4 In your L2 model, the p-value for the Blocks predictor should be 0.228620. True or false: This p-value means that there is a 23% chance of obtaining a Blocks coefficient estimate of 0.72 or higher in a random sample of 67 L2 speakers of English when there is actually no association between the results of the Blocks test and those of the Vocab test?\n\n\n\n\n\nTrue.\n\n\n\n\nFalse.",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Int`R`oduction to statistical modelling</span>"
    ]
  },
  {
    "objectID": "12_SimpleLinearRegression.html#sec-Ttestsregression",
    "href": "12_SimpleLinearRegression.html#sec-Ttestsregression",
    "title": "12¬† IntRoduction to statistical modelling",
    "section": "12.2 t-tests as regression over a binary variable",
    "text": "12.2 t-tests as regression over a binary variable\nLet us now see how much of the variation across all receptive English vocabulary test scores in the DƒÖbrowska (2019) data we can accurately predict if we only include a single categorical binary variable predictor in our model: whether the Vocab scores belong to L1 or L2 speakers of English. Hence, in the following model, we keep the same outcome variable (Vocab), but now we try to predict it using the Group variable as our single predictor.\nWe know that, on average, L1 speakers score better on the Vocab test than L2 speakers. However, as illustrated in Figure¬†12.7, we also know that there is quite a bit of variability around the mean scores of the L1 and L2 speakers.\n\n\nSee code to generate plot.\ngroup_means &lt;- Dabrowska.data |&gt; \n  group_by(Group) |&gt; \n  summarise(mean_vocab = mean(Vocab))\n\nggplot(data = Dabrowska.data, \n       aes(x = Group, \n           y = Vocab)) +\n  geom_boxplot(width = 0.5) +\n  stat_summary(\n    fun = mean,                     # what to plot\n    geom = \"point\",                 # as a point\n    shape = 18,                     # in a diamond shape\n    size = 4,                       # a little larger than the default\n    colour = \"blue\") +\n  geom_line(\n    data = group_means,                   \n    aes(x = as.numeric(Group), \n        y = mean_vocab),\n    colour = \"blue\",\n    linewidth = 1,\n    linetype = \"dashed\") +\n  geom_text(data = group_means,\n    aes(x = as.numeric(Group), \n        y = mean_vocab,\n        label = sprintf(\"%.2f\", mean_vocab)), # print the means to two decimal points\n    vjust = -1.4,\n    colour = \"purple\") +\n  labs(x = NULL,\n    y = \"Non-verbal IQ (Blocks) test\") +\n  theme_bw(base_size = 14)\n\n\n\n\n\n\n\n\nFigure¬†12.7: Comparison of non-verbal IQ (Blocks) test scores between L1 and L2 groups\n\n\n\n\n\nA t-test shows that the mean difference between L1 and L2 speakers is statistically significant (p¬†=¬†7.032e-06):\n\nt.test(Vocab ~ Group, \n       data = Dabrowska.data)\n\n\n    Welch Two Sample t-test\n\ndata:  Vocab by Group\nt = 4.6768, df = 133.83, p-value = 7.032e-06\nalternative hypothesis: true difference in means between group L1 and group L2 is not equal to 0\n95 percent confidence interval:\n  9.425801 23.240497\nsample estimates:\nmean in group L1 mean in group L2 \n        69.13580         52.80265 \n\n\nCohen‚Äôs d is large (0.77) and the 95% confidence interval does not go anywhere near zero:\n\nlibrary(effectsize)\n\ncohens_d(Vocab ~ Group, \n         data = Dabrowska.data)\n\nCohen's d |       95% CI\n------------------------\n0.77      | [0.44, 1.09]\n\n- Estimated using pooled SD.\n\n\nSo we can be confident that the Group variable will be a useful predictor to predict Vocab in a simple linear regression model. We can use exactly the same formula as earlier to fit our model:\n\nmodel2 &lt;- lm(formula = Vocab ~ Group, \n             data = Dabrowska.data)\n\nsummary(model2)\n\n\nCall:\nlm(formula = Vocab ~ Group, data = Dabrowska.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-66.136 -13.580   2.753  17.531  38.308 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   69.136      2.247  30.763  &lt; 2e-16 ***\nGroupL2      -16.333      3.440  -4.748 4.64e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.32 on 155 degrees of freedom\nMultiple R-squared:  0.127, Adjusted R-squared:  0.1213 \nF-statistic: 22.54 on 1 and 155 DF,  p-value: 4.644e-06\n\n\nLet‚Äôs break down the model summary:\n\nThe intercept coefficient estimate is the predicted Vocab score when Group is at its reference level. By default, in R, the reference level of a categorical variable is the level that comes first alphabetically. Here, it‚Äôs therefore ‚ÄòL1‚Äô for the English native speaker group. The model‚Äôs estimated Vocab test score for an L1 participant is therefore 69.136 points. If you check Figure¬†12.7, you will see that this value corresponds to the mean L1 Vocab score in our data.\nThe GroupL2 coefficient estimate is the estimated change in Vocab score for an L2 speaker as compared to the reference level of an L1 speaker. The estimate is -16.333, which means that L2 speakers are predicted to score 16.333 points lower than L1 speakers on this test. If you subtract this GroupL2 coefficient estimate from the Intercept, you will find that it corresponds to the mean L2 Vocab score in our data:\n\n69.136 - 16.333\n\n[1] 52.803\n\n\nThe p-value associated with the coefficient GroupL2 is extremely small (4.64e-06). It informs us that Group is a statistically significant predictor of receptive English vocabulary knowledge based on our data.\nThe adjusted R-squared value indicates the proportion of variation in Vocab scores that the model can account for when we know a participant‚Äôs native-speaker status: 12.13% (0.1213). This value is not particularly impressive. However, this is not surprising given that our model attempts to predict vocabulary scores based solely on whether someone is an L1 or L2 speaker of English. With only this information, the model can only predict that L1 speakers will achieve the mean L1 score and L2 speakers the mean L2 score of the sample data.\n\n\n\n\n\n\n\nNoteIt‚Äôs (still) all linear regression!\n\n\n\n\n\nHere, too, it is worth noticing that fitting a simple linear regression model with a single binary predictor is ‚Äî under the hood ‚Äî exactly the same thing as conducting an independent two-sample t-test. Compare the t-statistic of the t-test that we computed earlier with that of the t-statistic of the GroupL2 coefficient reported in the summary of our second model. If we ignore the signs of the t-values, we can see that they are almost identical!\nAlso, notice how the p-value computed by the t.test() function and that reported for our GroupL2 coefficient are almost identical. They both correspond to values that are extremely close to zero.\nThe very small differences between these values are due to the default correction that the t.test() function in R makes for unequal group variances (see Section 8.3). If we switch off this correction, both the t-statistic and the p-value are exactly the same as those reported in the summary of our second linear model above:\n\nt.test(Vocab ~ Group, \n       data = Dabrowska.data,\n       var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  Vocab by Group\nt = 4.7478, df = 155, p-value = 4.644e-06\nalternative hypothesis: true difference in means between group L1 and group L2 is not equal to 0\n95 percent confidence interval:\n  9.537477 23.128821\nsample estimates:\nmean in group L1 mean in group L2 \n        69.13580         52.80265 \n\n\n\n\n\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nQ12.5 Draw a boxplot showing the distribution of Vocab scores among male and female participants in Dabrowska.data. Based on your boxplot, do you expect Gender to be a statistically significant predictor of Vocab scores?\n\n\n\n\n\nNo, because there is a lot of overlap between the two distributions and the female and male median Vocab scores are very close to each other.\n\n\n\n\nNo, because the female and male mean Vocab scores are very close to each other.\n\n\n\n\nYes, because the mean Vocab score of male participants is higher than that of female participants.\n\n\n\n\nYes, because there are a lot of very low Vocab scores among female participants.\n\n\n\n\nYes, because there are significantly more female participants in this dataset.\n\n\n\n\n\n\n\n\n\n\nShow sample code to answer Q12.5.\nDabrowska.data |&gt; \n  ggplot(mapping = aes(x = Gender,\n                       y = Vocab)) +\n  geom_boxplot() +\n  labs(x = \"Gender\", \n       y = \"Vocab scores\") +\n  theme_minimal()\n\n\nQ12.6 Fit a model with Vocab as the outcome variable and Gender as the predictor to test your intuition based on your boxplot. Is Gender a statistically significant predictor of Vocab score in this simple linear regression model?\n\n\n\n\n\nNo, the p-value associated with GenderM coefficient is higher than 0.05.\n\n\n\n\nYes, the p-value associated with GenderM coefficient is higher than 0.05.\n\n\n\n\nNo, the p-value associated with GenderM coefficient is lower than 0.05.\n\n\n\n\nYes, the p-value associated with GenderM coefficient is lower than 0.05.\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n\n\nShow code to answer Q12.6\ntaskmodel3 &lt;- lm(formula = Vocab ~ Gender,\n                 data = Dabrowska.data)\n\nsummary(taskmodel3) # p-value associated with coefficient estimate for GenderM = 0.957\n\n\nQ12.7 The adjusted R2 coefficient of the model is -0.006433. True or false: This means that male participants, on average, score 0.006433 fewer points than female participants on the Vocab test?\n\n\n\n\n\nTrue.\n\n\n\n\nFalse.",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Int`R`oduction to statistical modelling</span>"
    ]
  },
  {
    "objectID": "12_SimpleLinearRegression.html#sec-Categoricalpredictor",
    "href": "12_SimpleLinearRegression.html#sec-Categoricalpredictor",
    "title": "12¬† IntRoduction to statistical modelling",
    "section": "12.3 Regressing over a categorical predictor with more than two levels",
    "text": "12.3 Regressing over a categorical predictor with more than two levels\nThe beauty of the statistical modelling approach ‚Äî as opposed to the testing approach ‚Äî is that we can include all kinds of predictors in our models using a single function, lm(), and R‚Äôs formula syntax. So far, we have seen that predictors can be numeric variables (e.g.¬†EduTotal) and categorical binary variables (e.g.¬†Group). In this section, we see how a categorical variable with more than two levels can be entered in a linear regression model.\nTo demonstrate this, we will now attempt to predict participants‚Äô Vocab scores based on their occupational group (OccupGroup), which can be one of four broad categories (see Section 10.1.2):\n\nC: Clerical positions\nI: Occupationally inactive (i.e.¬†unemployed, retired, or homemakers)\nM: Manual jobs\nPS: Professional-level jobs or studying for a degree\n\nAccording to the descriptive statistics visualised in Figure¬†12.8, it looks like participants‚Äô professional occupation group is unlikely to be terribly useful to predict their vocabulary knowledge. Nonetheless, we can see that ‚Äî perhaps somewhat counter-intuitively ‚Äî so-called ‚Äúoccupationally inactive‚Äù participants (‚ÄúI‚Äù) tend to score higher than the other participants.\n\n\nSee code to generate plot.\nDabrowska.data |&gt; \n  ggplot(mapping = \n           aes(y = Vocab, \n               x = OccupGroup,\n               fill = OccupGroup,\n               colour = OccupGroup)) +\n  geom_boxplot(alpha = 0.3, \n               outliers = FALSE) + # If the outliers are plotted as part of the boxplot, these data points will be duplicated by the geom_jitter() function.\n  geom_jitter(alpha = 0.8, \n              width = 0.2) +\n  scale_color_viridis_d(guide = \"none\") +\n  scale_fill_viridis_d(guide = \"none\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nFigure¬†12.8: Distribution of vocabulary test scores across four occupational groups\n\n\n\n\n\nSo, let‚Äôs compute a simple linear model with OccupGroup as a predictor of Vocab and examine its summary:\n\nmodel3 &lt;- lm(formula = Vocab ~ OccupGroup, \n             data = Dabrowska.data)\n\nsummary(model3)\n\n\nCall:\nlm(formula = Vocab ~ OccupGroup, data = Dabrowska.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-74.406 -13.504   3.372  16.705  34.688 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    63.333      3.867  16.379   &lt;2e-16 ***\nOccupGroupI    12.393      5.775   2.146   0.0335 *  \nOccupGroupM    -9.133      5.160  -1.770   0.0787 .  \nOccupGroupPS   -2.261      4.817  -0.469   0.6395    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.87 on 153 degrees of freedom\nMultiple R-squared:  0.09288,   Adjusted R-squared:  0.07509 \nF-statistic: 5.222 on 3 and 153 DF,  p-value: 0.001849\n\n\n\nThe estimate of the intercept corresponds to the reference level of the OccupGroup variable. Remember that the reference level is always the first level. We can check the order of the levels in any factor variable using the levels() function:\n\nlevels(Dabrowska.data$OccupGroup)\n\n[1] \"C\"  \"I\"  \"M\"  \"PS\"\n\n\nBy default, the levels are ordered alphabetically. In the OccupGroup variable, the first level is ‚ÄúC‚Äù, which corresponds to a clerical position. This means that our third model predicts that English speakers in a clerical position will score 63.333 points on the Vocab test. Those that belong to the ‚Äúinactive‚Äù group (‚ÄúI‚Äù) will score 12.393 more points than those, i.e.:\n\n63.333 + 12.393\n\n[1] 75.726\n\n\nParticipants who have manual jobs (‚ÄúM‚Äù) are predicted to score -9.133 points fewer than those in clerical positions, and so-called ‚Äúprofessionals‚Äù (‚ÄúPS‚Äù) will do slightly worse than clerks (-2.261 points). Compare these predicted values to those observed in the data shown in Figure¬†12.8.\nThe adjusted R2 value for our model is relatively low (0.07509). This means that our model accounts for about 7.5% of the total variance in Vocab scores across the dataset. Given that the R2 of model1 was 0.1213, this indicates that OccupGroup is a considerably less useful predictor variable to help us predict participants‚Äô Vocab scores than whether or not they are an L1 speaker (Group).\nWhat‚Äôs more, the model summary reveals that only one of the model3‚Äôs three coefficient estimates is statistically significantly different from 0 at the Œ±-level of 0.05: OccupGroupI with a p-value of 0.0335. This means that we can reject the null hypothesis that there is no difference in Vocab scores between speakers of the occupational group ‚ÄúC‚Äù (the reference level) and those of the occupational group ‚ÄúI‚Äù. For the other two occupational groups, we do not have enough evidence to reject the null hypothesis of no difference compared to the reference level ‚ÄúC‚Äù.\n\nWe can display the predicted Vocab scores for each occupational group, together with a 95% confidence interval around these predicted values using the emmeans() function from the {emmeans} package (Lenth 2025):\n\n#install.packages(\"emmeans\")\nlibrary(emmeans)\n\nemmeans(model3, ~ OccupGroup)\n\n OccupGroup emmean   SE  df lower.CL upper.CL\n C            63.3 3.87 153     55.7     71.0\n I            75.7 4.29 153     67.3     84.2\n M            54.2 3.42 153     47.5     60.9\n PS           61.1 2.87 153     55.4     66.7\n\nConfidence level used: 0.95 \n\n\nThe {visreg} package (Breheny & Burchett 2017) provides an efficient way of visualising model predictions and residuals. In Figure¬†12.9 we visualise the model‚Äôs predicted values (as blue lines) and the confidence intervals output by the emmeans() function (as grey bands), together with the model‚Äôs residuals (as grey dots).\n\nlibrary(visreg)\n\nvisreg(model3, gg = TRUE) +\n  labs(x = \"Occupational groups\",\n       y = \"Predicted Vocab scores\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure¬†12.9: Distribution of predicted vocabulary test scores across four occupational groups\n\n\n\n\n\nWhen we use the argument ‚Äúgg = TRUE‚Äù, the visreg() function outputs a ggplot object that we can then manipulate and customise just like any other ggplot object (see Chapter 10).\n\n\n\n\n\n\nNoteWhat about ANOVAs?\n\n\n\n\n\nYou may come across the type of simple linear regression model that we have just covered, involving a continuous numeric outcome variable and a categorical predictor with more than two levels, under the guise of a statistical significance test, namely the one-way ANOVA (ANalysis Of VAriance).\nIf you compare the summary of the following one-way ANOVA with that of the third model (model3), you will spot some similarities. However, it is necessary to carry out so-called post-hoc tests to get as much information out of an ANOVA as we can get from the summary of our simple linear regression model.\n\nanova1 &lt;- aov(formula = Vocab ~ OccupGroup, \n    data = Dabrowska.data)\n\nsummary(anova1)\n\n             Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nOccupGroup    3   7495  2498.5   5.222 0.00185 **\nResiduals   153  73205   478.5                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nIn this task, you will explore whether L2 participants‚Äô native language is a useful predictor when trying to model their English Vocab scores.\nQ12.8 Fit a linear model to model L2 participants‚Äô Vocab score based on their native language (NativeLg). According to the model‚Äôs adjusted R2 coefficient, how much of the variance in Vocab scores among L2 participants does this model account for?\n\n\n\n\n\nPractically 0%\n\n\n\n\nAbout 5%\n\n\n\n\nAbout 13%\n\n\n\n\nAbout 26%\n\n\n\n\nAbout 49%\n\n\n\n\n\n\n\n\n\n\nShow sample code to answer Q12.8.\ntaskmodel3 &lt;- lm(formula = Vocab ~ NativeLg,\n                data = L2.data) \n\nsummary(taskmodel3) # Adjusted R-squared:  0.1331 \n\n\nQ12.9 In this model, there is a greater difference between the multiple R2 and the adjusted R2 coefficients than in all previous models in this chapter. Why might that be?\n\n\n\n\n\nBecause knowing a participant's native language is a particularly useful predictor of Vocab scores.\n\n\n\n\nBecause the NativeLg variable includes 10 different levels.\n\n\n\n\nBecause a participant's native language is inherently connected to their English vocabulary knowledge.\n\n\n\n\nBecause the model's p-value is only just below 0.05.\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\nQ12.10 One way to reduce the number of levels in the NativeLg variable is to model Vocab scores based on L2 participants‚Äô native language family, instead. Fit a model that attempts to predict Vocab scores among L2 participants based on the NativeLgFamily variable (the creation of this variable was a Your turn! task in Section 9.5.2). Based on your comparison of the two models, which of the following statements is/are true?\n\n\n\n\n\nWhen adjusted for the number of predictors, the language family model is better at predicting Vocab scores.\n\n\n\n\nWhen adjusted for the number of predictors, the language family model is worse at predicting Vocab scores.\n\n\n\n\nThe more predictors there are, the easier it is to predict the Vocab scores of the sample. This is why the language family model has a lower multiple R-squared coefficient.\n\n\n\n\nThe language family model is statistically less significant than the other model because it features fewer predictors.\n\n\n\n\n\n\n\n\n\n\nShow code to answer Q.12.10\ntaskmodel4 &lt;- lm(formula = Vocab ~ NativeLgFamily,\n                data = L2.data) \n\nsummary(taskmodel4)\n\n\nQ12.11 According to your NativeLgFamily model, what is the predicted Vocab score of an L2 participant with a Baltic L1?\n\n\n\n\n\n-23.749\n\n\n\n\n9.497\n\n\n\n\n0.1133\n\n\n\n\n73.778\n\n\n\n\n7.768\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n\n\nShow sample code to answer Q12.11\n# The Baltic language family is the first level in this categorical predictor (because we haven't changed the order which, by default, is alphabetical):\nlevels(L2.data$NativeLgFamily)\n\n# The Intercept coefficient corresponds to an L2 participant with a Baltic native language:\nsummary(taskmodel4)\n\n\nQ12.12 According to your NativeLgFamily model, what is the predicted Vocab score of an L2 participant with a Slavic L1?\n\n\n\n\n\n73.778\n\n\n\n\n23.749\n\n\n\n\n50.029\n\n\n\n\n-23.749\n\n\n\n\n10.088\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n\n\nShow sample code to answer Q12.12\n# a) Numeric approach\nsummary(taskmodel4)\n\n73.778 + -23.749\n\n# b) Graphical approach\nlibrary(visreg)\n\nvisreg(taskmodel4, gg = TRUE) +\n  labs(title = \"Participants' L1 language family\",\n       x = NULL,\n       y = \"Vocab scores\") +\n  theme_bw()\n\n# The predicted value for the Slavic L1 speakers is visualised by the last blue line.\n\n\nQ12.13 According to the NativeLgFamily model, Hellenic L1 speakers are predicted to perform considerably better than the reference level of Baltic L1 speakers. However, the p-value associated with this coefficient estimate is very large (0.4591). Why is that?\n\n\n\n\n\nBecause the dataset only includes one Greek L1 speaker.\n\n\n\n\nBecause p-values have nothing to do with effect sizes.\n\n\n\n\nBecause the model chose a particularly low Œ±-level.\n\n\n\n\nBecause the data for Hellenic speakers is less reliable than for the other L2 speakers.\n\n\n\n\n\n\n\n\nü¶â Hover over the owl for a first hint.\n\n\n\n\nüê≠ Click on the mouse for a second hint.\n\n\n\n\n\n\nShow sample code to answer Q12.13\n# The plot of predicted values (see code below) shows that the coefficient estimate for Hellenic speakers is based on only one data point (as is also the case for Germanic speakers).\nlibrary(visreg)\nvisreg(taskmodel4, gg = TRUE) +\n  labs(title = \"Participants' L1 language family\",\n       x = NULL,\n       y = \"Vocab scores\") +\n  theme_bw()\n\n# We can check the distribution of native language family using the `table()`, `summary()` or `count()` functions:\ntable(L2.data$NativeLgFamily)\n\nsummary(L2.data$NativeLgFamily)\n\nL2.data |&gt; \n  count(NativeLgFamily)\n\n\nQ12.14 Apply the emmeans function from the {emmeans} package to the NativeLgFamily model to compute the predicted mean Vocab scores of each L1 language family group. Romance L1 speakers are predicted to attain a mean score of 66.3. This predicted score is associated with a 95% confidence interval that spans from 43.2 to 56.8. What does this mean?\n\n\n\n\n\nIf we were to repeat this study many times, in the long run, 95% of the intervals that would calculate for this prediction would contain the true mean score.\n\n\n\n\nWe can be 95% confident that the true mean score is located within this confidence interval.\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a second hint.\n\n\n\n\n\n\nShow sample code to answer Q12.14\n#install.packages(\"emmeans\")\nlibrary(emmeans)\n\nemmeans(taskmodel4, ~ NativeLgFamily)",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Int`R`oduction to statistical modelling</span>"
    ]
  },
  {
    "objectID": "12_SimpleLinearRegression.html#sec-AssumptionsLR",
    "href": "12_SimpleLinearRegression.html#sec-AssumptionsLR",
    "title": "12¬† IntRoduction to statistical modelling",
    "section": "12.4 Regression assumptions",
    "text": "12.4 Regression assumptions\nSimilar to the statistical tests that we conducted in Chapter 11, linear regression also has a number of assumptions that should be met if we want to obtain reliable results that we can trust. It is therefore very important that we check that these assumptions are met.\n\n12.4.1 Assumption 1: Independence\nThis is the same assumption as for the statistical tests that we covered in Chapter 11 (see Section 11.6.2). It means that each observation in our dataset should be unrelated to every other observation. In other words, knowing the value of one data point shouldn‚Äôt give us any information about any other data point in our dataset. This assumption is critical because violations can lead to unjustifiably low p-values and narrower confidence intervals.\nIndependence violations arise from the way data were collected. For example, if we test the same person several times, their test results are likely to be more similar to each other than to the results of other participants. Similarly, if we collect data over time, today‚Äôs measurement might be influenced by yesterday‚Äôs value. When pupils are sampled from different schools, pupils within the same school or class might be more similar to each other than to pupils from other schools and/or classes.\nIf our data do not meet the assumption of independence, we need to use a different analysis method that can account for the fact that our data points are related to each other. In the language sciences, this is most commonly achieved using mixed-effects models. In this textbook, however, we only cover fixed-effects models for which the assumption of independence must be held.\n\n\n\n\n\n\nNoteFurther reading üìö\n\n\n\n\n\nHere are some great starting points (in alphabetical order) to learn about mixed-effects models for linguistics research and how to fit them in R:\n\nGries (2021): Chapter 6\nLevshina (2015): Chapter 8\nWinter (2020): Chapters 14-15\n\n\n\n\n\n\n12.4.2 Assumption 2: Linearity\nThe linearity assumption requires that the relationship between the numeric predictor variables entered into a model and the outcome variable follows a straight (i.e.¬†linear) line rather than a curved one. This assumption is fundamental because linear regression, as the name suggests, can only ever fit a straight line through the data. If the true relationship between our predictors is curved, forcing a straight line through the data will lead to poor predictions and misleading conclusions.\nTo check this assumption, it is best to plot the predictor variable against the outcome variable in the form of a scatter plot (see Figure¬†12.1). When examining such plots, we are looking for rough straight-line patterns. We are not expecting all the same data points to fall on the linear regression line (that would be entirely unrealistic with real data). However, the points should be scattered around an imaginary straight line without showing any obvious curved patterns.\nIn Figure¬†12.10, by contrast, the data points follow a curved pattern. The regression line is forced to be straight, creating systematic prediction errors. Such a pattern of errors indicates that the assumption of linearity is violated.\n\n\n\n\n\n\n\n\nFigure¬†12.10\n\n\n\n\n\n\n\n12.4.3 Assumption 3: Homogeneity of residuals\nLinear regression models assume homogeneity ‚Äî or constant variance ‚Äî of the residuals (see Section 11.6.5). This assumption is best checked by visually examining the model residuals. In Figure¬†12.11, we plot the residuals of model1 and :check-mark-button: that their variability does not systematically increase or decrease as the outcome variable increases or decreases.\n\n\nSee code to generate plot.\n# Create a data frame containing the model's residuals and predictions:\nresidual_data &lt;- data.frame(\n  fitted_values = fitted(model1),\n  residuals = residuals(model1)\n)\n\n# Create plot of fitted values vs. residuals:\nggplot(residual_data, \n       aes(x = fitted_values, y = residuals)) +\n  geom_point(alpha = 0.6) +\n  geom_hline(yintercept = 0, color = \"blue\", linewidth = 1) +\n  labs(x = \"Predicted Vocab scores (fitted values)\", \n       y = \"Model residuals\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure¬†12.11: Comparison of model residuals with predicted vocabulary scores\n\n\n\n\n\nIn Figure¬†12.11, we can see that the residuals are not randomly spread across both sides of the blue line, but rather that they form a funnel-like shape: the spread of residuals tends to be larger for low predicted Vocab scores and becomes progressively smaller for higher predicted scores. This pattern indicates that our model is more accurate when it comes to predicting high Vocab scores than lower ones. This constitutes a violation of the assumption of equal variance or homoscedasticity.\n\n\n12.4.4 Assumption 4: Normality of residuals\nIdeally, model residuals also ought to be normally distributed, with a mean of zero. However, this assumption becomes less important as the sample size increases (Williams, Grajales & Kurkiewicz 2013: 10). Again, it is best to check the distribution of residuals visually. As shown in Figure¬†12.12, the residuals of model1 are fairly normally distributed.\n\n\nSee code to generate plot.\nggplot(residual_data, \n       aes(x = residuals)) +\n  geom_density(fill = \"purple\",\n               alpha = 0.6) +\n  geom_vline(xintercept = 0,\n             colour = \"blue\",\n             linewidth = 0.8,\n             linetype = \"dotted\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure¬†12.12: Distribution of model residuals\n\n\n\n\n\nFigure¬†12.12 shows that the distribution of the residuals of model1 are slightly left-skewed, but that the distribution is more or less centered around zero. Indeed, the model‚Äôs mean residual is almost exactly zero:\n\nsummary(model1$residuals)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-57.7253 -10.7158   0.5255   0.0000  12.4168  36.0334 \n\n\nWhen the sample size is small and the model residuals are not normally distributed, the inferences that we can draw from the model are less trustworthy. However, with larger sample sizes, regression is relatively robust to the assumption of normally distributed residuals (Williams, Grajales & Kurkiewicz 2013: 3).",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Int`R`oduction to statistical modelling</span>"
    ]
  },
  {
    "objectID": "12_SimpleLinearRegression.html#sec-IntSummary",
    "href": "12_SimpleLinearRegression.html#sec-IntSummary",
    "title": "12¬† IntRoduction to statistical modelling",
    "section": "12.5 Intermediary summary",
    "text": "12.5 Intermediary summary\nIn this chapter, we fitted simple linear regression models. These models demonstrate that we can ‚Äì to a greater or lesser degree ‚Äì predict (or model) participants‚Äô receptive vocabulary knowledge test scores on the basis of a single predictor such as the number of years that they spent in formal education or their occupational group.\nWe have referred to the variable that we are trying to predict as the outcome variable. It is worth knowing that it is sometimes also called the dependent variable because we are attempting to model its ‚Äòdependence‚Äô on one or more independent variables (which we have called predictors). In Chapter 13, we will continue to use the terminology of outcome and predictor variables, but you should be aware that some textbooks and researchers will speak of dependent and independent variables, instead.3\nWhile our simple linear regression models did allow us to account for some of the variance in participants‚Äô Vocab test scores, like the statistical tests that we conducted in Chapter 11, they still only allow us to capture one aspect of receptive vocabulary knowledge at a time. What we really want to do is to be able to enter several predictor variables into a single model. For example, it would be interesting to know whether the number of years spent in formal education remains a significant predictor of Vocab scores when controlling for participants‚Äô occupational groups. This can be achieved with multiple linear regression, and that‚Äôs coming up in the next chapter!\n\nCheck your progress üåü\nWell done! You have successfully completed this chapter introducing the linear regression modelling. You have answered 0 out of 14 questions correctly.\nAre you confident that you can‚Ä¶?\n\nFit a simple linear model to predict a numeric outcome variable\nVisualise the predictions of a simple linear regression model\nFind out what the intercept of a model corresponds to and interpret the model‚Äôs coefficient estimates\nInterpret a model‚Äôs adjusted R2 coefficient\nCheck for the main assumptions of linear regression: independence, linearity, homoscedasticity, and normality of residuals (Section 12.4)\n\n\n\n\n\nBreheny, Patrick & Woodrow Burchett. 2017. Visualization of Regression Models Using visreg. The R Journal 9(2). 56. https://doi.org/10.32614/RJ-2017-046.\n\n\nDƒÖbrowska, Ewa. 2019. Experience, aptitude, and individual differences in linguistic attainment: A comparison of native and nonnative speakers. Language Learning 69(S1). 72‚Äì100. https://doi.org/10.1111/lang.12323.\n\n\nGries, Stefan Thomas. 2021. Statistics for linguistics with R: A practical introduction (De Gruyter Mouton Textbook). 3rd revised edition. de Gruyter Mouton.\n\n\nKaufman, Allison B. & James C. Kaufman (eds.). 2018. The illusion of causality: A cognitive bias underlying pseudoscience. In Pseudoscience. The MIT Press. https://doi.org/10.7551/mitpress/10747.003.0007.\n\n\nLenth, Russell V. 2025. Emmeans: Estimated marginal means, aka least-squares means. https://rvlenth.github.io/emmeans/.\n\n\nLevshina, Natalia. 2015. How to do linguistics with R: Data exploration and statistical analysis. John Benjamins.\n\n\nMatute, Helena, Fernando Blanco, Ion Yarritu, Marcos D√≠az-Lago, Miguel A. Vadillo & Itxaso Barberia. 2015. Illusions of causality: How they bias our everyday thinking and how they could be reduced. Frontiers in Psychology. Frontiers 6. https://doi.org/10.3389/fpsyg.2015.00888.\n\n\nWilliams, Matt N., Carlos Alberto G√≥mez Grajales & Dason Kurkiewicz. 2013. Assumptions of multiple regression: Correcting two misconceptions. Practical Assessment, Research, and Evaluation 18(11).\n\n\nWinter, Bodo. 2020. Statistics for linguists: An introduction using R. Routledge. https://doi.org/10.4324/9781315165547.",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Int`R`oduction to statistical modelling</span>"
    ]
  },
  {
    "objectID": "12_SimpleLinearRegression.html#footnotes",
    "href": "12_SimpleLinearRegression.html#footnotes",
    "title": "12¬† IntRoduction to statistical modelling",
    "section": "",
    "text": "This variable is sometimes referred to as the dependent variable (see Section 12.5).‚Ü©Ô∏é\nThese variables are also sometimes referred to as independent variables (see Section 12.5).‚Ü©Ô∏é\nThis choice of terminology is largely a personal one but, in my experience, learners find this terminology less confusing as it is immediately obvious what is being predicted (the outcome or dependent variable) and what is used to make the prediction (the predictors or the independent variables). Another advantage is that the term predictor can be used by itself (i.e.¬†without the word ‚Äúvariable‚Äù), which is handy because when we enter a categorical variable into a model, there are as many predictors as there are variable levels.‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Int`R`oduction to statistical modelling</span>"
    ]
  },
  {
    "objectID": "13_MultipleLinearRegression.html",
    "href": "13_MultipleLinearRegression.html",
    "title": "13¬† Multiple linear regRession modelling",
    "section": "",
    "text": "Chapter overview\nIn this chapter, you will learn how to:",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Multiple linear reg`R`ession modelling</span>"
    ]
  },
  {
    "objectID": "13_MultipleLinearRegression.html#from-simple-to-multiple-linear-regression-models",
    "href": "13_MultipleLinearRegression.html#from-simple-to-multiple-linear-regression-models",
    "title": "13¬† Multiple linear regRession modelling",
    "section": "13.1 From simple to multiple linear regression models",
    "text": "13.1 From simple to multiple linear regression models\nIn Chapter 12, we used the lm() function to fit simple linear regression models. We saw that, like the t.test() and the cor.test() functions, the lm() function takes a formula as its first argument. Schematically, the formula syntax for a simple linear regression model takes the form of:\noutcome.variable ~ predictor\nIn the second half of this chapter, we will continue to try to predict (i.e.¬†try to better understand) Vocab scores among L1 and L2 English speakers, but this time, we will do so with multiple predictors. To this end, we will use the + operator to add predictors to our model formula like this:\noutcome.variable ~ predictor1 + predictor2 + predictor3\nA linear regression model can include as many predictors as we like ‚Äì or rather, as is meaningful and we have data for! The predictors can be a mixture of numeric and categorical predictors. Multiple linear regression modelling is much more powerful than conducting individual statistical tests (as in Chapter 11) or several simple linear regression models (as in Chapter 12) because it enables us to quantify the strength of the association between an outcome variable and a predictor, while controlling for the other predictors ‚Äì in other words, while holding all the other predictors constant. Moreover, it also reduces the risk of reporting false positive results (i.e.¬†Type 1 error, see Section 11.7). In this chapter, we will see that we can learn much more about our data from a single multiple linear regression model than from a series of individual statistical tests or simple regression models.\n\n\n\n\n\n\nWarningPrerequisites\n\n\n\n\n\nAs with previous chapters, all the examples, tasks, and quiz questions from this chapter are based on data from:\n\nDƒÖbrowska, Ewa. 2019. Experience, Aptitude, and Individual Differences in Linguistic Attainment: A Comparison of Native and Nonnative Speakers. Language Learning 69(S1). 72‚Äì100. https://doi.org/10.1111/lang.12323.\n\nOur starting point for this chapter is the wrangled combined dataset that we created and saved in Chapter 9. Follow the instructions in Section 9.7 to create this R object.\nAlternatively, you can download Dabrowska2019.zip from the textbook‚Äôs GitHub repository. To launch the project correctly, unzip the file and then double-click on the Dabrowska2019.Rproj file.\nTo begin, load the combined_L1_L2_data.rds file that we created in Chapter 9. This file contains the full data of all the L1 and L2 participants from DƒÖbrowska (2019). The categorical variables are stored as factors, and obvious data entry inconsistencies and typos have been corrected.\n\nlibrary(here)\n\nDabrowska.data &lt;- readRDS(file = here(\"data\", \"processed\", \"combined_L1_L2_data.rds\"))\n\nBefore you get started, check that you have correctly imported the data by examining the output of View(Dabrowska.data) and str(Dabrowska.data). In addition, run the following lines of code to load the {tidyverse} and create ‚Äúclean‚Äù versions of both the L1 and L2 datasets as separate R objects.\n\nlibrary(tidyverse)\n\nL1.data &lt;- Dabrowska.data |&gt; \n  filter(Group == \"L1\")\n\nL2.data &lt;- Dabrowska.data |&gt; \n  filter(Group == \"L2\")\n\nOnce you are satisfied that the data have been correctly imported and that you are familiar with the dataset, you are ready to tap into the potential of multiple linear regression modelling! üöÄ",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Multiple linear reg`R`ession modelling</span>"
    ]
  },
  {
    "objectID": "13_MultipleLinearRegression.html#sec-MultipleLM",
    "href": "13_MultipleLinearRegression.html#sec-MultipleLM",
    "title": "13¬† Multiple linear regRession modelling",
    "section": "13.2 Combining multiple predictors",
    "text": "13.2 Combining multiple predictors\nIn the following, we will attempt to model the variability in the receptive English vocabulary test scores of L1 and L2 English participants from DƒÖbrowska (2019). To this end, we will use multiple numeric and categorical predictors:\n\nparticipants‚Äô native-speaker status (Group)\ntheir Age,\ntheir occupational group (OccupGroup),\ntheir Gender\ntheir non-verbal IQ test score (Blocks), and\nthe number of years they were in formal education (EduTotal).\n\nWe use the + operator to construct the model formula. It doesn‚Äôt matter in which order we list the predictors, as the model will consider them all simultaneously.\n\nmodel4 &lt;- lm(Vocab ~ Group + Age + OccupGroup + Gender + Blocks + EduTotal, \n             data = Dabrowska.data)\n\nWe can then examine the model summary just like we did in Chapter 12 using the summary() function:\n\nsummary(model4)\n\n\nCall:\nlm(formula = Vocab ~ Group + Age + OccupGroup + Gender + Blocks + \n    EduTotal, data = Dabrowska.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-62.825 -10.838   1.831  12.185  38.345 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    4.2113    10.7831   0.391 0.696691    \nGroupL2      -22.2896     3.4854  -6.395 1.98e-09 ***\nAge            0.3718     0.1401   2.654 0.008812 ** \nOccupGroupI    9.9519     5.5999   1.777 0.077600 .  \nOccupGroupM   -3.9993     4.5139  -0.886 0.377050    \nOccupGroupPS  -1.0629     4.3876  -0.242 0.808919    \nGenderM       -3.7177     3.1387  -1.184 0.238131    \nBlocks         1.3011     0.3218   4.043 8.44e-05 ***\nEduTotal       2.4308     0.6293   3.863 0.000167 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.65 on 148 degrees of freedom\nMultiple R-squared:  0.3621,    Adjusted R-squared:  0.3276 \nF-statistic:  10.5 on 8 and 148 DF,  p-value: 1.327e-11\n\n\nAs with the simple linear regression models, we begin our interpretation of the model summary with the coefficient estimate for the intercept (see Section 12.2). The first question we ask ourselves is:\n\nIn this model, what does the intercept correspond to?\n\nRemember that the reference levels of categorical predictors correspond to the first level of these variables. This is why, here, the coefficient estimate for the intercept corresponds to a female English native speaker with a clerical occupation:\n\nlevels(Dabrowska.data$Gender) # 'Female' is the first level.\n\n[1] \"F\" \"M\"\n\nlevels(Dabrowska.data$Group) # 'L1' is the first level.\n\n[1] \"L1\" \"L2\"\n\nlevels(Dabrowska.data$OccupGroup) # 'C' corresponding to a clerical professional occupation is the first level.\n\n[1] \"C\"  \"I\"  \"M\"  \"PS\"\n\n\nThe reference level of the numeric predictors, by contrast, corresponds to the value of¬†zero. In model4, therefore, the estimated coefficient for the intercept corresponds to the predicted Vocab score of an adult English native speaker who belongs to the occupational group ‚ÄúC‚Äù, is female, who is aged 0 (!), scored 0 on the Blocks test, and spent 0 years (!) in formal education. Needless to say that trying to interpret this value is utterly meaningless! This is why, in this case, it makes sense to center our numeric predictor variables before entering them into our model. Another way to go about this would be to standardise the predictors using a z-transformation (see e.g. Winter 2020: Section 5.2).",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Multiple linear reg`R`ession modelling</span>"
    ]
  },
  {
    "objectID": "13_MultipleLinearRegression.html#centering-numeric-predictors",
    "href": "13_MultipleLinearRegression.html#centering-numeric-predictors",
    "title": "13¬† Multiple linear regRession modelling",
    "section": "13.3 Centering numeric predictors",
    "text": "13.3 Centering numeric predictors\nCentering involves subtracting a variable‚Äôs average from each value in the variable. Typically, we subtract the mean from each value but, given that we saw that many of the numeric variables in our dataset are not at all normally distributed (see Section 8.2.2), here, we will subtract the median instead.\nTo this end, we use the mutate() function to add three columns to the R data object Dabrowska.data. These new columns contain transformed versions of the predictor variables that we previously entered into our model:\n\nDabrowska.data &lt;- Dabrowska.data |&gt; \n  mutate(Age_c = Age - median(Age),\n         Blocks_c = Blocks - median(Blocks),\n         EduTotal_c = EduTotal - median(EduTotal))\n\nWe have centred the values of the three numeric predictor variables so that a value of zero in the transformed version corresponds to the variable‚Äôs original median value (see Table¬†13.1).\n\n\n\n\nTable¬†13.1: Comparison of the original, untransformed variables with the new, centered ones in a random sample of observations from the data\n\n\n\n\n\n\nAge\nAge_c\nBlocks\nBlocks_c\nEduTotal\nEduTotal_c\n\n\n\n\n21\n-10\n16\n0\n17\n3\n\n\n27\n-4\n21\n5\n18\n4\n\n\n25\n-6\n21\n5\n18\n4\n\n\n31\n0\n8\n-8\n13\n-1\n\n\n37\n6\n9\n-7\n12\n-2\n\n\n60\n29\n1\n-15\n10\n-4\n\n\n20\n-11\n17\n1\n12\n-2\n\n\n25\n-6\n20\n4\n12\n-2\n\n\n62\n31\n8\n-8\n14\n0\n\n\n42\n11\n21\n5\n15\n1\n\n\n\n\n\n\n\n\nFor each pair of variables, the value of¬†0 in the centered variable corresponds to the median value of the untransformed variable. As a result, in the centered variables, each data point is expressed in terms of how much it is either above the median (positive score) or below the median (negative score).",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Multiple linear reg`R`ession modelling</span>"
    ]
  },
  {
    "objectID": "13_MultipleLinearRegression.html#interpreting-a-model-summary",
    "href": "13_MultipleLinearRegression.html#interpreting-a-model-summary",
    "title": "13¬† Multiple linear regRession modelling",
    "section": "13.4 Interpreting a model summary",
    "text": "13.4 Interpreting a model summary\nWe can now fit a new multiple linear regression model that attempts to predict Vocab scores with the same predictors as model4 above, except that we are now entering the centered numeric predictor variables instead of the original untransformed ones. The categorical predictor variables remain the same.\n\nmodel4_c &lt;- lm(Vocab ~ Group + Age_c + OccupGroup + Gender + Blocks_c + EduTotal_c, \n             data = Dabrowska.data)\n\nsummary(model4_c)\n\n\nCall:\nlm(formula = Vocab ~ Group + Age_c + OccupGroup + Gender + Blocks_c + \n    EduTotal_c, data = Dabrowska.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-62.825 -10.838   1.831  12.185  38.345 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   70.5852     3.7045  19.054  &lt; 2e-16 ***\nGroupL2      -22.2896     3.4854  -6.395 1.98e-09 ***\nAge_c          0.3718     0.1401   2.654 0.008812 ** \nOccupGroupI    9.9519     5.5999   1.777 0.077600 .  \nOccupGroupM   -3.9993     4.5139  -0.886 0.377050    \nOccupGroupPS  -1.0629     4.3876  -0.242 0.808919    \nGenderM       -3.7177     3.1387  -1.184 0.238131    \nBlocks_c       1.3011     0.3218   4.043 8.44e-05 ***\nEduTotal_c     2.4308     0.6293   3.863 0.000167 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.65 on 148 degrees of freedom\nMultiple R-squared:  0.3621,    Adjusted R-squared:  0.3276 \nF-statistic:  10.5 on 8 and 148 DF,  p-value: 1.327e-11\n\n\nIf you compare the summary of model4 with that of model4_c, you will notice that, whilst the intercept coefficient estimate has changed, all the other coefficient estimates have stayed the same.\nLet‚Äôs decipher the summary of model4_c step-by-step:\n\nIn this model, the estimated coefficient for the intercept corresponds to the predicted Vocab score of an English native speaker (Group¬†=¬†L1) who is aged 31 (the median Age), belongs to the occupational group C, is female, scored 16 on the Blocks test (the median Blocks score), and was in formal education for 14 years (the median number of years).\nAs with the simple linear regression models that we computed in Chapter 12, the coefficient estimates of the numeric predictors (Age_c, Blocks_c, and EduTotal_c) correspond to increases or decreases in Vocab scores for each additional unit of that predictor variable, whilst keeping all other predictors at their reference level. Remember that, for numeric predictors, the reference level is 0, which, given that we have centered them, corresponds to the variable‚Äôs median value. Looking at the coefficient estimate for Blocks_c (1.3011), this means that someone who scored one point more than the median score in the Blocks test is predicted to have a Vocab test result that is 1.3011 points higher than the intercept, namely:\n\n70.5852 + 1.3011\n\n[1] 71.8863\n\n\nTo calculate the predicted Vocab score of an L1 female speaker in a clerical occupation, of median age, who spent the median number of years in formal education, but scored an impressive 26 points on the Blocks test, we multiply the estimated Blocks_c coefficient (1.3011) by 26 minus the median Blocks score (16) and add this to the intercept coefficient:\n\n70.5852 + 1.3011*(26 - median(Dabrowska.data$Blocks))\n\n[1] 83.5962\n\n\nYou‚Äôll be pleased to read that the interpretation of coefficient estimates of categorical predictors is less involved. Recall that, for categorical variables, the reference level is always the variable‚Äôs first level (see Section 13.2). If we want to make a prediction for an L2 instead of an L1 female speaker, but keep all other predictors at the reference level (i.e.¬†clerical occupation, median age, median number of years in formal education, and median Blocks score), all we need to do is add the coefficient estimate for GroupL2 (-22.2896) to the intercept coefficient. Given that this coefficient is negative, this addition will result in a predicted Vocab score that is lower than the model‚Äôs reference level:\n\n70.5852 + -22.2896\n\n[1] 48.2956\n\n\nSince very few people are perfectly average, let us now calculate the predicted Vocab score of an actual, randomly chosen person: the 154th participant in our dataset. As shown below using the {tidyverse} function slice(), the 154th participant is a 46-year-old Polish male driver who scored 23 points on the Blocks test and attended formal education for 10 years.\n\nDabrowska.data |&gt; \n  slice(154) |&gt; \n  select(Group, Age, NativeLg, Occupation, OccupGroup, Gender, Blocks, EduTotal)\n\n  Group Age NativeLg Occupation OccupGroup Gender Blocks EduTotal\n1    L2  46   Polish     Driver          M      M     23       10\n\n\nTo obtain the model‚Äôs predicted Vocab score for a 46-year-old male L2 speaker with a manual occupation (M) who scored 23 points on the Blocks test and was in formal education for 10 years (EduTotal), we combine the coefficient estimates as follows:\n\n170.5852 +\n2-22.2896 +\n30.3718 * (46 - median(Dabrowska.data$Age)) +\n4-3.9993 +\n5-3.7177 +\n61.3011 * (23 - median(Dabrowska.data$Blocks)) +\n72.4308 * (10 - median(Dabrowska.data$EduTotal))\n\n\n1\n\nIntercept coefficient\n\n2\n\nL2 speaker\n\n3\n\n46 years old\n\n4\n\nManual occupation (OccupGroupM)\n\n5\n\nMale\n\n6\n\n23 points on Blocks test\n\n7\n\n10 years in formal education\n\n\n\n\n[1] 45.5401\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou can hover your mouse over the circled numbers to find out how this predicted score was calculated. All the coefficient estimates were copied from the model summary output by summary(model4_c).\n\n\nWe can check that we did the maths correctly by outputting the model‚Äôs prediction for the 154th observation directly. To this end, we apply the predict() function to the model object model4_c and extract the model‚Äôs predicted score for the 154th data point:\n\npredict(model4_c)[154]\n\n     154 \n45.53989 \n\n\nAs you can see, we obtain the same predicted Vocab score. The minor difference after the decimal point is due to us using coefficient estimates rounded-off to four decimal places (as displayed in the model‚Äôs summary output) rather than the exact values to which the predict() function has access.\nThis is all very well, but how accurate is this model prediction? To find out, we can compare this predicted score (that our model predicts for any male 46-year old with a manual occupation, a Blocks score of 23, and 10 years in formal education) to the 46-year-old Polish driver‚Äôs actual Vocab score:\n\nDabrowska.data |&gt; \n  slice(154) |&gt; \n  select(Group, Age, NativeLg, Occupation, OccupGroup, Gender, Blocks, EduTotal, Vocab)\n\n  Group Age NativeLg Occupation OccupGroup Gender Blocks EduTotal    Vocab\n1    L2  46   Polish     Driver          M      M     23       10 48.88889\n\n\nOur model prediction (45.53989) is close to our Polish driver‚Äôs real Vocab test score (48.88889), but our model has slightly underestimated his performance. This underestimation results in a positive residual. The residual is positive because there are points ‚Äúleft over‚Äù by the model. Model residuals are calculated by subtracting a model‚Äôs prediction from the real, observed value of the outcome variable for any specific data point. In our case, we substract our model‚Äôs predicted Vocab score for a male 46-year old with a manual occupation, a Blocks score of 23, and 10 years in formal education from the Polish 46-year-old driver‚Äôs actual Vocab score:\n\n48.88889 - 45.53989\n\n[1] 3.349\n\n\nResiduals are also stored in the model object and be accesssed like this:\n\nmodel4_c$residuals[154]\n\n     154 \n3.348998 \n\n\nWe can compare this particular residual (corresponding to the 154th participant in the dataset) to all model residuals (corresponding to all participants) by examining the summary statistics of all residuals, which are displayed at the top of the model summary output (summary(model4_c)). These descriptive statistics inform us that the residual for the 154th participant is slightly higher than the average (median) residual, but well within the IQR (see Section 8.3.2) of model residuals:\nResiduals:\n    Min      1Q  Median      3Q     Max \n-62.825 -10.838   1.831  12.185  38.345 \nLooking at the minimum and maximum residuals, we can see that our model overestimates at least one person‚Äôs Vocab score by 63 points (this is the most negative residual: Min), whilst in another case it underestimates it by 38 points (this is the largest positive residual: Max).\n\n\nClick here to find out whose score was most underestimated!\n# Run the following code to find out more about the participant whose Vocab score was most dramatically underestimated by our model:\n\nDabrowska.data |&gt; \n  slice(which.max(model4_c$residuals)) |&gt; \n  select(Group, NativeLg, Age, Occupation, Gender, Blocks, EduTotal, Vocab)\n\n\nThe p-values associated with each coefficient estimate in the model summary indicate which predictor variables (or predictor variable levels, in the case of categorical variables) make a statistically significant contribution to the model. You may recall that, in Section 12.3, we fitted a simple linear regression model (model3) which included only OccupGroup as a single predictor. In this simple model, the coefficient estimate for OccupGroupI made a statistically significant contribution to the model at an Œ±-level of 0.05 (p¬†=¬†0.0335). By contrast, in the present multiple linear regression model, the predictor level OccupGroupI does not make a statistically significant contribution (p¬†=¬†0.077600 which is greater than 0.05). This is because our multiple regression model model4_c includes predictors that account for some of the same variance in Vocab scores in the data that occupational groups accounted for in our earlier model. This leaves less unique variance attributable to occupational group, thereby rendering it statistically non-significant.\nFrom the adjusted R-squared value (0.3276) displayed at the bottom of the model summary output, we can see that our multiple linear regression model accounts for around 33% of the total variance in Vocab scores found in the DƒÖbrowska (2019) data. This is considerably more than we achieved with any of the simple linear models that we fitted in Chapter 12.\n\n\n13.4.1 Interpreting model predictions\nWhilst it is important to understand how to interpret the coefficients of a multiple linear regression model from the model summary, in practice, it is also always a very good idea to visualise model predictions. On the one hand, this reduces the risk of making any obvious interpretation errors and, on the other, it is much easier to interpret the residuals of a model when they are visualised alongside model predictions.\nTo fully visualise the predictions of a multiple linear model, we would need to be able to plot as many dimensions as there are predictors in the model. The trouble is that, as humans, we find it difficult to interpret data visualisations with more than two dimensions. Indeed, although 3D plots can sometimes be useful (and can easily be generated in R), we are much better at interpreting two-dimensional plots. To bypass this inherent human weakness, we will plot the values predicted by our model on several plots: one for each predictor variable (see Figure¬†13.1). Run the following command and then follow the instructions displayed in the Console pane to view the plots one by one. You may need to resize your Plot pane for the plots to be displayed. If you have a small screen, you may find the ‚Äú:magnifying-glass-tilted-right: Zoom‚Äù option in RStudio‚Äôs Plots pane useful as it allows you to view the plots in a separate window.\n\nlibrary(visreg)\n\nvisreg(model4_c)\n\n\n\n\n\n\n\n\n\nFigure¬†13.1: Vocab scores as predicted by model4_c (blue lines) as a function of various predictor variables and partial model residuals (grey points)\n\n\n\n\n\nOn plots produced by the visreg() function, as in Figure¬†12.9, the model‚Äôs predicted values are displayed as blue lines. By default, these predictions are surrounded by 95% confidence bands, which are visualised as grey areas. Now that we have several predictors in our model, the points in visreg() plots represent the model‚Äôs partial residuals. Partial residuals are the left-over variance (i.e.¬†the residual) relative to the predictor that we are examining after having subtracted off the contribution of all the other predictors in the model.\nWhen interpreting the numeric predictors plotted in Figure¬†13.1 above, it is important to remember that we entered centered numeric predictors in model4_c. This means that an Age_c value of¬†0 corresponds to the median age in the dataset: 31 years. This is why Figure¬†13.1 features negative ages: the negative values correspond to participants who are younger than 31. By contrast, positive scores correspond to participants who are older than 31 years. This is hardly intuitive so, for the purposes of visualising and interpreting our model, it is best to transform these variables back to their original scale (see Figure¬†13.2). This is achieved by adding the following xtrans argument within the visreg() function.\n\n\n\n\n\n\nWarning\n\n\n\nDue to a bug that was introduced in the latest version of the {visreg} package, it is currently necessary to install version 2.7 of the {visreg} package for the xtrans argument to work as expected. You can find out which package version you currently have installed, if any, using this function:\n\npackageVersion(\"visreg\")\n\nIf you have version 2.8.0, begin by deleting your current installation:\n\nremove.packages(\"visreg\")\n\nThen, install the {remotes} package, which allows you to install older versions of packages (if they are compatible with your R version). You will likely get a message notifying you that it is necessary to restart your R session. Click ‚Äúyes‚Äù.\n\ninstall.packages(\"remotes\")\n\nFinally, load the {remotes} library and then install version 2.7.0 of the {visreg} package:\n\nlibrary(remotes)\ninstall_version(\"visreg\", \"2.7.0\")\n\n\n\n\nlibrary(visreg)\n\nvisreg(model4_c, \n       xvar = \"Age_c\", \n       xtrans = function(x) x + median(Dabrowska.data$Age), \n       gg = TRUE) + \n  labs(x = \"Age (in years)\",\n       y = \"Predicted Vocab scores\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nFigure¬†13.2: Vocab scores as predicted by model4_c for speakers of various ages (blue line) and partial model residuals represented as points. In the version printed in the textbook, two data points are highlighted: one representing a very large positive residual and one representing a very small residual.\n\n\n\n\n\nIn Figure¬†13.2, we focus on one predictor from our model: Age. We know from our model summary that the coefficient estimate for age is positive (0.3718), hence the upward blue line. The grey 95% confidence band visualises the uncertainty around the estimated mean predicted Vocab scores.\nThe points on Figure¬†13.2 represent the partial residuals. This means that there are as many points as there are participants in the dataset used to fit the model. The further away a point is from the predicted value (the blue line), the poorer the prediction is for that particular participant.\nSome of the partial residuals are particularly large: the Vocab score of the 23-year-old Lithuanian bar staff, for instance, is vastly underestimated. By contrast, the 46-year-old Polish driver‚Äôs predicted score is well within the confidence band of the predicted Vocab score for his age, indicating that our model makes a pretty accurate prediction for this L2 speaker.\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nThe dataset from DƒÖbrowska (2019) includes a variable that we have not explored yet: ART. It stands for Author Recognition Test (Acheson, Wells & MacDonald 2008) and is a measure of print exposure to (almost exclusively Western, English-language) literature. DƒÖbrowska (2019:¬†9) explains the testing instrument and her motivation for including it in her battery of tests as follows:\n\nThe test consists of a list of 130 names, half of which are names of real authors. The participants‚Äô task is to mark the names that they know to be those of published authors. To penalize guessing, the score is computed by subtracting the number of foils [wrong guesses] from the number of real authors selected. Thus, the maximum possible score is 65, and the minimum score could be negative if a participant selects more foils than real authors. When this happened, the negative number was replaced with 0.\nThe ART has been shown to be a valid and reliable measure of print exposure, which, unlike questionnaire-based measures, is not contaminated by socially desirable responses and assesses lifetime reading experience as opposed to current reading (see Acheson et al., 2008; Stanovich & Cunningham, 1992).\n\nQ13.1 For this task, you will fit a new multiple linear regression model that predicts Vocab scores among L1 and L2 speakers using all of the predictors that we used in model4_c plus ART scores. Which formula do you need to specify inside the lm()function to fit this new model (which we will name model.ART)?\n\n\n\n\n\nVocab ~ Group + Age_c + OccupGroup + Gender + Blocks_c + EduTotal_c * ART\n\n\n\n\nVocab ~ Group + Age_c + OccupGroup + Gender + Blocks_c + EduTotal_c + ART\n\n\n\n\nVocab ~ model4_c + ART\n\n\n\n\nART ~ Group + Age_c + OccupGroup + Gender + Blocks_c + EduTotal_c\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\nQ13.2 Fit a model using the correct formula from above and save it as model.ART. Examine the summary of your new model. Which of these statements is/are true?\n\n\n\n\n\nRecognising more real authors on the ART is associated with higher Vocab scores.\n\n\n\n\nThe model that does not include ART (model4_c) accounts for more of the variance in Vocab scores than the model that does.\n\n\n\n\nART makes a statistically significant contribution to the model at the Œ±-level of 0.05.\n\n\n\n\nThe model that includes ART accounts for more of the variance in Vocab scores than the model that does not (model4_c).\n\n\n\n\nRecognising more real authors on the ART is associated with lower Vocab scores.\n\n\n\n\nART does not make a statistically significant contribution to the model at the Œ±-level of 0.05.\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n\n\nShow sample code to answer Q13.2.\n# First, it is always a good idea to visualise the data before fitting a model to have an idea of what to expect:\nDabrowska.data |&gt; \n  ggplot(mapping = aes(y = Vocab,\n                       x = ART)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  theme_bw()\n\n# We fit the new model with ART as an additional predictor using the formula from Q13.1:\nmodel.ART &lt;- lm(Vocab ~ Group + Age_c + OccupGroup + Gender + Blocks_c + EduTotal_c + ART, \n             data = Dabrowska.data)\n\n# We examine the coefficients of this new model:\nsummary(model.ART)\n\n# To find out how much variance the model accounts for compared to our prior model, compare the adjusted R-squared values of the two models:\nsummary(model4_c)\nsummary(model.ART)\n\n\nHave you noticed how, having added ART as a predictor to your model, Age no longer makes a statistically significant contribution to the model (p¬†=¬†0.070994)? How come? Well, it turns out that ART and Age are correlated (r¬†=¬†0.32, see Figure¬†13.3). This moderate positive correlation makes intuitive sense: the older you are, the more likely you are to have come across more authors and therefore be able to correctly identify them in the Author Recognition Test. Of the two predictors, our model suggests that ART scores are a more useful predictor of Vocab scores than age. In other words, if we know the ART score of a participant from the DƒÖbrowska (2019) dataset, their age does not provide additional information that can help us to better predict their Vocab score.\n\n\n\n\n\n\n\n\nFigure¬†13.3: Observed relationship between participants‚Äô age and their Author Recognition Test (ART) scores. The blue line shows that there is a moderate positive correlation between these two variables.\n\n\n\n\n\nQ13.3 What is the predicted Vocab score of a female L1 speaker aged 31 (the median age) with a manual occupation, who scored 16 on the Blocks test (the median score), spent 14 years in formal education (median time period), and scored 20 on the Author Recognition Test?\n\n\n\n\n\n71.1133\n\n\n\n\n8.5405\n\n\n\n\n74.1007\n\n\n\n\n73.6248\n\n\n\n\n74.3556\n\n\n\n\n123.3076\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n\n\nShow sample code to answer Q13.3.\n# We must add together the coefficient for the intercept, the coefficient for the manual occupation (which is negative), and the ART coefficient multiplied by 20 (as we did not center this predictor):\n62.5728 + -2.5115 + 0.5526*20\n\n\nQ13.4 By default, the visreg() function displays a 95% confidence band around predicted values, corresponding to a significance level of 0.05. How can you change this default to display a 99% confidence band instead? Run the command ?visreg() to find out how to achieve this from the function‚Äôs help file.\n\n\n\n\n\nvisreg(model.ART, xvar = \"ART\", trans = CI(0.99))\n\n\n\n\nvisreg(model.ART, xvar = \"ART\", alpha = 99)\n\n\n\n\nvisreg(model.ART, xvar = \"ART\", CI = 0.01)\n\n\n\n\nvisreg(model.ART, xvar = \"ART\", alpha = \"99%\")\n\n\n\n\nvisreg(model.ART, xvar = \"ART\", alpha = 0.01)\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n\n\nShow sample code to answer Q13.4.\n# The argument that you need to change is called \"alpha\". This is because the significance level is also called the alpha level (see Section 11.3). \n\n# The alpha value that corresponds to a 99% confidence interval is equal to:\n1 - 0.99\n\n# Hence the code should (minimally) include these arguments:\nvisreg(model.ART, \n       xvar = \"ART\", \n       alpha = .01)\n\n\nObserve how the width of the confidence bands changes when you increase the significance level from 0.01 to 0.1: the higher the significance level, the narrower the confidence band becomes. Remember that the significance level corresponds to the risk that we are willing to take of wrongly rejecting the null hypothesis when it is actually true. Therefore, when we lower this risk by choosing a lower significance level (i.e.¬†0.01), we end up with wider confidence bands that are more likely to allow us to draw a flat line (see Section 11.5). If we can draw a flat line through a 99% confidence band, this means that we do not have enough evidence to reject the null hypothesis at the significance level of 0.01.",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Multiple linear reg`R`ession modelling</span>"
    ]
  },
  {
    "objectID": "13_MultipleLinearRegression.html#relative-importance-of-predictors",
    "href": "13_MultipleLinearRegression.html#relative-importance-of-predictors",
    "title": "13¬† Multiple linear regRession modelling",
    "section": "13.5 Relative importance of predictors",
    "text": "13.5 Relative importance of predictors\nWhen interpreting a model summary, it is important to remember that the coefficient estimates of each predictor correspond to a change in prediction for a one-unit change in that predictor, e.g.¬†for the predictor Age, an increase of one year. However, within a single model, it‚Äôs quite common for the predictor variables to be measured in different units. For example, in summary(model4_c), the coefficient estimate for Age_c is 0.3718, which means that, holding all other predictors constant, for every year that a participant is older than the median age, their predicted Vocab score increases by 0.3718. By contrast, the coefficient estimate for the Blocks_c predictor (1.3011) corresponds to an increase in Vocab scores for every additional point that participants score on the non-verbal IQ Blocks test. Its unit is therefore test points. For categorical predictor variables, a single-unit change represents a change from one predictor level to another, e.g.¬†from L1 to L2, or from female to male.\nBecause they are in different units that represent different quantities or levels, the raw coefficient estimates of a model cannot be compared to each other. Indeed, depending on the predictor, a one-unit change may correspond to either a big or a small change. This is where measures of the relative importance of predictors come in handy. In this chapter, we will use the lmg metric (that was first proposed by Lindeman, Merenda & Gold in 1980: 119 ff.) to compare the importance of predictors within a single multiple linear regression model.\nAlthough not currently widely used in the language sciences (but see, e.g. DƒÖbrowska 2019: 14), lmg has a number of advantages. Its interpretation is fairly intuitive because it is similar to a coefficient of determination (R2): a value of¬†0 means that, in this model, a predictor accounts for 0% of the variance in the outcome variable, while a value of¬†1 would mean that it can be used to perfectly predict the outcome variable. Calculating lmg values is computationally involved because the metric includes both direct effects and is adjusted for all the other predictors of the model. But this need not worry us because a researcher and statistician, Ulrike Gr√∂mping, has developed and published an R package that will do the computation for us. :smiling-face-with-smiling-eyes:\nOnce we have installed and loaded the {relaimpo} package1 (Gr√∂mping 2006), we can use its calc.relimp() function to calculate a range of relative importance metrics for linear models. With the argument ‚Äútype‚Äù, we specify that we are interested in the lmg metric:\n\ninstall.packages(\"relaimpo\")\nlibrary(relaimpo)\nselect &lt;- dplyr::select\n\nrel.imp.metric &lt;- calc.relimp(model4_c, \n                              type = \"lmg\")\n\nThe output of the function is long. We have therefore saved it as a new R object (rel.imp.metric) in order to retrieve only the part that we are interested in, namely the lmp values for each predictor, as a single data frame (rel.imp.metric.df):\n\nrel.imp.metric.df &lt;- data.frame(lmg = rel.imp.metric$lmg) |&gt; \n    rownames_to_column(var = \"Predictor\")\n\nWe display this data frame in descending order of lmp values, rounded to two decimal values:\n\nrel.imp.metric.df |&gt; \n  mutate(lmg = round(lmg, digits = 2)) |&gt; \n  arrange(-lmg)\n\n   Predictor  lmg\n1      Group 0.15\n2 OccupGroup 0.07\n3 EduTotal_c 0.05\n4      Age_c 0.04\n5   Blocks_c 0.04\n6     Gender 0.00\n\n\nThese values indicate that, on average, whether or not a participant is a native or non-native speaker of English (Group) makes the biggest difference in terms of predicted Vocab scores. Gender, by contrast, is practically irrelevant in this model.\nWe can also visualise these values in the form of a bar plot (see Figure¬†13.4). Note that another nice thing about the lmg metric is that the lmg values for all the predictors entered in model4_c add up to the model‚Äôs (unadjusted) multiple R2 (0.3621).\n\nrel.imp.metric.df |&gt; \n  ggplot(mapping = aes(x = fct_reorder(Predictor, lmg),\n                       y = lmg)) +\n  geom_col() +\n  theme_minimal() +\n  labs(x = \"Predictor in model4_c\") +\n  coord_flip() \n\n\n\n\n\n\n\nFigure¬†13.4: Relative importance of predictors in model 4_c\n\n\n\n\n\nIt is also worth noting that the second most important predictor is OccupGroup. This may come as a surprise given that none of its levels (or rather none of the differences between the reference level of C for clerical occupations and the remaining levels) make a statistically significant contribution to the model. This result illustrates the value of this type of analysis compared to only looking at a model‚Äôs coefficient estimates.\nMany studies will compare the coefficient estimates of multiple regression models using standardised coefficients (see e.g. Winter 2020: Section 6.2); however, this approach can lead to misleading conclusions (see e.g. Mizumoto 2023).\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nIn the last Your turn! section, you fitted a multiple linear regression model to predict Vocab scores among L1 and L2 speakers using all of the predictors that we used in model4_c plus ART scores.\nQ13.5 Calculate the relative importance of each of the predictors in this model using the {relaimpo} package. What is the relative importance of the predictor ART in this model as estimated by the lmg metric and rounded to two decimal places?\n\n\n\n\n\n\n\n\n\n\n\n\nShow sample code to answer Q13.5.\n# Fit the model:\nmodel.ART &lt;- lm(Vocab ~ Group + Age_c + OccupGroup + Gender + Blocks_c + EduTotal_c + ART, \n             data = Dabrowska.data)\n\n# Compute the lmg values and extract them as a data frame\nlibrary(relaimpo)\nrel.imp.metric &lt;- calc.relimp(model.ART, \n                              type = \"lmg\")\n\nrel.imp.metric.df &lt;- data.frame(lmg = rel.imp.metric$lmg) |&gt; \n    rownames_to_column(var = \"Predictor\")\n\nrel.imp.metric.df |&gt; \n  mutate(lmg = round(lmg, digits = 2)) |&gt; \n  arrange(-lmg)",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Multiple linear reg`R`ession modelling</span>"
    ]
  },
  {
    "objectID": "13_MultipleLinearRegression.html#modelling-interactions-between-predictors",
    "href": "13_MultipleLinearRegression.html#modelling-interactions-between-predictors",
    "title": "13¬† Multiple linear regRession modelling",
    "section": "13.6 Modelling interactions between predictors",
    "text": "13.6 Modelling interactions between predictors\nOne of the strengths of multiple linear regression is that we can also model interactions between predictors. This is important because a predictor‚Äôs relationship with the outcome variable may depend on another predictor. Consider Age as a predictor of Vocab scores. In model4_c, we saw that this predictor made a statistically significant contribution to the model. The coefficient was positive, which means that the model predicts that the older the participants, the higher their receptive English vocabulary.\nHowever, if you completed Q11.10, you might remember that Age correlates positively with Vocab scores among L1 participants, but does not among L2 participants (see Figure¬†13.5 below). If anything, the correlation visualised in the right-hand panel of Figure¬†13.5 is slightly negative. Moreover, the confidence band is wide enough to draw a horizontal line through it, which suggests that the data are also compatible with the null hypothesis of no correlation. As a result, we can conclude that this slightly negative correlation is not statistically significant.\n\nDabrowska.data |&gt; \n   ggplot(mapping = aes(x = Age, \n                        y = Vocab)) +\n     geom_point() +\n     facet_wrap(~ Group) +\n     geom_smooth(method = \"lm\", \n                 se = TRUE) +\n  theme_bw()\n\n\n\n\n\n\n\nFigure¬†13.5: Observed correlation between participants‚Äô vocabulary scores and their age in both the L1 and L2 groups\n\n\n\n\n\nFigure¬†13.5 informs us that what we really need is a model that can account for the fact that Age is probably a useful predictor of Vocab scores for L1 speakers, but not so much for L2 speakers. In other words, we‚Äôd like to model an interaction between the predictors Age and Group.\nIn R‚Äôs formula syntax, interaction terms are denoted with a colon (:) or an asterisk (*). In the following model, therefore, we are attempting to predict Vocab scores on the basis of a person‚Äôs native-speaker status (Group), their age, occupational group, gender, Blocks test score, number of years in formal education, and the interaction between their age and native-speaker status (Age_c:Group):\n\nmodel5 &lt;- lm(Vocab ~ Group + Age_c + OccupGroup + Gender + Blocks_c + EduTotal_c + Age_c:Group, \n             data = Dabrowska.data)\n\nsummary(model5)\n\n\nCall:\nlm(formula = Vocab ~ Group + Age_c + OccupGroup + Gender + Blocks_c + \n    EduTotal_c + Age_c:Group, data = Dabrowska.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-66.720 -10.651   1.224  11.939  45.415 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    69.1314     3.6286  19.052  &lt; 2e-16 ***\nGroupL2       -19.9128     3.4699  -5.739 5.26e-08 ***\nAge_c           0.5466     0.1471   3.717 0.000286 ***\nOccupGroupI     7.8267     5.4824   1.428 0.155525    \nOccupGroupM    -4.0769     4.3852  -0.930 0.354058    \nOccupGroupPS   -1.7774     4.2686  -0.416 0.677729    \nGenderM        -1.6285     3.1213  -0.522 0.602644    \nBlocks_c        1.2414     0.3132   3.964 0.000115 ***\nEduTotal_c      2.5520     0.6126   4.166 5.27e-05 ***\nGroupL2:Age_c  -0.8982     0.2867  -3.133 0.002089 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.12 on 147 degrees of freedom\nMultiple R-squared:  0.402, Adjusted R-squared:  0.3654 \nF-statistic: 10.98 on 9 and 147 DF,  p-value: 5.537e-13\n\n\nIn the model summary above, the interaction term between Age and Group is the last coefficient listed (GroupL2:Age_c). The values confirm our intuition based on our descriptive visualisation of the data (Figure¬†13.5): there is a statistically significant interaction between age and native-speaker status (p¬†=¬†0.002089) and the interaction coefficient for this interaction term (Age_c:Group) is negative (-0.8982). This means that, whilst being older is generally associated with higher Vocab scores for the reference level of L1 speakers, if a participant is an L2 English speaker, this trend is reversed.\nTo understand how this works in practice, let‚Äôs compare the predicted Vocab scores of two 35-year-olds: one a native English speaker and the other a non-native. For the purposes of this illustration, we will assume that, apart from their native-speaker status, all their other characteristics correspond to the reference level in our model (i.e.¬†they are both female, have a clerical occupation, scored average on the Blocks test and were in formal education for the median number of years). For the L1 speaker, we calculate their predicted Vocab score as before. We take the intercept (69.1314) as our starting point and then add the Age_c coefficient (0.5466) multiplied by the difference between their age and the median age (which is the reference level of our centered predictor) (i.e.¬†35¬†-¬†31¬†=¬†4). This means that their predicted Vocab score is:\n\n69.1314 + 0.5466*4\n\n[1] 71.3178\n\n\nFor the L2 speaker, we begin by combining the same coefficient estimates as above but, this time, we also add the GroupL2 coefficient (-19.9128), and the interaction coefficient (GroupL2:Age_c). The interaction coefficient (-0.8982) has to be multiplied by the difference between the speaker‚Äôs age and the median age (which remains 4 years). This L2 speaker‚Äôs predicted score is therefore:\n\n69.1314 + 0.5466*4 + -19.9128 + -0.8982*4\n\n[1] 47.8122\n\n\n\n\n\n\n\n\nCautionImportant\n\n\n\nWhenever we have a model with an interaction, we can no longer interpret the individual coefficient estimates of the predictors that enter into the interaction by themselves: instead, we must interpret our model‚Äôs main effects together with their interaction!\nFor example, in model5, if we only took account of the model‚Äôs main effect for age, we would be misled into thinking that age is always positively associated with Vocab scores. However, as illustrated in Figure¬†13.5, we know that this is not true for L2 speakers ‚Äî hence the statistically significant interaction between Age_c and Group in model5.\n\n\nThe best way to avoid misinterpreting interaction effects is to make sure you always visualise the predictions of models that involve interactions. The visreg() function includes a ‚Äúby‚Äù argument, which is ideal for this:\n\nvisreg(model5, \n       xvar = \"Age_c\", \n       by = \"Group\",\n       xtrans = function(x) x + median(Dabrowska.data$Age), \n       gg = TRUE) + \n  labs(x = \"Age (in years)\",\n       y = \"Predicted Vocab scores\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure¬†13.6: Vocab scores as predicted by model5 for L1 and L2 speakers of various ages (blue lines) and partial model residuals (grey points)\n\n\n\n\n\nIf we compare Figure¬†13.6 (in which the points represent the model‚Äôs partial residuals) to Figure¬†13.5 (in which the points represent the actual, observed values), we can see that the partial residuals are, on average, smaller than the differences between the observed values and the regression lines of Figure¬†13.5. This is because the regression lines of Figure¬†13.5 correspond to a model that only includes three coefficients, Age_c, Group and their interaction (Age_c:Group), whereas the partial residuals in Figure¬†13.6 correspond to the left-over variance in Vocab scores once all other predictors of the model5 have been taken into account.\nComparing the model summaries of model4_c and model5, we can see that adding the interaction term between age and native-speaker status considerably boosted the amount of variance in Vocab scores that our model now accounts for: whereas the adjusted R2 of the model without the interaction was 0.3276 (33%), it has now reached 0.3654 (37%) thanks to the added interaction.\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nIn this task, we will explore the possibility of an interaction between participants‚Äô native-speaker status (Group) and the number of years they spent in formal education (EduTotal). The underlying idea is that, for the L1 speakers, their formal education will most likely have been in English; hence the longer they were in formal education, the higher they might score on the Vocab test. For the L2 speakers, however, their formal education will more likely have been in a language other than English. As a result, years in formal education may be a weaker predictor of Vocab scores than for L1 speakers.\nQ13.6 To begin, generate a plot to visualise this interaction in the observed data: map the EduTotal variable onto the x-axis and Vocab onto the y-axis, and split the plot into two facets, one for L1 and the other for L2 participants. Based on your interpretation of your plot, which of these statements is/are correct?\n\n\n\n\n\nFor both L1 and L2 participants, the correlation between the number of years that they spent in formal education and their Vocab scores is not statistically significant.\n\n\n\n\nFor both L1 and L2 participants, the number of years that they spent in formal education is positively correlated with their Vocab scores.\n\n\n\n\nFor both L1 and L2 participants, the number of years that they spent in formal education is negatively correlated with their Vocab scores.\n\n\n\n\nThe number of years that L1 participants spent in formal education is positively correlated with their Vocab scores. This is not the case for L2 participants.\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n\n\nShow code to generate the plot needed to complete Q13.6\nDabrowska.data |&gt; \n   ggplot(mapping = aes(x = EduTotal, \n                        y = Vocab)) +\n     geom_point() +\n     facet_wrap(~ Group) +\n     geom_smooth(method = \"lm\", \n                 se = TRUE) +\n     theme_bw()\n\n\nQ13.7 Below is the model formula for model5:\nVocab ~ Group + Age_c + OccupGroup + Gender + Blocks_c + EduTotal_c + Age_c:Group\nWhat do you need to add to this formula to also model the interaction that you just visualised?\n\n\n\n\n\n+ EduTotal_c:Group:Vocab\n\n\n\n\n+ Vocab ~ EduTotal_c:Group\n\n\n\n\n+ EduTotal_c:Group\n\n\n\n\n\n\n\n\nQ13.8 Fit the model with the added interaction. Does the interaction make a statistically significant contribution to the model at the significance level of 0.05?\n\n\n\n\n\nNo, it does not.\n\n\n\n\nYes, it does.\n\n\n\n\n\n\n\n\n\n\nShow code to complete Q13.8\nmodel5.int &lt;- lm(Vocab ~ Group + Age_c + OccupGroup + Gender + Blocks_c + EduTotal_c + Age_c:Group + EduTotal_c:Group,\n                 data = Dabrowska.data)\n\nsummary(model5.int)\n\n\nQ13.9 Use the {visreg} package to visualise the interaction effect EduTotal_c:Group as predicted by your model. Looking at your plot of predicted values and partial residuals, which conclusion can you draw?\n\n\n\n\n\nYears in formal education is a statistically significant predictor of Vocab scores for both L1 and L2 speakers, and there is no striking difference in the strength of this association between the L1 and the L2 group.\n\n\n\n\nYears in formal education is a statistically significant predictor of Vocab scores for L1 speakers, but not L2 speakers.\n\n\n\n\nYears in formal education is a statistically significant predictor of Vocab scores for L2 speakers, but not L1 speakers.\n\n\n\n\nThe model makes almost perfect predictions for L1 speakers, but not L2 speakers.\n\n\n\n\nNone of these conclusions can be drawn from the plot alone.\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n\n\nShow code to help you complete Q13.9\nvisreg(model5.int, \n       xvar = \"EduTotal_c\", \n       by = \"Group\",\n       xtrans = function(x) x + median(Dabrowska.data$EduTotal), \n       gg = TRUE) + \n  labs(x = \"Years in formal education\",\n       y = \"Predicted Vocab scores\") +\n  theme_bw()\n\n\n\n\n\n\n13.6.1 Interactions between two numeric predictors\nWe can also model interactions between two numeric predictors. For instance, we may hypothesise that the positive effect of time spent in formal education is moderated by age. If this interaction effect were negative, this would mean that, as people get older, the fact that they spent longer in formal education becomes less relevant to predict their current vocabulary knowledge.\nLet‚Äôs add an Age_c:EduTotal_c interaction to our model to find out if our data support this hypothesis.\n\nmodel6 &lt;- lm(Vocab ~ Group + Age_c + OccupGroup + Gender + Blocks_c + EduTotal_c + Group:Age_c + Age_c:EduTotal_c,\n             data = Dabrowska.data)\n\nsummary(model6)\n\n\nCall:\nlm(formula = Vocab ~ Group + Age_c + OccupGroup + Gender + Blocks_c + \n    EduTotal_c + Group:Age_c + Age_c:EduTotal_c, data = Dabrowska.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-68.346 -10.460   0.981  11.830  45.360 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       69.27826    3.65827  18.937  &lt; 2e-16 ***\nGroupL2          -20.11637    3.51829  -5.718 5.89e-08 ***\nAge_c              0.53820    0.14902   3.612 0.000418 ***\nOccupGroupI        7.56857    5.53734   1.367 0.173781    \nOccupGroupM       -4.14901    4.40173  -0.943 0.347449    \nOccupGroupPS      -1.88806    4.29021  -0.440 0.660525    \nGenderM           -1.74885    3.14526  -0.556 0.579044    \nBlocks_c           1.24165    0.31409   3.953 0.000120 ***\nEduTotal_c         2.66662    0.68007   3.921 0.000135 ***\nGroupL2:Age_c     -0.87396    0.29409  -2.972 0.003464 ** \nAge_c:EduTotal_c  -0.01776    0.04520  -0.393 0.694947    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.17 on 146 degrees of freedom\nMultiple R-squared:  0.4027,    Adjusted R-squared:  0.3618 \nF-statistic: 9.842 on 10 and 146 DF,  p-value: 1.78e-12\n\n\nLooking at the last coefficient estimate in the model summary (Age_c:EduTotal_c), we can see that this second interaction coefficient is negative ‚Äì in line with our hypothesis. However, this coefficient is also very small (-0.01776). Moreover, its associated p-value (0.694947) warns us that, if there were no interaction effect (i.e.¬†under the null hypothesis), we would have a 69% probability of observing such a small effect in a dataset this size simply due to random variation alone.\nThe model summary also tells us that the amount of variance in Vocab scores that model6 accounts for is 36.18% (Adjusted R-squared:  0.3618). This is actually slightly less than model5 (Adjusted R-squared:  0.3654), which did not include this interaction. In other words, this additional interaction does not help us to model the association between our predictor variables and Vocab scores more accurately.\nIn Figure¬†13.7, we visualise this statistically non-significant interaction to better understand what it corresponds to. When used to visualise an interaction effect between two numeric predictors, the visreg() function automatically splits the second predictor variable (the ‚Äúby‚Äù variable) into three categories corresponding to low, middle, and high values of the variable. It therefore shows the predicted effect of the numeric predictor predicted on the x-axis in these three different contexts. If, as in Figure¬†13.7, the three slopes are of the same gradient and the regression lines therefore run parallel to each other, this indicates that there is no noteworthy interaction between the two numeric predictors.\n\nvisreg(model6,\n       xvar = \"Age_c\", \n       by = \"EduTotal_c\",\n       xtrans = function(x) x + median(Dabrowska.data$Age), \n       gg = TRUE) + \n  labs(x = \"Age\",\n       y = \"Predicted Vocab scores\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure¬†13.7: Vocab scores as predicted by model6 for English speakers of various ages who were in formal education for a below-average, average, and above-average length of time (in blue) and partial model residuals (grey points)\n\n\n\n\n\nExamining Figure¬†13.7, we can therefore conclude that the effect of age on Vocab scores is not moderated by the number of years that participants spent in formal education.\nThe visreg() function provides two ways to visualise interaction effects between two numeric variables: in Figure¬†13.7, below-average, average, and above-average number of years in formal education were visualised across three panels. Figure¬†13.8 shows the same predictions but, this time, the code includes the argument ‚Äúoverlay = TRUE‚Äù, which results in a single panel with three coloured regression lines superimposed. This can make it easier to check whether the lines are parallel. From Figure¬†13.8, it is easier to see that the three lines are pretty much parallel to each other. This suggests that the positive effect of Age on Vocab scores does not change as a function of the number of years that participants were in formal education.\n\nvisreg(model6,\n       xvar = \"Age_c\", \n       by = \"EduTotal_c\",\n       overlay = TRUE,\n       xtrans = function(x) x + median(Dabrowska.data$Age), \n       gg = TRUE) + \n  labs(x = \"Age\",\n       y = \"Predicted Vocab scores\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure¬†13.8: Vocab scores as predicted by model4_c for English speakers of various ages who were in formal education for a below-average (red line), average (green line), and above-average (blue line) period of time and partial model residuals (coloured points).\n\n\n\n\n\nIn order to interpret the output of a model featuring a significant interaction between two numeric predictors, we will now fit a simpler model predicting the Vocab scores of the L1 participants only (L1.data) using only two predictor variables: years in formal education (EduTotal) and Author Recognition Test (ART) scores.\nThe Author Recognition Test (Acheson, Wells & MacDonald 2008) is a measure of print exposure to literature, which is known to be a strong predictor of vocabulary knowledge (see Your turn! section in Section 13.4.1 above). This is confirmed in the L1 dataset as ART and Vocab scores are strongly correlated:\n\ncor(L1.data$ART, L1.data$Vocab)\n\n[1] 0.6032758\n\n\nAt the same time, we also know that there is a positive, though less strong, correlation between the number of years that L1 participants were in formal education and their Vocab test results:\n\ncor(L1.data$EduTotal, L1.data$Vocab)\n\n[1] 0.4268695\n\n\nIn the following, we explore the possibility that this positive association between Author Recognition Test (ART) scores and Vocab scores may be moderated by the number of years spent in formal education (EduTotal). If this interaction effect were negative, this would mean that, if two individuals both have the same high ART score, but one spent longer in formal education, then the effect of those extra years of education would not be as strong as they would be for someone with a lower ART score. To test this hypothesis, we formulate the following null hypothesis:\n\nH0: The number of years spent in formal education does not moderate the association of L1 English speakers‚Äô ART scores with their Vocab scores.\n\nWe now fit a model to find out if we have enough evidence to reject this null hypothesis:\n\n# First, we center the numeric variables in the L1 dataset:\nL1.data &lt;- L1.data |&gt; \n   mutate(EduTotal_c = EduTotal - median(EduTotal),\n          Blocks_c = Blocks - median(Blocks),\n          Age_c = Age - median(Age))\n\n# Second, we fit the model with both variables and their two-way interaction as predictors:\nL1.model &lt;- lm(Vocab ~ ART + EduTotal_c + ART:EduTotal_c,\n              data = L1.data)\n\n# Finally, we inspect the model:\nsummary(L1.model)\n\n\nCall:\nlm(formula = Vocab ~ ART + EduTotal_c + ART:EduTotal_c, data = L1.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-54.807  -6.113   0.065  10.927  26.379 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     51.9162     2.7622  18.795  &lt; 2e-16 ***\nART              1.3088     0.1955   6.696 2.09e-09 ***\nEduTotal_c       5.2626     1.3272   3.965 0.000151 ***\nART:EduTotal_c  -0.1803     0.0530  -3.402 0.001017 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.18 on 86 degrees of freedom\nMultiple R-squared:  0.4625,    Adjusted R-squared:  0.4437 \nF-statistic: 24.67 on 3 and 86 DF,  p-value: 1.311e-11\n\n\nThe model summary confirms that both predictors individually (i.e.¬†as main effects) make statistically significant, positive contributions to the prediction of Vocab scores. Crucially, the summary also indicates that the interaction effect (ART:EduTotal_c) is statistically significant at Œ±¬†=¬†0.05 (p¬†=¬†0.001017). The negative coefficient estimate of -0.1803 means that, for each additional year of formal education, our model predicts that the effect of a participant‚Äôs ART score on Vocab decreases by 0.1803 points. In simpler terms, the positive effect of reading literature, as measured by the Author Recognition Test (Acheson, Wells & MacDonald 2008), decreases slightly for every year spent in formal education.\nRemember that, whenever we report an interaction, we can no longer interpret the estimated coefficients of the individual predictors in isolation. That is because we must also consider the interaction effect. To understand what this means in practice, let‚Äôs compare two speakers from the L1 dataset:\n\nL1.data |&gt; \n  slice(2, 18) |&gt; \n  select(Occupation, OccupGroup, ART, EduTotal, Vocab)\n\n              Occupation OccupGroup ART EduTotal    Vocab\n1 Student/Support Worker         PS  31       13 95.55556\n2              Housewife          I  31       17 84.44444\n\n\n\nParticipant No.¬†2 is a student and support worker with an ART score of 31 points, who has (so far) spent 13 years in formal education.\nParticipant No.¬†18 is a housewife who also scored 31 points on the ART, but who was in formal education for a total of 17 years.\n\nIn general, we can calculate the Vocab scores that our L1.model predicts by combining the model‚Äôs coefficient estimates like this:\n\nIntercept +\nART coefficient*ART score +\nEduTotal coefficient*(EduTotal in years - median EduTotal) +\nART:EduTotal coefficient*ART score*(EduTotal in years - median EduTotal)\n\nBased on the coefficient estimates of the model summary, we can therefore calculate the predicted Vocab score of the student / support worker as follows:\n\n151.9162 +\n21.3088 * 31 +\n35.2626 * (13 - median(L1.data$EduTotal)) +\n4-0.1803 * 31 * (13 - median(L1.data$EduTotal))\n\n\n1\n\nIntercept coefficient\n\n2\n\nMain effect of 31 points on the ART test\n\n3\n\nMain effect of 13 years in formal education\n\n4\n\nInteraction effect between 31 points on the ART and having spent 13 years in formal education\n\n\n\n\n[1] 92.489\n\n\nAnd similarly for the housewife with the same ART score:\n\n151.9162 +\n21.3088 * 31 +\n35.2626 * (17 - median(L1.data$EduTotal)) +\n4-0.1803 * 31 * (17 - median(L1.data$EduTotal))\n\n\n1\n\nIntercept coefficient\n\n2\n\nMain effect of 31 points on ART test\n\n3\n\nMain effect of 17 years in formal education\n\n4\n\nInteraction effect between scoring 31 points on the ART and having spent 17 years in formal education\n\n\n\n\n[1] 91.1822\n\n\nWe can check that we did the maths correctly by checking the model‚Äôs prediction for these two individuals. Remember that minor differences after the decimal point are due to us using rounded coefficient estimates.\n\npredict(L1.model)[2]\n\n       2 \n92.49038 \n\npredict(L1.model)[18]\n\n      18 \n91.18172 \n\n\nNotice how these two predicted scores are very similar, even though the housewife spent longer in formal education than the student / support worker. This is because, for L1 speakers, greater print exposure and more education generally lead to a larger vocabulary (as indicated by the positive main-effect coefficient estimates in L1.model), but the increase in vocabulary for each additional year of education is predicted to be smaller for individuals with higher ART scores.\nFigure¬†13.9 visualises the predictions of L1.model. How can we interpret these three regression lines?\n\n\nShow code to generate plot.\nvisreg(L1.model,\n       xvar = \"EduTotal_c\", \n       by = \"ART\",\n       xtrans = function(x) x + median(L1.data$EduTotal), \n       gg = TRUE) + \n  labs(x = \"Years in formal education\",\n       y = \"Predicted Vocab scores\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nFigure¬†13.9: Vocab scores as predicted by L1.model (in blue) and partial model residuals (grey points) as a function of the number of years speakers were in formal education and for three ART test scores\n\n\n\n\n\nFor low and mid-level ART scores, the model predicts a positive correlation between the number of years a participant has spent in formal education and their predicted Vocab scores. However, this is not the case for participants who scored high on the ART test (third panel). The three regression lines representing the model‚Äôs predicted scores are clearly not parallel, and it would not be possible to draw three parallel lines that each stay within their respective 95% confidence bands. This confirms that, in this L1 model, the interaction between ART scores and years in formal education is statistically significant at Œ±¬†=¬†0.05. The main effects of these two predictors cannot be meaningfully interpreted without considering this interaction.",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Multiple linear reg`R`ession modelling</span>"
    ]
  },
  {
    "objectID": "13_MultipleLinearRegression.html#model-selection",
    "href": "13_MultipleLinearRegression.html#model-selection",
    "title": "13¬† Multiple linear regRession modelling",
    "section": "13.7 Model selection",
    "text": "13.7 Model selection\nTypically, the more predictors we enter in a model, the better the model fits the data. However, having predictors that contribute very little to the model and/or whose contributions are not statistically significant risks lowering the accuracy of the model‚Äôs predictions on new data. In this case, we say that the model overfits. A model that overfits the sample data risks not generalising to other data. If the aim of our statistical modelling is to infer from our sample to the general population (see Chapter 11), it can sometimes make sense to try to find an ‚Äòoptimal‚Äô model that accounts for as much of the variance in the outcome variable as possible, relying only on association effects that we can be fairly confident could not have occurred due to chance only. This is where model selection comes into play.\nSome people consider model selection to be a bit of an art. This chapter only aims to introduce the topic and does not cover ‚Äî let alone compare or endorse ‚Äî different model selection procedures. Ultimately, what really matters is that, if we intend to apply a model selection procedure, we decide on the method before analysing the data. The most credible way to settle on a model selection procedure prior to data analysis is to preregister an analysis plan on an online platform such as AsPredicted.org, Open Science Framework (OSF), and Zenodo (see Haroz 2022):\n\nPreregistration [or pre-registration] is the practice of posting a time-stamped, read-only version of your study plan to a public repository before beginning data collection or analysis. This establishes a transparent record of your research intentions (Center for OpenScience 2025).\n\n\n\n\n\n\nPreregistration Open Science badge\n\n\nModel selection bears the very real risk of (consciously or unconsciously) ‚Äúfishing‚Äù for statistically significant results. Fishing is a highly Questionable Research Practice (QRP, see Section 11.7) that consists in trying out different methods until we arrive at findings that match our theory and/or hypotheses. For this reason, selection procedures in regression modelling have been heavily criticised over the years and there are very good arguments for not engaging in any model selection at all (Thompson 1995; Smith 2018; Harrell 2015: Section 4.3).\n\nA fundamental problem with stepwise regression is that some real explanatory variables that have causal effects on the dependent variable may happen to not be statistically significant, while nuisance variables may be coincidentally significant. As a result, the model may fit the data well in-sample, but do poorly out-of-sample (Smith 2018: 1).\n\nNonetheless, in the following section, we will see how we might - if we decide to narrow down predictor variables using model selection - arrive at an optimal model of Vocab scores among L1 and L2 speakers of English using the adjusted R2 as our model selection decision criterion. Recall that R2 values correspond to the amount of variance in the outcome variable that a model can predict. The adjusted R2 is particularly useful because it is adjusted for the number of predictors entered in the model; models with more predictors are penalised.\n\nWhen using adjusted R2 as the decision criterion, we seek to eliminate or add predictors depending on whether they lead to the largest improvement in adjusted R2 and we stop when adding or eliminating another predictor does not lead to further improvement in adjusted R2.\nAdjusted R2 describes the strength of a model fit, and it is a useful tool for evaluating which predictors are adding value to the model, where adding value means they are (likely) improving the accuracy in predicting future outcomes. (√áetinkaya-Rundel & Hardin 2021: Section 8.4)\n\nThere are two common ways to add or remove predictors in a multiple regression model. These are called backward elimination and forward selection. They are often called stepwise selection because they add or remove one variable at a time.\n\nBackward elimination starts with the full model ‚Äì the model that includes all potential predictor variables. Predictors are eliminated one-at-a-time from the model until we cannot improve the model any further.\nForward selection is the reverse of the backward elimination technique. Instead of eliminating predictors one-at-a-time, we add predictors one-at-a-time until we cannot find any predictors that improve the model any further. (√áetinkaya-Rundel & Hardin 2021: Section 8.4)\n\nWe will use backward elimination as it allows us to start with a full model that includes all the predictors and any interactions that we believe are justified on the basis of theory and/or prior research. At this stage, it is absolutely crucial to think about which variables and which interactions are genuinely meaningful and which are not!\nWith the DƒÖbrowska (2019) data, several full models can be justified. In this section, we will attempt to model Vocab scores among L1 participants using four predictors (ART, Blocks_c, Age_c, EduTotal_c, and OccupGroup) and the following four interactions (ART:EduTotal_c, Blocks_c:EduTotal_c, ART:Age_c, and Blocks:Age_c). We can justify this choice of predictors based on our current understanding of language learning.\n\nL1.model.full &lt;- lm(Vocab ~ ART + Blocks_c + Age_c + EduTotal_c + OccupGroup + ART:EduTotal_c + Blocks_c:EduTotal_c + ART:Age_c + Blocks_c:Age_c, \n           data = L1.data)\n\nsummary(L1.model.full)\n\n\nCall:\nlm(formula = Vocab ~ ART + Blocks_c + Age_c + EduTotal_c + OccupGroup + \n    ART:EduTotal_c + Blocks_c:EduTotal_c + ART:Age_c + Blocks_c:Age_c, \n    data = L1.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-42.927  -4.064  -0.374   8.295  28.282 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         53.1677121  3.7269444  14.266  &lt; 2e-16 ***\nART                  1.1304963  0.2330505   4.851 6.16e-06 ***\nBlocks_c             1.4066157  0.3164938   4.444 2.88e-05 ***\nAge_c                0.5955643  0.1895092   3.143  0.00237 ** \nEduTotal_c           4.4423564  1.3560678   3.276  0.00157 ** \nOccupGroupI          4.0757889  4.7833634   0.852  0.39678    \nOccupGroupM          0.1584647  4.3523775   0.036  0.97105    \nOccupGroupPS         0.4096426  4.3947432   0.093  0.92597    \nART:EduTotal_c      -0.1523719  0.0513417  -2.968  0.00398 ** \nBlocks_c:EduTotal_c -0.1428729  0.1311735  -1.089  0.27942    \nART:Age_c           -0.0125418  0.0096286  -1.303  0.19656    \nBlocks_c:Age_c      -0.0007549  0.0194954  -0.039  0.96921    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.45 on 78 degrees of freedom\nMultiple R-squared:  0.6172,    Adjusted R-squared:  0.5632 \nF-statistic: 11.43 on 11 and 78 DF,  p-value: 2.492e-12\n\n\nOur full model has an adjusted R2 of 0.5632, which means that the combination of these variables and the four interactions account for about 56% of the variance in Vocab scores in the L1 data. However, many of the model coefficients in L1.model.full are not statistically significant, so we could try to remove them to see whether this leads to a lower adjusted R2 or not. Strictly speaking, a stepwise selection procedure would entail removing each interaction one-by-one. To save space here, we remove all three non-significant interaction terms in a single backward step:\n\nL1.model.back1 &lt;- lm(Vocab ~ ART + Blocks_c + Age_c + EduTotal_c + OccupGroup + ART:EduTotal_c, \n           data = L1.data)\n\nsummary(L1.model.back1)\n\n\nCall:\nlm(formula = Vocab ~ ART + Blocks_c + Age_c + EduTotal_c + OccupGroup + \n    ART:EduTotal_c, data = L1.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-42.282  -4.835   0.367   8.867  29.110 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    53.61458    3.60809  14.860  &lt; 2e-16 ***\nART             0.97262    0.20158   4.825 6.48e-06 ***\nBlocks_c        1.37827    0.31278   4.407 3.19e-05 ***\nAge_c           0.42996    0.13122   3.276  0.00155 ** \nEduTotal_c      4.17372    1.30919   3.188  0.00204 ** \nOccupGroupI     4.00028    4.65106   0.860  0.39228    \nOccupGroupM     0.74751    4.29748   0.174  0.86235    \nOccupGroupPS   -0.22682    4.17410  -0.054  0.95680    \nART:EduTotal_c -0.13709    0.04903  -2.796  0.00646 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.38 on 81 degrees of freedom\nMultiple R-squared:  0.6067,    Adjusted R-squared:  0.5679 \nF-statistic: 15.62 on 8 and 81 DF,  p-value: 1.153e-13\n\n\nThis procedure has actually led to a (very) small increase in our adjusted R2, which is now 0.5679, or 57%. Can we simplify our model even further and still account for as much variance in Vocab scores by dropping categorical predictor variable OccupGroup?\n\nL1.model.back2 &lt;-  lm(Vocab ~ ART + Blocks_c + Age_c + EduTotal_c + ART:EduTotal_c, \n           data = L1.data)\n\nsummary(L1.model.back2)\n\n\nCall:\nlm(formula = Vocab ~ ART + Blocks_c + Age_c + EduTotal_c + ART:EduTotal_c, \n    data = L1.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-43.379  -5.410   0.824   8.369  32.573 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     54.2022     2.4512  22.113  &lt; 2e-16 ***\nART              0.9970     0.1922   5.187 1.46e-06 ***\nBlocks_c         1.3251     0.3030   4.373 3.49e-05 ***\nAge_c            0.4843     0.1092   4.434 2.78e-05 ***\nEduTotal_c       4.3116     1.2812   3.365  0.00115 ** \nART:EduTotal_c  -0.1472     0.0471  -3.125  0.00244 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.21 on 84 degrees of freedom\nMultiple R-squared:  0.6025,    Adjusted R-squared:  0.5788 \nF-statistic: 25.46 on 5 and 84 DF,  p-value: 1.521e-15\n\n\nThis new model has a slightly higher adjusted R2 (0.5788), so it looks like this was another sensible simplification of our model. All of the remaining coefficient estimates in L1.model.back2 make statistically significant contributions to the model. If we try to remove one, we can expect that the amount of variance that our model can account for will drop. For example, we can try to remove Age as a predictor from our model to see what happens:\n\nL1.model.back3 &lt;- lm(Vocab ~ ART + Blocks_c + EduTotal_c + ART:EduTotal_c, \n           data = L1.data)\n\nsummary(L1.model.back3)\n\n\nCall:\nlm(formula = Vocab ~ ART + Blocks_c + EduTotal_c + ART:EduTotal_c, \n    data = L1.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-48.816  -7.486  -0.458   9.970  26.546 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    52.08245    2.65487  19.618  &lt; 2e-16 ***\nART             1.38099    0.18951   7.287  1.5e-10 ***\nBlocks_c        0.90754    0.31806   2.853  0.00543 ** \nEduTotal_c      3.59093    1.40344   2.559  0.01228 *  \nART:EduTotal_c -0.15028    0.05201  -2.890  0.00489 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.59 on 85 degrees of freedom\nMultiple R-squared:  0.5095,    Adjusted R-squared:  0.4864 \nF-statistic: 22.07 on 4 and 85 DF,  p-value: 1.615e-12\n\n\nIndeed, L1.model.back3 accounts for 49% of the variance (adjusted R2 = 0.4864), which is considerably less than L1.model.back2. We will therefore report and interpret L1.model.back2.\n\n\n\n\n\n\nNoteReporting a multiple regression model\n\n\n\nThere are many ways to report the numerical results of a statistical model. Researchers typically include a table reporting the model‚Äôs coefficient estimates (sometimes referred to as Œ≤, ‚Äúbeta‚Äù), together with a measure of variability around these estimates (e.g.¬†standard error or confidence intervals), as well as their associated p-values. In addition, it is important to report the accuracy of the model. Different so-called goodness-of-fit measures are used for this; one of the most common being the adjusted coefficient of determination, R2. The output of the summary() function includes all of these statistics and is therefore suitable for a research report.\nAlternatively, the {sjPlot} library (L√ºdecke 2020) includes a handy function that produces nicely formatted tables to report all kinds of models, including multiple linear regression models. When you run the tab_model() function in RStudio, the table will be displayed in the Viewer pane. By default, it includes the model‚Äôs coefficient estimates and 95% confidence intervals around these estimates, as well as p-values formatted in bold if they are below 0.05.\n\ninstall.packages(\"sjPlot\")\nlibrary(sjPlot)\n\ntab_model(L1.model.back2)\n\n\n\n\n\n\n¬†\nVocab\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n54.20\n49.33¬†‚Äì¬†59.08\n&lt;0.001\n\n\nART\n1.00\n0.61¬†‚Äì¬†1.38\n&lt;0.001\n\n\nBlocks c\n1.33\n0.72¬†‚Äì¬†1.93\n&lt;0.001\n\n\nAge c\n0.48\n0.27¬†‚Äì¬†0.70\n&lt;0.001\n\n\nEduTotal c\n4.31\n1.76¬†‚Äì¬†6.86\n0.001\n\n\nART √ó EduTotal c\n-0.15\n-0.24¬†‚Äì¬†-0.05\n0.002\n\n\nObservations\n90\n\n\nR2 / R2 adjusted\n0.603 / 0.579\n\n\n\n\n\n\nCheck the documentation of the tab_model() function and the sjPlot website to learn about its many useful formatting options:\n\n?tab_model\n\nIt is also recommended to visualise the model‚Äôs predictions and its (partial) residuals. Again, there are many ways to achieve this in R, but we will stick to using the {visreg} library. Run the following command and follow the instructions displayed in the Console to view all the plots in RStudio. You may need to resize your Plot pane or use the Zoom button to properly view the plots.\n\nvisreg(L1.model.back2)\n\nRunning the visreg() function on this multiple regression model outputs several warning messages in the Console. These messages are important and should not be ignored:\nNote that you are attempting to plot a 'main effect' in a model that contains an interaction. This is potentially misleading; you may wish to consider using the 'by' argument.\nThe message warns us that we should not attempt to interpret ART and EduTotal_c as main effects because our model includes an interaction that involves these two variables. Indeed, the fourth plot (see Figure¬†13.10) suggests that the more years an L1 speaker spends in formal education, the greater their Vocab score; however, because our model includes an interaction effect, the authors of the {visreg} package are warning us that the strength or even the direction of this effect could change depending on individuals‚Äô ART score.\n\nvisreg(fit = L1.model.back2, \n       xvar = \"EduTotal_c\",\n       xtrans = function(x) x + median(L1.data$EduTotal), \n       gg = TRUE) +\n  labs(x = \"Years in formal education\",\n       y = \"Predicted Vocab scores\") +\n  theme_bw()    \n\nConditions used in construction of plot\nART: 10.5\nBlocks_c: 0\nAge_c: 0\n\n\n\n\n\n\n\n\nFigure¬†13.10: Predicted vocabulary scores for L1 speakers as a function of the number of years that they were in formal education (in blue) and partial residuals (grey points).\n\n\n\n\n\nAs suggested by the warning message, we can (and should!) visualise interactions like this one using the ‚Äúby‚Äù argument.\n\nvisreg(fit = L1.model.back2, \n       xvar = \"EduTotal_c\", \n       xtrans = function(x) x + median(L1.data$EduTotal), \n       by = \"ART\",\n       gg = TRUE) +\n  labs(x = \"Years in formal education\",\n       y = \"Predicted Vocab scores\") +\n  theme_bw()  \n\n\n\n\n\n\n\nFigure¬†13.11: Predicted vocabulary scores for L1 speakers as a function of the number of years that they were in formal education and across three different ART scores (in blue) and partial residuals (grey points).\n\n\n\n\n\nFigure¬†13.11 shows the predicted effect of the number of years in formal education on Vocab scores for participants who scored 3, 10, and 31 points on the ART. The regression line shows a positive relation between Vocab scores and years in formal education for below-average and average ART scores, but the regression line is almost flat for above-average ART scores. This indicates that, for individuals who score very high on the ART, years in education is no longer a useful predictor of their Vocab scores.\nFinally, we should also report the outcome of our model assumption checks. This is covered in the following section.\n\n\n\n\n\n\nHex sticker of the {sjPlot} package",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Multiple linear reg`R`ession modelling</span>"
    ]
  },
  {
    "objectID": "13_MultipleLinearRegression.html#sec-ModelAssumptions",
    "href": "13_MultipleLinearRegression.html#sec-ModelAssumptions",
    "title": "13¬† Multiple linear regRession modelling",
    "section": "13.8 Checking model assumptions",
    "text": "13.8 Checking model assumptions\nAs we saw in Section 12.4, it is crucial that we check that our models meet the assumptions of linear regression models before we interpret them because, if they don‚Äôt, our models may be unreliable. In some cases, modelling issues may already be visible from the model summary. Warning signs include very large model residuals, coefficient estimates reported as NA, and warning messages stating that the model is ‚Äúsingular‚Äù or that it has ‚Äúfailed to converge‚Äù.\nThese problems can occur for a number of reasons, but most often because the model is too complex given the data available: the sample size may be too small or the data too sparse for certain combinations of predictors (e.g.¬†if you try to enter gender and native language as predictors in a model, but for some languages only female native speakers are represented in the dataset). There are often ways around these issues, but they are beyond the scope of this introductory textbook (see recommended readings below and next-step resources).\nThe model assumptions for multiple linear regression models are the same as for simple linear regression models (see Section 12.4):\n\nIndependence of the data points (see Section 11.6.2 and Section 12.4.1)\nLinear relationships between the predictors (see Section 11.6.4 and Section 12.4.2)\nHomogeneity of the model residuals (see Section 11.6.5 and Section 12.4.3)\nNormality of the model residuals (see Section 11.6.3)\nNo overly influential outliers (see Section 11.6.4)\n\nThere is only one additional assumption that is specific to models that include multiple predictors:\n\nNo multicollinearity\n\nIn the following, we use the check_model() function from the {performance} package (L√ºdecke, Ben-Shachar, et al. 2021) to check these assumptions graphically. The check_model() function also requires the installation of the {qqplotr} (Almeida, Loy & Hofmann 2018) and {see} (L√ºdecke, Patil, et al. 2021) packages to work.\n\n\n\n\n\nHex sticker of the {performance} package\n\n\n\ninstall.packages(c(\"performance\", \"qqplotr\", \"see\"))\nlibrary(performance)\n\nRunning the function on the saved model object should generate a large figure comprising six plots2 (see Figure¬†13.12). In the following, we will learn to interpret them one-by-one.\n\ncheck_model(L1.model.back2)\n\n\n\n\n\n\n\nFigure¬†13.12: Six plots output by the check_model() function for assessing key assumptions of linear regression models\n\n\n\n\n\nBy default, check_model() outputs a single figure that allows us To check the most important model assumptions. This large figure is useful for reporting purposes, but it is rather unwieldy for interpretation. Changing the ‚Äúpanel‚Äù argument of the check_model() function to FALSE returns a list of {ggplot} objects that we can save to our local environment as diagnostic.plots:\n\ndiagnostic.plots &lt;- plot(check_model(L1.model.back2, panel = FALSE))\n\nThen we can use the double square bracket operator to display a single diagnostic plot from this saved list:\n\ndiagnostic.plots[[1]]\n\n\n\n\n\n\n\nFigure¬†13.13: Comparing the distribution of observed vocabulary scores (green) with data simulated based on model predictions (blue)\n\n\n\n\n\nThis first plot (Figure¬†13.13) is another way to compare the model‚Äôs predicted values (represented here as blue distributions) with the real-life (i.e observed) outcome variable (represented here in green). We can see that the center of the distribution of observed Vocab scores is slightly shifted to the right compared to most distributions of model-predicted data. That said, the simulated distributions are close to the real distribution and largely follow a similar shape. If they didn‚Äôt, this would suggest that a linear regression model may not be suitable for our data.\nThe second plot (Figure¬†13.14) is designed To check the assumption of linearity. If the predictors are linearly related, the green reference line should be flat and horizontal. Our reference line is slightly curved, but it remains possible to draw a horizontal line through the grey band, hence we can conclude that the assumption of linearity is not severely violated.\n\ndiagnostic.plots[[2]]\n\n\n\n\n\n\n\nFigure¬†13.14: The relationship between model predictions (fitted values) and model residuals\n\n\n\n\n\nNote that Figure¬†13.14 is actually the same kind of plot as Figure¬†12.11 that we generated to check the assumption of equal (or constant) variance, i.e.¬†homoscedasticity. Fitted values is another term for predicted values. Thus, in Figure¬†13.14, the x-axis represents the Vocab scores predicted by the model. When the assumption of homoscedasticity is met, the model residuals are randomly distributed above and below 0, i.e.¬†they do not notably increase or decrease as predicted values increase.\nThe {performance} package proposes a different kind of diagnostic plot to check the assumption of homoscedasticity (see Figure¬†13.15) with the square-root of the absolute standardised residuals on the y-axis. A roughly flat and horizontal green reference line indicates homoscedasticity. Again, although the reference line in Figure¬†13.15 is by no means perfectly flat, a flat line can be drawn within the grey band, suggesting that the assumption is met.\n\ndiagnostic.plots[[3]]\n\n\n\n\n\n\n\nFigure¬†13.15: The relationship between the model predictions (fitted values) and the square root of absolute standardised residuals\n\n\n\n\n\nThe fourth diagnostic plot (Figure¬†13.16) helps to detect outliers in the data that may have a particularly strong influence on our model. It is based on Cook‚Äôs distance (see Levshina 2015: Section 7.2.4; Sonderegger 2023: Section 5.7.3). Any points that fall outside the dashed green lines fall outside Cook‚Äôs distance and are considered influential observations.\n\ndiagnostic.plots[[4]]\n\n\n\n\n\n\n\nFigure¬†13.16: The relationship between leverage as estimated using Cook‚Äôs distance and standardised residuals\n\n\n\n\n\nIn Figure¬†13.16, no data point falls outside of Cook‚Äôs distance so we are not concerned about influential observations violating the assumptions of our model. Still, it is interesting to briefly explore the most influential observations, which are labelled by their index number in the dataset. In Figure¬†13.16, one of these is participant number¬†21:\n\nL1.data |&gt; \n  slice(21) |&gt; \n  select(Age, Gender, Occupation, Blocks, EduTotal, ART, Vocab)\n\n  Age Gender      Occupation Blocks EduTotal ART    Vocab\n1  41      F Senior Lecturer      9       21  43 93.33333\n\n\nAs you can see, this female senior lecturer is rather unusual: she performed below average in the non-verbal IQ test (Blocks), yet achieved the second-highest Vocab score. She also performed far above average on the author recognition test (ART) and reported the longest period in formal education among the L1 participants (EduTotal).\nIn some cases, it may be justified to remove overly influential outliers; however, this should typically only be done when we are fairly certain that the outliers were caused by a technical error, such as a measuring instrument not functioning properly, or a human error, such as a participant misunderstanding the direction of a response scale. All other outliers may be theoretically interesting: if our aim is to generalise our model to the full population, we must be prepared to include some unusual observations that also belong to that population. In the case of L1 English speakers, this includes people who spent more than 20 years in formal education and are seemingly much more into languages than the kind of abstract puzzles typically found in non-verbal IQ tests!\nThe fifth diagnostic plot output by the check_model() function (Figure¬†13.17) serves to check the assumption of a no multicollinearity. Multicollinearity, or high collinearity, refers to a strong linear dependence between predictors such that they do not contribute unique or independent information to the model. Multicollinearity should not be confused with a strong correlation between two individual predictors as measured using the cor.test() function (see Section 11.5). What matters here is the association between one or more predictor variables, conditional on the other variables in the model. This is considerably more complex to calculate, but luckily, there are several functions that allow us to do just that in R.\nThe {performance} package relies on the Variance Inflation Factor (VIF) to quantify collinearity. Typically, VIF scores above 10 are considered to indicate a problematic degree of collinearity between some predictors. Figure¬†13.17 indicates that our model does not suffer from multicollinearity.\n\ndiagnostic.plots[[5]]\n\n\n\n\n\n\n\nFigure¬†13.17: VIF factor and 95% confidence interval for each predictor in the model\n\n\n\n\n\nFinally, the sixth plot output by the check_model() function serves to check the assumption of the normality of the residuals. In Section 12.4.4, we achieved this by visualising the distribution of residuals as a density plot (see Figure¬†12.12). The diagnostic plot presented in Figure¬†13.18, by contrast, is a so-called Q-Q plot (quantile-quantile plot). These plots are designed to compare the shapes of distributions. If the residuals follow a perfect normal distribution, the points should all fall along the straight reference line (in green).\n\ndiagnostic.plots[[6]]\n\n\n\n\n\n\n\nFigure¬†13.18: Q-Q plot\n\n\n\n\n\nIn Figure¬†13.18, we see that most residuals follow a normal distribution, except some observations at the tails of the distribution. This is fairly typical and, if the other model assumptions are not violated, minor deviations from normality like this are unlikely to be a problem, especially with larger sample sizes.\n\n\n\n\n\n\nNote\n\n\n\nThe check_model() vignette provides more detailed information on these diagnostic plots, how to interpret them, and what to do if the assumptions are not met: https://easystats.github.io/performance/articles/check_model.html.\n\n\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nSo far in this chapter, we have fitted a model of L1 speakers‚Äô Vocab scores. We have found that participants‚Äô print exposure (as measured by the ART), non-verbal intelligence (as measured by the Blocks test), their age, the number of years they were in formal education, and the interaction between ART scores and years in education all contribute to being able to predict how well they performed on the Vocab test. The model that we selected as the optimal model (L1.model.back2) was able to predict 58% of the variance in Vocab scores among L1 speakers.\nIn this task, your aim is to find out which predictors and interactions among predictors can be used to predict Vocab scores among L2 speakers and to what degree of accuracy. In addition to the predictors that we entered in our full L1 model, you can enter three additional predictors in your model that are relevant to L2 speakers only:\n\nAge at which they started living in an English-speaking country (Arrival)\nTotal number of years they have been living in an English-speaking country (LoR)\nNumber of hours they spend reading in English in a typical week (ReadEng)\n\nRun the following code to median-center some of these numeric variables in L2.data:\n\nL2.data &lt;- L2.data |&gt; \n  mutate(Blocks_c = Blocks - median(Blocks),\n         Age_c = Age - median(Age),\n         EduTotal_c = EduTotal - median(EduTotal),\n         Arrival_c = Arrival - median(Arrival),\n         LoR_c = LoR - median(LoR))\n\nThen, fit the following multiple linear regression model on the L2 dataset and examine the model summary:\n\nL2.model &lt;- lm(formula = Vocab ~ ART + Blocks_c + Age_c + EduTotal_c + Arrival_c + LoR_c + ReadEng + ART:EduTotal_c + ART:ReadEng + EduTotal_c:ReadEng + Arrival_c:Age_c + LoR_c:Arrival_c,\n             data = L2.data)\n\nsummary(L2.model)\n\nQ13.10 How many interaction terms contribute to the predictions made by L2.model?\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\nQ13.11 Use the check_model() function to visually check whether L2.model meets the most important assumptions of multiple linear regression models. Which assumption(s) are obviously violated?\n\n\n\n\n\nIndependence of the data points\n\n\n\n\nLinear relationships between the predictors\n\n\n\n\nHomogeneity of the model residuals\n\n\n\n\nNormality of the model residuals\n\n\n\n\nNo overly influential outliers\n\n\n\n\nNo multicollinearity\n\n\n\n\nNone of these assumptions\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n\n\nShow sample code to answer Q13.11.\ndiagnostic.plots.L2 &lt;- plot(check_model(L2.model, panel = FALSE))\n\ndiagnostic.plots.L2[[1]] # No issues here.\n\ndiagnostic.plots.L2[[2]] # Also no issues here.\n\ndiagnostic.plots.L2[[3]] # Nothing major to worry about: the reference line is not perfectly flat, but it's not far off.\n\ndiagnostic.plots.L2[[4]] # No observation points appear to have an overly large influence on the model\n\ndiagnostic.plots.L2[[5]] # Here we have three coefficients with VIF scores &gt; 10. This is problematic!\n\ndiagnostic.plots.L2[[6]] # The residuals are not normally distributed, but the largest deviation is at lowest end of the distribution which is not terribly unusual.\n\n\nWe can display the exact VIF values of a model using the vif() function from the {car} package (Fox & Weisberg 2019):\n\ninstall.packages(\"car\")\nlibrary(car)\n\nvif(L2.model)\n\n\n\n               ART           Blocks_c              Age_c         EduTotal_c \n          5.987790           1.216035          25.347847           5.490682 \n         Arrival_c              LoR_c            ReadEng     ART:EduTotal_c \n         23.746278          11.102277           3.661388           2.237536 \n       ART:ReadEng EduTotal_c:ReadEng    Age_c:Arrival_c    Arrival_c:LoR_c \n          8.550867           4.412402           3.349661           1.398937 \n\n\nFrom this list of VIF values, we can see that three predictors display high collinearity: Age_c, Arrival_c, and LoR_c. This makes sense because, for most L2 participants recruited in the UK, if we know how old they were when they arrived in the UK (Arrival), and their length of residence in the country in years (LoR), then we can probably almost predict their age (Age) to a high degree of accuracy. We can easily :check-mark-button: this hypothesis in a random sample of six participants from our L2.data:\n\nL2.data |&gt; \n  select(Arrival, LoR, Age) |&gt; \n  slice_sample(n = 6)\n\n  Arrival LoR Age\n1      26   3  29\n2      32   6  38\n3      22   5  28\n4      17   3  20\n5      23   4  27\n6      16  14  30\n\n\nAs you can see, in most cases, participants‚Äô age is equal to their age when they first arrived in the country plus the number of years they‚Äôve been living there since. This is why Age and the combination of the predictors Arrival and LoR are almost perfectly correlated:\n\ncor(L2.data$Age, (L2.data$Arrival + L2.data$LoR))\n\n[1] 0.9768833\n\n\nTo remedy this, we must remove one of these three predictors from our model. Fit a new model that does not include participants‚Äô age when they first arrived in the UK (Arrival). Save this new model as L2.model.b. Check that it now meets all the model assumptions and then examine its summary.\nQ13.12 Adjusted for the number of predictors entered into L2.model.b, what percentage of the variance in Vocab scores in the L2 dataset does this model accurately predict?\n\n\n\n\nPractically 0%\nAbout 3%\nAbout 15%\nAbout 20%\nAbout 22%\nAbout 31%\nAbout 57%\n\n\n\n\n\n\n\n\n\nShow sample code to answer Q13.12.\nL2.model.b &lt;- lm(formula = Vocab ~ ART + Blocks_c + Age_c + EduTotal_c + LoR_c + ReadEng + ART:EduTotal_c + ART:ReadEng + EduTotal_c:ReadEng,\n             data = L2.data)\n\ncheck_model(L2.model.b) # Now all looking much better than earlier!\n\nsummary(L2.model.b) # Adjusted R-squared:  0.2027\n\n\nQ13.13 Which main-effect predictor(s) make a statistically significant contribution to L2.model.b at Œ±¬†=¬†0.05?\n\n\n\n\n\nART\n\n\n\n\nBlocks_c\n\n\n\n\nAge_c\n\n\n\n\nEduTotal_c\n\n\n\n\nArrival_c\n\n\n\n\nLoR_c\n\n\n\n\nReadEng\n\n\n\n\n\n\n\n\nQ13.14 Does the model‚Äôs coefficient estimate for ReadEng make intuitive sense?\n\n\n\n\n\nYes, the ReadEng coefficient is positive, which means that the more time L2 speakers regularly spend reading in English, the higher their predicted receptive English vocabulary test scores.\n\n\n\n\nNo, the ReadEng coefficient is larger than all other coefficients, which violates the model assumption of no overly influential outliers.\n\n\n\n\n\n\n\n\nQ13.15 Which interaction effect(s) make a statistically significant contribution to L2.model.b at Œ±¬†=¬†0.05?\n\n\n\n\n\nART:EduTotal_c\n\n\n\n\nART:ReadEng\n\n\n\n\nEduTotal_c:ReadEng\n\n\n\n\nNone of them\n\n\n\n\n\n\n\n\nQ13.16 Use the {visreg} library to visualise the model‚Äôs predicted Vocab scores (on the y-axis) as a function of all possible ReadEng values (on the x-axis) and subdivide the figure into three panels corresponding to low, mid-level, and high ART scores. Which arguments do you have to use inside the visreg() function to achieve this?\n\n\n\n\n\nfit = L2.model.b\n\n\n\n\nxvar = \"ART\"\n\n\n\n\nxvar = \"ReadEng\"\n\n\n\n\nxtrans = function(x) x + median(L2.data$ReadEng)\n\n\n\n\nby = \"ReadEng\"\n\n\n\n\nby = \"ART\"\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n\n\nShow sample code to answer Q13.16.\nvisreg(fit = L2.model.b,\n       xvar = \"ReadEng\",\n       by = \"ART\")\n\n\n\n\n\n\n\n\n\n\nHex sticker of the {car} package",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Multiple linear reg`R`ession modelling</span>"
    ]
  },
  {
    "objectID": "13_MultipleLinearRegression.html#tapping-into-the-potential-of-statistical-modelling",
    "href": "13_MultipleLinearRegression.html#tapping-into-the-potential-of-statistical-modelling",
    "title": "13¬† Multiple linear regRession modelling",
    "section": "13.9 Tapping into the potential of statistical modelling",
    "text": "13.9 Tapping into the potential of statistical modelling\nThis chapter aimed to provide first insights into the potential of statistical modelling with multiple predictors. It has demonstrated the importance of considering interactions between predictors and of visualising both the observed sample data, as well as simulated values based on the predictions of a statistical model.\nYou may have noted that p-values were not at the heart of this chapter. Instead, we focused on model accuracy (as measured by the adjusted R2), coefficient estimates and their relative importance (as measured by the lmg metric), predicted values, and (partial) model residuals. This is because there is an unfortunate tendency among some students and researchers to be misled into thinking that, if a result turns out to be statistically significant, it must be true. As statistician Andrew Gelman puts it, some confuse statistics for ‚Äúa form of modern alchemy, transforming the uncertainty and variation of the laboratory and field measurements into clean scientific conclusions that can be taken as truth‚Äù (Gelman 2018: 43).\nI hope that working through Chapter 11 to 13 has shown you that a big part of learning to work with quantitative data in the language sciences is really about ‚Äúembracing variation and accepting uncertainty‚Äù (Gelman 2019). If we intend to report inferential statistics based on sample data, it is important that we decide on significance level thresholds, model selection criteria, and other such parameters before conducting our data analysis (on the benefits of preregistrating protocols and methods in the language sciences, see e.g. Mertzen, Lago & Vasishth 2021; Roettger 2021). If we want to test hypotheses, these must be well-defined hypotheses and we must strive to quantify (and ideally also visualise!) uncertainty. Ultimately, it is worth remembering that ‚Äúin almost all practical data analysis situations ‚Äì we can only draw uncertain conclusions from data, regardless of whether we manage to obtain statistical significance or not‚Äù (Vasishth & Gelman 2021: 1311). Crucially, we must be extremely careful when extrapolating our results beyond the range of our observed data.\nAs explained in Chapter 11, statistical inference based on NHST and p-values comes from a statistical framework called frequentism, which happens to (currently) be the most widely used framework in the language sciences. Taking a frequentist approach means that the statistical properties of the hypothesis tests that we conduct are considered under hypothetical replications of our study with new sample data. This is because, in the frequentist framework, we estimate the long-run probability of observing certain effects, were we to repeat the study many times. This is one of the reasons why replication (see Section 14.2) is key to advancing our knowledge of linguistics and language teaching and learning. Whenever we have small sample sizes, small effect sizes (i.e.¬†small coefficient estimates in statistical models), large measurement error, and/or lots of variability among the target population ‚Äì as is often the case in the language sciences ‚Äì we simply cannot get reliable results from a single sample.\nDoes this mean that we should give up with quantitative data analysis and statistics all together? No, of course not. On the contrary, this uncertainty makes research in the language sciences all the more interesting and worth pursuing! It also means that there is much more to be learnt in terms of methods. In this chapter, you have learnt about frequentist fixed-effects linear regression models. These models are incredibly useful and can be used in many contexts, but they make a number of important assumptions that do not always hold:\n\nPerhaps the most obvious, yet one that we have not discussed so far, is that the outcome variable of a linear regression model must be quantitative, like the Vocab variable in Dabrowska.data, which ranges from -13.33 to 95.56 (see Section 7.2). Other types of statistical models can be used to model other types of outcome variables, including:\n\nBinomial (or binary) logistic regression models allow us to predict binary outcomes (e.g.¬†whether or not a verb is negated) (to find out more, see e.g. Levshina 2015: Chapter¬†12; Sonderegger 2023: Chapter¬†6; Winter 2020: Chapter¬†12).\nMultinomial logistic regression models can be used to model categorical outcome variables with more than two levels (e.g.¬†which modal verb is used in certain constructions) (to find out more, see e.g. Levshina 2015: Chapter¬†13).\nPoisson regression models are used to model count variables, i.e.¬†discrete numeric variables such as the frequency of fillers (such as uh and oh) in certain contexts (to find out more, see e.g. Winter 2020: Chapter¬†13).\n\nThe observations (i.e.¬†the data points) used to fit a fixed-effect model must be independent of each other. In the language sciences, such a situation is actually quite rare (see Section 12.4.1). To model interdependencies between observations, we can fit mixed-effects models (also called multilevel or hierarchical models) (to learn more, see e.g. Gries 2021: Chapter¬†6; Sonderegger 2023: Chapters¬†8-10; Winter 2020: Chapters¬†14-15).\nLinear regression models assume linear relationships between the predictors. There are different ways to circumvent this problem. In some cases, predictors can be transformed to meet this assumption (see Winter 2020: Chapter¬†5). In others, it may be wiser to model non-linear associations with other kinds of models such as Generalised Additive Mixed Models Wieling (2018).\nFinally, we need not stick to the frequentist school of statistics. In fact, quantitative linguists are increasingly turning to Bayesian statistics and finding that Bayesian models help them work with the particularities of linguistic data (to learn more about Bayesian statistics, see e.g. Levshina 2022; Nicenboim, Schad & Vasishth 2026).\n\n\n\n\n\n\n\nNoteRecommended further reading üìö\n\n\n\nIn this textbook, we have only just scratched the surface of statistical modelling. We can do much, much more with these kinds of models, and there are lots of additional things to take into account. The good news is that there are lots of excellent resources to help you continue your statistical modelling journey. Here are some good places to get started (in alphabetical order):\n\nGries, Stefan Thomas. 2021. Statistics for linguistics with R: A practical introduction (De Gruyter Mouton Textbook). 3rd revised edition. De Gruyter Mouton.\nLevshina, Natalia. 2015. How to do linguistics with R: Data exploration and statistical analysis. John Benjamins.\nNicenboim, Bruno, Daniel Schad & Shravan Vasishth. 2026. Introduction to Bayesian Data Analysis for cognitive science (Chapman & Hall/CRC Statistics in the Social and Behavioral Sciences Series). CRC Press. Open Access version: https://bruno.nicenboim.me/bayescogsci/.\nSonderegger, Morgan. 2023. Regression modeling for linguistic data. Cambridge, Massachusetts: The MIT Press. Open Access version: https://osf.io/pnumg/.\nWinter, Bodo. 2020. Statistics for Linguists: An Introduction Using R. Routledge.\n\nAlthough they go further than the present textbook, you will also find that these resources begin by explaining many of the things already covered in this chapter and previous chapters, and that‚Äôs actually a good thing. There‚Äôs no harm in revising these complex topics from a different perspective, with different examples, R packages, and coding styles.\n\n\n\nCheck your progress üåü\nCongratulations: you have successfully completed the most difficult chapter of this textbook! You have answered 0 out of 16 questions correctly.\nAre you confident that you can‚Ä¶?\n\nFit a linear regression model in R with multiple numeric and categorical predictors and interpret the intercept and predictor coefficients\nCenter numeric predictors to make the intercept more meaningful and improve model interpretability\nAssess the importance of predictors using metrics like lmg\nVisualise and interpret model predictions (with confidence bands) and partial residuals using the {visreg} library\nModel and interpret interactions between predictors in multiple linear regression models, and visualise these interactions to see how one predictor moderates the effect of another on the outcome variable\n\n\n\n\n\nAcheson, Daniel J., Justine B. Wells & Maryellen C. MacDonald. 2008. New and updated tests of print exposure and reading abilities in college students. Behavior Research Methods 40(1). 278‚Äì289. https://doi.org/10.3758/brm.40.1.278.\n\n\nAlmeida, Alexandre, Adam Loy & Heike Hofmann. 2018. ggplot2 compatible quantile-quantile plots in R. The R Journal 10(2). 248‚Äì261. https://doi.org/10.32614/RJ-2018-051.\n\n\nCenter for OpenScience. 2025. Choosing the Right Preregistration Template: A Guide for Researchers. https://www.cos.io/blog/choosing-preregistration-template-guide-for-researchers.\n\n\n√áetinkaya-Rundel, Mine & Johanna Hardin. 2021. Introduction to modern statistics. Second. Leanpub. https://openintro-ims.netlify.app/.\n\n\nDƒÖbrowska, Ewa. 2019. Experience, aptitude, and individual differences in linguistic attainment: A comparison of native and nonnative speakers. Language Learning 69(S1). 72‚Äì100. https://doi.org/10.1111/lang.12323.\n\n\nFox, John & Sanford Weisberg. 2019. An R companion to applied regression. Third edition. SAGE.\n\n\nGelman, Andrew. 2018. Ethics in statistical practice and communication: Five recommendations. Significance 15(5). 40‚Äì43. https://doi.org/10.1111/j.1740-9713.2018.01193.x.\n\n\nGelman, Andrew. 2019. Embracing variation and accepting uncertainty: Implications for science and metascience. https://www.youtube.com/watch?v=VQCcMP4A5Ks.\n\n\nGries, Stefan Thomas. 2021. Statistics for linguistics with R: A practical introduction (De Gruyter Mouton Textbook). 3rd revised edition. de Gruyter Mouton.\n\n\nGr√∂mping, Ulrike. 2006. Relative Importance for Linear Regression inR: The Package relaimpo. Journal of Statistical Software 17(1). https://doi.org/10.18637/jss.v017.i01.\n\n\nHaroz, Steve. 2022. Comparison of preregistration platforms. https://doi.org/10.31222/osf.io/zry2u.\n\n\nHarrell, Frank E. 2015. Regression modeling strategies: With applications to linear models, logistic and ordinal regression, and survival analysis (Springer Series in Statistics). Springer International Publishing. https://doi.org/10.1007/978-3-319-19425-7.\n\n\nLevshina, Natalia. 2015. How to do linguistics with R: Data exploration and statistical analysis. John Benjamins.\n\n\nLevshina, Natalia. 2022. Comparing Bayesian and Frequentist Models of Language Variation: The Case of Help + (to-)Infinitive. In Ole Sch√ºtzler & Julia Schl√ºter (eds.), 224‚Äì258. 1st edn. Cambridge University Press. https://doi.org/10.1017/9781108589314.009.\n\n\nLindeman, Richard Harold, Peter Francis Merenda & Ruth Z. Gold. 1980. Introduction to bivariate and multivariate analysis. Scott, Foresman.\n\n\nL√ºdecke, Daniel. 2020. sjPlot: Data visualization for statistics in social science. https://CRAN.R-project.org/package=sjPlot.\n\n\nL√ºdecke, Daniel, Mattan S. Ben-Shachar, Indrajeet Patil, Philip Waggoner & Dominique Makowski. 2021. performance: An R package for assessment, comparison and testing of statistical models. Journal of Open Source Software 6(60). 3139. https://doi.org/10.21105/joss.03139.\n\n\nL√ºdecke, Daniel, Indrajeet Patil, Mattan S. Ben-Shachar, Brenton M. Wiernik, Philip Waggoner & Dominique Makowski. 2021. See: An R package for visualizing statistical models. Journal of Open Source Software 6(64). 3393. https://doi.org/10.21105/joss.03393.\n\n\nMertzen, Daniela, Sol Lago & Shravan Vasishth. 2021. The benefits of preregistration for hypothesis-driven bilingualism research. Bilingualism: Language and Cognition 24(5). 807‚Äì812. https://doi.org/10.1017/S1366728921000031.\n\n\nMizumoto, Atsushi. 2023. Calculating the relative importance of multiple regression predictor variables using dominance analysis and random forests. Language Learning 73(1). 161‚Äì196. https://doi.org/10.1111/lang.12518.\n\n\nNicenboim, Bruno, Daniel Schad & Shravan Vasishth. 2026. Introduction to Bayesian Data Analysis for cognitive science (Chapman & Hall/CRC Statistics in the social and behavioral sciences series). Boca Raton London New York: CRC Press, Taylor & Francis Group. https://doi.org/10.1201/9780429342646.\n\n\nRoettger, Timo B. 2021. Preregistration in experimental linguistics: Applications, challenges, and limitations. Linguistics. De 59(5). 1227‚Äì1249. https://doi.org/10.1515/ling-2019-0048.\n\n\nSmith, Gary. 2018. Step away from stepwise. Journal of Big Data. SpringerOpen 5(1). 1‚Äì12. https://doi.org/10.1186/s40537-018-0143-6.\n\n\nSonderegger, Morgan. 2023. Regression modeling for linguistic data. The MIT Press.\n\n\nS√≥skuthy, M√°rton. Generalised additive mixed models for dynamic analysis in linguistics: A practical introduction. https://doi.org/10.48550/arXiv.1703.05339.\n\n\nThompson, Bruce. 1995. Stepwise regression and stepwise discriminant analysis need not apply here: A guidelines editorial. Educational and Psychological Measurement. SAGE 55(4). 525‚Äì534. https://doi.org/10.1177/0013164495055004001.\n\n\nVasishth, Shravan & Andrew Gelman. 2021. How to embrace variation and accept uncertainty in linguistic and psycholinguistic data analysis. Linguistics 59(5). 1311‚Äì1342. https://doi.org/10.1515/ling-2019-0051.\n\n\nWieling, Martijn. 2018. Analyzing dynamic phonetic data using generalized additive mixed modeling: A tutorial focusing on articulatory differences between L1 and L2 speakers of english. Journal of Phonetics 70. 86‚Äì116. https://doi.org/10.1016/j.wocn.2018.03.002.\n\n\nWinter, Bodo. 2020. Statistics for linguists: An introduction using R. Routledge. https://doi.org/10.4324/9781315165547.",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Multiple linear reg`R`ession modelling</span>"
    ]
  },
  {
    "objectID": "13_MultipleLinearRegression.html#footnotes",
    "href": "13_MultipleLinearRegression.html#footnotes",
    "title": "13¬† Multiple linear regRession modelling",
    "section": "",
    "text": "Loading the {relaimpo} package returns several warnings informing us, on the one hand, about packages that {relaimpo} requires to work and which are therefore also automatically loaded (these are called dependencies) and, on the other, about objects being masked by different packages, e.g.:\nThe following object is masked from ‚Äòpackage:dplyr‚Äô:\n\nselect\nIt is worth paying attention to the latter set of warnings because it means that some function names, e.g.¬†select(), are now shared by more than one package in our R environment. This can cause code that previously worked fine to suddenly return errors. For example, after loading the {relaimpo} package, you may find that the select() function no longer works as expected because R attempts to use the select() function from the {MASS} package rather than from the tidyverse {dplyr} package. To avoid this happening, we can manually assign the correct package to the function name like this:\n\nselect &lt;- dplyr::select\n\nThis ensures that any future mentions of the select() function are, by default, interpreted as the function defined in the {dplyr} package.‚Ü©Ô∏é\nIf your version of RStudio does not display the six-panelled figure but instead only a white canvas, this is probably because your Plots pane is too small. In this case, you will need to make it as large as possible and then try running the function again. If that doesn‚Äôt work either, don‚Äôt worry as we will save and examine individual plots from now on.‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Multiple linear reg`R`ession modelling</span>"
    ]
  },
  {
    "objectID": "14_LiterateProgramming.html",
    "href": "14_LiterateProgramming.html",
    "title": "14¬† RepRoducible research and academic wRiting in Quarto",
    "section": "",
    "text": "Chapter overview\nSo far, we have seen how we can export the outputs of our analyses conducted in R in the form of tables (Section 9.8) and graphics (Section 10.3). However, in most situations, we want to communicate our research in a way that allows us to combine both text and analysis outputs. This is where literate programming comes into play!\nIn this chapter, you will learn:",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Rep`R`oducible research and academic w`R`iting in Quarto</span>"
    ]
  },
  {
    "objectID": "14_LiterateProgramming.html#sec-LitProgramming",
    "href": "14_LiterateProgramming.html#sec-LitProgramming",
    "title": "14¬† RepRoducible research and academic wRiting in Quarto",
    "section": "14.1 Literate programming",
    "text": "14.1 Literate programming\nThe basic idea of literate programming is that we combine text, code, and code outputs (i.e.¬†tables, statistics, and plots) within a single document that can be exported into different formats for sharing and publishing.\nLiterate programming can be implemented in different authoring formats. Up until very recently, the most common format for R projects was R Markdown. For Python projects, Jupyter Notebooks remains the standard to date. In this chapter, we will focus on Quarto, a relatively new open-source scientific and technical authoring and publishing system that has the advantage of supporting many different programming languages. This means that code in R, Python, Julia, and other languages can be combined into one document, making project management and collaboration much easier. Quarto also allows us to readily export (or render) our documents to multiple formats such as HTML, PDF, and Word (see Figure¬†14.1 and Section 14.12).\n\n\n\n\n\n\nFigure¬†14.1: A schematic representation showing Quarto can understand multi-language input and produce multi-format outputs (Artwork CC BY 4.0 Allison Horst from the ‚ÄúHello, Quarto‚Äù keynote by Julia Lowndes and Mine √áetinkaya-Rundel, first presented at the RStudio Conference 2022).\n\n\n\nLiterate programming is particularly useful for academic research and data science. Did you know that this entire textbook was written in Quarto? I chose this format because it allows for the seamless combination of explanations with nicely formatted R code and code outputs (i.e.¬†all of the textbook‚Äôs tables, data visualisations, quiz questions, etc.). It also automatically generates consistent section and figure numbers, cross-references, bibliographic references, and much more. By the end of this chapter, you‚Äôll be ready to start writing your own term paper, dissertation, thesis, journal article, blog, or presentation slides in Quarto.\n\n\n\n\n\n\nNote\n\n\n\nQuarto documents are designed to:\n\nHelp you collaborate with other researchers (including your future self!) who are interested in both reproducing your results and understanding how you reached them (i.e.¬†the code).\nProvide you with a convenient environment in which to do research - a kind of ‚Äúmodern-day lab notebook where you can capture not only what you did, but also what you were thinking‚Äù (Wickham, √áetinkaya-Rundel & Grolemund 2023).\nCommunicate your analyses to others, including those who are not familiar with any programming language.",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Rep`R`oducible research and academic w`R`iting in Quarto</span>"
    ]
  },
  {
    "objectID": "14_LiterateProgramming.html#sec-Reproducibility",
    "href": "14_LiterateProgramming.html#sec-Reproducibility",
    "title": "14¬† RepRoducible research and academic wRiting in Quarto",
    "section": "14.2 Reproducible research",
    "text": "14.2 Reproducible research\nNot only is using Quarto (or any other literate programming format, see Section 14.1) very convenient, it also helps us make our research more reproducible. Unfortunately, the terms reproducible, replicable and repeatable are often confused and, not helping matters, some definitions in the literature contradict each other. In this textbook, we will adopt the terminology of The Turing Way. We thus define reproducibility as the ability of an independent researcher or team to obtain the same results as in a study using the same data and methods as the original study (see Figure¬†14.2).\n\n\n\nThis is in contrast to replicability, where the same methods, but different data are used; and robustness, where the same data, but different methods are used. Finally, if a finding can be reliably observed across different datasets with different methods, then we can say that the finding is generalisable.\n\n\n\n\n\n\nFigure¬†14.2: Defining reproducibility and related terms (CC BY 4.0 The Turing Way Community )\n\n\n\nGiven this definition, reproducibility might seem like a low bar to pass. You might be thinking: shouldn‚Äôt it be obvious that we‚Äôll get the same results if we repeat a study using exactly the same data and method? Well, yes, it should be. But it very often isn‚Äôt! For a start, to be able to even attempt to reproduce the results of a study, the underlying data must be available. Linguists that share their primary data as Ewa DƒÖbrowska did as part of her 2019 publication (DƒÖbrowska 2019) remain the exception rather than the norm (see Bochynska et al. 2023)1. Second, the data must be available in an accessible format and must be published together with enough documentation to be understandable to an independent researcher. Third, the author(s) of the original study need to have very diligently documented all their data wrangling and analyses steps. The best way to do this is undoubtedly to use code that does not require closed-source software (e.g.¬†a researcher without a license for SPSS or Stata will not be able to run SPSS or Stata scripts, see Section 1.2). This open code must be shared in an accessible format, too. Fourth, independent researchers need to be able to run these scripts. To this end, it is important that they know exactly which tools were used. Thus, if the analyses were conducted in R, they need to know which R version and which packages and package versions were used (Section 14.10). They also need to know in which order the scripts were run and, finally, the scripts must run on their own computers without any errors. So now, reproducibility doesn‚Äôt sound quite so easy, right? Luckily, if we apply the principles of literate programming in Quarto, we can go a long way towards ensuring that our research is reproducible.\n\n\n\n\n\n\nNoteGoing furtherÔ∏è üöÄ\n\n\n\nTo find out more about best practices for reproducible research, check out The Turing Way‚Äôs excellent Guide for Reproducible Research.\n\n\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nWatch this video from 2019, in which Garrett Grolemund (data scientist and instructor at RStudio) explains why literate programming is key to improving medical science, data science, and ultimately all empirical research endeavours. Be aware that everything that Garrett says about R Markdown is also true of Quarto.\n\nQ14.1 What is meant by the replication crisis?2\n\n\n\n\n\nThe application of statistical principles to arrive at well-founded ‚Äîi.e. likely corresponding accurately to the real world‚Äî concepts, conclusions or measurement.\n\n\n\n\nA set of good research practices based on fundamental principles: honesty, reliability, respect and accountability.\n\n\n\n\nAn aphorism describing the pressure researchers feel to publish academic manuscripts, often in high prestige academic journals, in order to have a successful academic career.\n\n\n\n\nAn approach that integrates external criticism by colleagues and peers into the research process.\n\n\n\n\nThe tendency to report only significant results in the abstract, while reporting non-significant results within the main body of the manuscript (not reporting non-significant results altogether would constitute selective reporting).\n\n\n\n\nThe finding, and related shift in academic culture and thinking, that a large proportion of scientific studies published across different disciplines do not replicate.\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\nQ14.2 Which stages of the research process are potential sources of uncertainty?\n\n\n\n\n\nThe choice of research questions\n\n\n\n\nThe sampling procedure\n\n\n\n\nThe measurement of variables\n\n\n\n\nThe handling of missing data\n\n\n\n\nThe choice of statistical tests/models\n\n\n\n\nThe handling of confounding variables\n\n\n\n\nThe publication process\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\nQ14.3 What would it take for a linguist to fully understand the conclusions of another linguist‚Äôs quantitative study?\n\n\n\n\n\nThe original data from the study\n\n\n\n\nThe analysis code\n\n\n\n\nThe software used to run the code\n\n\n\n\nRemote access to the author's computer\n\n\n\n\nA detailed report explaining the author's reasoning for the choices they made\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Rep`R`oducible research and academic w`R`iting in Quarto</span>"
    ]
  },
  {
    "objectID": "14_LiterateProgramming.html#sec-StartQuarto",
    "href": "14_LiterateProgramming.html#sec-StartQuarto",
    "title": "14¬† RepRoducible research and academic wRiting in Quarto",
    "section": "14.3 Getting started with Quarto",
    "text": "14.3 Getting started with Quarto\n\n\n\nWe will be writing Quarto documents from the RStudio IDE3, which conveniently ships with a version of Quarto, meaning that no additional installation4 is required for you to use Quarto on your computer.\nTo get started with Quarto:\n\nIn RStudio, create a new Project by selecting File¬†&gt;¬†New Project‚Ä¶ in the main menu, or by clicking on the ‚Äúnew project‚Äù button.\n\nYou can choose the first option to create a new folder for your project if you‚Äôve not yet got one,\nor the second option to select an existing project directory.\n\nThen, create a new Quarto document by navigating to File¬†&gt;¬†New File¬†&gt;¬†Quarto Document‚Ä¶, or clicking on the ‚Äúnew document‚Äù button and selecting ‚ÄúQuarto Document‚Ä¶‚Äù. A dialogue menu will appear (Figure¬†14.3). Leave everything as is and simply click on‚ÄùCreate‚Äù at the bottom.\n\n\n\n\n\n\n\nFigure¬†14.3: Creating a new Quarto document\n\n\n\n\nRStudio has now opened a new, untitled Quarto file (.qmd). Depending on your settings, this new Quarto document may include some template material, which you can delete. Change the title of your Quarto document (which is not the same as its filename!) and add three further lines to the document header by copying and pasting the following lines at the top of the document, replacing the default header. Quarto document headers5 are written in YAML which, I kid you not, stands for Yet Another Markup Language! :grinning-face-with-sweat:\n\n\n---\ntitle: Learning Quarto\nsubtitle: \"by reproducing the descriptive statistics of DƒÖbrowska's (2019) study\"\nauthor: Write your name here\ndate: last-modified\n---\n\n\nClick on the ‚Äúsave‚Äù button in the menu bar or navigate to File¬†&gt;¬†Save to save your .qmd file. You will be prompted to give it a name. This could be LearningQuarto.qmd (see Section 3.2 for tips on how to name files).\nTo check your Quarto installation, render your document by either selecting File¬†&gt;¬†Render Document in the main menu, or clicking on ‚ÄúRender‚Äù button in the Quarto menu bar (see Figure¬†14.4). Your .qmd file will automatically be rendered to HTML (Quarto‚Äôs default rendering format).\nNavigate to the folder where you saved your .qmd file to find the rendered HTML file. You can use a Finder (on macOS) or File Explorer window (on Windows) or go to the ‚ÄúFiles‚Äù pane in RStudio to do this. The rendered version of your file will have the same filename as your Quarto document, but with the file extension .html (e.g.¬†LearningQuarto.html). If you open on the file, it will appear in your default web browser (e.g.¬†Firefox, Chrome, Safari). You should see that the HTML document features the title of your document, your name as the author, and today‚Äôs date (see Figure¬†14.5).\n\n\n\n\n\n\n\n\n\n\nFigure¬†14.4: The .qmd file as opened in RStudio\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†14.5: The .html file as opened in a web browser\n\n\n\n\n\n\nFor now, the document is empty. In the next sections, you will learn how to add text, code, and code outputs to your Quarto document.\n\n14.3.1 RStudio‚Äôs visual editor\nYou may have noticed that RStudio proposes two different modes in which Quarto documents can be edited: Source and Visual (see Figure¬†14.6).\n\n\n\n\n\n\nFigure¬†14.6: Source and Visual mode in RStudio\n\n\n\nThe Visual mode offers a WYSIWYM (What You See Is What You Mean) authoring experience. This means that, in Visual model, you will immediately see the effect of your formatting on screen. For example, to format a word in italics, you can click on the corresponding button in the toolbar (see Figure¬†14.6) or use the keyboard shortcut (Cmd/Ctrl¬†+¬†I) ‚Äî just like you would in text-processing software ‚Äî and this will immediately display the text in italics.\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nQ14.4 In this task, you will practice using RStudio‚Äôs Visual model to format text in a Quarto document.\n\nIn a new line beginning after the final --- of the YAML header, paste the introduction text below.\nUsing the Quarto editing toolbar, format the text so that, in the Visual mode, it looks like the text displayed in Figure¬†14.7.\nRender the document and compare how it is formatted in the HTML version.\n\n\nIntroduction\n\nThe aim of this report is to reproduce the descriptive statistics reported in DƒÖbrowska (2019: 5-6) using the original datasets (DƒÖbrowska 2019: Appendix S4):\n\nMethod\n\nParticipants\n\nNinety native speakers (42 male and 48 female) and 67 nonnative speakers of English (21 male and 46 female) were recruited through personal contacts, church and social clubs, and advertisements in local newspapers. Participants were told that the purpose of the study was to examine individual differences in native and nonnative speakers‚Äô knowledge of English and whether these differences are related to their linguistic experience and abilities. All participants signed a written consent form before the research commenced.\n\nThe L1 participants were all born and raised in the United Kingdom and were selected to ensure a range of ages, occupations, and educational backgrounds. The age range was from 17 to 65 years (M = 38, SD = 16). Twenty-two percent of the participants held manual jobs, 24% held clerical positions, and 28% had professional-level jobs or were studying for a degree; the remaining 26% were occupationally inactive (i.e. unemployed, retired, or homemakers). In terms of education, participants‚Äô backgrounds ranged from no formal qualifications to Ph.D., with corresponding differences in the number of years spent in full-time education (from 10 to 21; M = 14, SD = 2). Six participants reported a working knowledge of another language; the rest described themselves as monolinguals.\n\n\n\n\n\n\n\nFigure¬†14.7: RStudio‚Äôs Visual mode\n\n\n\nIn RStudio‚Äôs visual mode, what is the name of the formatting option that indents and adds a grey line to the left of a quoted paragraph as in Figure¬†14.7?\n\n\n\n\n\nLine Block\n\n\n\n\nSpan\n\n\n\n\nCode Block\n\n\n\n\nDiv\n\n\n\n\nBlockquote\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteClick here for the full solution to Q14.4\n\n\n\n\n\nIn the Visual mode (see Figure¬†14.8 (a)), click on the ‚ÄúNormal‚Äù drop-down menu (see Figure¬†14.8 (b)) to change the formatting of the word Introduction to the ‚ÄúHeader 1‚Äù style. To format the long citation, choose the ‚ÄúBlockquote‚Äù option from the the ‚ÄúFormat‚Äù drop-down menu (see Figure¬†14.8 (c)).\n\n\n\n\n\n\nFigure¬†14.8: Text formatting using the Visual mode in RStudio",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Rep`R`oducible research and academic w`R`iting in Quarto</span>"
    ]
  },
  {
    "objectID": "14_LiterateProgramming.html#sec-Markdown",
    "href": "14_LiterateProgramming.html#sec-Markdown",
    "title": "14¬† RepRoducible research and academic wRiting in Quarto",
    "section": "14.4 Markdown text",
    "text": "14.4 Markdown text\nWriting and formatting text in RStudio‚Äôs Visual editor is very similar to writing in a word-processing software such as LibreOffice Writer or Microsoft Word. In the background, however, RStudio automatically converts your formatted text to Markdown in the underlying source code of your .qmd file. Markdown is a plain-text format. For example, in Markdown, words in italics are enclosed in asterisks like this: *italics*. Table¬†14.1 displays the Markdown syntax for other formatting options commonly used in academic writing.\nThe best way to get the hang of Markdown is simply to try things out. You will also find a handy cheatsheet under Help &gt; Markdown Quick Reference. Remember that you can always go back to the Visual mode to format your text if that‚Äôs easier for you. When it comes to debugging any Quarto syntax errors, however, it‚Äôs usually easier to catch these in plain text, so you‚Äôll typically want to switch to the Source mode for that.\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nSwitch to the Source mode to view the text that you formatted in the Visual editor for Q14.4 in Markdown format.\nQ14.5 How is text highlighted in bold displayed in Markdown?\n\n\n\n\n\n**bold text**\n\n\n\n\n# bold text\n\n\n\n\nBOLD TEXT\n\n\n\n\n[bold text]{.bold}\n\n\n\n\n*bold text*\n\n\n\n\n\n\n\n\n¬†\nQ14.6 How is a first-level heading displayed in Markdown?\n\n\n\n\n\n[*Heading 1*]\n\n\n\n\n&lt;h1&gt;Heading 1&lt;/h1&gt;\n\n\n\n\n# Heading 1\n\n\n\n\n**Heading 1**\n\n\n\n\nHEADING 1\n\n\n\n\n\n\n\n\n¬†\nQ14.7 How are block quotes formatted in Markdown?\n\n\n\n\n\nThe text is coloured green\n\n\n\n\nEvery line begins with &gt;\n\n\n\n\nEvery line begins with &gt; followed by a tab\n\n\n\n\nEvery line begins with &gt; followed by a space\n\n\n\n\n\n\n\n\n¬†\nQ14.8 How will the word ~~mystery~~ be formatted in Markdown?\n\n\n\n\n\nin italics\n\n\n\n\nin grey\n\n\n\n\nas a subsection heading\n\n\n\n\ncrossed-out\n\n\n\n\nas computer code\n\n\n\n\n\n\n\n\n¬†\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nMarkdown is gaining in popularity and is now widely supported across many platforms, from text editors to content management systems, ensuring that your formatting remains consistent and portable. Whether you‚Äôre writing documentation, creating blog posts, or taking notes, Markdown‚Äôs simplicity and versatility make it a valuable skill to have beyond academic writing and Quarto.\nYou can learn more about Markdown here: https://www.markdownguide.org/basic-syntax/.\n\n\n\n\n\n\nTable¬†14.1: Commonly used formatting options and their markdown syntax (adapted from the official Quarto Guide)\n\n\n\n\n\n\n\n\n\nMarkdown syntax\nRendered output\n\n\n\n\n*italics*, **bold**, ***bold italics***\nitalics, bold, bold italics\n\n\nsuperscript^2^ / subscript~2~\nsuperscript2 / subscript2\n\n\n~~strikethrough~~\nstrikethrough\n\n\n`verbatim code`\nverbatim code\n\n\n# Heading 1\nHeading 1\n\n\n## Heading 2\nHeading 2\n\n\n### Heading 3\nHeading 3\n\n\n#### Heading 4\nHeading 4\n\n\n&lt;https://quarto.org&gt;\nhttps://quarto.org\n\n\n[Quarto guide](https://quarto.org)\nQuarto guide\n\n\n* bullet-point list\n  + sub-item 1\n  + sub-item 2\n    - sub-sub-item 1\n\nbullet-point list\n\nsub-item 1\nsub-item 2\n\nsub-sub-item 1\n\n\n\n\n\n1. numbered list\n2. item 2\n   i. sub-item 1\n      A.  sub-sub-item 1\n\nnumbered list\nitem 2\n\nsub-item 1\n\nsub-sub-item 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nTo make writing in Quarto more convenient and less error-prone, you can switch on a spell-checker within RStudio. To do so, go to Tools¬†&gt;¬†Global Options‚Ä¶¬†&gt;¬†Spelling. You may need to restart RStudio for the change to take effect.",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Rep`R`oducible research and academic w`R`iting in Quarto</span>"
    ]
  },
  {
    "objectID": "14_LiterateProgramming.html#sec-Chunks",
    "href": "14_LiterateProgramming.html#sec-Chunks",
    "title": "14¬† RepRoducible research and academic wRiting in Quarto",
    "section": "14.5 Code chunks",
    "text": "14.5 Code chunks\nTo run code inside a Quarto document, we need to insert a code chunk. There are three ways to do so:\n\nUsing the keyboard shortcut Cmd/Ctrl¬†+¬†Option¬†+¬†i\nClicking on the green ‚ÄúInsert chunk‚Äù button icon in the editor toolbar\nManually typing the chunk delimiters ```{r} and ```\n\nIt is definitely worth learning the keyboard shortcut as it will save you a lot of time in the long run!\nIn the code chunk below, {r} tells Quarto that this chunk is written in the programming language R. If we wanted to embed a chunk of Python code, we must begin it with ```{python} instead.\nUsing one of the three aforementioned options, insert the following R code chunk in your document.\n\n```{r}\nlibrary(here)\nlibrary(tidyverse)\n```\n\nTo run code within a Quarto document, we can either run:\n\neach individual line of code using the keyboard shortcut Cmd/Ctrl¬†+¬†Enter or\nthe entire code chunk either by clicking the ‚ÄúRun‚Äùicon or with shortcut Shift¬†+¬†Cmd/Ctrl¬†+¬†Enter.\n\nRStudio will execute the code and display the results either within your document (below each chunk) or in the Console, depending on your RStudio settings.6\nChunk output can be customised with chunk options. There are many options to choose from, but the most important options control whether a code block should be executed when the Quarto document is rendered and what results are inserted in the rendered version:\n\neval: false prevents code from being evaluated. Given that the code is not run, no code outputs are generated either.\ninclude: false runs the code, but does not show the code or its outputs in the rendered document. This option is useful for code chunks that are not informative to the readers of your document.\necho: false prevents the code from appearing in the rendered document, but displays the code outputs. This option is useful when you want to present the results of your analyses to people who are not interested in the underlying code.\nmessage: false or warning: false prevents messages or warnings from appearing in the rendered document.\n\nIt is also possible to label code chunks using the label option (see code chunk below). This can help to navigate long Quarto documents using the drop-down menu available in the bottom-left corner of the Source pane (see Figure¬†14.9). It also helps to quickly identify which code chunk is causing errors during rendering. Chunk labels should be short but meaningful.\n```{r}\n#| echo: false\n#| label: \"Example plot\"\nplot(1:10)\n```\nIn RStudio the easiest way to set a chunk option is by clicking the gear icon in the top right corner of the chunk that you want to modify. This way, you can both choose a label and set chunk options. If you prefer to write code chunk options manually, these are placed at the top of the corresponding chunk following #|, as in the chunk above and Figure¬†14.9. As you can see in Figure¬†14.10, the echo: false chunk option means that the rendered document includes the chunk output, but not the code itself.\n\n\n\n\n\n\n\n\n\nFigure¬†14.9: Quarto document in Source mode in RStudio\n\n\n\n\n\n¬†\n\n\n\n\n\n\n\n\nFigure¬†14.10: Rendered HTML document opened in a browser\n\n\n\n\n\n\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nIn your Quarto document, add a label to your first R chunk and render your document to HTML.\n\n```{r}\n#| label: setup\n\nlibrary(here)\nlibrary(tidyverse)\n```\n\nQ14.9 What is the output of the setup chunk in your rendered .html document?\n\n\n\n\n\nNothing.\n\n\n\n\nTwo messages, one per loaded library.\n\n\n\n\nA conflict error message.\n\n\n\n\nAn error message beginning with: \"Error in `library()`: ! there is no package called...\n\n\n\n\nAn error message ending in: \"Execution halted\".\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\nQ14.10 Which code chunk option can you use to remove the two messages from the rendered version of your Quarto document, whilst still ensuring that the setup chunk is displayed and executed so that the libraries can be used in future code chunks?\n\n\n\n\n\n#| message: true\n\n\n\n\n#| echo: false\n\n\n\n\n#| messages: false\n\n\n\n\n#| message: false\n\n\n\n\n#| eval: true\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\nQ14.11 Which code chunk option can you use to remove both the setup chunk and its outputs from the rendered version of your Quarto document, whilst still ensuring that the libraries are loaded so that their functions can be used further down in the document?\n\n\n\n\n\n#| eval: false\n\n\n\n\n#| display: false\n\n\n\n\n#| echo: true\n\n\n\n\n#| echo: false\n\n\n\n\n#| include: false\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Rep`R`oducible research and academic w`R`iting in Quarto</span>"
    ]
  },
  {
    "objectID": "14_LiterateProgramming.html#sec-Inline",
    "href": "14_LiterateProgramming.html#sec-Inline",
    "title": "14¬† RepRoducible research and academic wRiting in Quarto",
    "section": "14.6 Inline code",
    "text": "14.6 Inline code\nSo far, we have seen how we can insert and format text in Quarto and how we can add code chunks with various options. But, to make the most of literate programming, we want to combine the two.\n\n\n\n\n\n\nWarningPrerequisites\n\n\n\nThis chapter assumes that you are familiar with the following research article (which was first introduced in Section 6.1):\n\nDƒÖbrowska, Ewa. 2019. Experience, Aptitude, and Individual Differences in Linguistic Attainment: A Comparison of Native and Nonnative Speakers. Language Learning 69(S1). 72-100. https://doi.org/10.1111/lang.12323.\n\nOur starting point for this chapter are the author‚Äôs original datasets, which are linked in the article‚Äôs Appendix S4.\n\nAppendix S4: Datasets\nDƒÖbrowska, E. (2018). L1 data [Data set]. Retrieved from https://www.iris-database.org/iris/app/home/detail?id=york:935513\nDƒÖbrowska, E. (2018). L2 data [Data set]. Retrieved from https://www.iris-database.org/iris/app/home/detail?id=york:935514\n\nYou will only be able to reproduce the analyses and answer the quiz questions from this chapter if you have successfully imported these two two datasets. To do so, follow the instructions from Section 6.3 to Section 6.5 and complete Q6.8‚ÄîQ6.12.\nAlternatively, you can download Dabrowska2019.zip from the textbook‚Äôs GitHub repository, which contains both datasets. To launch the project correctly, first unzip the file and then double-click on the Dabrowska2019.Rproj file.\n\n\nInsert the following R chunk to load the DƒÖbrowska (2019) data so that they may be used in your Quarto document. As this import-data chunk requires the here() function, it must come after the setup chunk because, when the document is rendered, code chunks will be executed in the order that they appear. If the {here} library is not loaded before the data are imported, the rendering process will be aborted and an error message will be displayed in the Console.\nTo begin, we will reproduce the following basic descriptive statistics about the two datasets:\n\n‚ÄúNinety native speakers (42 male and 48 female) and 67 nonnative speakers of English (21 male and 46 female) were recruited through personal contacts, church and social clubs, and advertisements in local newspapers‚Äù (DƒÖbrowska 2019: 5).\n\nAs you may recall from Chapter 7, the number of native and non-native participants corresponds to the number of rows in the corresponding dataset:\n\nnrow(L1.data)\n\n[1] 90\n\nnrow(L2.data)\n\n[1] 67\n\n\nIn Quarto, we can use inline code to dynamically insert these numbers into our paragraph. Inline code in R begins with `{r} and ends with a single backtick `. It is best to use the Source mode to insert inline code. Using the Source mode, add the following section to your Quarto document and render it to HTML.\n## Descriptive statistics about the participants\n\n¬†`{r} nrow(L1.data)` native speakers and `{r} nrow(L2.data)` nonnative speakers of English were recruited through personal contacts, church and social clubs, and advertisements in local newspapers.\nThe rendered version should read like this (if you are obtaining different numbers, this either means that you have tempered with the original data files or that they have been corrupted)7:\n\nDescriptive statistics about the participants\n90 native speakers and 67 nonnative speakers of English were recruited through personal contacts, church and social clubs, and advertisements in local newspapers.\n\nInline code should only be used for very simple code, ideally with no more than one function, as in `{r} nrow(L1.data)`. To insert the output of more complex operations, it is best to write the code and save its output(s) to the local environment in a hidden code chunk (using the option #| include: false, see Section 14.5).\nThe saved objects (L1.males and L1.females) each contain one number. They can therefore be directly called within the text as inline code:\n¬†`{r} nrow(L1.data)` native speakers (`{r} L1.males` male and `{r} L1.females` female) and `{r} nrow(L2.data)` nonnative speakers of English were recruited through personal contacts, church and social clubs, and advertisements in local newspapers.\nWhen rendered, the paragraph will read:\n\n90 native speakers (42 male and 48 female) and 67 nonnative speakers of English were recruited through personal contacts, church and social clubs, and advertisements in local newspapers.\n\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nQ14.12 In your Quarto document, add a code chunk called L2-gender in which you compute the values necessary to complete the missing descriptive statistics in the sentence above. When rendered, your sentence should read:\n\n90 native speakers (42 male and 48 female) and 67 nonnative speakers of English (21 male and 46 female) were recruited through personal contacts, church and social clubs, and advertisements in local newspapers.\n\n¬† Which value requires more than just one line of code?\n\n\n\n\n\nThe total number of L2 participants.\n\n\n\n\nThe number of L2 female speakers\n\n\n\n\nThe number of L2 male speakers\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteClick here for the solution to Q14.12\n\n\n\n\n\nTo save the number of male L2 participants as an R object, we can follow the same procedure as above.\n\nL2.males &lt;- L2.data |&gt;  \n  filter(Gender == \"M\") |&gt;\n  count()\n\nFor the number of female L2 participants, however, it‚Äôs not so simple because some are labelled f, while others are labelled F (see Section 9.4.2).\n\ntable(L2.data$Gender)\n\n\n f  F  M \n 6 40 21 \n\n\nBelow are four possible methods to solve this issue (and there are many more still!):\n\n# Method 1:\nL2.Females &lt;- L2.data |&gt; \n  filter(Gender == \"F\") |&gt; \n  count()\n\nL2.females &lt;- L2.data |&gt; \n  filter(Gender == \"f\") |&gt; \n  count()\n\nL2.allfemales &lt;- L2.Females + L2.females \n\n# Method 2:\nL2.allfemales &lt;- L2.data |&gt; \n  filter(Gender == \"F\" | Gender == \"f\") |&gt; \n  count()\n\n# Method 3:\nL2.allfemales &lt;- L2.data |&gt; \n  filter(Gender %in% c(\"F\", \"f\")) |&gt; \n  count()\n\n# Method 4:\nL2.allfemales &lt;- L2.data |&gt; \n  mutate(Gender = toupper(Gender)) |&gt; \n  filter(Gender == \"F\") |&gt; \n  count()\n\nSome of these methods are perhaps more elegant than others, but they are all acceptable. After all, they all work! :upside-down-face:\nOnce they are saved to the local environment, the values can be inserted inline in the usual way:\n\n `{r} nrow(L1.data)` native speakers (`{r} L1.males` male and `{r} L1.females` female) and `{r} nrow(L2.data)` nonnative speakers of English (`{r} L2.males` male and `{r} L2.allfemales` female) were recruited through personal contacts, church and social clubs, and advertisements in local newspapers. \n\n\n\nIf we want to start our paragraph with 90 written in as a word rather than in digits, we can use the numbers_to_words function() function from the {xfun} package. First, you‚Äôll need to install the {xfun} package and then add a line to your setup chunk to load it.8\n\n```{r}\n#install.packages(\"xfun\")\nlibrary(xfun)\n```\n\nFirst, let‚Äôs test that the package works by running the following line of code from the Console:\n\nnumbers_to_words(nrow(L1.data))\n\n[1] \"ninety\"\n\n\nTo start our paragraph with a capital letter, we‚Äôll need to set the function‚Äôs cap argument to TRUE.\n¬†`{r} numbers_to_words(nrow(L1.data), cap = TRUE)` native speakers (`{r} L1.males` male and `{r} L1.females` female) and `{r} nrow(L2.data)` nonnative speakers of English...\nNext, we want to reproduce the following descriptive statistics about the L1 participants:\n\n‚ÄúThe L1 participants were all born and raised in the United Kingdom and were selected to ensure a range of ages, occupations, and educational backgrounds. The age range was from 17 to 65 years (M = 38, SD = 16)‚Äù (DƒÖbrowska 2019: 5).\n\nWe can use the base R functions min(), max(), mean(), and sd() to compute these values.\nThe L1 participants were all born and raised in the United Kingdom and were selected to ensure a range of ages, occupations, and educational backgrounds. The age range was from `{r} min(L1.data$Age)` to `{r} max(L1.data$Age)` years (*M* = `{r} mean(L1.data$Age)`, *SD* = `{r} sd(L1.data$Age)`).\nThe rendered document will read:\n\nThe L1 participants were all born and raised in the United Kingdom and were selected to ensure a range of ages, occupations, and educational backgrounds. The age range was from 17 to 65 years (M = 37.5444444, SD = 16.148998).\n\nWhilst these values are correct, in practice, we want to round them off to the nearest integer. To this end, we can wrap the round() function around the mean() and sd() function (see Section 7.5.1).\nThe L1 participants were all born and raised in the United Kingdom and were selected to ensure a range of ages, occupations, and educational backgrounds. The age range was from `{r} min(L1.data$Age)` to `{r} max(L1.data$Age)` years (*M* = `{r} round(mean(L1.data$Age))`, *SD* = `{r} round(sd(L1.data$Age))`).\nThe rendered document will read:\n\nThe L1 participants were all born and raised in the United Kingdom and were selected to ensure a range of ages, occupations, and educational backgrounds. The age range was from 17 to 65 years (M = 38, SD = 16).\n\n\n\n\n\n\n\nNoteMore complex inline computations\n\n\n\n\n\nFor more complex computations, it is much better to compute the values in a dedicated code chunk. This also allows you to add code annotation which is important to ensure that other researchers (and your future self!) understand the reasoning behind the code.\nFor example, the following annotated code chunk can be used to reproduce the descriptive statistics concerning L1 participants‚Äô professional occupations and foreign language skills.\n\n```{r}\n#| label: L1-jobs\n\n# Counting manual job participants using a tidyverse solution:\nL1.manualjobs &lt;- L1.data |&gt; # Select L1 dataset\n  count(OccupGroup) |&gt; # Tally each level of OccupGroup\n  mutate(proportion = n/sum(n)) |&gt; # Calculate proportion\n  filter(OccupGroup == \"M\") |&gt; # Select only the manual occupations\n  pull(proportion) |&gt; # Select just the proportion value\n  round(2)\n\n# Alternative: Counting manual job participants using a base R solution:\nL1.manualjobs &lt;- round(proportions(table(L1.data$OccupGroup))[\"M\"], digits = 2)\n\n# Counting clerical job participants\nL1.clerical &lt;- round(proportions(table(L1.data$OccupGroup))[\"C\"], digits = 2)\n\n# Counting professional job participants\nL1.pro.num &lt;- L1.data |&gt;\n  filter(OccupGroup %in% c('PS', 'PS ')) |&gt;\n  count()\n\nL1.pro &lt;- round((L1.pro.num/ nrow(L1.data)), digits = 2)\n\n# Counting professionally inactive participants\nL1.inactive &lt;- round(proportions(table(L1.data$OccupGroup))[\"I\"], digits = 2)\n\n# Counting participants who speak at least one language other than English\nL1.otherlgs &lt;- L1.data |&gt;\n  filter(OtherLgs != \"None\") |&gt;\n  count()\n```\n\nThe values saved to the local environment as R objects can then be inserted inline within the Markdown text as follows:\n¬†`{r} numbers_to_words((L1.manualjobs*100), cap = TRUE)` percent of the participants held manual jobs, `{r} L1.clerical*100`% held clerical positions, and `{r} L1.pro*100`% had professional-level jobs or were studying for a degree; the remaining `{r} L1.inactive*100`% were occupationally inactive (i.e. unemployed, retired, or homemakers). In terms of education, participants‚Äô backgrounds ranged from no formal qualifications to Ph.D., with corresponding differences in the number of years spent in full-time education (from `{r} min(L1.data$EduYrs)` to `{r} max(L1.data$EduYrs)`; *M* = `{r} round(mean(L1.data$EduYrs))`, *SD* = `{r} round(sd(L1.data$EduYrs))`). `{r} L1.otherlgs` participants reported a working knowledge of another language; the rest described themselves as monolinguals.\n\n\n\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nüë©üèæ‚Äçüíª Copy the code and text sections corresponding to the description of participants‚Äô professional occupations and education displayed Section 14.6 (in the textbox ‚ÄúMore complex inline computations‚Äù) into your Quarto document and render it to HTML. Compare the values in your rendered document with the original ones from the published study (see below).\n\n‚ÄúTwenty-two percent of the participants held manual jobs, 24% held clerical positions, and 28% had professional-level jobs or were studying for a degree; the remaining 26% were occupationally inactive (i.e.¬†unemployed, retired, or homemakers). In terms of education, participants‚Äô backgrounds ranged from no formal qualifications to Ph.D., with corresponding differences in the number of years spent in full-time education (from 10 to 21; M = 14, SD = 2). Six participants reported a working knowledge of another language; the rest described themselves as monolinguals‚Äù (DƒÖbrowska 2019: 6).\n\nQ14.13 Compare the rendered version of your document with the original descriptive statistics reported in DƒÖbrowska (2019: 6). Could you successfully reproduce these descriptive statistics? Which values are different?\n\n\n\n\n\nThe number of non-monolingual L1 participants.\n\n\n\n\nNone of them.\n\n\n\n\nThe values expressed in percentages.\n\n\n\n\nThe standard deviations.",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Rep`R`oducible research and academic w`R`iting in Quarto</span>"
    ]
  },
  {
    "objectID": "14_LiterateProgramming.html#sec-QuartoTables",
    "href": "14_LiterateProgramming.html#sec-QuartoTables",
    "title": "14¬† RepRoducible research and academic wRiting in Quarto",
    "section": "14.7 Tables",
    "text": "14.7 Tables\nThe easiest way to manually construct a table in a Quarto document in RStudio is to switch to Visual mode and click on Insert¬†&gt;¬†Table. You can choose how many rows and columns you need and then fill in your table in the Visual editor.\n\nTerminology used in this chapter\n\n\n\nSame data\nDifferent data\n\n\n\n\nSame analysis method\nReproducible\nReplicable\n\n\nDifferent analysis method\nRobust\nGeneralisable\n\n\n\n¬†\nWhen you switch to the Source mode, you will see that, in Markdown (see Section 14.4), your table has been converted to a pipe table. Pipe tables allow for column alignment and captions.\n|                               | Same data    | Different data |\n|-------------------------------|--------------|----------------|\n| **Same analysis method**      | Reproducible | Replicable     |\n| **Different analysis method** | Robust       | Generalisable  |\n\n: Terminology used in this chapter\nMost of the time, however, you will want to display tabular results based on data that you have imported, wrangled, and/or analysed in R. If the output of a code chunk within your Quarto document is a table, it will be displayed in your rendered document by default (unless you specify a chunk option to hide its output, see Section 14.5).\n\nL1.data |&gt; \n  count(OtherLgs,\n        sort = TRUE)\n\n  OtherLgs  n\n1     None 84\n2   German  3\n3   French  2\n4  Spanish  1\n\n\nHowever, this output is not particularly nicely formatted. There are several R packages designed to create tables that are ‚Äúpresentation-ready‚Äù. One of these is the {gt} package. Beyond its main function gt(), it offers many more functions to further style tables such as cols_label() to change the column headers. You will need to install this package before you can use it (see Section 14.10).\n\n```{r}\n#| label: tbl-L1-languages\n#| tbl-cap: \"Example of a {gt} table\"\n#| tbl-cap-location: top\n#| tbl-colwidths: [80,20]\n\n#install.packages(\"gt\")\nlibrary(gt)\n\nL1.data |&gt; \n  count(OtherLgs, \n        sort = TRUE) |&gt; \n  gt() |&gt; \n  cols_label(\n    OtherLgs = \"Additional language\", \n    n = \"N\")\n```\n\n\n\nTable¬†14.2: Example of a {gt} table\n\n\n\n\n\n\n\n\n\nAdditional language\nN\n\n\n\n\nNone\n84\n\n\nGerman\n3\n\n\nFrench\n2\n\n\nSpanish\n1\n\n\n\n\n\n\n\n\n\n\nIn addition, Quarto also has a range of chunk options to customise the display of tables, including tbl-cap for the addition of a table caption and tbl-cap-location to determine where the caption is placed. Note that, in the above chunk, the table‚Äôs label chunk option begins with tbl-. This allows for in-text cross-referencing to the table with the insertion of @tbl-L1-languages within the text of the Quarto document, which will automatically be rendered as the following linked and numbered cross-reference: Table¬†14.2.\n\n\n\n\n\n\nNoteGoing further\n\n\n\nThe Quarto guide provides further information about formatting tables: https://quarto.org/docs/authoring/tables.html.",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Rep`R`oducible research and academic w`R`iting in Quarto</span>"
    ]
  },
  {
    "objectID": "14_LiterateProgramming.html#sec-QuartoFigures",
    "href": "14_LiterateProgramming.html#sec-QuartoFigures",
    "title": "14¬† RepRoducible research and academic wRiting in Quarto",
    "section": "14.8 Figures",
    "text": "14.8 Figures\nIn Quarto documents, figures can either be inserted from image files (e.g.¬†.png or .jpeg files, see Section 2.3) or from the output of a code chunk (e.g.¬†a plot, see Chapter 10).\n\n14.8.1 Images\nTo embed an image from an external file, you can use the ‚ÄúInsert‚Äù menu in RStudio‚Äôs Visual editor and select ‚ÄúFigure / Image‚Äù (see Figure¬†14.11). This will open up a menu where you can select the image that you want to insert, as well as add alt-text (see Section 10.1.3) and a caption. The easiest way to adjust the size of an embedded image is to click on the image and then adjust the size of the image with the blue circle in the bottom-right corner of the image (see Figure¬†14.11).\n\n\n\n\n\n\nFigure¬†14.11: Adjusting the size of an image in Quarto\n\n\n\nBelow is the source code for Figure¬†14.12 in Markdown. The code includes the relative path to the image file (see Section 3.3) relative to the project directory (see Section 6.3). In the example below, the image file BERD_pipeline-real.jpg is located in a subfolder called images. If you want to try this out yourself, you will need to create this subfolder within your own project directory and save Figure¬†14.12 to this subfolder.\n![A more realistic research pipeline [CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/) @seiboldBERDCourseMake2023]](images/BERD_pipeline-real.jpg){#fig-RealisticPipeline fig-alt=\"Cartoon drawing of a complex set of pipes with various entry points for \\\"data\\\" and a single output: a research paper with text, a table, and a plot. Sections of the pipe are coloured according to the processes that they correspond to. These include data cleaning, overview, figures, modelling, and text.\" width=\"480\"}\n\n\n\n\n\n\nFigure¬†14.12: A more realistic research pipeline (CC BY 4.0 Seibold & M√ºller 2023)\n\n\n\nThis example embedded image includes a caption (that, itself, includes a link), an alt-text (see Section 10.1.3), and a custom width in pixel. Note that, in the source code, special characters such as quotation marks need to be escaped using a backslash \\. Tags beginning with #fig- can be used to cross-reference images by replacing the # with @. Hence, in this chapter, @fig-RealisticPipeline in the Quarto source code is rendered as Figure¬†14.12.\nFigures can be arranged in many ways. The example below uses the ::: div syntax to display two images side-by-side. This syntax also allows for subcaptions as shown in Figure¬†14.13.\n::: {#fig-Pipelines layout-ncol=\"2\"}\n![An idealised research pipeline](images/BERD_pipeline-simple.jpg){#fig-IdealisedPipeline}\n\n![A more realistic research pipeline](images/BERD_pipeline-real.jpg){#fig-RealisticPipeline2}\n\nResearch workflows as pipelines [[CC BY 4.0](https://creativecommons.org/licenses/by/4.0/) @seiboldBERDCourseMake2023]\n:::\n\n\n\n\n\n\n\n\n\n\n\n(a) An idealised research pipeline\n\n\n\n\n\n\n\n\n\n\n\n(b) A more realistic research pipeline\n\n\n\n\n\n\n\nFigure¬†14.13: Research workflows as pipelines (CC BY 4.0 Seibold & M√ºller 2023)\n\n\n\n\n\n\n\n\n\nNoteGoing further\n\n\n\nTo find out more about inserting and arranging figures, check out the detailed Quarto guide: https://quarto.org/docs/authoring/figures.html.\n\n\n\n\n14.8.2 Plots\nIf your Quarto document includes code chunks that generate plots, they will automatically be integrated in your rendered document. Plots will either appear immediately after the corresponding code chunk or where the code chunk would be, if you chose to hide the code chunk that generated the plot with the echo: false option.\nAs with computed tables (see Section 14.7), various code chunk options can be added to customise the look of computed figures in rendered documents. Compare the code chunk options below and the generated output in Figure¬†14.14.\n\n```{r}\n#| label: fig-scatterplot\n#| fig-cap: \"L2 participants' lexical proficiency in English and their professional occupational group\"\n#| fig-height: 5\n#| fig-asp: 0.618\n#| message: false\n\nL2.data |&gt; \n  ggplot(mapping = aes(x = VocabR, \n                       y = CollocR)) +\n  geom_point(aes(colour = OccupGroup),\n             size = 2) +\n  geom_smooth(method = \"lm\") +\n  scale_colour_viridis_d() +\n  labs(x = \"Vocabulary test scores\",\n       y = \"Collocation test scores\",\n       colour = \"Occupational\\ngroups\") +\n  theme_bw()\n```\n\n\n\n\n\n\n\nFigure¬†14.14: L2 participants‚Äô lexical proficiency in English and their professional occupational group\n\n\n\n\n\nAccording to the authors of ‚ÄúR for Data Science‚Äù, figure sizing and scaling in R is ‚Äúan art and science and getting things right can require an iterative trial-and-error approach‚Äù (Wickham, √áetinkaya-Rundel & Grolemund 2023). This is because there are five main options that control figure sizing: fig-width, fig-height, fig-asp, out-width and out-height. The first three control the size of the figure created by R, whereas the latter two control the size at which it is inserted in the rendered document.\nIf you are sharing your research analyses and results in HTML format, you can also embed interactive plots (see Section 10.2.8) in your Quarto documents. In HTML format, it is therefore possible to hover over Figure¬†14.15 to explore the data interactively.\n\n\nShow R code to generate the interactive plot below.\n#install.packages(\"plotly\")\nlibrary(plotly)\n\nL2.scatter2 &lt;- L2.data |&gt; \n  ggplot(mapping = aes(x = VocabR, \n                       y = CollocR,\n                       text = paste(\"L1:\", NativeLg, \"&lt;/br&gt;Age:\", Age, \"&lt;/br&gt;Years in formal education:\", EduTotal, \"&lt;/br&gt;Job:\", Occupation))) +      \n  geom_point(aes(colour = OccupGroup),\n             size = 2) +\n  scale_colour_viridis_d() +\n  labs(x = \"Vocabulary test scores\",\n       y = \"Grammar test scores\",\n       colour = \"Occupational\\ngroups\") +\n  theme_bw()\n\nggplotly(L2.scatter2)\n\n\n\n\n\n\n\n\nFigure¬†14.15: An interactive plot of L2 participants‚Äô lexical proficiency in English",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Rep`R`oducible research and academic w`R`iting in Quarto</span>"
    ]
  },
  {
    "objectID": "14_LiterateProgramming.html#sec-Citations",
    "href": "14_LiterateProgramming.html#sec-Citations",
    "title": "14¬† RepRoducible research and academic wRiting in Quarto",
    "section": "14.9 References",
    "text": "14.9 References\nAn important aspect of academic writing is the inclusion of in-text bibliographic references (citations) and a well-formatted list of references (also referred to as a bibliography). RStudio‚Äôs Visual editor makes inserting bibliographic references very convenient. To insert a reference, click on ‚ÄúInsert‚Äù and then select ‚ÄúCitation‚Äù or use the keyboard shortcut Cmd/Ctrl¬†+¬†Shift¬†+¬†F8. This opens up a menu (see Figure¬†14.16) giving you the option to search for the source that you‚Äôd like to cite on your own computer (e.g.¬†in your own Zotero database, if you use Zotero), via the Crossref database, or directly using a DOI.\n\n\n\n\n\n\nFigure¬†14.16: Search menu for bibliographic reference\n\n\n\nAlternatively, if you start typing @ in the Visual editor, a quick reference menu will appear. Either way, any references that you add will be displayed as @ followed by a reference identifier. For example, in the source code of this Quarto document, every reference to DƒÖbrowska (2019) is indicated as @DabrowskaExperienceAptitudeIndividual2019.\n\n\n\n\n\n\nNote\n\n\n\nFor more information on how to format your in-text citations, see the Quarto guide.\n\n\nWhen you insert your first reference in a Quarto document, RStudio will automatically create a references.bib file in your project folder. All references are automatically added to this new BibLaTeX file. As shown below, .bib files contain entries that begin with @ followed by the type of reference (article, book, manual, url, etc.) and the reference identifier (e.g.¬†DabrowskaExperienceAptitudeIndividual2019, wickhamDataScienceImport2023). The rest of the entries contains structured information about each reference including its title, date of publication, and DOI or ISBN.\n\n\nreferences.bib\n\n@article{\n  DabrowskaExperienceAptitudeIndividual2019,\n  title={Experience, Aptitude, and Individual Differences in Linguistic Attainment: A Comparison of Native and Nonnative Speakers},\n  volume={69},\n  ISSN={1467-9922},\n  url={https://onlinelibrary.wiley.com/doi/abs/10.1111/lang.12323},\n  DOI={10.1111/lang.12323},\n  number={S1},\n  journal={Language Learning},\n  author={DƒÖbrowska, Ewa},\n  year={2019},\n  pages={72‚Äì100}\n}\n\n@book{\n  wickhamDataScienceImport2023,\n  place={Beijing, Boston, Farnham, Sebastopol, Tokyo},\n  edition={2},\n  title={R for Data Science: Import, tidy, transform, visualize, and model data},\n  ISBN={978-1-4920-9740-2},\n  url={https://r4ds.hadley.nz/},\n  publisher={O‚ÄôReilly},\n  author={Wickham, Hadley and √áetinkaya-Rundel, Mine and Grolemund, Garrett},\n  year={2023} \n}\n\nIn order to connect this bibliography.bib file with our Quarto document, we need to add a bibliography key to our YAML header (see Section 14.3). Provided that our references.bib file is located in the same folder as our Quarto document (which is what RStudio does by default), we can simply add the following line to our document header:\n\n--- \ntitle: Learning Quarto\nsubtitle: \"by reproducing the descriptive statistics of DƒÖbrowska's (2019) study\"\nauthor: Elen Le Foll\ndate: last-modified\nbibliography: references.bib\n---\n\nWith this modified YAML header, when the document is rendered, a bibliography will automatically be added to the end of the document. This means that, if you have citations in your document, it is a good idea to include a header section # References at the end of the document.\n\nReferences\nDƒÖbrowska, Ewa. 2019. ‚ÄúExperience, Aptitude, and Individual Differences in Linguistic Attainment: A Comparison of Native and Nonnative Speakers.‚Äù Language Learning 69 (S1): 72-100. https://doi.org/10.1111/lang.12323.\nWickham, Hadley, Mine √áetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 2nd ed.¬†O‚ÄôReilly. https://r4ds.hadley.nz/.\n\nBy default, Quarto will use the Chicago Manual of Style author-date citation format (as above). However, we can point to a different citation stylesheet in the form of a .csl (Citation Style Language) file in the YAML header. This allows us to determine exactly how our bibliography and in-text citations should be formatted. Many institutions, publishers, and journals have their own (sometimes annoyingly specific!) requirements. Luckily, the open-source research community has put together a large repository of citation stylesheets for you to choose from: https://www.zotero.org/styles. You can download any of these stylesheets (as a .csl file), place the file in your project folder, and then link it to your Quarto document by adding a cls key to your header.\n\n---\ntitle: Learning Quarto\nsubtitle: \"by reproducing the descriptive statistics of DƒÖbrowska's (2019) study\"\nauthor: Elen Le Foll\ndate: last-modified\nbibliography: references.bib\ncsl: international-journal-of-learner-corpus-research.csl\n---\n\nFor example, if you wanted to submit your paper to the International Journal of Learner Corpus Research, you can download the corresponding CLS stylesheet from the Zotero styles database, save it in your project folder, and link to it in your YAML header as above. When rendered, your document‚Äôs bibliography will then read:\n\nReferences\nDƒÖbrowska, E. (2019). Experience, Aptitude, and Individual Differences in Linguistic Attainment: A Comparison of Native and Nonnative Speakers. Language Learning, 69(S1), 72-100. https://doi.org/10.1111/lang.12323.\nWickham, H., √áetinkaya-Rundel, M., & Grolemund, G. (2023). R for data science: Import, tidy, transform, visualize, and model data (2nd ed.). O‚ÄôReilly. Retrieved from https://r4ds.hadley.nz/.\n\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nUsing any of the methods described above, add an in-text bibliographic reference to the following article in your Quarto document:\n\nIn‚Äônami, Yo, Atsushi Mizumoto, Luke Plonsky & Rie Koizumi. 2022. Promoting computationally reproducible research in applied linguistics: Recommended practices and considerations. Research Methods in Applied Linguistics 1(3). 100030. https://doi.org/10.1016/j.rmal.2022.100030.\n\nSpecifically, we want to cite this passage from page 8:\n\nAs implementing these steps may seem daunting, we recommend that researchers engage in reproducible research incrementally. That may be one small step for a researcher, but it will represent a giant leap for the field of applied linguistics when consolidated and accumulated in the long run.\n\nQ14.14 If the key to this article in the .bib file is innami2022, which in-text citation can be used to cite this specific page within a Quarto document?\n\n\n\n\n\n[@innami2022], p. 8]\n\n\n\n\n([@innami2022], p. 8)\n\n\n\n\n[@innami2022, p. 8]\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\nGo to the Zotero style repository and download the .csl citation stylesheet to format references according to the American Psychological Association (APA) 7th edition. Link this stylesheet to your Quarto document and render to HTML.\nQ14.15 Now that your document includes references formatted in APA7, how are the authors‚Äô names listed in your bibliography?\n\n\n\n\n\nIn‚Äônami, Yo, Atsushi Mizumoto, Luke Plonsky & Rie Koizumi\n\n\n\n\nIn‚Äônami, Y., Mizumoto, A., Plonsky, L., and Koizumi, R.\n\n\n\n\nIn‚Äônami et al.\n\n\n\n\nIn‚Äônami, Y., Mizumoto, A., Plonsky, L., & Koizumi, R.\n\n\n\n\nIn‚Äônami, Yo, Atsushi Mizumoto, Luke Plonsky and Rie Koizumi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteLiterature management\n\n\n\nManaging the large number of references that we need to consult, read, and cite when doing research can be a real challenge. The good news is that reference management software are there to help you overcome this challenge! Whether you are working on a term paper, a Master‚Äôs dissertation, PhD thesis, or post-doctoral project, it is always worth investing the time to learn to use a reference manager!\nZotero is a free and open-source bibliographic reference manager that will help you organise all your sources and generate beautifully formatted bibliographies for all your projects. It offers various browser extensions that enable you to quickly add references to your library directly from your web browser.\n\nWhat‚Äôs more, Zotero can be integrated in RStudio, making it very easy to include BibTeX-formatted references in your Quarto documents. Find out more in the RStudio documentation.",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Rep`R`oducible research and academic w`R`iting in Quarto</span>"
    ]
  },
  {
    "objectID": "14_LiterateProgramming.html#sec-Packages",
    "href": "14_LiterateProgramming.html#sec-Packages",
    "title": "14¬† RepRoducible research and academic wRiting in Quarto",
    "section": "14.10 Computing environment",
    "text": "14.10 Computing environment\nIn addition to referencing academic papers, it is also very important that we reference which R version we used for our analyses and which packages and package versions. This serves two purposes:\n\nIndependent researchers (and our future selves!) know exactly what they need to be able to reproduce our analyses (see Section 14.2).\nWe give credit to the kind people who spent time and effort developing and sharing the R packages that we used for our analyses (see Section 1.2).\n\nThe easiest way to ‚Äúgive credit where credit is due‚Äù to R package developers is to use the {grateful} package. Its cite_packages() function will scan your project for all the R packages that are used and generate a BibTeX file called grateful-refs.bib that contains the package references. Having first installed the package, load the {grateful} library:\n\n#install.packages(\"grateful\")\nlibrary(grateful)\n\n\n\n\nMake sure that all the packages that your script relies on are loaded and then run the following command once to generate a bibliography of all loaded packages:\n\ncite_packages(out.dir = getwd(),\n              omit = NULL)\n\nThe .bib text generated by the {grateful} library should now be in your Quarto project directory. Next, add a reference to this BibTeX file in your YAML header. This means that your Quarto document will now have be linked to two bibliography files, which is fine as long as you use the following YAML syntax to reference them both (watch the indentation!):\n\n---\nbibliography: \n  - references.bib\n  - grateful-refs.bib\n---\n\nWe can now call the cite_packages(output = \"paragraph\") function to generate a paragraph that mentions all the packages used in the document and add their references to the bibliography (either at the bottom of your rendered Quarto document or in a specific References section as in this textbook).\n\ncite_packages(output = \"paragraph\", \n              out.dir = getwd(), \n              pkgs = \"Session\",\n              omit = NULL)\n\nWe used R v. 4.5.2 (R Core Team 2025) and the following R packages: checkdown v. 0.0.13 (Moroz 2020), grateful v. 0.3.0 (Rodriguez-Sanchez & Jackson 2025), gt v. 1.2.0 (Iannone et al. 2025), here v. 1.0.2 (M√ºller 2025), plotly v. 4.11.0 (Sievert 2020), tidyverse v. 2.0.0 (Wickham et al. 2019), webexercises v. 1.1.0 (Barr & DeBruine 2023), xfun v. 0.55 (Xie 2025).\n\n\nAlternatively, cite_packages() can generate a table with all the package names, versions, and references. Table¬†14.3 lists the packages used in this chapter. To display functioning links and references, the table is rendered using the kable() function from the {knitr} package.\n\n#install.packages(\"knitr\")\npkgs &lt;- cite_packages(output = \"table\", \n                      out.dir = getwd(), \n                      omit = NULL)\nknitr::kable(pkgs)\n\n\n\n\n\nTable¬†14.3\n\n\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\n\nbase\n4.5.2\nR Core Team (2025)\n\n\ncheckdown\n0.0.13\nMoroz (2020)\n\n\ngrateful\n0.3.0\nRodriguez-Sanchez & Jackson (2025)\n\n\ngt\n1.2.0\nIannone et al. (2025)\n\n\nhere\n1.0.2\nM√ºller (2025)\n\n\nplotly\n4.11.0\nSievert (2020)\n\n\ntidyverse\n2.0.0\nWickham et al. (2019)\n\n\nwebexercises\n1.1.0\nBarr & DeBruine (2023)\n\n\nxfun\n0.55\nXie (2025)\n\n\n\n\n\n\n\n\nTracking the versions of the packages that your code relies on is important if you want your analysis code to be reproducible in the long-run (i.e.¬†so that you or a colleague can run it next month or next year). However, manually installing these packages with these exact versions is hardly feasible. To simplify the process of re-creating the same project environment, consider using {renv} or {rix}.\n\n\n\nThe {renv} library (Ushey & Wickham 2023) keeps track of the package versions that your project depends on, and ensures that those exact versions are installed whenever and wherever your project is opened. {renv} provides each project with its own isolated package library, ensuring that you can update packages in new projects without risking breaking older projects.\nTo create project-specific environments that additionally include system dependencies, you will need to check out the {rix} package (Rodrigues & Baumann 2026). Both of these packages aim to make R projects more isolated, portable and therefore reproducible.\n\n\n\n\n\n\n\n\n\nNoteGoing further üìö\n\n\n\nAccessible introductions to stabilising your computing environment can be found in the BERD course ‚ÄúMake Your Research Reproducible‚Äù (Seibold & M√ºller 2023) and The Turing Way‚Äôs guide to reproducible environments (The Turing Way Community 2022).",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Rep`R`oducible research and academic w`R`iting in Quarto</span>"
    ]
  },
  {
    "objectID": "14_LiterateProgramming.html#sec-EmbedResources",
    "href": "14_LiterateProgramming.html#sec-EmbedResources",
    "title": "14¬† RepRoducible research and academic wRiting in Quarto",
    "section": "14.11 Sharing HTML documents",
    "text": "14.11 Sharing HTML documents\nYou may have noticed that, in addition to creating an .html file, rendering your Quarto document has also generated a folder containing any necessary data, images, stylesheets or other files required to display the HTML version of your document. This is because, by default, Quarto keeps external resources separate from the main HTML file. While this is advantageous for large documents and complex projects, it does mean that your HTML document can only be viewed if both the .html file and its associated folder are shared.\nIf you want to share a single, self-contained .html file with someone else, you will need to embed all the necessary files directly inside your HTML file. This is achieved by adding the following option at the end of your document‚Äôs YAML header:\n\n---\nformat:\n  html:\n    embed-resources: true\n---\n\nWith this setting, Quarto will package all the necessary resources inside the HTML file, resulting in a self-contained document that is easy to share as it can be viewed in any web browser (e.g.¬†Firefox, Google Chrome, Safari).\nIf you intend to share a longer Quarto document, it may be a good idea to number the headings and sub-headings (number-sections) and to include a table of content (toc). You can do this by adding the following two lines to the format section of your YAML header:\n\n---\nformat:\n  html:\n    embed-resources: true\n    number-sections: true\n    toc: true\n---",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Rep`R`oducible research and academic w`R`iting in Quarto</span>"
    ]
  },
  {
    "objectID": "14_LiterateProgramming.html#sec-PublishingFormats",
    "href": "14_LiterateProgramming.html#sec-PublishingFormats",
    "title": "14¬† RepRoducible research and academic wRiting in Quarto",
    "section": "14.12 Other publishing formats",
    "text": "14.12 Other publishing formats\nSo far, we have only tried rendering our Quarto document to HTML, which is the default publishing format for Quarto documents. HTML has many advantages and is great for publishing online, but the beauty of Quarto is that you can share and publish your research in many other formats, too.\n\n\n\n\n\n\nFigure¬†14.17: Artwork (CC BY 4.0 Allison Horst from the ‚ÄúHello, Quarto‚Äù keynote by Julia Lowndes and Mine √áetinkaya-Rundel, presented at RStudio Conference 2022.\n\n\n\n\n14.12.1 Word, LibreOffice & co.\nYour supervisor or colleague may request a Microsoft Word version of your Quarto document and, thankfully, this is no problem. You can change the rendering format to a .docx file by amending the format option in your YAML header:\n\n---\nformat: docx\n---\n\nWith this format option, rendering your Quarto document will generate a .docx file that includes your text, any code that you wanted to show in your document, and all of the code outputs that you wanted to share, such as your statistics, graphs, and tables.\nSome of the formatting options available for HTML also work in the .docx format:\n\n---\nformat:\n  docx:\n    embed-resources: true\n    number-sections: true\n    toc: true\n---\n\nNote, however, that any options that are not available in the rendering format specified are ignored without warning or error messages.\n\n\n\n\n\n\nWarningNot rendering code chunks in specific formats\n\n\n\nDynamic code outputs, such as the interactive {plotly} graph displayed in Figure¬†14.15, cannot be meaningfully rendered to static formats, such as Microsoft Word or PDF. Attempting to do so can cause rendering errors such as:\nError: Functions that produce HTML output found in document targeting docx output.\nPlease change the output type of this document to HTML.\nTo fix this, add the following options to any code chunk that generates content that only works in HTML:\n\n```{r}\n#| eval: !expr 'knitr::is_html_output()'\nggplotly(L2.scatter2)\n```\n\n\n\n\n\nThese options ensure that the code chunk is ignored when the document is rendered to any format other than HTML.\n\n\nWhen you open the .docx version of your Quarto document in Microsoft Word, you may get a number of warnings (e.g. Figure¬†14.18). You can safely click ‚ÄúYes‚Äù or ‚ÄúClose‚Äù to get rid of these warnings and open up your Word file. If, for some reason, you cannot open a rendered document in Microsoft Word, try rendering to .odt instead (see below).\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure¬†14.18: Examples of popup menus that may appear when opening the .docx version of a Quarto document.\n\n\n\nTo share your work with LibreOffice, OnlyOffice, and OpenOffice users, use the .odt rendering option. This will generate an OpenDocument ‚Äî an open standard file format that can be opened in any text-processing software, including Microsoft Word.\n\n---\nformat: odt\n---\n\nBy default, the quality of the images and graphs in rendered .docx and .odt files is low. This is to keep the file size reasonable. High-quality images can be rendered by specifying the image definition in the YAML option. To do so, replace the format line that you added above with the following lines. Make sure that you indent each line correctly as shown below; otherwise, you will get an error when you try to render your document.\n\n---\nformat: \n  odt:\n    fig-dpi: 300\n---\n\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nQ14.15 If you completed Q8.15 (Section 8.3.1), you copied-and-pasted a paragraph with gaps into LibreOffice Writer or Microsoft Word and then manually inserted descriptive statistics that you had calculated in R. This method of copying-and-pasting across different programmes is very error-prone: What if you accidentally pasted the wrong number in the wrong place? And what if there was an update to the dataset or you made some changes to the data cleaning procedure? You‚Äôd have to manually change all the numbers again! :exploding-head: This is time-consuming and, more worryingly, very likely to result in errors!\nNow that you know about literate programming in Quarto, rewrite the following paragraph describing the GrammarR variable in L1.data and L2.data in Quarto. Use in-text code chunks to fill the gaps and then render your paragraph to .docx or .odt format to check the results.\n\nOn average, English native speakers performed only marginally better in the English grammatical comprehension test (median = ______) than English L2 learners (median = ______). However, L1 participants‚Äô grammatical comprehension test results ranged from ______to ______, whereas L2 participants‚Äô results ranged from ______to ______.\n\n¬†\n\n\n\n\n\n\n\n\n\nNoteClick here for the solution to Q14.15\n\n\n\n\n\nBelow is a screenshot of a Quarto document with the inline code chunks and its rendered .odt version as opened in LibreOffice Writer. You can click on the images to zoom in.\n\n\n\n\n\n\n\n\n\nFigure¬†14.19: Quarto source code\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†14.20: Rendered document as opened in LibreOffice Writer\n\n\n\n\n\n\n\n\n\n\n\n14.12.2 PDF\nIt is also possible to render Quarto documents to PDF; however, this requires you to have LaTeX installed on your computer. Alternatively, you can use Typst ‚Äî a new open-source markup-based typesetting system designed to be as powerful as LaTeX but easier to use.\nIf you don‚Äôt already have your favourite LaTex distribution, Quarto developers recommend that you use the TinyTeX distribution to render .qmd files to PDF. To install (or update) TinyTeX, go to the Terminal pane in RStudio and run the following command:\n\n\nTerminal\n\nquarto install tinytex\n\nThis is likely to take a few minutes but you will only need to do it once. Afterwards, you can add the following line to your Quarto YAML header and you‚Äôre ready to render to PDF! If you run into any issues installing {tinytex}, consult the tinytex FAQ page.\n\n---\nformat: pdf\n---\n\nHTML being the default format, some options available for HTML are not ‚Äì at least by default ‚Äì available in other publishing formats. Many of the basic options, however, work across different formats. The YAML header options below can be used to include a table of content with numbered sections at the start of the PDF version of your document. It also includes two options that are specific to the PDF format and which are particularly useful for academic writing: the first will print a list of figures (lof) and the second a list of tables (lot).\n\n---\nformat:  \n  pdf:\n    number-sections: true\n    toc: true\n    lof: true\n    lot: true\n---    \n\n\n\n14.12.3 Slides\nIn research, it‚Äôs quite common that you will be working on a project that will be submitted as a paper or thesis (e.g.¬†in PDF format) and that you‚Äôll also want to present in class, to your research group, or at a conference. Conveniently, we can turn any Quarto document into presentation slides. At the time of writing, there are three presentation formats to choose from:\n\n\n\nRevealjs\nAn open-source HTML presentation framework.\nformat: revealjs\n\n\nPower-Point\nMicrosoft Office‚Äôs presentation editing software.\nformat: pptx\n\n\nBeamer\nA LaTeX class for producing presentations and slides in PDF format.\nformat: beamer\n\n\n\nI recommend using Revealjs. The best way to get a sense of what is possible is to explore the demo presentation from the Quarto Guide.\n\n\n\nTo view the demo in a standalone browser tab, head to the Quarto Guide. You can also out the source code to see how the slides were created.",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Rep`R`oducible research and academic w`R`iting in Quarto</span>"
    ]
  },
  {
    "objectID": "14_LiterateProgramming.html#conclusion",
    "href": "14_LiterateProgramming.html#conclusion",
    "title": "14¬† RepRoducible research and academic wRiting in Quarto",
    "section": "14.13 Conclusion",
    "text": "14.13 Conclusion\nThis chapter only just scratched the surface of what‚Äôs possible in Quarto. The Quarto documentation is very detailed and well worth exploring to find out what else you can do in Quarto: https://quarto.org/docs/guide/. From books to blogs and interactive dashboards, the world‚Äôs your oyster! üöÄ\n\n\n\n\n\n\nNoteGoing further with Quarto\n\n\n\n‚û°Ô∏è For those of you who want to dive a little deeper, I heartily recommend the final chapter of ‚ÄúAn Introduction to Quantitative Text Analysis for Linguistics: Reproducible Research Using R‚Äù by Jerid Francom: https://qtalr.com/book/part_5/11_contribute.html.\n‚û°Ô∏è Quarto has many functionalities that are particularly attractive to those of us involved in higher education teaching and academic research. Watch Quarto for Academics (20 minutes) by Mine √áetinkaya-Rundel to find out more.\n‚û°Ô∏è Thinking of writing an entire M.A./PhD thesis or book in Quarto? Cameron Patrick wrote his PhD thesis in Quarto and has helpfully put together some great tips so that his ‚Äúpain and suffering can help reduce yours‚Äù. Well worth reading if you‚Äôre thinking of rendering a complex (set of) Quarto document(s) to PDF: https://cameronpatrick.com/post/2023/07/quarto-thesis-formatting/.\n‚û°Ô∏è Last but not least, the latest edition of ‚ÄúR for Data Science‚Äù also has a great chapter on communicating the results of data science projects using Quarto: https://r4ds.hadley.nz/communicate.\n\n\n\nCheck your progress üåü\nWell done! You have successfully completed this chapter on literate programming using Quarto. You have answered 0 out of 16 questions correctly.\nAre you confident that you can‚Ä¶?\n\nExplain the concepts of literate programming and reproducible research to a friend or colleague (Section 14.1)\nWrite and format text (bold, first-level heading, italics, etc.) in a Quarto document (Section 14.4)\nInsert a code chunk in a Quarto document and use inline codes (Section 14.5)\nInsert code chunks in a Quarto document and make use of inline code (Section 14.6)\nInsert tables in a Quarto document (Section 14.7)\nEmbed images and plots in a Quarto document (Section 14.8)\nRender (i.e.¬†export) your .qmd document to HTML, Microsoft Word, and PDF (Section 14.12)\n\n\n\n\n\nBarr, Dale & Lisa DeBruine. 2023. webexercises: Create interactive web exercises in ‚ÄúR Markdown‚Äù (formerly ‚Äúwebex‚Äù). https://doi.org/10.32614/CRAN.package.webexercises.\n\n\nBochynska, Agata, Liam Keeble, Caitlin Halfacre, Joseph V. Casillas, Irys-Am√©lie Champagne, Kaidi Chen, Melanie R√∂thlisberger, Erin M. Buchanan & Timo B. Roettger. 2023. Reproducible research practices and transparency across linguistics. Glossa Psycholinguistics 2(1). https://doi.org/10.5070/G6011239.\n\n\nDƒÖbrowska, Ewa. 2019. Experience, aptitude, and individual differences in linguistic attainment: A comparison of native and nonnative speakers. Language Learning 69(S1). 72‚Äì100. https://doi.org/10.1111/lang.12323.\n\n\nIannone, Richard, Joe Cheng, Barret Schloerke, Shannon Haughton, Ellis Hughes, Alexandra Lauer, Romain Fran√ßois, JooYoung Seo, Ken Brevoort & Olivier Roy. 2025. gt: Easily create presentation-ready display tables. https://doi.org/10.32614/CRAN.package.gt.\n\n\nMoroz, George. 2020. Create check-fields and check-boxes with checkdown. https://CRAN.R-project.org/package=checkdown.\n\n\nM√ºller, Kirill. 2025. here: A simpler way to find your files. https://doi.org/10.32614/CRAN.package.here.\n\n\nParsons, Sam, Fl√°vio Azevedo, Mahmoud M. Elsherif, Samuel Guay, Owen N. Shahim, Gisela H. Govaart, Emma Norris, et al. 2022. A community-sourced glossary of open scholarship terms. Nature Human Behaviour. Nature 6(3). 312‚Äì318. https://doi.org/10.1038/s41562-021-01269-4.\n\n\nR Core Team. 2025. R: A language and environment for statistical computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nRodrigues, Bruno & Philipp Baumann. 2026. Rix: Reproducible data science environments with ‚Äúnix‚Äù. https://docs.ropensci.org/rix/.\n\n\nRodriguez-Sanchez, Francisco & Connor P. Jackson. 2025. grateful: Facilitate citation of R packages. https://pakillo.github.io/grateful/.\n\n\nSeibold, Heidi & Rabea M√ºller. 2023. BERD course: Make your research reproducible. https://doi.org/10.17605/OSF.IO/RUPT7.\n\n\nSievert, Carson. 2020. Interactive web-based data visualization with r, plotly, and shiny. Chapman; Hall/CRC. https://plotly-r.com.\n\n\nThe Turing Way Community. 2022. The Turing Way: A handbook for reproducible, ethical and collaborative research (1.0.2). Zenodo. https://doi.org/10.5281/zenodo.3233853.\n\n\nUshey, Kevin & Hadley Wickham. 2023. Renv: Project environments. https://CRAN.R-project.org/package=renv.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D‚ÄôAgostino McGowan, Romain Fran√ßois, Garrett Grolemund, et al. 2019. Welcome to the tidyverse. Journal of Open Source Software 4(43). 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Mine √áetinkaya-Rundel & Garrett Grolemund. 2023. R for data science: Import, tidy, transform, visualize, and model data. 2nd edition. O‚ÄôReilly. https://r4ds.hadley.nz/.\n\n\nXie, Yihui. 2025. xfun: Supporting functions for packages maintained by ‚ÄúYihui Xie‚Äù. https://doi.org/10.32614/CRAN.package.xfun.",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Rep`R`oducible research and academic w`R`iting in Quarto</span>"
    ]
  },
  {
    "objectID": "14_LiterateProgramming.html#footnotes",
    "href": "14_LiterateProgramming.html#footnotes",
    "title": "14¬† RepRoducible research and academic wRiting in Quarto",
    "section": "",
    "text": "Although there is ground for optimism here as more and more linguists and language education scholars are beginning to make their data open in repositories such as IRIS, TROLLing, and the Open Science Framework (OSF) (see Section 2.4).‚Ü©Ô∏é\nThe above definitions are all from the community-sourced FORRT glossary (Parsons et al. 2022).‚Ü©Ô∏é\nMany other IDEs (Integrated Developer Environments, see Section 4.2.1) support Quarto, including JupyterLab, Neovim, Positron, and VS Code. Feel free to pick the IDE that you are most comfortable with!‚Ü©Ô∏é\nIf you need to install Quarto, go to https://quarto.org/docs/get-started/ and download the latest Quarto version that is compatible with your operating system. Once the download is completed (which may take several minutes), double-click on the installer file that you downloaded and click your way through the installation process.‚Ü©Ô∏é\nNote that, in YAML syntax, character strings that include special characters (e.g.¬†') need to be enclosed in quotation marks.‚Ü©Ô∏é\nYou can change this behaviour in your RStudio preferences under Tools &gt; Global Options &gt; R Markdown by selecting or unselecting the option: ‚ÄúShow output inline for all R Markdown documents‚Äù.‚Ü©Ô∏é\nUsing Microsoft Excel to open these .csv files can corrupt the files and can happen even if you did not use Excel yourself (e.g.¬†on some Windows computers, this is sometimes done automatically as part of the download process). To find out more, see Section 2.6.‚Ü©Ô∏é\nTo make your Quarto document even more reproducible, you can replace your setup chunk with the following function that will automatically check if a package needs to be installed before it is loaded:\n\n```{r}\n#| label: improved-setup\n# List of packages necessary in this Quarto document:\npackages &lt;- c(\"here\", \"tidyverse\", \"xfun\")\n\n# Function to install the packages that are not yet installed:\ninstalled_packages &lt;- packages %in%               rownames(installed.packages()) \nif (any(installed_packages == FALSE)) { install.packages(packages[!installed_packages], repos = \"https://packagemanager.rstudio.com/all/latest\") }\n\n# Function to load the packages without printing any messages:\ninvisible(lapply(packages, library, character.only = TRUE))\n```\n\nTo ensure that the correct package version is installed, consider using {renv} or {rix} for your project (see Section 14.10).‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Rep`R`oducible research and academic w`R`iting in Quarto</span>"
    ]
  },
  {
    "objectID": "15_PartingWords.html",
    "href": "15_PartingWords.html",
    "title": "15¬† What‚Äôs next? AI-assisted reseaRch?",
    "section": "",
    "text": "15.1 On the technology behind ‚ÄúAI‚Äù ü§ñ\nIf you are reading the last chapter of what is by no means a short textbook, you are likely someone who is eager to acquire new skills and knowledge. As such, you are well aware that learning is a process that requires intrinsic motivation, time, and a great deal of effort and patience. Many tech companies, by contrast, are keen for us to believe that this tiring, time-consuming experience of human learning is no longer necessary: So-called ‚Äòartificial intelligence‚Äô (AI) systems are designed to spare us the burden of thinking for ourselves. If the answers to our (research) questions are just a few, effortless prompts away, it is legitimate to ask: Is is still worth devoting so much time and effort to acquire and further develop knowledge and skills in data analysis, programming, and statistics?\nThis concluding chapter explains why we can answer this question with a decisive ‚Äúno‚Äù. To this end, we consider the role of AI in research and learning before recapping what we have learnt so far and where to go from there.\nContrary to popular belief, ‚Äòartificial intelligence‚Äô is not a technology. It is, and always has been, a marketing term used to sell (or get research funding for) a wide, diverse, and shifting array of ideas, research projects, systems, and technologies see e.g.¬†15.1. Despite marketing claims, these systems and technologies are not ‚Äòintelligent‚Äô; they cannot think, understand, or reason (see e.g. Bender & Hanna 2025; Quattrociocchi, Capraro & Perc 2025). This is why I put quotation marks around the terms ‚Äúartificial intelligence‚Äù and ‚ÄúAI‚Äù when they are used to refer to technologies, systems, and products.\nPopular, commercial chat-based ‚ÄúAI‚Äù products such as ChatGPT, Claude, CoPilot, and Gemini are powered by Large Language Models (LLMs). LLMs are statistical models fitted to huge amounts of training data to generate a probable response given a prompt based on statistical patterns found in the training data, additional reinforcement learning from human feedback, and additional profit-driven criteria (see e.g. Bender & Hanna 2025). The underlying principle of next-token prediction is comparable to the autocomplete functions from our phone and Google search bar.\nIt goes without saying that modern LLMs are much more powerful predictive text generators than the models that once offered to help us draft our text messages. This is due to four key factors:\nDespite its name, OpenAI ‚Äî the main company behind ChatGPT ‚Äî does not develop open-source LLMs, nor is the company a not-for-profit initiative. So why have OpenAI and other large tech firms been offering their ‚ÄúAI‚Äù-products for free or at prices well below actual running (let alone development) costs? Two reasons are worth considering. First, because one major bottleneck to improving current LLMs is access to new data. Human-generated data is highly valuable and chatbot users are providing lots of it. ‚ÄúAI‚Äù-companies are harvesting this data to train the next generation of LLMs. Second, because free or cheap access to ‚ÄúAI‚Äù-products encourage us to become reliant on them for all kinds of work-related tasks and personal activities. As they become ubiquitous to our everyday lives, we are naturally inclined to rely on and trust their outputs. It is only a matter of time until subscriptions prices are hiked up and/or promoted contents are (more or less transparently) integrated in model outputs (see e.g. M√ºhlhoff 2025: 91-97).",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>What's next? AI-assisted resea`R`ch?</span>"
    ]
  },
  {
    "objectID": "15_PartingWords.html#on-the-technology-behind-ai",
    "href": "15_PartingWords.html#on-the-technology-behind-ai",
    "title": "15¬† What‚Äôs next? AI-assisted reseaRch?",
    "section": "",
    "text": "Figure¬†15.1: A cartoon set theoretic view of terms commonly used when discussing the superset ‚ÄúAI‚Äù from Guest et al. (2025): Figure¬†1 (CC BY 4.0)\n\n\n\n\n\n\n\n\n\n\nFigure¬†15.2: Autocomplete function on web search query (google.com on 8 February 2026)\n\n\n\n\n\nThe development of a novel algorithmic architecture known as a transformer that is based on attention mechanisms (Vaswani et al. 2017)\nThe availability and largely unethical, if not outright illegal (see e.g. Samuelson 2023; Lucchi 2024) scraping of huge amounts of training data from the internet, including (academic) books and articles, but also vast amounts of blog posts, social media data, forum discussions, Wikipedia articles, and YouTube videos.\nThe availability of (relatively) cheap, large-scale computational power (which, however, still comes at a high environmental cost, see e.g. Luccioni et al.).\nReinforcement learning from human feedback, a process whereby mostly underpaid, crowdsourced human workers from low-income countries (see e.g. Perrigo 2023) provide extensive feedback on LLM outputs to fine-tune models for what humans want to obtain when they query a model (see Bender & Hanna 2025: Ch¬†3).\n\n\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nQ15.1 Which of the following analogies have been used by different scholars to refer to Large Language Models (LLMs)?\n\n\n\n\n\nStochastic parrots\n\n\n\n\nAutomated plagiarism\n\n\n\n\nSpicy autocomplete\n\n\n\n\nAsk-Einstein-anything engine\n\n\n\n\nBullshit\n\n\n\n\nMagic research wand\n\n\n\n\nSynthetic text extruding machines\n\n\n\n\nKitsch\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteUsing LLMs in R ‚ú®\n\n\n\n\n\nIf you‚Äôre interested in trying to run, query, and/or manipulate the outputs of LLMs in R, check out Luis D. Verde Arregoitia‚Äôs curated list of R packages and other useful resources: ‚ÄòLarge Language Model tools for R‚Äô (also available in Spanish). I also highly recommend reading the ‚ÄúRead first‚Äù and ‚ÄúFurther reading‚Äù sections.\n\n\n\n\n\n\nFigure¬†15.3: Selection of hex logos of LLM-related R packages from Arregoitia (2026)",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>What's next? AI-assisted resea`R`ch?</span>"
    ]
  },
  {
    "objectID": "15_PartingWords.html#on-the-value-of-critical-thinking",
    "href": "15_PartingWords.html#on-the-value-of-critical-thinking",
    "title": "15¬† What‚Äôs next? AI-assisted reseaRch?",
    "section": "15.2 On the value of critical thinking",
    "text": "15.2 On the value of critical thinking\nIn educational contexts, the challenges brought about by artificial ‚Äúintelligence‚Äù have been compared to the introduction of pocket calculators in the 1960s. The comparison is an interesting one. Although calculators are now widely available and perform school-level arithmetic to perfection, we still first teach school pupils to do mathematics without a calculator because we know that this is necessary to develop an understanding of what a calculator does. Following this analogy, we should still teach and learn data analysis, statistics, and programming, even if ‚ÄúAI‚Äù were to generate error-free solutions to these tasks. The crux of the problem is how to convince ourselves that it is worth learning to do things the hard way given how convenient and effortless ‚ÄúAI‚Äù-products seem. More on that in Section 15.3 but, for now, let‚Äôs return to our calculator analogy and consider another, crucial issue: the reliability of LLM outputs.\nA calculator is a deterministic system that will always provide the same, correct answer to a mathematical operation. By contrast, an LLM is a stochastic model that is fitted to generate probable outputs. These are generated by highly complex black-box algorithms that are based on (often illegally and/or unethically acquired) training data, reinforcement learning from (exploited) human feedback, and additional company-internal fine-tuning, as well as a degree of randomness. This means that, unlike calculators, their outputs are irreproducible and therefore unreliable.\nA marketing term that tech companies have been pushing to describe this inherent lack of reliability is ‚Äúhallucination‚Äù. However, as LLMs have no way of representing what is true or false, it is misleading to speak of LLMs ‚Äúhallucinating‚Äù. LLMs are more likely to generate outputs that are truthful when they have been trained a lot of reliable data on the subject but, by definition, they cannot evaluate their sources ‚Äî which can range from high-quality, peer-reviewed academic journals to satiric Reddit comments written by cheeky teenagers ‚Äî and, crucially, have no understanding of their contents.\n\nAI isn‚Äôt helping you build something novel. It can‚Äôt. It only knows what‚Äôs been done before. It‚Äôs autocomplete with a superiority complex (Jj 2025: n.p.).\n\nGiven that their outputs are both irreproducible and unreliable, commercial ‚ÄúAI‚Äù tools are unsuitable for most research-related activities. For writing, LLMs not only output texts without any form of fact-checking, they also automate plagiarism as they fail to credit the authors whose texts they were trained on. In this context, it is worth noting that any bibliographic references output by LLMs are also randomly generated text strings. They may or may not correspond to real sources. When the sources exist, they may or may not contain (some of) the information regurgitated by the LLMs. We have no way of knowing from the model output.\nThe same goes for literature reviews: having no access to the training data, this is not a task that we can responsibly delegate to an LLM. To make matters worse, ‚ÄúAI‚Äù-systems are known to perpetuate and exacerbate biases (e.g.¬†the ‚ÄúMatthew effect,‚Äù see Pooley 2025). High-quality research requires us to put in the intellectual effort of searching, reading, and critically evaluating the literature ourselves. Like writing, this is an integral part of the research process.\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nQ15.2 Read Jeff Pooley‚Äôs short paper ‚ÄòThe Matthew Effect in AI Summary‚Äô. What does the Matthew Effect refer to?\n\n\n\n\n\nThe Matthew effect describes how successful researchers tend to be harsher in peer reviewing processes in order to remain successful themselves.\n\n\n\n\nThe Matthew effect refers to the practice of LLM-assisted plagiarism in academic writing.\n\n\n\n\nThe Matthew effect refers to the positive impact of open-access publishing on research dissemination.\n\n\n\n\nThe Matthew effect describes how successful researchers receive more recognition and opportunities, leading to more success.\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\nQ15.3 Which term describes the phenomenon whereby the contributions of marginalised female scientists are overlooked or attributed to their male colleagues?\n\n\n\n\n\nThe Matilda effect\n\n\n\n\nThe Dunning-Kruger effect\n\n\n\n\nThe Harriet effect\n\n\n\n\nThe Margaret Rossiter effect\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\nQ15.4 According to Pooley (2025), which of the following biases in academia are likely to be aggravated by the use of Large Language Models (LLMs) to summarise academic literature and write research articles?\n\n\n\n\n\nRacial biases\n\n\n\n\nLanguage biases, particularly favouring English\n\n\n\n\nGender biases\n\n\n\n\nInequalities favouring central, influential regions over less influential, so-called \"peripheral\" regions\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n\n\n\nWriting code for data (pre)processing, analysis, visualisation, and statistical modelling is also increasingly becoming part and parcel of research processes in the language sciences. Whilst it may be tempting to outsource (part of) the data analysis process to a machine learning algorithm, an LLM or an ‚ÄúAI‚Äù agent, time spent examining our data (e.g.¬†cleaning, wrangling, and visualising them) allows us to gain in-depth knowledge of our data with which we can spot issues in complex statistical analyses further down the road. It is much easier to critically interpret the outputs of complex models if we have a good intuitive sense of what is plausible based on our data. This is not something that we can responsibly delegate to a even the most sophisticated ‚Äòautocomplete machine‚Äô.\n\n\n\nA xkcd comic depicting a pile of algebra as a machine learning algorithm (xkcd CC BY-NC 2.5)",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>What's next? AI-assisted resea`R`ch?</span>"
    ]
  },
  {
    "objectID": "15_PartingWords.html#sec-HumanLearning",
    "href": "15_PartingWords.html#sec-HumanLearning",
    "title": "15¬† What‚Äôs next? AI-assisted reseaRch?",
    "section": "15.3 On the value of human learning",
    "text": "15.3 On the value of human learning\nAll this is not to say that ‚ÄúAI‚Äù-products are never useful. Many people report successfully using ‚ÄúAI‚Äù for designing experiments, writing, and coding ‚Äî including in academia. Here, two factors are worth considering. First, the less we know about a domain, the more we tend to overestimate the quality of LLM outputs in this domain. When it comes to learning how to code and do statistics, political scientist, R package developer, and educator Andrew Heiss1 (2024) goes as far as saying that ‚Äúusing ChatGPT and other LLMs when learning R is actually really detrimental to learning, especially if you just copy/paste directly from what it spits out.‚Äù He goes on to explain that:\n\nUsing ChatGPT with R requires a good baseline knowledge of R to actually be useful. A good analogy for this is with recipes. ChatGPT is really confident at spitting out plausible-looking recipes. A few months ago, for fun, I asked it to give me a cookie recipe. I got back something with flour, eggs, sugar, and all other standard-looking ingredients, but it also said to include 3/4 cup of baking powder. That‚Äôs wild and obviously wrong, but I only knew that because I‚Äôve made cookies before (Heiss 2024: n.p.).\n\nThis begs the question as to how novices can reach the level of expertise necessary to be able to reliably assess LLM outputs in a specific domain. Researchers are often short on time and LLMs are sold to us as a convenient way to take shortcuts. Indeed, ‚ÄúAI‚Äù-products are designed to give us the illusion that we are more efficient and productive when we use and trust them. The risk is that, once we have become dependent, we are no longer able to compare how long we would have needed to complete the same task had we not relied on a third-party ‚ÄúAI‚Äù-system to do so.\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nRead through Google‚Äôs ‚ÄúAI overview‚Äù displayed in Figure¬†15.4 (a). You can click on the images to enlarge them.\n\n\n\n\n\n\n\n\n\n\n\n(a) Google AI overview to the query ‚Äúcheese not sticking to pizza‚Äù\n\n\n\n\n\n\n\n\n\n\n\n(b) Satirical Reddit post from 2013\n\n\n\n\n\n\n\nFigure¬†15.4: Screenshots from r/Pizza illustrating the infamous ‚Äòpizza glue‚Äô AI fail (see also Notopoulos 2024)\n\n\n\nQ15.5 Among a number of sensible suggestions, we find a recommendation for adding non-toxic glue to pizza sauce. This mention is thought to have come from an old Reddit post (see Figure¬†15.4 (b)). Which aspect(s) of the ‚ÄúAI‚Äù overview point to this theory?\n\n\n\n\n\nThe AI overview's mention of non-toxic glue to the sauce is likely a hallucination.\n\n\n\n\nThe AI overview mentions the same type of glue as in the Reddit post.\n\n\n\n\nThe format of the AI overview is similar to that of the Reddit post.\n\n\n\n\nThe AI overview mentions the same quantity of glue as in the Reddit post.\n\n\n\n\nThe AI overview uses the same, in the context of cooking, unusual word as fucksmith: 'tackiness'.\n\n\n\n\nReddit is usually one of the most reliable source of recipes and cooking tips on the Internet.\n\n\n\n\n\n\n\n\n\n\n\nWhen it comes to programming, the use of LLMs may seem less risky than when conducting literature reviews, writing prose, or analysing data. After all, we can test that generated code runs as expected. If it does not (which is often the case although we might not immediately spot this), we can debug the code with the support of one or more LLMs until it does. Whilst this might feel like the path of least resistance, data scientist, R developer, and educator, Mine √áetinkaya-Rundel, explains how debugging someone else‚Äôs code (e.g., what an LLM produced) is considerably harder than debugging our own code (Chow, Wickham & Mckinney 2025). This is because when it‚Äôs our own code, we had an idea, followed a certain example, or a specific strategy and this knowledge helps us debug in a systematic way. Crucially, each problem or error is an opportunity to learn. Relying on LLM-generated code to solve these errors robs us of these opportunities.\n\nAI can‚Äôt learn from its mistakes‚Äîit doesn‚Äôt understand why something failed. It just pattern-matches from training data (Stetskov 2025: n.p.).\n\nResearch on the mid- to long-term impact of ‚ÄúAI‚Äù usage on cognitive abilities such as writing, coding, and critical thinking is still in its infancy; however, a number of studies point towards a very genuine risk of deskilling (see e.g. Ferdman 2025) in many domains of use (e.g.¬†medicine, see Natali et al. 2025). In the context of ‚ÄúAI‚Äù-assisted coding, researchers from Anthropic (the company behind the Claude family of ‚ÄúAI‚Äù-products) conducted a pre-registered (Tamkin & Shen 2026) experiment in which 52 (mostly junior) software developers completed a programming task using a Python library that they were not familiar with and were subsequently tested for their understanding of the code. Tamkin & Shen (2026) compared:\n\nhow quickly the programmers completed the task with and without ‚ÄúAI‚Äù assistance and\nwhether using ‚ÄúAI‚Äù made them less, more, or equally likely to understand the code they had just written.\n\n\n\n\n\n\n\nFigure¬†15.5: Difference in means (and 95% CI error bars) of overall task time and test scores between the treatment (AI Assistant) and control (No AI) groups (n¬†=¬†52) (Source: Tamkin & Shen 2026: Fig.¬†1)\n\n\n\nOn average, the participants assigned to the ‚ÄúAI‚Äù-assistance group completed the programming task about two minutes faster than those who worked without ‚ÄúAI‚Äù, but this difference was not statistically significant (p¬†=¬†0.391). There was, however, a significant difference in their code comprehension test scores: the AI group averaged 50% on the test, compared to 67% in the group that did not have access to ‚ÄúAI‚Äù (Cohen‚Äôs¬†d¬†=¬†0.738, p¬†=¬†0.01), which the authors claim corresponds to a difference of ‚Äútwo grade points‚Äù (Tamkin & Shen 2026: 2). Tamkin & Shen (2026) also report that the largest gap in scores between the two groups was on debugging questions. They attribute the gains in skill development of the group that did not use ‚ÄúAI‚Äù ‚Äúto the process of encountering and subsequently resolving errors independently‚Äù (Tamkin & Shen 2026: 2-3).\n\nSenior developers don‚Äôt grow out of thin air [‚Ä¶¬†They] develop intuition through thousands of small failures (Stetskov 2025: n.p.).\n\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nAs part of an exploratory, not pre-registered, analysis, Tamkin & Shen (2026) from Anthropic (see Section 15.3) decomposed the quiz scores into sub-areas and question types (see Figure¬†15.6). Each question in the quiz belonged to exactly one task (e.g., Task 1 or Task 2) and exactly one question type (e.g., Conceptual, Debugging, or Code Reading). Figure¬†15.6 shows that, for both tasks, the control (no AI) group performed better than the AI group.\n\n\n\n\n\n\nFigure¬†15.6: Score breakdown by questions type relating to each task and skill area (Source: Tamkin & Shen 2026: Fig.¬†8)\n\n\n\nQ15.6 Looking at the results displayed in Figure¬†15.6, which question type shows the largest difference in average quiz scores between the treatment and control groups?\n\n\n\n\n\nConceptual\n\n\n\n\nDebugging\n\n\n\n\nCode Reading\n\n\n\n\n\n\n\n\n¬†\nQ15.7 Why might the control group have, on average, performed better on debugging questions compared to the AI group?\n\n\n\n\n\nThe AI tool was specifically designed to help with debugging, giving the treatment group a clear advantage.\n\n\n\n\nThe control group had no AI assistance, so they encountered more errors during the task and gained more experience debugging.\n\n\n\n\nThe treatment group had more practice with debugging during the experimental task.\n\n\n\n\nThe debugging questions were easier than the rest, so both groups scored high, and the treatment group only scored slightly higher by chance.\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\nQ15.8 What do the results displayed in Figure¬†15.6 suggest about the impact of ‚ÄúAI‚Äù assistance on code reading skills?\n\n\n\n\n\nThe confidence intervals corresponding to the code reading questions overlap too much to be able to draw any conclusions.\n\n\n\n\nBoth groups had similar exposure to reading code through the task, so the AI and non-AI groups performed similarly.\n\n\n\n\nThe AI tool significantly improved code reading performance for the treatment group.\n\n\n\n\nThe control group had more experience reading code, so they scored considerably higher.",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>What's next? AI-assisted resea`R`ch?</span>"
    ]
  },
  {
    "objectID": "15_PartingWords.html#on-the-value-of-the-commons",
    "href": "15_PartingWords.html#on-the-value-of-the-commons",
    "title": "15¬† What‚Äôs next? AI-assisted reseaRch?",
    "section": "15.4 On the value of the commons",
    "text": "15.4 On the value of the commons\nIt is worth pointing out that the programmers in the control, no AI group in the aforementioned Anthropic study (Tamkin & Shen 2026) did not write code with no help whatsoever. Instead, they had access to regular web searching and code documentation. It is a common misconception that learning to code involves memorising lots of functions and commands. In practice, programming always involves looking things up. Up until very recently, researchers would typically search the web to find answers to their coding problems instead of prompting an ‚ÄúAI‚Äù product. These searches led us to interesting forum discussions, blog posts by fellow researchers and developers, and helpful documentation files (e.g.¬†in the form of so-called ‚Äòvignettes‚Äô). If our web search did not lead to anything use, we prepared a reprex, short for minimal reproducible example (see Wickham, √áetinkaya-Rundel & Grolemund 2023: Ch. 8 on getting help), posted it on a dedicated forum and community members would typically provide helpful answers within days or even hours.\nAll of these contents were human-generated. Not all were 100% reliable, but high-quality answers to problems on StackOverflow, for example, were upvoted by readers and errors in the documentation of open-source packages were quickly identified and corrected by the community of users. The same principle was true of statistical questions. Today, this strategy is still viable but, unfortunately, there are a lot of AI-generated webpages that one first needs to ignore.\n\n\n\n\n\n\nTipYour turn!\n\n\n\n\n\nQ15.9 What is Stack Exchange?\n\n\n\n\n\nAn online game for learning about programming and statistics, among other topics.\n\n\n\n\nAn open-source tool for managing project tasks and workflows that is widely used in academic research.\n\n\n\n\nA network of websites where people can ask and answer questions on a wide range of topics, the most popular of which is StackOverflow for programming questions.\n\n\n\n\nA free cloud-based service for hosting and managing data and code repositories.\n\n\n\n\nAn open-source social media platform for sharing photos and videos about research and programming.\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\nQ15.10 Open this archived version of a Q&A about confidence intervals hosted on Cross Validated, Stack Exchange‚Äôs statistics forum. In which year did Eliott originally ask their question?\n\n\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\nQ15.11 As of 9 February 2026 when the page was archived, how many Cross Validated members had upvoted the top answer?\n\n\n\n\n\n34 members\n\n\n\n\n28 members\n\n\n\n\n30 members\n\n\n\n\n32 members\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\nIn Section 11.2, we conducted the following t-test to find out whether the observed difference between the L1 and L2 speakers‚Äô non-verbal IQ ‚ÄòBlocks‚Äô test scores was significant or not:\n\nt.test(formula = Blocks ~ Group, \n       data = Dabrowska.data)\n\n\n    Welch Two Sample t-test\n\ndata:  Blocks by Group\nt = -4.4084, df = 152.46, p-value = 1.956e-05\nalternative hypothesis: true difference in means between group L1 and group L2 is not equal to 0\n95 percent confidence interval:\n -5.239791 -1.996693\nsample estimates:\nmean in group L1 mean in group L2 \n        13.84444         17.46269 \n\n\nThe output of the t.test() function above was fed into a LLM (DeepSeek R1 Distill Llama 70B) with a prompt requesting a short paragraph interpreting these results. The model‚Äôs response is printed below:\n\nThe Welch two‚Äësample t‚Äëtest shows a significant difference between the groups (t‚ÄØ=‚ÄØ‚Äë4.41, df‚ÄØ‚âà‚ÄØ152.5, p‚ÄØ=‚ÄØ1.96‚ÄØ√ó‚ÄØ10‚Åª‚Åµ). Group‚ÄØL1 has a mean of 13.84 blocks, while group‚ÄØL2‚Äôs mean is 17.46 blocks, a mean difference of ‚Äë3.62 blocks. The 95‚ÄØ% confidence interval for this difference (‚Äë5.24 to ‚Äë1.99) means we can be 95‚ÄØ% confident that the true average in L2 exceeds that in L1 by roughly 2 to 5 blocks. Because the interval does not include zero, the null hypothesis of equal means is rejected.\n\nQ15.12 Aside from the fact that ‚Äúblocks‚Äù is not a unit, which major statistical error does the LLM‚Äôs interpretation of the t-test output include?\n\n\n\n\n\nThe p-value is incorrectly translated and is in fact too large to reject the null hypothesis.\n\n\n\n\nThe t-test output makes a statement about the alternative hypothesis, not the null hypothesis.\n\n\n\n\nThe confidence interval is incorrectly interpreted as a probability statement about the true mean difference.\n\n\n\n\nThe LLM fails to recognise that the t-test was not appropriate because the data are not normally distributed.\n\n\n\n\nThe mean difference should be positive, not negative, since L2 has a higher mean.\n\n\n\n\n\n\n\n\n\n\n\nIn addition to being flooded by AI slop making it difficult to find reliable information, human content creators are finding that their intellectual property is being scraped without consent to be used as training data for commercial ‚ÄúAI‚Äù-products. In the era of Open Scholarship, researchers, software developers, and educators have been sharing their work with the world for the benefit of scientific progress, typically with only authorship attribution as a reward. However, this may change as AI companies appropriate their work and LLMs automate plagiarism (Rooij 2022).\nThis may sound like an entirely dystopian situation but, up until fairly recently, researchers had to pay to use programming languages for statistical analyses and scientific computing. FORTAN compilers, MatLab, S, and SPPS were (and still are) proprietary software which were inaccessible to many researchers and students. It is no exaggeration to say that open-source, community-led programming languages such as R, Python, and Julia revolutionised data analysis, making state-of-the-art methods accessible to far more people. However, as high-quality open-access resources become rarer due the contamination of AI slop and illegal scraping, we may be returning to an era of restricted access to scientific computing. Hence, it is worth remembering that it is communities of humans who have been developing programming languages such as R and Python, their many extensions such as the {tidyverse} packages, open-source software such as R Studio, and high-quality documentation and Open Educational Resources.\nThese communities depend on collaboration, interactions, and mutual support. Yet, we have entered an age where human interactions are marketed as unproductive, time-consuming, and burdensome. We are told that they can easily be replaced by more efficient and ‚Äúobjective‚Äù chatbots. Aside from the fact that LLMs are known to be prone to all kinds of very serious biases and that their efficiency is far from proven (see e.g. Loker 2025 on how AI code creates more problems than human-generated code), we should not loose sight of the value of subjective, human interactions. This is not to say that interacting with an LLM is never useful but rather that I also strongly encourage you to devote time to learning from reliable, human-generated resources (see e.g.¬†Next-step resources), join a course with other human beings to continue your learning journey, and/or find a learning buddy to discuss and solve problems together. This textbook was entirely human-generated and benefited greatly from countless rounds of revisions thanks to interactions with and feedback from (human!) students and colleagues (see Acknowledgements). Of course, it would have been quicker to write the textbook without asking for regular feedback and to get an LLM to generate first drafts of sections, code, and/or quiz questions. But in research, teaching, and learning, quality matters more than velocity.\n\nCheck your progress üåü\nIn this concluding chapter, you have answered 0 out of 12 questions correctly. Congratulations!\n\n\n\n\n\n\nNoteMore food for thought üçèüçé\n\n\n\n\nBender, Emily M. & Alex Hanna. 2025. The AI Con: How to Fight Big Tech‚Äôs Hype and Create the Future We Want. HarperCollins.\nBergstrom, Carl T. & Jevin D. West. Modern-Day Oracles or Bullshit Machines: How to thrive in a ChatGPT world. Online course. https://thebullshitmachines.com. [Open Educational Resource].\nDingemanse, Mark. 2024. Generative AI and Research Integrity. OSF. https://doi.org/10.31219/osf.io/2c48n. [Open Access].\nGuest, Olivia, Marcela Suarez, Barbara M√ºller, Edwin van Meerkerk, Arnoud Oude Groote Beverborg, Ronald de Haan, Andrea Reyes Elizondo, et al.¬†2025. Against the Uncritical Adoption of ‚ÄúAI‚Äù Technologies in Academia. Zenodo. https://zenodo.org/records/17065099. [Open Access].\n‚û°Ô∏è See also Olivia Guest‚Äôs curated list of readings on critical AI literacy: https://olivia.science/ai/ [Open Access].\nM√ºhlhoff, Rainer. 2025. The ethics of AI: Power, critique, responsibility. Bristol: Bristol University Press. https://doi.org/10.51952/9781529249262. [Open Access].\n‚û°Ô∏è See also Rainer M√ºhlhoff‚Äôs Introduction to the Ethics of AI 2025 lecture videos. https://rainermuehlhoff.de/en/EoAI2025/. [Open Educational Resource].",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>What's next? AI-assisted resea`R`ch?</span>"
    ]
  },
  {
    "objectID": "15_PartingWords.html#whats-next",
    "href": "15_PartingWords.html#whats-next",
    "title": "15¬† What‚Äôs next? AI-assisted reseaRch?",
    "section": "15.5 What‚Äôs next? üß≠",
    "text": "15.5 What‚Äôs next? üß≠\nThis textbook has taken you on a journey: first introducing Open Scholarship (Chapter 1), which forms the backbone of this textbook‚Äôs approach to doing science, then consolidating knowledge about file formats, file naming, and project organisation (Chapter 2 and Chapter 3), which are often major hurdles for successful data analysis pipelines. Having installed and set up R and RStudio (Chapter 4), you took your first steps in learning to code in R (Chapter 5). Next, you learnt to import real research data into an R project (Chapter 6). From Chapter 7 onwards, you learnt how to analyse data in R using descriptive and inferential statistics and data visualisations. This entailed wrangling data to prepare them for such analyses, as well as developing an understanding of key statistical concepts such as measures of central tendency and variability, distributions, effect sizes, confidence intervals, and p-values. Chapter 12 introduced statistical modelling with simple linear regression and Chapter 13 expanded this concept to multiple predictor variables and interactions between predictors. Having mastered this foundational knowledge, you are now ready to tackle more advanced statistical methods such as mixed-effects and non-linear regression models, logistic and other classification models, machine learning algorithms, and much more (see Next-step resources). In Chapter 14, you learnt how to apply literate programming skills to conduct and publish reproducible research in Quarto. Finally, in this concluding chapter, you reflected on the values of critical thinking, human learning, and of nurturing communities in the day and age of ‚ÄúAI‚Äù.\n\nLearning statistical theory and practice are inseparable from scientific reasoning (Vasishth & Gelman 2021: 1312).\n\nI personally believe that it will always be worth investing in learning complex matters, developing critical thinking skills, and building meaningful human relationships. No matter how powerful and efficient future ‚ÄúAI‚Äù products may be, the skills that will be valued in the future will continue to be collaboration, computational thinking, and critical (statistical) literacy, not ‚Äúadvanced‚Äù prompting techniques. The resources listed in the Appendix are a great starting point to continuing your learning journey.\nIn addition, the online version of this textbook features a growing set of case-study chapters, which students from my ‚ÄòIntroduction to Data Analysis in R‚Äô class have (co-)authored. Each chapter attempts to computationally reproduce a published a linguistics study using the authors‚Äô original data. The student authors document all the steps necessary to reproduce the results and discuss the success (or not!) of the reproduction. Picking a case study that interests you or that covers a method that you‚Äôd like to learn more about and attempting to reproduce the steps outlined in the chapter is a great to consolidate and expand on the skills and knowledge that you have acquired so far.\n\n\n\nHappy leaRning!Ô∏è (artwork by Allison Horst CC BY 4.0)\n\n\n\n\n\n\nArregoitia, Luis D. Verde. 2026. Large language model tools for r. https://luisdva.github.io/llmsr-book/.\n\n\nBender, Emily M. & Alex Hanna. 2025. The AI con: How to fight big tech‚Äôs hype and create the future we want. HarperCollins.\n\n\nChow, Michael, Hadley Wickham & Wes Mckinney. 2025. Episode 4 : Mine √ßetinkaya-rundel: Teaching in the AI era ‚Äî and keeping students engaged. https://posit.co/thetestset/episode/mine-cetinkaya-rundel-teaching-in-the-ai-era-and-keeping-students-engaged/.\n\n\nFerdman, Avigail. 2025. AI deskilling is a structural problem. AI & SOCIETY. https://doi.org/10.1007/s00146-025-02686-z.\n\n\nGuest, Olivia, Marcela Suarez, Barbara M√ºller, Edwin van Meerkerk, Arnoud Oude Groote Beverborg, Ronald de Haan, Andrea Reyes Elizondo, et al. 2025. Against the uncritical adoption of ‚ÄúAI‚Äù technologies in academia. https://zenodo.org/records/17065099.\n\n\nHeiss. 2024. Data visualization with r: Can we use ChatGPT? Data Visualization with R. https://datavizs24.classes.andrewheiss.com/news/2024-06-11_faqs_session-01.html#can-we-use-chatgpt-can-you-even-tell-if-we-do.\n\n\nJj. 2025. The copilot delusion. Blogmobly. https://deplet.ing/the-copilot-delusion/.\n\n\nLoker, David. 2025. AI vs human code gen report: AI code creates 1.7x more issues. CodeRabbit. https://www.coderabbit.ai/blog/state-of-ai-vs-human-code-generation-report.\n\n\nLucchi, Nicola. 2024. ChatGPT: A case study on copyright challenges for generative artificial intelligence systems. European Journal of Risk Regulation 15(3). 602‚Äì624. https://doi.org/10.1017/err.2023.59.\n\n\nLuccioni, Sasha, Boris Gamazaychikov, Theo Alves da Costa & Emma Strubell. Misinformation by omission: The need for more environmental transparency in AI. https://doi.org/10.48550/arXiv.2506.15572.\n\n\nM√ºhlhoff, Rainer. 2025. The ethics of AI: Power, critique, responsibility. Bristol University Press.\n\n\nNatali, Chiara, Luca Marconi, Leslye Denisse Dias Duran & Federico Cabitza. 2025. AI-induced Deskilling in Medicine: A Mixed-Method Review and Research Agenda for Healthcare and Beyond. Artificial Intelligence Review 58(11). https://doi.org/10.1007/s10462-025-11352-1.\n\n\nNotopoulos, Katie. 2024. Google AI said to put glue in pizza ‚Äî so i made a pizza with glue and ate it. Business Insider. https://www.businessinsider.com/google-ai-glue-pizza-i-tried-it-2024-5.\n\n\nPerrigo, Billy. 2023. TIME. https://web.archive.org/web/20260103132216/https://time.com/6247678/openai-chatgpt-kenya-workers/.\n\n\nPooley, Jeff. 2025. The matthew effect in AI summary. https://www.jeffpooley.com/2025/11/the-matthew-effect-in-ai-summary/.\n\n\nQuattrociocchi, Walter, Valerio Capraro & Matja≈æ Perc. 2025. Epistemological fault lines between human and artificial intelligence. https://doi.org/10.48550/arXiv.2512.19466.\n\n\nRooij, Iris Van. 2022. Against automated plagiarism. Iris van Rooij. https://irisvanrooijcogsci.com/2022/12/29/against-automated-plagiarism/.\n\n\nSamuelson, Pamela. 2023. Generative AI meets copyright. Science. American Association for the Advancement of Science 381(6654). 158‚Äì161. https://doi.org/10.1126/science.adi0656.\n\n\nStetskov, Denis. 2025. The great software quality collapse: How we normalized catastrophe. From the Trenches. https://techtrenches.dev/p/the-great-software-quality-collapse.\n\n\nTamkin, Alex & Judy Hanwen Shen. 2026. How AI impacts skill formation. https://doi.org/10.48550/arXiv.2601.20245.\n\n\nVasishth, Shravan & Andrew Gelman. 2021. How to embrace variation and accept uncertainty in linguistic and psycholinguistic data analysis. Linguistics 59(5). 1311‚Äì1342. https://doi.org/10.1515/ling-2019-0051.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser & Illia Polosukhin. 2017. Attention is all you need. In. arXiv. https://doi.org/10.48550/arXiv.1706.03762.\n\n\nWickham, Hadley, Mine √áetinkaya-Rundel & Garrett Grolemund. 2023. R for data science: Import, tidy, transform, visualize, and model data. 2nd edition. O‚ÄôReilly. https://r4ds.hadley.nz/.",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>What's next? AI-assisted resea`R`ch?</span>"
    ]
  },
  {
    "objectID": "15_PartingWords.html#footnotes",
    "href": "15_PartingWords.html#footnotes",
    "title": "15¬† What‚Äôs next? AI-assisted reseaRch?",
    "section": "",
    "text": "I highly recommend Andrew Heiss‚Äô blog and his beautiful teaching resources. Fun fact for language students and linguists: Andrew majored in Arabic and Italian and didn‚Äôt learn about statistics or R until he started his second master‚Äôs!‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>What's next? AI-assisted resea`R`ch?</span>"
    ]
  },
  {
    "objectID": "99_references.html",
    "href": "99_references.html",
    "title": "References",
    "section": "",
    "text": "Abeysooriya, Mandhri, Megan Soria, Mary Sravya Kasu & Mark Ziemann.\n2021. Gene name errors: Lessons not learned. PLOS Computational\nBiology. Public 17(7). e1008984. https://doi.org/10.1371/journal.pcbi.1008984.\n\n\nAcheson, Daniel J., Justine B. Wells & Maryellen C. MacDonald. 2008.\nNew and updated tests of print exposure and reading abilities in college\nstudents. Behavior Research Methods 40(1). 278‚Äì289. https://doi.org/10.3758/brm.40.1.278.\n\n\nAlhazmi, Fahd. 2020. A visual interpretation of the standard deviation.\nMedium. https://towardsdatascience.com/a-visual-interpretation-of-the-standard-deviation-30f4676c291c.\n\n\nAlmeida, Alexandre, Adam Loy & Heike Hofmann. 2018. ggplot2\ncompatible quantile-quantile plots in R. The R\nJournal 10(2). 248‚Äì261. https://doi.org/10.32614/RJ-2018-051.\n\n\nalvinashcraft, alexbuckgit, ArcticLampyrid & bearmannl. 2022.\nMaximum path length limitation. Learn Microsoft. https://learn.microsoft.com/en-us/windows/win32/fileio/maximum-file-path-limitation.\n\n\nBaayen, R. Harald. 2008. Analyzing linguistic data: A practical\nintroduction to statistics using R. Cambridge\nUniversity Press.\n\n\nBarr, Dale & Lisa DeBruine. 2023. webexercises: Create interactive web exercises in\n‚ÄúR Markdown‚Äù (formerly ‚Äúwebex‚Äù). https://doi.org/10.32614/CRAN.package.webexercises.\n\n\nBarrett, Malcolm. 2018. Why should I use the here package\nwhen I‚Äôm already using projects? https://malco.io/articles/2018-11-05-why-should-i-use-the-here-package-when-i-m-already-using-projects.\n\n\nBender, Emily M. & Alex Hanna. 2025. The AI con: How to fight\nbig tech‚Äôs hype and create the future we want. HarperCollins.\n\n\nBen-Shachar, Mattan, Daniel L√ºdecke & Dominique Makowski. 2020.\nEffectsize: Estimation of effect size indices and standardized\nparameters. Journal of Open Source Software 5(56). 2815. https://doi.org/10.21105/joss.02815.\n\n\nBerez-Kroeker, Andrea L., Bradley McDonnell, Eve Koller & Lauren B.\nCollister. 2022. The Open Handbook of\nLinguistic Data Management.\nMIT Press. https://doi.org/10.7551/mitpress/12200.001.0001.\n\n\nBochynska, Agata, Liam Keeble, Caitlin Halfacre, Joseph V. Casillas,\nIrys-Am√©lie Champagne, Kaidi Chen, Melanie R√∂thlisberger, Erin M.\nBuchanan & Timo B. Roettger. 2023. Reproducible research practices\nand transparency across linguistics. Glossa Psycholinguistics\n2(1). https://doi.org/10.5070/G6011239.\n\n\nBreheny, Patrick & Woodrow Burchett. 2017. Visualization of\nRegression Models Using visreg. The R Journal 9(2). 56. https://doi.org/10.32614/RJ-2017-046.\n\n\nBryan, Jennifer. 2018. Let‚Äôs Git started: Happy\nGit and GitHub for the useR. Open\nEducation Resource. https://happygitwithr.com/.\n\n\nBryan, Jenny. 2017. Project-oriented workflow. Tidyverse.org.\nhttps://www.tidyverse.org/blog/2017/12/workflow-vs-script/.\n\n\nBusterud, Guro, Anne Dahl, Dave Kush & Kjersti Faldet Listhaug.\n2023. Verb placement in L3 french and L3 german: The role of\nlanguage-internal factors in determining cross-linguistic influence from\nprior languages. Linguistic Approaches to Bilingualism. John\n13(5). 693‚Äì716. https://doi.org/10.1075/lab.22058.bus.\n\n\nCenter for OpenScience. 2025. Choosing the Right Preregistration\nTemplate: A Guide for Researchers. https://www.cos.io/blog/choosing-preregistration-template-guide-for-researchers.\n\n\n√áetinkaya-Rundel, Mine & Johanna Hardin. 2021. Introduction to\nmodern statistics. Second. Leanpub. https://openintro-ims.netlify.app/.\n\n\nChow, Michael, Hadley Wickham & Wes Mckinney. 2025. Episode 4 :\nMine √ßetinkaya-rundel: Teaching in the AI era ‚Äî and keeping students\nengaged. https://posit.co/thetestset/episode/mine-cetinkaya-rundel-teaching-in-the-ai-era-and-keeping-students-engaged/.\n\n\nCleveland, William S. & Robert McGill. 1987. Graphical perception:\nThe visual decoding of quantitative information on graphical displays of\ndata. Journal of the Royal Statistical Society: Series A\n(General) 150(3). 192‚Äì210. https://doi.org/10.2307/2981473.\n\n\nCohen, Jacob. 1988. Statistical power analysis for the behavioral\nsciences. 2. ed., reprint. New York, NY: Psychology Press.\n\n\nConsortium, TEI. 2025. TEI P5: Guidelines for electronic text encoding\nand interchange. Zenodo. https://doi.org/10.5281/zenodo.17161156.\n\n\nDƒÖbrowska, Ewa. 2019. Experience, aptitude, and individual differences\nin linguistic attainment: A comparison of native and nonnative speakers.\nLanguage Learning 69(S1). 72‚Äì100. https://doi.org/10.1111/lang.12323.\n\n\nDauber, Daniel. 2024. R for non-programmers: A guide for social\nscientists. Open Education Resource. https://bookdown.org/daniel_dauber_io/r4np_book/.\n\n\nDouglas, Alex, Deon Roos, Francesca Mancini & David Lusseau. 2024.\nAn introduction to R. https://intro2r.com/.\n\n\nEkman, Paul & Wallace V Friesen. 1978. Facial action coding system.\nEnvironmental Psychology & Nonverbal Behavior.\n\n\nFerdman, Avigail. 2025. AI deskilling is a structural problem. AI\n& SOCIETY. https://doi.org/10.1007/s00146-025-02686-z.\n\n\nFew, Stephen. Save the pies for dessert. August 2007. http://www.perceptualedge.com/articles/08-21-07.pdf.\n\n\nField, Andy P., Jeremy Miles & Zo√´ Field. 2012. Discovering\nstatistics using r. Sage.\n\n\nFox, John & Sanford Weisberg. 2019. An R companion\nto applied regression. Third edition. SAGE.\n\n\nFricke, Lea, Patrick G Grosz & Tatjana Scheffler. 2024. Semantic\ndifferences in visually similar face emojis. Language and\nCognition. Cambridge University Press 1‚Äì15. https://doi.org/10.1017/langcog.2024.12.\n\n\nFugate, Jennifer MB & Courtny L Franco. 2021. Implications for\nemotion: Using anatomically based facial coding to compare emoji faces\nacross platforms. Frontiers in Psychology. Frontiers Media SA\n12. 605928. https://doi.org/10.3389/fpsyg.2021.605928.\n\n\nGarnier, Simon, Noam Ross, BoB Rudis, Antoine Filipovic-Pierucci, Tal\nGalili, Timelyportfolio, Alan O‚ÄôCallaghan, et al. 2023.\nSjmgarnier/viridis: CRAN release v0.6.3. Zenodo. https://doi.org/10.5281/ZENODO.4679423.\n\n\nGelman, Andrew. 2018. Ethics in statistical practice and communication:\nFive recommendations. Significance 15(5). 40‚Äì43. https://doi.org/10.1111/j.1740-9713.2018.01193.x.\n\n\nGelman, Andrew. 2019. Embracing variation and accepting uncertainty:\nImplications for science and metascience. https://www.youtube.com/watch?v=VQCcMP4A5Ks.\n\n\nGodfrey, A. Jonathan R., Debra Warren, Deepayan Sarkar, Gabriel Becker,\nJames Thompson, Paul Murrell, Timothy Bilton & Volker Sorge. 2025.\nBrailleR: Improved access for blind users. https://github.com/ajrgodfrey/BrailleR.\n\n\nGood, Jeff. 2022. The scope of linguistic data. In Andrea L.\nBerez-Kroeker, Bradley McDonnell, Eve Koller & Lauren B. Collister\n(eds.), The open handbook of linguistic data management, 27‚Äì47.\nMIT Press. https://doi.org/10.7551/mitpress/12200.001.0001.\n\n\nGries, Stefan Th. & Nick C. Ellis. 2015. Statistical measures for\nusage-based linguistics. Language Learning 65(S1). 228‚Äì255. https://doi.org/10.1111/lang.12119.\n\n\nGries, Stefan Thomas. 2021. Statistics for linguistics with\nR: A practical introduction (De Gruyter Mouton\nTextbook). 3rd revised edition. de Gruyter Mouton.\n\n\nGr√∂mping, Ulrike. 2006. Relative Importance for Linear Regression\ninR: The Package relaimpo. Journal of Statistical\nSoftware 17(1). https://doi.org/10.18637/jss.v017.i01.\n\n\nGrosz, Patrick Georg, Gabriel Greenberg, Christian De Leon & Elsi\nKaiser. 2023. A semantics of face emoji in discourse. Linguistics\nand Philosophy. Springer 46(4). 905‚Äì957. https://doi.org/10.1007/s10988-022-09369-8.\n\n\nGuest, Olivia, Marcela Suarez, Barbara M√ºller, Edwin van Meerkerk,\nArnoud Oude Groote Beverborg, Ronald de Haan, Andrea Reyes Elizondo, et\nal. 2025. Against the uncritical adoption of ‚ÄúAI‚Äù\ntechnologies in academia. https://zenodo.org/records/17065099.\n\n\nHaroz, Steve. 2022. Comparison of preregistration platforms. https://doi.org/10.31222/osf.io/zry2u.\n\n\nHarrell, Frank E. 2015. Regression modeling strategies: With\napplications to linear models, logistic and ordinal regression, and\nsurvival analysis (Springer Series in Statistics). Springer\nInternational Publishing. https://doi.org/10.1007/978-3-319-19425-7.\n\n\nHeiss. 2024. Data visualization with r: Can we use ChatGPT? Data\nVisualization with R. https://datavizs24.classes.andrewheiss.com/news/2024-06-11_faqs_session-01.html#can-we-use-chatgpt-can-you-even-tell-if-we-do.\n\n\nHorst, Allison & Julie Lowndes. 2020. Openscapes - Tidy\ndata for efficiency, reproducibility, and collaboration. https://openscapes.org/blog/2020-10-12-tidy-data/.\n\n\nHvitfeldt, Emil. 2021. Paletteer: Comprehensive collection of color\npalettes. https://github.com/EmilHvitfeldt/paletteer.\n\n\nIannone, Richard, Joe Cheng, Barret Schloerke, Shannon Haughton, Ellis\nHughes, Alexandra Lauer, Romain Fran√ßois, JooYoung Seo, Ken Brevoort\n& Olivier Roy. 2025. gt: Easily\ncreate presentation-ready display tables. https://doi.org/10.32614/CRAN.package.gt.\n\n\niris-database.org. 2011. IRIS. https://iris-database.org/.\n\n\nJj. 2025. The copilot delusion. Blogmobly. https://deplet.ing/the-copilot-delusion/.\n\n\nKaufman, Allison B. & James C. Kaufman (eds.). 2018. The illusion of\ncausality: A cognitive bias underlying pseudoscience. In\nPseudoscience. The MIT Press. https://doi.org/10.7551/mitpress/10747.003.0007.\n\n\nKung, Susan Smythe. 2022. Developing a data management plan. In Andrea\nL. Berez-Kroeker, Bradley McDonnell, Eve Koller & Lauren B.\nCollister (eds.), The open handbook of linguistic data\nmanagement, 101‚Äì115. MIT Press. https://doi.org/10.7551/mitpress/12200.001.0001.\n\n\nLakens, Dani√´l. 2022. Improving your statistical inferences.\nZenodo. https://doi.org/10.5281/ZENODO.6409077.\n\n\nLausberg, Hedda & Han Sloetjes. 2009. Coding gestural behavior with\nthe NEUROGES-ELAN system. Behavior Research Methods 41(3).\n841‚Äì849. https://doi.org/10.3758/BRM.41.3.841.\n\n\nLe Foll, Elen. 2022. Textbook English: A\ncorpus-based analysis of the language of EFL textbooks used in secondary\nschools in France, Germany and\nSpain. Osnabr√ºck University PhD thesis. https://doi.org/10.48693/278.\n\n\nLenth, Russell V. 2025. Emmeans: Estimated marginal means, aka\nleast-squares means. https://rvlenth.github.io/emmeans/.\n\n\nLevshina, Natalia. 2015. How to do linguistics with R:\nData exploration and statistical analysis. John Benjamins.\n\n\nLevshina, Natalia. 2022. Comparing Bayesian and Frequentist Models of\nLanguage Variation: The Case of Help + (to-)Infinitive. In Ole Sch√ºtzler\n& Julia Schl√ºter (eds.), 224‚Äì258. 1st edn. Cambridge University\nPress. https://doi.org/10.1017/9781108589314.009.\n\n\nLindeman, Richard Harold, Peter Francis Merenda & Ruth Z. Gold.\n1980. Introduction to bivariate and multivariate analysis.\nScott, Foresman.\n\n\nLoker, David. 2025. AI vs human code gen report: AI code creates 1.7x\nmore issues. CodeRabbit. https://www.coderabbit.ai/blog/state-of-ai-vs-human-code-generation-report.\n\n\nLucchi, Nicola. 2024. ChatGPT: A case study on copyright challenges for\ngenerative artificial intelligence systems. European Journal of Risk\nRegulation 15(3). 602‚Äì624. https://doi.org/10.1017/err.2023.59.\n\n\nL√ºdecke, Daniel. 2020. sjPlot: Data visualization for statistics in\nsocial science. https://CRAN.R-project.org/package=sjPlot.\n\n\nL√ºdecke, Daniel, Mattan S. Ben-Shachar, Indrajeet Patil, Philip Waggoner\n& Dominique Makowski. 2021. performance:\nAn R package for assessment, comparison and testing of\nstatistical models. Journal of Open Source Software 6(60).\n3139. https://doi.org/10.21105/joss.03139.\n\n\nL√ºdecke, Daniel, Indrajeet Patil, Mattan S. Ben-Shachar, Brenton M.\nWiernik, Philip Waggoner & Dominique Makowski. 2021. See: An\nR package for visualizing statistical models. Journal\nof Open Source Software 6(64). 3393. https://doi.org/10.21105/joss.03393.\n\n\nMaier, Emar. 2023. Emojis as pictures. Ergo 10. https://doi.org/10.3998/ergo.4641.\n\n\nMatejka, Justin & George Fitzmaurice. 2017. Same stats, different\ngraphs: Generating datasets with varied appearance and identical\nstatistics through simulated annealing. In, 12901294. New York, NY, USA:\nAssociation for Computing Machinery. https://doi.org/10.1145/3025453.3025912.\n\n\nMatute, Helena, Fernando Blanco, Ion Yarritu, Marcos D√≠az-Lago, Miguel\nA. Vadillo & Itxaso Barberia. 2015. Illusions of causality: How they\nbias our everyday thinking and how they could be reduced. Frontiers\nin Psychology. Frontiers 6. https://doi.org/10.3389/fpsyg.2015.00888.\n\n\nMertzen, Daniela, Sol Lago & Shravan Vasishth. 2021. The benefits of\npreregistration for hypothesis-driven bilingualism research.\nBilingualism: Language and Cognition 24(5). 807‚Äì812. https://doi.org/10.1017/S1366728921000031.\n\n\nMizumoto, Atsushi. 2023. Calculating the relative importance of multiple\nregression predictor variables using dominance analysis and random\nforests. Language Learning 73(1). 161‚Äì196. https://doi.org/10.1111/lang.12518.\n\n\nMizumoto, Atsushi & Luke Plonsky. 2016. R as a lingua franca:\nAdvantages of using r for quantitative research in applied linguistics.\nApplied Linguistics 37(2). 284‚Äì291. https://doi.org/10.1093/applin/amv025.\n\n\nMoroz, George. 2020. Create check-fields and check-boxes with\ncheckdown. https://CRAN.R-project.org/package=checkdown.\n\n\nM√ºhlhoff, Rainer. 2025. The ethics of AI: Power, critique,\nresponsibility. Bristol University Press.\n\n\nM√ºller, Kirill. 2025. here: A simpler\nway to find your files. https://doi.org/10.32614/CRAN.package.here.\n\n\nNatali, Chiara, Luca Marconi, Leslye Denisse Dias Duran & Federico\nCabitza. 2025. AI-induced Deskilling in Medicine: A Mixed-Method Review\nand Research Agenda for Healthcare and Beyond. Artificial\nIntelligence Review 58(11). https://doi.org/10.1007/s10462-025-11352-1.\n\n\nNeuwirth, Erich. 2022. Package ‚ÄúRColorBrewer.‚Äù\nColorBrewer palettes 991. https://cran.r-project.org/web/packages/RColorBrewer/RColorBrewer.pdf.\n\n\nNicenboim, Bruno, Daniel Schad & Shravan Vasishth. 2026.\nIntroduction to Bayesian Data Analysis for cognitive science\n(Chapman & Hall/CRC Statistics in the social and behavioral sciences\nseries). Boca Raton London New York: CRC Press, Taylor & Francis\nGroup. https://doi.org/10.1201/9780429342646.\n\n\nNimon, Kim F. 2012. Statistical assumptions of substantive analyses\nacross the general linear model: A mini-review. Frontiers in\nPsychology 3. https://doi.org/10.3389/fpsyg.2012.00322.\n\n\nNotopoulos, Katie. 2024. Google AI said to put glue in pizza ‚Äî so i made\na pizza with glue and ate it. Business Insider. https://www.businessinsider.com/google-ai-glue-pizza-i-tried-it-2024-5.\n\n\nOu, Jianhong. 2021. colorBlindness: Safe color set for color\nblindness. https://CRAN.R-project.org/package=colorBlindness.\n\n\nPaquot, Magali, Alexander K√∂nig, Egon W. Stemle & Jennifer-Carmen\nFrey. 2024. The core metadata schema for learner corpora (LC-meta):\nCollaborative efforts to advance data discoverability, metadata quality\nand study comparability in L2 research. International Journal of\nLearner Corpus Research 10(2). 280‚Äì300. https://doi.org/10.1075/ijlcr.24010.paq.\n\n\nParsons, Sam, Fl√°vio Azevedo, Mahmoud M. Elsherif, Samuel Guay, Owen N.\nShahim, Gisela H. Govaart, Emma Norris, et al. 2022. A community-sourced\nglossary of open scholarship terms. Nature Human\nBehaviour. Nature 6(3). 312‚Äì318. https://doi.org/10.1038/s41562-021-01269-4.\n\n\nPedersen, Thomas Lin. 2024. Patchwork: The Composer of\nPlots. https://patchwork.data-imaginist.com.\n\n\nPedersen, Thomas Lin & Maxim Shemanarev. 2024. Ragg: Graphic\ndevices based on AGG. https://ragg.r-lib.org.\n\n\nPerrigo, Billy. 2023. TIME. https://web.archive.org/web/20260103132216/https://time.com/6247678/openai-chatgpt-kenya-workers/.\n\n\nPfadenhauer, Katrin & Evelyn Wiesinger (eds.). 2024. Romance\nmotion verbs in language change: Grammar, lexicon, discourse. De\nGruyter. https://doi.org/10.1515/9783111248141.\n\n\nPfeifer, Valeria A, Emma L Armstrong & Vicky Tzuyin Lai. 2022. Do\nall facial emojis communicate emotion? The impact of facial emojis on\nperceived sender emotion and text processing. Computers in Human\nBehavior. Elsevier 126. 107016. https://doi.org/10.1016/j.chb.2021.107016.\n\n\nPlonsky, Luke & Frederick L. Oswald. 2014. How big is\n‚Äúbig‚Äù? Interpreting effect sizes in L2 research.\nLanguage Learning 64(4). 878‚Äì912. https://doi.org/10.1111/lang.12079.\n\n\nPooley, Jeff. 2025. The matthew effect in AI summary. https://www.jeffpooley.com/2025/11/the-matthew-effect-in-ai-summary/.\n\n\nPrat, Chantel S., Tara M. Madhyastha, Malayka J. Mottarella &\nChu-Hsuan Kuo. 2020. Relating natural language aptitude to individual\ndifferences in learning programming languages. Scientific\nReports. Nature 10(1). 3817. https://doi.org/10.1038/s41598-020-60661-8.\n\n\nQuattrociocchi, Walter, Valerio Capraro & Matja≈æ Perc. 2025.\nEpistemological fault lines between human and artificial intelligence.\nhttps://doi.org/10.48550/arXiv.2512.19466.\n\n\nR Core Team. 2024. R: A language and environment for statistical\ncomputing. R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nR Core Team. 2025. R: A language and environment for\nstatistical computing. Vienna, Austria: R Foundation for\nStatistical Computing. https://www.R-project.org/.\n\n\nRodrigues, Bruno & Philipp Baumann. 2026. Rix: Reproducible data\nscience environments with ‚Äúnix‚Äù. https://docs.ropensci.org/rix/.\n\n\nRodriguez-Sanchez, Francisco & Connor P. Jackson. 2025. grateful: Facilitate citation of R\npackages. https://pakillo.github.io/grateful/.\n\n\nRoettger, Timo B. 2021. Preregistration in experimental linguistics:\nApplications, challenges, and limitations. Linguistics. De\n59(5). 1227‚Äì1249. https://doi.org/10.1515/ling-2019-0048.\n\n\nRooij, Iris Van. 2022. Against automated plagiarism. Iris van\nRooij. https://irisvanrooijcogsci.com/2022/12/29/against-automated-plagiarism/.\n\n\nRooij, Iris van, Olivia Guest, Federico Adolfi, Ronald de Haan, Antonina\nKolokolova & Patricia Rich. 2024. Reclaiming AI as a theoretical\ntool for cognitive science. Computational Brain & Behavior.\nhttps://doi.org/10.1007/s42113-024-00217-5.\n\n\nSamuelson, Pamela. 2023. Generative AI meets copyright.\nScience. American Association for the Advancement of Science\n381(6654). 158‚Äì161. https://doi.org/10.1126/science.adi0656.\n\n\nScheffler, Tatjana & Ivan Nenchev. 2024. Affective, semantic,\nfrequency, and descriptive norms for 107 face emojis. Behavior\nResearch Methods. Springer 1‚Äì22. https://doi.org/10.3758/s13428-024-02444-x.\n\n\nSchimke, Sarah, Israel de la Fuente, Barbara Hemforth & Saveria\nColonna. 2018. First language influence on second language offline and\nonline ambiguous pronoun resolution. Language Learning 68(3).\n744‚Äì779. https://doi.org/10.1111/lang.12293.\n\n\nSchweinberger, Martin. 2022. Data management, version control, and\nreproducibility. https://ladal.edu.au/repro.html.\n\n\nSeibold, Heidi & Rabea M√ºller. 2023. BERD course: Make your research\nreproducible. https://doi.org/10.17605/OSF.IO/RUPT7.\n\n\nSievert, Carson. 2020. Interactive web-based data visualization with\nr, plotly, and shiny. Chapman; Hall/CRC. https://plotly-r.com.\n\n\nSilge, Julia. 2022. Janeaustenr: Jane Austen‚Äôs complete\nnovels. https://CRAN.R-project.org/package=janeaustenr.\n\n\nSmith, Gary. 2018. Step away from stepwise. Journal of Big\nData. SpringerOpen 5(1). 1‚Äì12. https://doi.org/10.1186/s40537-018-0143-6.\n\n\nSonderegger, Morgan. 2023. Regression modeling for linguistic\ndata. The MIT Press.\n\n\nS√≥skuthy, M√°rton. Generalised additive mixed models for dynamic analysis\nin linguistics: A practical introduction. https://doi.org/10.48550/arXiv.1703.05339.\n\n\nSouth Carolina, University of. Alternative text. Digital\nAccessibility. https://sc.edu/about/offices_and_divisions/digital-accessibility/toolbox/best_practices/alternative_text/.\n\n\nStefanowitsch, Anatol & Susanne Flach. 2017. The corpus-based\nperspective on entrenchment. In Hans-J√∂rg Schmid (ed.), Entrenchment\nand the psychology of language learning: How we reorganize and adapt\nlinguistic knowledge, 101‚Äì127. De Gruyter. https://doi.org/10.1037/15969-006.\n\n\nStetskov, Denis. 2025. The great software quality collapse: How we\nnormalized catastrophe. From the Trenches. https://techtrenches.dev/p/the-great-software-quality-collapse.\n\n\nTabachnick, Barbara G. & Linda S. Fidell. 2014. Using\nmultivariate statistics (Always Learning). Pearson new\ninternational edition, sixth edition. Pearson.\n\n\nTamkin, Alex & Judy Hanwen Shen. 2026. How AI impacts skill\nformation. https://doi.org/10.48550/arXiv.2601.20245.\n\n\nThe Turing Way Community. 2022. The Turing\nWay: A handbook for reproducible, ethical and collaborative\nresearch (1.0.2). Zenodo. https://doi.org/10.5281/zenodo.3233853.\n\n\nThompson, Bruce. 1995. Stepwise regression and stepwise discriminant\nanalysis need not apply here: A guidelines editorial. Educational\nand Psychological Measurement. SAGE 55(4). 525‚Äì534. https://doi.org/10.1177/0013164495055004001.\n\n\nTrippel, Thorsten. 2025. Metadata for research data. In Piotr Ba≈Ñski,\nUlrich Heid & Laura Herzberg (eds.), Harmonizing language data:\nStandards for linguistic resources, 251‚Äì279. De Gruyter. https://www.degruyterbrill.com/document/doi/10.1515/9783112208212-011/html.\n\n\nUshey, Kevin & Hadley Wickham. 2023. Renv: Project\nenvironments. https://CRAN.R-project.org/package=renv.\n\n\nVan Hulle, Sven & Renata Enghels. 2024a. The category of throw verbs\nas productive source of the spanish inchoative construction. In Katrin\nPfadenhauer & Evelyn Wiesinger (eds.), Romance motion verbs in\nlanguage change, 213‚Äì240. De Gruyter. https://doi.org/10.1515/9783111248141-009.\n\n\nVan Hulle, Sven & Renata Enghels. 2024b. TROLLing replication data\nfor: ‚ÄúThe category of throw verbs as productive source of the\nspanish inchoative construction. DataverseNO, V1.‚Äù https://doi.org/10.18710/TR2PWJ.\n\n\nVasishth, Shravan & Andrew Gelman. 2021. How to embrace variation\nand accept uncertainty in linguistic and psycholinguistic data analysis.\nLinguistics 59(5). 1311‚Äì1342. https://doi.org/10.1515/ling-2019-0051.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N. Gomez, Lukasz Kaiser & Illia Polosukhin. 2017.\nAttention is all you need. In. arXiv. https://doi.org/10.48550/arXiv.1706.03762.\n\n\nWickham, Hadley. 2016. ggplot2: Elegant graphics for data\nanalysis. New York: Springer. https://ggplot2.tidyverse.org.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD‚ÄôAgostino McGowan, Romain Fran√ßois, Garrett Grolemund, et al. 2019.\nWelcome to the tidyverse. Journal of\nOpen Source Software 4(43). 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Mine √áetinkaya-Rundel & Garrett Grolemund. 2023.\nR for data science: Import, tidy, transform, visualize, and model\ndata. 2nd edition. O‚ÄôReilly. https://r4ds.hadley.nz/.\n\n\nWickham, Hadley, Romain Fran√ßois & Lucy D‚ÄôAgostino McGowan. 2024.\nEmo: Easily insert ‚Äôemoji‚Äô. https://github.com/hadley/emo.\n\n\nWickham, Hadley, Davis Vaughan & Maximilian Girlich. Tidy messy\ndata. https://tidyr.tidyverse.org/.\n\n\nWilkinson, Leland. 2005. The Grammar of\nGraphics (Statistics and Computing). New York:\nSpringer. https://doi.org/10.1007/0-387-28695-0.\n\n\nWilliams, Matt N., Carlos Alberto G√≥mez Grajales & Dason Kurkiewicz.\n2013. Assumptions of multiple regression: Correcting two misconceptions.\nPractical Assessment, Research, and Evaluation 18(11).\n\n\nWindhouwer, Menzo & Twan Goosen. 2022. Component metadata\ninfrastructure. In Darja Fi≈°er & Andreas Witt (eds.), CLARIN:\nThe infrastructure for language resources, 191‚Äì222. De Gruyter. https://doi.org/10.1515/9783110767377-008.\n\n\nWinter, Bodo. 2020. Statistics for linguists: An introduction using\nR. Routledge. https://doi.org/10.4324/9781315165547.\n\n\nWithers, Peter. 2012. Metadata management with arbil. In V. D. Arranz,\nB. Broeder, M. Gaiffe, M. Gavrilidou & M. Monachini (eds.),\nProceedings of the workshop describing LRs with metadata: Towards\nflexibility and interoperability in the documentation of LR, 72‚Äì75.\nEuropean Language Resources Association (ELRA). http://www.lrec-conf.org/proceedings/lrec2012/workshops/11.LREC2012%20Metadata%20Proceedings.pdf#page=79.\n\n\nXie, Yihui. 2025. xfun: Supporting\nfunctions for packages maintained by ‚ÄúYihui\nXie‚Äù. https://doi.org/10.32614/CRAN.package.xfun.\n\n\nYe, Jiachu, Xiaoyan Lai & Gary Ka-Wai Wong. 2022. The transfer\neffects of computational thinking: A systematic review with\nmeta-analysis and qualitative synthesis. Journal of Computer\nAssisted Learning 38(6). 1620‚Äì1638. https://doi.org/10.1111/jcal.12723.\n\n\nZiemann, Mark, Yotam Eren & Assam El-Osta. 2016. Gene name errors\nare widespread in the scientific literature. Genome Biology\n17(1). 177. https://doi.org/10.1186/s13059-016-1044-7.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "A_FurtherResources.html",
    "href": "A_FurtherResources.html",
    "title": "Next-step resources",
    "section": "",
    "text": "Recommended resources specific to the language sciences (in alphabetical order)\nIn the hope that this textbook has inspired you to dive deeper into the wonderful world of quantitative linguistics, data analysis, statistics, data visualisation, and coding in R, here is a (work-in-progress) curated list of further resources to continue your learning journey! üöÄ",
    "crumbs": [
      "Next-step resources"
    ]
  },
  {
    "objectID": "A_FurtherResources.html#recommended-resources-specific-to-the-language-sciences-in-alphabetical-order",
    "href": "A_FurtherResources.html#recommended-resources-specific-to-the-language-sciences-in-alphabetical-order",
    "title": "Next-step resources",
    "section": "",
    "text": "Brezina, Vaclav. 2018. Statistics in Corpus Linguistics: A Practical Guide. Cambridge University Press. https://doi.org/10.1017/9781316410899.\nDesagulier, Guillaume. 2017. Corpus Linguistics and Statistics with R: Introduction to Quantitative Methods in Linguistics (Quantitative Methods in the Humanities and Social Sciences). Springer International Publishing.\nGries, Stefan Thomas. 2021. Statistics for linguistics with R: a practical introduction (De Gruyter Mouton Textbook). 3rd revised edition. de Gruyter Mouton.\nImai, Kosuke & Nora Webb Williams. 2022. Quantitative Social Science: An Introduction in tidyverse. https://press.princeton.edu/books/paperback/9780691222288/quantitative-social-science.\nLADAL contributors. Tutorials of the Language Technology and Data Analysis Laboratory. https://ladal.edu.au/tutorials.html. Open Educational Resource.\nLevshina, Natalia. 2015. How to do linguistics with R: Data exploration and statistical analysis. John Benjamins. https://doi.org/10.1075/z.195.\nFrancom, Jerid. 2025. An Introduction to Quantitative Text Analysis for Linguistics: Reproducible Research Using R. Routledge. https://doi.org/10.4324/9781003393764. Open Access.\nR√ºhlemann, Christoph. 2020. Visual Linguistics with R: A practical introduction to quantitative Interactional Linguistics. John Benjamins. https://doi.org/10.1075/z.228.\nSchneider, Gerold. 2024. Text analytics for corpus linguistics and digital humanities: Simple R scripts and tools (Language, Data Science and Digital Humanities). Bloomsbury Academic.\nSchneider, Gerold & Max Lauber. 2020. Statistics for Linguists. https://dlf.uzh.ch/openbooks/statisticsforlinguists/. Open Educational Resource.\nSonderegger, Morgan. 2023. Regression modeling for linguistic data. MIT Press. Open Access version.\nSpeelman, Dirk, Kris Heylen & Dirk Geeraerts (eds.). 2018. Mixed-Effects Regression Models in Linguistics (Quantitative Methods in the Humanities and Social Sciences). Springer International Publishing. https://doi.org/10.1007/978-3-319-69830-4.\nWinter, Bodo. 2020. Statistics for Linguists: An Introduction Using R. Routledge. https://doi.org/10.4324/9781315165547.",
    "crumbs": [
      "Next-step resources"
    ]
  },
  {
    "objectID": "A_FurtherResources.html#further-open-educational-resources-work-in-progress",
    "href": "A_FurtherResources.html#further-open-educational-resources-work-in-progress",
    "title": "Next-step resources",
    "section": "Further Open Educational Resources (work in progress)",
    "text": "Further Open Educational Resources (work in progress)\nThis list focuses on OERs on statistics, data visualisation, reproducibility, and Open Science. For open-access textbooks on linguistics, see Roberta D‚ÄôAlessandro‚Äôs curated list: https://www.robertadalessandro.it/oa-textbooks.\n\nStatistics\n\n√áetinkaya-Rundel, Mina & Johanna Hardin. 2024. Introduction to Modern Statistics. 2nd Edition. https://openintro-ims.netlify.app/.\nClark, Michael & Seth Berry. 2025. Models Demystified: A Practical Guide from t-tests to Deep Learning. CRC Press. https://m-clark.github.io/book-of-models/.\nGelman, Andrew & Aki Vehtari. 2024. Active statistics: Stories, games, problems, and hands-on demonstrations for applied regression and causal inference. Cambridge University Press. https://avehtari.github.io/ActiveStatistics/.\nGreenwood, Mark C. 2022. Intermediate Statistics with R. Version 3.1. https://greenwood-stat.github.io/GreenwoodBookHTML/\nJan√©, Matthew B., Qinyu Xiao, Siu Kit Yeung, Mattan S. Ben-Shachar, Aaron R. Caldwell, Denis Cousineau, Daniel J. Dunleavy, Mahmoud Elsherif, Blair T. Johnson, David Moreau, Paul Riesthuis, Lukas R√∂seler, James Steele, Felipe Fontana Vieira, Mircea Zloteanu & Gilad Feldman. 2024. Guide to Effect Sizes and Confidence Intervals. https://matthewbjane.quarto.pub/guide-to-effect-sizes-and-confidence-intervals/.\nJohnson, Alicia A., Miles Q. Ott & Mine Doƒüucu. 2021. Bayes Rules! An Introduction to Applied Bayesian Modeling. CRC Press. https://www.bayesrulesbook.com/.\nLakens, Dani√´l. 2022. Improving Your Statistical Inferences. https://lakens.github.io/statistical_inferences/.\nLLaudet, Elena. 2025. Data Analysis for Social Science (DSS). https://ellaudet.github.io/dss_instructor_resources/.\nNavarro, Daniel. n.d. Learning statistics with R: A tutorial for psychology students and other beginners Version 0.6. https://learningstatisticswithr.com.\n\n\n\nData Visualization\n\nHeiss, Andrew. 2023. Data Visualization with R. https://datavizf23.classes.andrewheiss.com/.\nHoltz, Yan & Conor Healy. 2018. from Data to Viz. https://www.data-to-viz.com/.\nKabacoff, Robert. 2024. Modern Data Visualization with R. CRC Press. https://doi.org/10.1201/9781003299271. Open access version: https://rkabacoff.github.io/datavis.\nMulvaney, Nora, Audrey Wubbenhorst & Amtoj Kaur 2022. Critical Data Literacy: Strategies to Effectively Interpret and Evaluate Data Visualizations. https://pressbooks.library.torontomu.ca/criticaldataliteracy/.\nWilke, Claus O. 2019. Fundamentals of Data Visualization: A Primer on Making Informative and Compelling Figures. https://clauswilke.com/dataviz/.\n\n\n\nData Science\n\nDauber, Daniel. 2025. R for Non-Programmers (R4NP). https://r4np.com/.\nEstrellado, Ryan A., Emily A. Freer, Joshua M. Rosenberg & Isabella C. VelaÃÅsquez. 2020. Data science in education using R. 2nd edn. London, England: Routledge. https://datascienceineducation.com/.\nWickham, Hadley, Mina √áetinkaya-Rundel & Garrett Grolemund. 2023. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 2nd edn. O‚ÄôReilly Media. https://r4ds.hadley.nz/intro.\n\n\n\nReproducibility and Open Science\n\nBryan, Jennifer. n.a. Happy Git and GitHub for the useR. https://happygitwithr.com/.\nRodrigues, Bruno. 2023. Building reproducible analytical pipelines with R. https://raps-with-r.dev/.\nR√∂seler, Lukas. 2025. Open Science: Wie sich die Wissenschaft √∂ffnet: Vertrauenskrise, Replikationsrevolution, und Open Science einfach und umfangreich erkl√§rt: https://lukasroeseler.github.io/opensciencebuch/.\nTierney, Nicholas. 2025. Quarto for scientists. https://qmd4sci.njtierney.com/.\nUCSB Carpentry. 2022. Introduction to Reproducible Publications with Quarto. https://carpentry.library.ucsb.edu/reproducible-publications-quarto/.\nWittkuhn, Lennart, Konrad Pagenstedt & Liese K√ºmmerle. 2024. The Version Control Book: Track, organize and share your work: An introduction to Git for research. Last update: October 29, 2025. https://lennartwittkuhn.com/version-control-book/.\n\n\n\nComputational literacy\n\nBryan, Jennifer, Jim Heste, Shannon Pileggi & E. David Aja. n.a. What they forgot to teach you about R: The stuff you need to know about R, besides data analysis. https://rstats.wtf/.\nHealy, Kieran. 2025. Modern Plain Text Computing: https://mptc.io/content/.\n\n\n\nText Analysis\n\nBuskin, Vladimir, Thomas Brunner & Philippa Adolf. n.a. Statistics and Data Analysis for Corpus Linguists: From Theory to Practice with R. https://vbuskin.github.io/Stats_with_R/.\nReinhart, Alex & David Brown. n.a. Text Analysis for Statistics and Data Science. https://browndw.github.io/textstat_docs/.\nChen, Alvin Cheng-Hsien. 2023. Corpus Linguistics. https://alvinntnu.github.io/NTNU_ENC2036_LECTURES/.\nSilge, Julia, & David Robinson. 2017. Text Mining with R: A Tidy Approach. O‚ÄôReilly Media. https://www.tidytextmining.com.\n\n\n\nPython\n\nHuber, Florian. 2025. Introduction to Data Science with Python. v.0.23. https://florian-huber.github.io/data_science_course/book/cover.html.\nQuen√©, Hugo & Huub van den Bergh. 2024. Quantitative Methods and Statistics. Retrieved 27 Aug 2024 from https://hugoquene.github.io/QMS-EN/.\n\n\n\nGlossaries\n\nCarpetries Glosario. https://glosario.carpentries.org/en/.\nFORRT Glossary. https://forrt.org/glossary/: Parsons, Sam, Fl√°vio Azevedo, Mahmoud M. Elsherif, Samuel Guay, Owen N. Shahim, Gisela H. Govaart, Emma Norris, et al.¬†2022. A community-sourced glossary of open scholarship terms. Nature Human Behaviour. Nature 6(3). 312‚Äì318. https://doi.org/10.1038/s41562-021-01269-4.",
    "crumbs": [
      "Next-step resources"
    ]
  },
  {
    "objectID": "A_FurtherResources.html#corpora-and-other-language-resources",
    "href": "A_FurtherResources.html#corpora-and-other-language-resources",
    "title": "Next-step resources",
    "section": "Corpora and other language resources",
    "text": "Corpora and other language resources\n\nCLARIN Resource Families: https://www.clarin.eu/resource-families.\nLanguage Archive Cologne: https://lac.uni-koeln.de/.",
    "crumbs": [
      "Next-step resources"
    ]
  },
  {
    "objectID": "A_FurtherResources.html#selected-r-packages-for-the-language-sciences",
    "href": "A_FurtherResources.html#selected-r-packages-for-the-language-sciences",
    "title": "Next-step resources",
    "section": "Selected R packages for the language sciences",
    "text": "Selected R packages for the language sciences\n\nSchmitz, Dominic & Janina Esser. 2021. SfL: Statistics for Linguistics. R package version 0.4. https://github.com/dosc91/SfL.\nNini, Andrea & David van Leeuwen. 2024. idiolect: Forensic Authorship Analysis. https://github.com/andreanini/idiolect. The package contains tools for authorship analysis functions and is based on {quanteda}.\nBenoit, Kenneth, Kohei Watanabe, Haiyan Wang, Paul Nulty, Adam Obeng, Stefan M√ºller & Akitaka Matsuo. 2018. quanteda: An R package for the quantitative analysis of textual data.‚Äù¬†Journal of Open Source Software,¬†3(30), 774.¬†doi:10.21105/joss.00774. https://quanteda.io. The package is useful ‚Äúfor managing and analyzing text‚Äù (https://github.com/quanteda/quanteda)\nBenoit, Kenneth, Johannes Gruber & Akitaka Matsuo. 2023. spacyr. https://github.com/quanteda/spacyr. The package is useful for tokenization, lemmatizing tokens, parsing dependencies and identifying token sequences (https://cran.r-project.org/web/packages/spacyr/vignettes/using_spacyr.html).\nGagolewski Marek. 2022. stringi: Fast and portable character string processing in R, Journal of Statistical Software 103(2), 2022, 1‚Äì59, https://dx.doi.org/10.18637/jss.v103.i02. https://github.com/gagolews/stringi. The package is useful for work with string data and text mining (https://www.jstatsoft.org/article/view/v103i02).\nLuginbuhl, F√©lix & Hadley Wickham. 2021. polyglot. https://github.com/lgnbhl/polyglot. The package uses R as ‚Äúan interactive learning environment‚Äù (https://github.com/lgnbhl/polyglot?tab=readme-ov-file) to learn vocabulary of a foreign language.\nMontes, Mariana. 2024. glossr: Use Interlinear Glosses in R Markdown. R package version 0.8.0. https://montesmariana.github.io/glossr/. The package offers ‚Äútools to include interlinear glosses in your R Markdown or Quarto file‚Äù (https://montesmariana.github.io/glossr/).\nSchiborr, Nils Norman. 2025. multicastR: A Companion to the Multi-CAST Collection. https://cran.r-project.org/web/packages/multicastR/index.html. The package is useful for accessing the annotated database of spoken natural language from the Multi-CAST collection.\nS√∂nning, Lukas. 2025. tlda. https://github.com/lsoenning/tlda. The package includes functions for a corpus-linguistic dispersion analysis.\nTaylor, Jack. 2025. LexOPS. https://jackedtaylor.github.io/LexOPSdocs/index.html. The package is useful for generating experimentally matched stimuli.\nPackages with corpus data:\n\nBlaette, Anreas. 2021. GermaParl. https://polmine.github.io/GermaParl/. The package provides access to the annotaed data of the CWB-indexed GermaParl corpus.\nHarmon, Jon, Myfanwy Johnston, Jordan Bradford & David Robinson. 2025. gutenbergr. https://github.com/ropensci/gutenbergr. The package is useful to downloads works from Project Gutenberg with the metadata.\nKupietz, Marc & Nils Diewald. 2025. RKorAPClient: ‚ÄòKorAP‚Äô Web Service Client Package. https://github.com/KorAP/RKorAPClient. The package is useful for accessing corpora of the corpus analysis platform ‚ÄòKorAP‚Äô.\n\nPackages to work with ELAN data:\n\nMoroz, George. 2025. RCaucTile. https://cran.rstudio.com/web/packages/RCaucTile/index.html. The package is useful to create maps for the East Caucasian language family.\nShim, Ryan Soh-Eun & John Nerbonne. 2022. dialectR. https://github.com/b05102139/dialectR. The package is useful ‚Äúfor performing quantitative analyses of dialects based on categorical measures of difference and on variants of edit distance.‚Äù https://aclanthology.org/2022.vardial-1.3.pdf.",
    "crumbs": [
      "Next-step resources"
    ]
  },
  {
    "objectID": "B_CaseStudies/Poppy/ThrowVerbs.html",
    "href": "B_CaseStudies/Poppy/ThrowVerbs.html",
    "title": "16¬† ‚ÄòThrow‚Äô verbs in Spanish: RepRoducing the results of a corpus linguistics study",
    "section": "",
    "text": "Chapter overview\nThis chapter will guide you through the steps to reproduce the results of a published corpus linguistics study (Van Hulle & Enghels 2024a) using R.\nThe chapter will walk you through how to:",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>'Throw' verbs in Spanish: Rep`R`oducing the results of a corpus linguistics study</span>"
    ]
  },
  {
    "objectID": "B_CaseStudies/Poppy/ThrowVerbs.html#introducing-the-study",
    "href": "B_CaseStudies/Poppy/ThrowVerbs.html#introducing-the-study",
    "title": "16¬† ‚ÄòThrow‚Äô verbs in Spanish: RepRoducing the results of a corpus linguistics study",
    "section": "16.1 Introducing the study",
    "text": "16.1 Introducing the study\nIn this chapter, we attempt to reproduce the results of a corpus linguistics study by Van Hulle & Enghels (2024a), published as a book chapter in a volume edited by Pfadenhauer & Wiesinger (2024). The study focuses on the development of five throw verbs in Peninsular Spanish: echar, lanzar, disparar, tirar, and arrojar (Van Hulle & Enghels 2024a). These verbs have evolved into aspectual auxiliaries in inchoative constructions that convey the beginning of an event. Van Hulle & Enghels (2024a) use historical data to trace the evolution of these verbs, and contemporary data to analyse their usage in micro-constructions. Below are examples of the five throw verbs in inchoative constructions (all taken from Van Hulle & Enghels 2024a).\n\nLos nuevos rebeldes se arrojaron a atacar al sistema de control social. (‚ÄòThe new rebels started (lit. ‚Äòthrew/launched themselves‚Äô) to attack the system of social control.‚Äô)\nEl ni√±o abri√≥ los ojos y ech√≥ a correr de regreso a su casa. (‚ÄòThe child opened his eyes and started (lit. ‚Äòthrew‚Äô) to run back to his house.‚Äô)\nEl grupo de investigaci√≥n se lanz√≥ a analizar otros par√°metros. (‚ÄòThe investigation group started (lit. ‚Äòlaunched itself‚Äô) to analyse other parameters.‚Äô)\nDecid√≠ no tirarme a llorar y empec√© a buscar algo que me ayudara. (‚ÄòI decided not to start (lit. ‚Äòthrow myself‚Äô) to cry and I started to look for something that would help me.‚Äô)\nY todos dispararon a correr, sin volver la cabeza atr√°s. (‚ÄòAnd everybody started (lit. ‚Äòshot‚Äô) to run, without looking back.‚Äô)\n\n\n\n\n\n\n\nTipQuiz time!\n\n\n\nFollow the study‚Äôs DOI link and read the abstract to learn about the study‚Äôs research focus.\n\nVan Hulle, Sven & Renata Enghels. 2024. The category of throw verbs as productive source of the Spanish inchoative construction. In Katrin Pfadenhauer & Evelyn Wiesinger (eds.), Romance motion verbs in language change, 213‚Äì240. De Gruyter. https://doi.org/10.1515/9783111248141-009.\n\nQ1. What is the main focus of this study?\n\n\n\n\n\nThe development of 'throw' verbs as aspectual auxiliaries.\n\n\n\n\nThe role of spatial expressions in the process of grammaticalization.\n\n\n\n\nThe relationship between inchoative constructions and nouns of motion.\n\n\n\n\nSemantic differences between the five Spanish 'throw' verbs.\n\n\n\n\n\n\n\n\nQ2. According to the study, what semantic features help explain the connection between ‚Äòthrow‚Äô verbs and inchoative constructions?\n\n\n\n\n\nThe meaning of abruptness and the interruption of inertia.\n\n\n\n\nThe ability of 'throw' verbs to convey accuracy and control.\n\n\n\n\nThe aspect of moving towards a specified destination.\n\n\n\n\nThe shift in meaning from the concrete to the abstract.\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n\n\nIn this chapter, we will use the authors‚Äô original data to reproduce Tables 5 and 8 (Van Hulle & Enghels 2024a: 227, 232), as well as visualising the data with a series of informative line plots to facilitate interpretation.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>'Throw' verbs in Spanish: Rep`R`oducing the results of a corpus linguistics study</span>"
    ]
  },
  {
    "objectID": "B_CaseStudies/Poppy/ThrowVerbs.html#sec-ThrowData",
    "href": "B_CaseStudies/Poppy/ThrowVerbs.html#sec-ThrowData",
    "title": "16¬† ‚ÄòThrow‚Äô verbs in Spanish: RepRoducing the results of a corpus linguistics study",
    "section": "16.2 Retrieving the authors‚Äô original data",
    "text": "16.2 Retrieving the authors‚Äô original data\nIn the spirit of Open Science (see Section 1.1), Van Hulle & Enghels (2024a) have made their research data openly accessible on the Troms√∏ Repository of Language and Linguistics (TROLLing):\n\nVan Hulle, Sven & Renata Enghels. 2024. Replication Data for: ‚ÄúThe category of throw verbs as productive source of the Spanish inchoative construction.‚Äù DataverseNO. Version 1. https://doi.org/10.18710/TR2PWJ.\n\nFollow the link and read the description of the dataset. Next, scroll down the page where three different downloadable files are listed.\n\n0_ReadME_Spanish_ThrowVerbs_Inchoatives_20230413.txt This is a text ‚Äúfile which provides general information about the nature of the dataset and how the data was collected and annotated, and brief data-specific information for each file belonging to this dataset‚Äù (Van Hulle & Enghels 2024b).\nSpanish_ThrowVerbs_Inchoatives_20230413.csv This is a comma-separated file (see Section 2.5.1) which ‚Äúcontains the input data for the analysis, including the variables ‚ÄòAUX‚Äô, ‚ÄòCentury‚Äô, ‚ÄòINF‚Äô and ‚ÄòClass‚Äô, for the throw verbs arrojar, disparar, echar, lanzar and tirar‚Äù (Van Hulle & Enghels 2024b).\nSpanish_ThrowVerbs_Inchoatives_queries_20230413.txt ‚ÄúThis file specifies all corpus queries‚Äùthat were used to download the samples per auxiliary from the Spanish Web corpus (esTenTen18), that was accessed via Sketch Engine, and from the Corpus Diacr√≥nico del Espa√±ol (CORDE)‚Äù (Van Hulle & Enghels 2024b).\n\nIn corpus linguistics, it is often the case that corpora cannot be openly shared for copyright and/or data protection reasons. Instead, authors who strive to make their work transparent and reproducible can share details of the corpora that they analysed and of the specific corpus queries they used, so that the data that they share are only the results of the queries.\nAs we are interested in the frequencies retrieved from the corpora, we download the CSV file Spanish_ThrowVerbs_Inchoatives_20230413.csv.\n\n\n\n\n\n\nTipQuiz time!\n\n\n\nQ3. Where did the data for Van Hulle & Enghels (2024b)‚Äòs study on the five Spanish ‚Äôthrow‚Äô verbs come from?\n\n\n\n\n\nFor the historical data, from the Corpus Diacr√≥nico del Espa√±ol (CORDE).\n\n\n\n\nFor the contemporary data, from the European Spanish subcorpus of the Spanish Web Corpus (esTenTen18).\n\n\n\n\nFor all occurrences of 'throw' verbs, from the European Spanish Web Corpus (esTenTen18).\n\n\n\n\nFor the contemporary data, from the Spanish corpus of the Troms√∏ Repository of Language and Linguistics.\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n\nQ4. What does the term ‚Äúfalse positive‚Äù refer to in the context of this study?\n\n\n\n\n\nTokens removed due to being irrelevant to the study's focus on 'throw' verbs.\n\n\n\n\nTokens that were correctly identified as inchoative constructions.\n\n\n\n\nTags that incorrectly label nouns as infinitives or misidentify inchoative constructions.\n\n\n\n\nTokens that represent infinitives following 'a' with an incorrect auxiliary.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>'Throw' verbs in Spanish: Rep`R`oducing the results of a corpus linguistics study</span>"
    ]
  },
  {
    "objectID": "B_CaseStudies/Poppy/ThrowVerbs.html#sec-ImportingVanHulle",
    "href": "B_CaseStudies/Poppy/ThrowVerbs.html#sec-ImportingVanHulle",
    "title": "16¬† ‚ÄòThrow‚Äô verbs in Spanish: RepRoducing the results of a corpus linguistics study",
    "section": "16.3 ImpoRting the authors‚Äô original data",
    "text": "16.3 ImpoRting the authors‚Äô original data\nBefore we can import the dataset, we need to load all the packages that we will need for this project. Note that you may need to install some of these packages first (see Section 14.10 for instructions).\n\n# Loading required packages for this project\nlibrary(here)\nhere::i_am(\"B_CaseStudies/Poppy/ThrowVerbs.qmd\")\nlibrary(tidyverse)\nlibrary(xfun)\n\nNext, we import the dataset containing the number of occurrences of ‚Äòthrow‚Äô verbs in the corpora analysed in Van Hulle & Enghels (2024a) (Spanish_ThrowVerbs_Inchoatives_20230413.csv) as a new object called spanish.data. You will need to adjust the file path to match the folder structure of your computer (see Section 6.5).\n\n# Importing the Spanish verbs dataset\nspanish.data &lt;- read.csv(file = here(\"B_CaseStudies\", \"Poppy\", \"data\", \"Spanish_ThrowVerbs_Inchoatives_20230413.csv\"),\n                    header = TRUE,\n                    sep = \"\\t\",\n                    quote = \"\\\"\",\n                    dec = \".\")\n\nWe check the sanity of the imported data by visually examining the output of View(spanish.data) (Figure¬†16.1).\n\n\n\n\n\n\nFigure¬†16.1: Screenshot showing part of the dataset using the View() function.\n\n\n\nAs you can see in Figure¬†16.1, the dataset contains 2882 rows (i.e., the number of occurrences of ‚Äòthrow‚Äô verbs observed in the corpora) and 4 columns (i.e., variables describing these observations).\nThe readme file delivered with the data (0_ReadME_Spanish_ThrowVerbs_Inchoatives_20230413.txt) describes the variables as follows:\n-----------------------------------------\nDATA-SPECIFIC INFORMATION FOR: Spanish_ThrowVerbs_Inchoatives_20230413.csv\n-----------------------------------------\n\n#   Variable    Explanation\n\n1   AUX         This column contains the inchoative auxiliary. [...]\n\n2   Century     This column contains the century to which the concrete example belongs. \n\n3   INF         This column contains the infinitive observed in the filler slot of the inchoative construction. \n\n4   Class       This column contains the semantical class to which the infinitive belongs, based on the classification of ADESSE. This lexical classification classifies Spanish verbs in semantic groups, which we adopted for the annotation (http://adesse.uvigo.es/data) (@ref Garc√≠a-Miguel & Albertuz 2005). [...]\n\n¬†\nTo obtain a list of all the ‚Äòthrow‚Äô verbs included in the dataset and their total frequencies, we use the familiar count() function from {dyplr} (see Chapter 9).\n\nspanish.data |&gt; \n  count(AUX)\n\n       AUX    n\n1  arrojar  160\n2 disparar    8\n3    echar 1936\n4   lanzar  680\n5    tirar   98\n\n\nAs you can see, there are five ‚Äòthrow‚Äô verbs in the dataset: echar, lanzar, tirar, arrojar, and disparar. The most frequent one is echar.\n\n\n\n\n\n\nTipQuiz time!\n\n\n\nQ5. Which column in the dataset contains the general meaning of the verbs in the filler slot of the inchoative construction?\n\n\n\n\n\nClass\n\n\n\n\nCentury\n\n\n\n\nAUX\n\n\n\n\nINF\n\n\n\n\n\n\n\n\nQ6. Which of the following verbs is classified under the semantic category ‚ÄòDesplazamiento‚Äô (‚Äòmovement‚Äô)?\n\n\n\n\n\natacar\n\n\n\n\nhacer\n\n\n\n\nllevar\n\n\n\n\ndormir\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>'Throw' verbs in Spanish: Rep`R`oducing the results of a corpus linguistics study</span>"
    ]
  },
  {
    "objectID": "B_CaseStudies/Poppy/ThrowVerbs.html#token-absolute-frequency",
    "href": "B_CaseStudies/Poppy/ThrowVerbs.html#token-absolute-frequency",
    "title": "16¬† ‚ÄòThrow‚Äô verbs in Spanish: RepRoducing the results of a corpus linguistics study",
    "section": "16.4 Token (absolute) frequency",
    "text": "16.4 Token (absolute) frequency\nAccording to Gries & Ellis (2015: 232):\n\n‚ÄúToken frequency counts how often a particular form appears in the input.‚Äù\n\nIn Van Hulle & Enghels (2024a), token frequency refers to the number of occurrences of combinations of ‚Äòthrow‚Äô verbs and infinitives in inchoative constructions, as identified in the corpora queried for this study (see Section 16.2).\n\n16.4.1 Creating a table of token frequencies\nFirst of all, we want to find out how often each ‚Äòthrow‚Äô verb was observed in each century. To do so, we use the count() function to output the number of corpus occurrences for all possible combinations of the AUX and Century variables. Then, we pipe this output into an arrange() command to order the rows of the table by the values of the Century and AUX columns (as shown in Table¬†16.1 below), prioritising the order of the Century over the alphabetical order of the AUX. This ensures that the centuries are ordered correctly from the 13th to the 21st century, rather than being jumbled. We store this summary table (see Table¬†16.1) as a new R object called verbs.investigated.\n\nverbs.investigated &lt;- spanish.data |&gt;\n  count(AUX, Century, sort = TRUE) |&gt; \n  arrange(Century, AUX)\n\nTable¬†16.1 contains 26 rows and three columns, AUX, Century, both from the original dataset, and n which contains the number of occurrences for each combination of the Centuryand AUX variables. For example, the verb echar occurs 32, 15, and 101 times in the corpus data from the 13th, 14th, and 15th centuries respectively and so on.\n\n\n\n\nTable¬†16.1: Frequency of each Spanish ‚Äòthrow‚Äô verb in each century\n\n\n\n\n\n\nAUX\nCentury\nn\n\n\n\n\nechar\n13\n32\n\n\nechar\n14\n15\n\n\nechar\n15\n101\n\n\ntirar\n15\n2\n\n\narrojar\n16\n20\n\n\nechar\n16\n153\n\n\narrojar\n17\n47\n\n\ndisparar\n17\n3\n\n\nechar\n17\n95\n\n\narrojar\n18\n16\n\n\nechar\n18\n40\n\n\ntirar\n18\n8\n\n\narrojar\n19\n38\n\n\ndisparar\n19\n1\n\n\nechar\n19\n500\n\n\nlanzar\n19\n55\n\n\narrojar\n20\n11\n\n\ndisparar\n20\n1\n\n\nechar\n20\n500\n\n\nlanzar\n20\n125\n\n\ntirar\n20\n7\n\n\narrojar\n21\n28\n\n\ndisparar\n21\n3\n\n\nechar\n21\n500\n\n\nlanzar\n21\n500\n\n\ntirar\n21\n81\n\n\n\n\n\n\n\n\nWe will now attempt to reproduce ‚ÄúTable 4: General overview of the dataset‚Äù (Van Hulle & Enghels 2024a: 225), reprinted below as Table¬†16.2.\n\n\n\n\nTable¬†16.2: Absolute token frequency of Spanish ‚Äòthrow‚Äô verbs as reported in van Hulle & Enghels (2024a: 225)\n\n\n\n\n\n\nAUX\n13\n14\n15\n16\n17\n18\n19\n20\n21\nTotal\n\n\n\n\narrojar\n0\n0\n0\n20\n47\n16\n38\n11\n28\n160\n\n\ndisparar\n0\n0\n0\n0\n3\n0\n1\n1\n3\n8\n\n\nechar\n32\n15\n101\n153\n95\n40\n500\n500\n500\n1936\n\n\nlanzar\n0\n0\n0\n0\n0\n0\n55\n125\n500\n680\n\n\ntirar\n0\n0\n0\n0\n0\n0\n0\n7\n81\n88\n\n\nTotal\n32\n15\n101\n173\n145\n56\n594\n644\n1112\n2872\n\n\n\n\n\n\n\n\nTo reproduce this table on the basis of the data provided by the authors, we begin by reshaping the data frame spanish.data from long format to wide format using the pivot_wider() function (see Section 9.6). This function takes the arguments ‚Äúnames_from‚Äù to specify which column is to provide the names for the output columns, and ‚Äúvalues_from‚Äù to determine which column is to supply the cell values.\n\nverbs.investigated |&gt;\n  pivot_wider(names_from = Century, values_from = n) \n\n# A tibble: 5 √ó 10\n  AUX       `13`  `14`  `15`  `16`  `17`  `18`  `19`  `20`  `21`\n  &lt;chr&gt;    &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1 echar       32    15   101   153    95    40   500   500   500\n2 tirar       NA    NA     2    NA    NA     8    NA     7    81\n3 arrojar     NA    NA    NA    20    47    16    38    11    28\n4 disparar    NA    NA    NA    NA     3    NA     1     1     3\n5 lanzar      NA    NA    NA    NA    NA    NA    55   125   500\n\n\nAs you can see, this table includes a lot of NA values for the verbs for which zero occurrences were found in certain centuries. To replace missing values (NA) with a different value (here 0 to match Table¬†16.2), we can use the tidyverse function replace_na() in combination with mutate(). By applying this operation across(everything()), we ensure that the modifications are performed on all columns. We pipe this output into the arrange() function to order the rows of the table by the alphabetical order of the AUX. This is important as we will later use token.data to calculate the type/token frequency (see Table¬†16.11) for which we will merge token.data with the types.wide, which is also arranged by the alphabetical order of the AUX.\n\ntoken.data &lt;- verbs.investigated |&gt;\n  pivot_wider(names_from = Century, values_from = n) |&gt; \n  mutate(across(everything(), ~ replace_na(., 0))) |&gt;\n  arrange(AUX) \n\ntoken.data\n\n# A tibble: 5 √ó 10\n  AUX       `13`  `14`  `15`  `16`  `17`  `18`  `19`  `20`  `21`\n  &lt;chr&gt;    &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1 arrojar      0     0     0    20    47    16    38    11    28\n2 disparar     0     0     0     0     3     0     1     1     3\n3 echar       32    15   101   153    95    40   500   500   500\n4 lanzar       0     0     0     0     0     0    55   125   500\n5 tirar        0     0     2     0     0     8     0     7    81\n\n\nWe now want to add the total number of verb occurrences in each row and column of our table, as in (Van Hulle & Enghels 2024a, Table 4) (see also Table¬†16.2). We begin by calculating the total number of occurrences of each verb in token.data. We therefore first select just the columns containing numeric values.\n\nnumeric_columns &lt;- token.data |&gt; \n  select(where(is.numeric))\n\nnumeric_columns\n\n# A tibble: 5 √ó 9\n   `13`  `14`  `15`  `16`  `17`  `18`  `19`  `20`  `21`\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     0     0     0    20    47    16    38    11    28\n2     0     0     0     0     3     0     1     1     3\n3    32    15   101   153    95    40   500   500   500\n4     0     0     0     0     0     0    55   125   500\n5     0     0     2     0     0     8     0     7    81\n\n\nIt is important that we specify that we only add the values in columns representing numeric variables because if we ask R to do any mathematical operations with values of the AUX variable, we will get an error message indicating that it is impossible to add up character string values!\n\nsum(token.data$AUX)\n\nError in sum(token.data$AUX) : invalid 'type' (character) of argument\nThis is why we first created an R object that contains only the numeric variables of token.data: these are the columns that we will need to compute our sums. Next, we use the base R function rowSums() to calculate the total number of occurrences of each ‚Äòthrow‚Äô verb across all corpus texts queried, from the 13th to the 21th century.\n\nrow_sums &lt;- rowSums(numeric_columns)\n\nWe have saved the output of the rowSums() function to a new object called row_sums. This object is a numeric vector containing just the row totals.\n\nrow_sums\n\n[1]  160    8 1936  680   98\n\n\nTo check that these are in fact the correct totals, we can compare these row sums to the output of table(spanish.data$AUX) (see also Section 16.3). As the numbers match, we can now use mutate() to add row_sums as a new column to token.data.\n\ntoken.data.rowSums &lt;- token.data |&gt; \n  mutate(Total = row_sums)\n\ntoken.data.rowSums\n\n# A tibble: 5 √ó 11\n  AUX       `13`  `14`  `15`  `16`  `17`  `18`  `19`  `20`  `21` Total\n  &lt;chr&gt;    &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n1 arrojar      0     0     0    20    47    16    38    11    28   160\n2 disparar     0     0     0     0     3     0     1     1     3     8\n3 echar       32    15   101   153    95    40   500   500   500  1936\n4 lanzar       0     0     0     0     0     0    55   125   500   680\n5 tirar        0     0     2     0     0     8     0     7    81    98\n\n\nNow, let‚Äôs turn to the column totals. We can use colSums() to calculate the total number of ‚Äòthrow‚Äô verb occurrences in each century.\n\ncolumn_sums &lt;- colSums(numeric_columns)\n\ncolumn_sums\n\n  13   14   15   16   17   18   19   20   21 \n  32   15  103  173  145   64  594  644 1112 \n\n\nIn the original paper, the row of totals is labelled ‚ÄúTotal‚Äù. Furthermore, we also have a value representing the total number of verbs included in the dataset. Hence, the last row will be constructed as follows using the combine function c().\n\ntotal_row &lt;- c(\"Total\", column_sums, sum(row_sums))\n\ntotal_row\n\n             13      14      15      16      17      18      19      20      21 \n\"Total\"    \"32\"    \"15\"   \"103\"   \"173\"   \"145\"    \"64\"   \"594\"   \"644\"  \"1112\" \n        \n \"2882\" \n\n\nAgain, we can check that we have not ‚Äúlost‚Äù any verbs along the way by comparing the last value of total_row with the number of observations in our original long-format dataset.\n\nnrow(spanish.data)\n\n[1] 2882\n\n\nFinally, we use rbind() to append the total_row vector to token.data, creating a complete table with both row and column totals (Table¬†16.3).\n\n\n\nTable¬†16.3: Absolute token frequency of Spanish ‚Äòthrow‚Äô verbs based on dataset\n\n\ntoken.table.totals &lt;- rbind(token.data.rowSums, total_row)\n\ntoken.table.totals\n\n# A tibble: 6 √ó 11\n  AUX      `13`  `14`  `15`  `16`  `17`  `18`  `19`  `20`  `21`  Total\n  &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1 arrojar  0     0     0     20    47    16    38    11    28    160  \n2 disparar 0     0     0     0     3     0     1     1     3     8    \n3 echar    32    15    101   153   95    40    500   500   500   1936 \n4 lanzar   0     0     0     0     0     0     55    125   500   680  \n5 tirar    0     0     2     0     0     8     0     7     81    98   \n6 Total    32    15    103   173   145   64    594   644   1112  2882 \n\n\n\n\nIf we now compare the values of the table in the published study (reproduced as Table¬†16.2) with Table¬†16.3 based on the authors‚Äô archived data, we can see that the total number of ‚Äòthrow‚Äô verbs is only 2872, suggesting that ten verb occurrences are somehow missing in the summary table printed in the published paper. In the data frame spanish.data, these missing data points correspond to occurrences of the verb tirar, specifically two tokens from the 15th century and eight tokens from the 18th century (Table¬†16.3).\nSince Van Hulle & Enghels (2024a) focus their analyses on the other two verbs, echar and lanzar, this discrepancy is not particularly conspicuous. However, it suggests that the version of the dataset archived on TROLLing is not exactly the same as the one that the authors presumably used for the analyses presented in the 2024 paper.1\n\n\n16.4.2 Visualising the absolute frequencies in a tabular format\nAs Van Hulle & Enghels (Van Hulle & Enghels 2024a: 224) state,\n\n‚ÄúThe searches in the databases of CORDE and esTenTen18 were exhaustive, but, for reasons of feasibility, only the first 500 relevant cases were included in the final dataset.‚Äù\n\nThat is, the corpus contains much more data than what the authors could feasibly investigate. For example, the verb echar appears 799 times in the 19th century texts, 1,641 times in the 20th texts, and 10,347 times in those from the 21st century. However, in their final dataset Van Hulle & Enghels (2024a) included only 500 instances of echar in these centuries, as shown in Table¬†16.2 above.\nTo generate a table that includes the absolute token frequency in the corpus, similar to the ‚Äúabsolute token frequency‚Äù subsection of Table 5 in Van Hulle & Enghels (Van Hulle & Enghels 2024a: 227), we need to modify the values of echar in the 19th, 20th, and 21st centuries, and lanzar in the 21st century in verbs.investigated that we previously created.\nWe use the mutate() function to update specific columns and case_when() to define the conditions of the changes. For example, if the verb echar appears in the AUX variable and at the same time the value 19 is found in the Century variable, then the cell value should be changed to 799, and so on. The formula TRUE ~ n ensures that the original value is retained if no condition is met. The modified table is assigned to a new data frame object, which we name verbs.corpus.\nNext, we generate a contingency table with the altered values for those verbs by applying the pivot_wider() function as in Table¬†16.3 above. The result is displayed in Table¬†16.4 below.\n\nShow the R code to generate the table below.\nverbs.corpus &lt;- verbs.investigated |&gt;\n  # Modifying specific columns with mutate()\n  mutate(n = case_when(\n    AUX == \"echar\" & Century == 19 ~ 799,\n    AUX == \"echar\" & Century == 20 ~ 1641,\n    AUX == \"echar\" & Century == 21 ~ 10347,\n    AUX == \"lanzar\" & Century == 21 ~ 7625,\n    # Keep the original value if no condition is met\n    TRUE ~ n))\n\n# Generating a contingency table with the altered verb values using pivot_wider()\nverbs.corpus.wide &lt;- verbs.corpus |&gt;\n  pivot_wider(names_from = Century, values_from = n) |&gt;\n  mutate(across(everything(), ~ replace_na(., 0))) |&gt;\n  arrange(AUX)\n\nverbs.corpus.wide\n\n\n\n\nTable¬†16.4: Absolute token frequency of Spanish ‚Äòthrow‚Äô verbs as observed in the corpus\n\n\n\n# A tibble: 5 √ó 10\n  AUX       `13`  `14`  `15`  `16`  `17`  `18`  `19`  `20`  `21`\n  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 arrojar      0     0     0    20    47    16    38    11    28\n2 disparar     0     0     0     0     3     0     1     1     3\n3 echar       32    15   101   153    95    40   799  1641 10347\n4 lanzar       0     0     0     0     0     0    55   125  7625\n5 tirar        0     0     2     0     0     8     0     7    81\n\n\n\n\nThe difference between the corpus data (Table¬†16.4) and the final dataset (Table¬†16.3) is found only in echar in 19th, 20th, 21st, and in lanzar in 21st centuries. Following Van Hulle & Enghels (2024a), we will use the frequency of Spanish ‚Äòthrow‚Äô verbs (stored as a data frame named verbs.corpus) observed in the corpus (Table¬†16.4) to calculate the normalized frequency (see Table¬†16.7 below).",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>'Throw' verbs in Spanish: Rep`R`oducing the results of a corpus linguistics study</span>"
    ]
  },
  {
    "objectID": "B_CaseStudies/Poppy/ThrowVerbs.html#normalized-frequency",
    "href": "B_CaseStudies/Poppy/ThrowVerbs.html#normalized-frequency",
    "title": "16¬† ‚ÄòThrow‚Äô verbs in Spanish: RepRoducing the results of a corpus linguistics study",
    "section": "16.5 Normalized frequency",
    "text": "16.5 Normalized frequency\nA normalized frequency is an occurrence rate adjusted to a common base, such as per million words (pmw), to allow comparisons across datasets of different sizes.\nVan Hulle & Enghels (2024a) analyse Spanish corpora from different centuries, using the Corpus Diacr√≥nico del Espa√±ol (CORDE) for the 13th to 20th centuries and the esTenTen18 corpus for the 21st century, accessed via the Sketch Engine platform. To compare frequencies from these varying-sized corpora, we need to normalize them to ensure that large frequencies are not simply due to the corpus being larger. Van Hulle & Enghels (Van Hulle & Enghels 2024a: 227, footnote 2) explain:\n\nThe normalised token frequencies are calculated dividing the absolute token frequency by these total amounts of words, multiplied by 1 million. This number then shows how many times each micro-construction occurs per 1 million words, per century.\n\nThe formula for normalized frequency is as follows:\n\\[\nnormalized frequency = \\frac{token frequency}{total words *1000000}\n\\]\n\n16.5.1 Visualising normalized frequencies in a tabular format\nWe will now attempt to reproduce the ‚ÄúNormalized Token Frequency‚Äù sections of Tables 5 and 8 from the published paper using the authors‚Äô original data. For later comparison, the normalized frequencies as reported in Van Hulle & Enghels (Van Hulle & Enghels 2024a: 227, 232)2 are reproduced in this chapter as Table¬†16.5.\n\n\n\n\nTable¬†16.5: Normalized frequency (pmw) as reported in the published paper (Van Hulle & Enghels 2024a: Tables 5 and 8)\n\n\n\n\n\n\nAUX\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n\narrojar\n-\n-\n-\n0.4\n1.23\n1.11\n0.89\n0.19\n0.01\n\n\ndisparar\n-\n-\n-\n-\n0.08\n-\n0.02\n0.02\n0.0008\n\n\nechar\n4.09\n2\n4.43\n3.07\n2.49\n2.76\n18.7\n27.96\n2.91\n\n\nlanzar\n-\n-\n-\n-\n-\n-\n1.31\n2.15\n2.14\n\n\ntirar\n-\n-\n-\n-\n-\n-\n-\n0.12\n0.02\n\n\n\n\n\n\n\n\nThe sizes of the corpora for each century are provided in Van Hulle & Enghels (Van Hulle & Enghels 2024a: 227, footnote 12)3. We create a table of word counts for each century (Table¬†16.6) using the tibble() function from the tidyverse, by concatenating (using the c() function) the values of Words and Century and storing these as a new data frame named corpus_sizes.\n\ncorpus_sizes &lt;- tibble(Century = c(13, 14, 15, 16, 17, 18, 19, 20, 21),\n                       Words = c(7829566, 7483952, 22796824, 49912675, 38083322, 14466748, 42726881, 58686214, 3554986755))\n\n\n\n\nTable¬†16.6: Total numbers of words in the corpora\n\n\ncorpus_sizes\n\n# A tibble: 9 √ó 2\n  Century      Words\n    &lt;dbl&gt;      &lt;dbl&gt;\n1      13    7829566\n2      14    7483952\n3      15   22796824\n4      16   49912675\n5      17   38083322\n6      18   14466748\n7      19   42726881\n8      20   58686214\n9      21 3554986755\n\n\n\n\nWe will apply this formula to each verb for every century. First, we use the left_join function from {dplyr} to combine two data frames, i.e.¬†verbs.corpus, which we used to create Table¬†16.4, and corpus_sizes, which we just created (Table¬†16.6), based on the common Century column.\n\nleft_join(verbs.corpus, corpus_sizes, by = \"Century\")\n\n        AUX Century     n      Words\n1     echar      13    32    7829566\n2     echar      14    15    7483952\n3     echar      15   101   22796824\n4     tirar      15     2   22796824\n5   arrojar      16    20   49912675\n6     echar      16   153   49912675\n7   arrojar      17    47   38083322\n8  disparar      17     3   38083322\n9     echar      17    95   38083322\n10  arrojar      18    16   14466748\n11    echar      18    40   14466748\n12    tirar      18     8   14466748\n13  arrojar      19    38   42726881\n14 disparar      19     1   42726881\n15    echar      19   799   42726881\n16   lanzar      19    55   42726881\n17  arrojar      20    11   58686214\n18 disparar      20     1   58686214\n19    echar      20  1641   58686214\n20   lanzar      20   125   58686214\n21    tirar      20     7   58686214\n22  arrojar      21    28 3554986755\n23 disparar      21     3 3554986755\n24    echar      21 10347 3554986755\n25   lanzar      21  7625 3554986755\n26    tirar      21    81 3554986755\n\n\nNext, we pipe the combined data frames into a mutate() function to add a new column named normalized and apply the formula (n / Words) * 1000000 to normalize the frequency. In a second step, we round the result to two decimal places.\n\nleft_join(verbs.corpus, corpus_sizes, by = \"Century\") |&gt;\n  mutate(normalized = (n / Words) * 1000000) |&gt;\n  mutate(normalized = round(normalized,\n                            digits = 2))\n\n        AUX Century     n      Words normalized\n1     echar      13    32    7829566       4.09\n2     echar      14    15    7483952       2.00\n3     echar      15   101   22796824       4.43\n4     tirar      15     2   22796824       0.09\n5   arrojar      16    20   49912675       0.40\n6     echar      16   153   49912675       3.07\n7   arrojar      17    47   38083322       1.23\n8  disparar      17     3   38083322       0.08\n9     echar      17    95   38083322       2.49\n10  arrojar      18    16   14466748       1.11\n11    echar      18    40   14466748       2.76\n12    tirar      18     8   14466748       0.55\n13  arrojar      19    38   42726881       0.89\n14 disparar      19     1   42726881       0.02\n15    echar      19   799   42726881      18.70\n16   lanzar      19    55   42726881       1.29\n17  arrojar      20    11   58686214       0.19\n18 disparar      20     1   58686214       0.02\n19    echar      20  1641   58686214      27.96\n20   lanzar      20   125   58686214       2.13\n21    tirar      20     7   58686214       0.12\n22  arrojar      21    28 3554986755       0.01\n23 disparar      21     3 3554986755       0.00\n24    echar      21 10347 3554986755       2.91\n25   lanzar      21  7625 3554986755       2.14\n26    tirar      21    81 3554986755       0.02\n\n\nNext, we remove the n and Words columns that we no longer need here by combining the minus operator - and the select() function to ‚Äúunselect‚Äù these columns.\n\nverb.normalized &lt;- left_join(verbs.corpus,\n                           corpus_sizes,\n                           by = \"Century\") |&gt;\n  mutate(normalized = (n / Words) * 1000000) |&gt;\n  mutate(normalized = round(normalized,\n                          digits = 2)) |&gt; \n  select(-c(n, Words))\n\nverb.normalized\n\n        AUX Century normalized\n1     echar      13       4.09\n2     echar      14       2.00\n3     echar      15       4.43\n4     tirar      15       0.09\n5   arrojar      16       0.40\n6     echar      16       3.07\n7   arrojar      17       1.23\n8  disparar      17       0.08\n9     echar      17       2.49\n10  arrojar      18       1.11\n11    echar      18       2.76\n12    tirar      18       0.55\n13  arrojar      19       0.89\n14 disparar      19       0.02\n15    echar      19      18.70\n16   lanzar      19       1.29\n17  arrojar      20       0.19\n18 disparar      20       0.02\n19    echar      20      27.96\n20   lanzar      20       2.13\n21    tirar      20       0.12\n22  arrojar      21       0.01\n23 disparar      21       0.00\n24    echar      21       2.91\n25   lanzar      21       2.14\n26    tirar      21       0.02\n\n\nWe reshape the data frame verb.normalized from long format to wide format by replicating the pivot_wider() function, which we used to create Table¬†16.2 and Table¬†16.4. The new column names will be taken from Century. The values in the new column will come from normalized. As earlier, we sort the rows of the data frame according to the alphabetical order of AUX using arrange(). We convert the output into a data frame format with the as.data.frame() command and assign the output to normalized.wide.\n\nnormalized.wide &lt;- verb.normalized |&gt;\n  pivot_wider(names_from = Century, values_from = normalized) |&gt;\n  arrange(AUX) |&gt; \n  as.data.frame()\n\nnormalized.wide\n\n       AUX   13 14   15   16   17   18    19    20   21\n1  arrojar   NA NA   NA 0.40 1.23 1.11  0.89  0.19 0.01\n2 disparar   NA NA   NA   NA 0.08   NA  0.02  0.02 0.00\n3    echar 4.09  2 4.43 3.07 2.49 2.76 18.70 27.96 2.91\n4   lanzar   NA NA   NA   NA   NA   NA  1.29  2.13 2.14\n5    tirar   NA NA 0.09   NA   NA 0.55    NA  0.12 0.02\n\n\nNext, we use the is.na() function to find all missing values (NA) in the data frame normalized.wide. We replace all these NA values with a dash (\"-\") using the &lt;- operator.\n\nnormalized.wide[is.na(normalized.wide)] &lt;- \"-\"\n\nThe result can be seen in Table¬†16.7.\n\nShow the R code to generate the wide table below.\n# Use left_join to merge the dataframes\nverb.normalized &lt;- left_join(verbs.corpus,\n                             corpus_sizes,\n                             by = \"Century\") |&gt;\n# Use mutate to create a new column with n divided by words\n  mutate(normalized = (n / Words) * 1000000) |&gt;\n  mutate(normalized = round(normalized,\n                            digits = 2)) |&gt; \n# Remove raw frequencies (n) and corpus sizes (Words)\n  select(-c(n, Words))\n\n# Pivot to wide format and replace NAs with 0\nnormalized.wide &lt;- verb.normalized |&gt;\n  pivot_wider(names_from = Century, values_from = normalized) |&gt;\n  arrange(AUX) |&gt; \n  as.data.frame()\n\n# replace NA with \"-\"\nnormalized.wide[is.na(normalized.wide)] &lt;- \"-\"\n\nnormalized.wide\n\n\n\n\nTable¬†16.7: Normalized frequency of Spanish ‚Äòthrow‚Äô verbs (pmw) based on TROLLing data\n\n\n\n       AUX   13 14   15   16   17   18   19    20   21\n1  arrojar    -  -    -  0.4 1.23 1.11 0.89  0.19 0.01\n2 disparar    -  -    -    - 0.08    - 0.02  0.02 0.00\n3    echar 4.09  2 4.43 3.07 2.49 2.76 18.7 27.96 2.91\n4   lanzar    -  -    -    -    -    - 1.29  2.13 2.14\n5    tirar    -  - 0.09    -    - 0.55    -  0.12 0.02\n\n\n\n\nAt this stage, it is important to note some differences between Table¬†16.7 and Table¬†16.5. Van Hulle & Enghels (2024a) provided normalized frequencies for the three verbs arrojar, disparar and tirar only from the 16th until the 21st centuries, with no data for the 13th to 15th centuries. However, Table¬†16.7 shows the normalized frequency of tirar at 0.09 for the 15th century and 0.55 for the 18th century, filling in some missing data found in the dataset (Table¬†16.3). Additionally, there are slight differences in the normalized frequencies of lanzar for the 19th and 20th centuries, calculated as 1.29 and 2.13 based on TROLLing data and displayed in Table¬†16.7, compared to 1.31 and 2.15 in reported by Van Hulle & Enghels (2024a) and displayed in Table¬†16.5.\nAnother point to note is the apparent discrepancy in the normalized frequency of the verb disparar. In Table¬†16.5, it is reported in the original paper as 0.0008 for the 21st century, while Table¬†16.7 displays it as 0.00. However, this difference is due to Table¬†16.7 using a two-digit format; when rounded to four digits, the value would indeed be 0.0008. Thus, this is not a true discrepancy.\n\n\n16.5.2 Visualisation of the normalized frequencies as a line graph\nWe now visualize how the usage of Spanish ‚Äòthrow‚Äô verbs in inchoative constructions has evolved from the 13th to the 21st century. Although such a visualization is not provided in Van Hulle & Enghels (2024a), it is mentioned in the dataset description Van Hulle & Enghels (2024b), and it can facilitate the interpretation of the changes in normalized frequencies documented in Table¬†16.7.\nFor a diachronic study based on corpus data, it is reasonable to choose a connected scatterplot, which is essentially a combination of a scatterplot and a line plot. Using the {ggplot2} package, this entails combining a geom_point() layer on top of a geom_line() layer. The connected scatterplot provides a visualisation that helps to identify the usage of the five ‚Äòthrow‚Äô verbs in inchoative constructions over time.\nFigure¬†16.2 is created using a ggplot() function that takes the data frame verb.normalized as its first argument and the aesthetics (aes) as its second argument. For the aes argument, we choose the Century column for the x-axis and the column normalized for the y-axis. Additionally, we specify two more optional aesthetics mappings in aes: ‚Äúcolor‚Äù and ‚Äúgroup‚Äù. Both will be mapped onto the AUX variable, meaning that each verb will be displayed in a different color, and the line will be grouped by each verb over time. We also add a scale_x_continuous() layer ensures that the x-axis is labelled from the 13th to 21st century.\n\nggplot(verb.normalized, \n       aes(x = Century, \n           y = normalized, \n           color = AUX, \n           group = AUX)) +\n  geom_point() +  # Scatterplot points\n  geom_line() +   # Connect points with lines\n  scale_x_continuous(breaks = 13:21) + \n  labs(title = \"Normalized frequency of Spanish 'throw' verbs over time\",\n       x = \"Century\",\n       y = \"Normalized frequency (pmw)\",\n       color = \"Verbs\") +\n  theme_minimal()\n\n\n\n\n\n\n\nFigure¬†16.2: Normalized frequency of Spanish ‚Äòthrow‚Äô verbs over time\n\n\n\n\n\nFigure¬†16.2 shows that the verb echar is the most frequently used verb as an inchoative auxiliary, appearing in the corpus since the 13th century, while the other verbs only began to appear from the 15th century (tirar), the 16th century (arrojar), the 17th century (disparar), and the 19th century (lanzar). According to Van Hulle & Enghels (2024a: 223), the verb echar ‚Äúcan be considered as the exemplary verb which opened the pathway for other ‚Äòthrow‚Äô verbs towards the aspectual inchoative domain‚Äù. They further state, &gt; ‚ÄúThe relative token frequency increases remarkably in the 19th (n=18,70) and 20th (n=27,96) centuries, which can thus be defined as the time frames in which the micro-construction with echar was most frequently used. In the 21st century data, both micro-constructions appear with a comparable normalized token frequency in the corpus‚Äù (Van Hulle & Enghels (2024a)).\nThe normalized frequency graphic of Spanish ‚Äòthrow‚Äô verbs in Figure¬†16.2 effectively illustrates the authors‚Äô statement, providing a clear visual representation of how these verbs have evolved in usage over time.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>'Throw' verbs in Spanish: Rep`R`oducing the results of a corpus linguistics study</span>"
    ]
  },
  {
    "objectID": "B_CaseStudies/Poppy/ThrowVerbs.html#type-frequency",
    "href": "B_CaseStudies/Poppy/ThrowVerbs.html#type-frequency",
    "title": "16¬† ‚ÄòThrow‚Äô verbs in Spanish: RepRoducing the results of a corpus linguistics study",
    "section": "16.6 Type frequency",
    "text": "16.6 Type frequency\nType frequency refers to the number of unique words that can appear in a specific position, or ‚Äúslot,‚Äù within a particular grammatical construction. In the context of an inchoative construction, a specific slot refers to the position within the construction where an infinitive verb can occur.\nFor example, let‚Äôs look at the data in spanish.data (as shown in View(spanish.data) in the imported data, (see Figure¬†16.1). Here, we see a list of verb usages, with each row representing a token, or instance, of a verb in a sentence or construction. There are 15 rows, each representing a token of a verb in specific sentences.\nIf we focus on the verb lanzar, we can count a total of 7 tokens, meaning that lanzar appears 7 times in Figure¬†16.1 (in the 1st, 3rd, 5th, 6th, 7th, 8th, and 15th rows). However, among these tokens, lanzar pairs twice with hacer in an inchoative construction. Because hacer is repeated, this combination with lanzar is counted as only one type. Therefore, although we have 7 tokens (occurrences) of lanzar, we have only 6 unique types (distinct pairings) involving lanzar in the inchoative slot.\nVan Hulle & Enghels (2024a: 226) state that one may generally assume ‚Äúthat a higher type frequency indicates a higher degree of semantic productivity. As such, it is likely that a construction with a high number of different infinitives will accept even more types in the future‚Äù. Thus, type frequency is an important measure of how productive and adaptable a pattern is.\n\n16.6.1 Visualising type frequencies in a tabular format\nWe will now attempt to reproduce the type frequencies of Spanish ‚Äòthrow‚Äô verbs as displayed in the two subtables (both labelled ‚Äútype frequency‚Äù) of the original publication: one for echar and lanzar (Van Hulle & Enghels 2024a: Table 5) and the other for arrojar, disparar and tirar (Van Hulle & Enghels 2024a: Table 8). The values from these two subtables are reproduced in this chapter as Table¬†16.8.\n\n\n\n\nTable¬†16.8: Type frequency of Spanish ‚Äòthrow‚Äô verbs based on the published paper\n\n\n\n\n\n\nAUX\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n\narrojar\n-\n-\n-\n16\n34\n13\n32\n10\n27\n\n\ndisparar\n-\n-\n-\n-\n2\n-\n1\n1\n3\n\n\nechar\n8\n3\n15\n12\n12\n17\n19\n20\n20\n\n\nlanzar\n-\n-\n-\n-\n-\n-\n45\n95\n215\n\n\ntirar\n-\n-\n-\n-\n-\n-\n-\n7\n46\n\n\n\n\n\n\n\n\nBased on the object spanish.data (see Figure¬†16.1), which we created from on the TROLLing dataset Spanish_ThrowVerbs_Inchoatives_20230413.csv, we can calculate the type frequency of each ‚Äòthrow‚Äô verbs in inchoative construction (see Table¬†16.9). To achieve this, we first select the first three columns of spanish.data, i.e.¬†AUX, Century, INF. The result is a long table with the three columns and 2,882 rows. We check the first six lines of the table using the head() function.\n\ntype.token &lt;- select(spanish.data, 1:3)\nhead(type.token)\n\n     AUX Century      INF\n1 lanzar      21   llevar\n2  echar      21   dormir\n3 lanzar      21   probar\n4  tirar      21   atacar\n5 lanzar      21    hacer\n6 lanzar      21 estudiar\n\n\nWe then calculate the number of unique combinations among these variables using the distinct() function. We pipe the output into a group_by() function, which allows us to group all the corpus occurences according to Century and AUX. Then, using the summarize() function, we create a new column called Types with the number (n) of types corresponding to each combination of Century and AUX. We convert the output into a data frame format using as.data.frame() and assign it to a new R object called verb.types.\n\nverb.types &lt;- type.token |&gt; \n  distinct(Century, AUX, INF) |&gt; \n  group_by(Century, AUX) |&gt; \n  summarize(Types = n()) |&gt; \n  as.data.frame()\n\nWe reshape the object verb.types from long format to wide format using the pivot_wider() function. The new column names will be taken from Century. The values in the new column will come from Types. We use mutate(across(everything()) to modify all columns at once. The modification entails replacing all missing values (NA) with 0 using the replace_na function. Next, we sort the rows of the data frame in the alphabetical order of the AUX column using the arrange() function. We assign the output to types.wide.\n\ntypes.wide &lt;- verb.types |&gt;\n  pivot_wider(names_from = Century, values_from = Types) |&gt;\n  mutate(across(everything(), ~ replace_na(., 0))) |&gt;\n  arrange(AUX)\n\nThe result is displayed as Table¬†16.9.\n\nShow the R code to generate the table below.\n# Selecting the first three columns of spanish.data\n# Creating a type frequency table labelled as type.token\ntype.token &lt;- select(spanish.data, 1:3)\n\n# Calculating distinct combinations of Century, AUX, and INF using the distinct() function\n# Grouping data by Century and AUX using the group_by() function\n# Creating a new column Types with the summarize() function,\n# Returning the count (n) for each group\n\nverb.types &lt;- type.token |&gt; \n  distinct(Century, AUX, INF) |&gt; \n  group_by(Century, AUX) |&gt; \n  summarize(Types = n())\n\n# Converting verb.types to a data frame using the as.data.frame() function\nverb.types &lt;- as.data.frame(verb.types)\n\n# Using the pivot_wider() function to create a contigency table\ntypes.wide &lt;- verb.types |&gt;\n  pivot_wider(names_from = Century, values_from = Types) |&gt;\n  mutate(across(everything(), ~ replace_na(., 0))) |&gt;\n  arrange(AUX)\n\n# Printing the table in elegantly formatted HTML format\ntypes.wide\n\n\n\n\nTable¬†16.9: Type frequency of Spanish ‚Äòthrow‚Äô verbs based on data\n\n\n\n# A tibble: 5 √ó 10\n  AUX       `13`  `14`  `15`  `16`  `17`  `18`  `19`  `20`  `21`\n  &lt;chr&gt;    &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1 arrojar      0     0     0    16    34    13    32    10    27\n2 disparar     0     0     0     0     2     0     1     1     3\n3 echar        8     3    15    12    12    17    18    22    20\n4 lanzar       0     0     0     0     0     0    44    95   215\n5 tirar        0     0     2     0     0     7     0     7    46\n\n\n\n\nHere, too, we observe several discrepancies between Table¬†16.9 and Table¬†16.8. The discrepancies involve the type frequencies of echar for the 19th and 20th centuries, reported as 19 and 20 in the original paper, and of lanzar for the 19th century, originally reported as 45 (Table¬†16.8). Other discrepancies include the type frequencies of the verb tirar in the 15th and 18th centuries, which are two and seven according to the TROLLing data, but both reported as zero in the published study (see also Table¬†16.2).\n\n\n16.6.2 Visualisation of the type frequency as a line graph\nUsing the type frequency data that we calculated above (see Table¬†16.9), we can largely recycle the ggplot() code that we used to create Figure¬†16.2.\n\n\nShow the R code to generate the graph below.\n# Using the ggplot() function with the dataframe verb.types\n# The y-axis represents the type frequency\nggplot(verb.types, \n       aes(x = Century, \n           y = Types, \n           color = AUX, \n           group = AUX)) +\n  geom_point() +  # Scatterplot points\n  geom_line() +   # Connect points with lines\n  scale_x_continuous(breaks = 13:21) + \n  labs(title = \"Productivity of Spanish 'throw' verbs over time\",\n       x = \"Century\",\n       y = \"Type frequency\",\n       color = \"Verbs\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure¬†16.3: Type frequency of Spanish ‚Äòthrow‚Äô verbs over time\n\n\n\n\n\nThe connected scatterplot displayed in Figure¬†16.3 provides a visualisation that helps identify the productivity of the five ‚Äòthrow‚Äô verbs in inchoative constructions with respect to their type frequency.\nAs Van Hulle & Enghels (2024a: 226) state:\n\n‚ÄúIn general, it is assumed that a higher type frequency indicates a higher degree of semantic productivity. As such, it is likely that a construction with a high number of different infinitives will accept even more types in the future. In this sense, type frequency constitutes an important parameter to measure the extending productivity of a construction‚Äù.\n\nHowever, we should interpret this graphic carefully, keeping in mind that absence of evidence is not evidence of absence. Notably, there is almost no data for disparar, which raises the question: in the real world, is this verb rarely used in an inchoative construction, or are there simply no examples in the corpus?\n\n\n\n\n\n\nTipQuiz time!\n\n\n\nQ7. Which line of code can you add in the ggplot() code above to change the color scheme of the line graph in Figure¬†16.3 to a color-blind friendly one? Click on ‚ÄúShow the R code to generate the graph below.‚Äù to see the code for Figure¬†16.3.\n\n\n\n+ scale_color_viridis_d()\n\n\n\n\n+ scale_color_continuous()\n\n\n\n\n+ scale_color_viridis_c()\n\n\n\n\n+ scale_color_viridis_b()\n\n\n\n\n+ scale_color_blind()\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n Q8. Alternatively, we could opt for a black-and-white solution like below. How can we adapt the ggplot() code from Figure¬†16.3 to achieve this?\n\n\n\n\n\n\n\n\n\n\n\n\nChange `geom_line()` to `geom_line(linetype = AUX)`\n\n\n\n\nChange `aes(color = AUX)` to `aes(linetype = AUX)`\n\n\n\n\nChange `aes(color = AUX)` to `aes(line = AUX)`\n\n\n\n\nChange `geom_line()` to `geom_dotted()`",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>'Throw' verbs in Spanish: Rep`R`oducing the results of a corpus linguistics study</span>"
    ]
  },
  {
    "objectID": "B_CaseStudies/Poppy/ThrowVerbs.html#typetoken-ratio-ttr",
    "href": "B_CaseStudies/Poppy/ThrowVerbs.html#typetoken-ratio-ttr",
    "title": "16¬† ‚ÄòThrow‚Äô verbs in Spanish: RepRoducing the results of a corpus linguistics study",
    "section": "16.7 Type/token ratio (TTR)",
    "text": "16.7 Type/token ratio (TTR)\nAs stated by Van Hulle & Enghels (2024a: 226), the ‚Äútype/token ratio measures the realized productivity‚Äù of each verb. Furthermore,\n\nsince type frequency depends to some extent on token frequency (the more tokens, the more opportunities for different types to occur), the two must be put into some kind of relationship. The simplest measure suggested in the literature is the type/token ratio [‚Ä¶] (Stefanowitsch & Flach 2017: 118)\n\nType/token ratios (TTR) can range from zero and one. A TTR of zero indicates that there are no examples of the type in the given occurrences, while a TTR of one signifies that all types are unique to those given occurrences.\n\\[\nTTR = \\frac{types}{tokens}\n\\]\nAs type/token ratios depend on corpus size, Van Hulle & Enghels (2024a: 227) explain that:\n\n‚ÄúTo be representative, the measures of type/token and hapax/token ratio are calculated on a maximum of 500 tokens per auxiliary. Specifically, for echar in the 19th, 20th and 21st century and for lanzar in the 21st century, token frequency is reduced to 500.‚Äù\n\n\n16.7.1 Visualising type/token ratios in a tabular format\nWe will now attempt to reproduce the type/token ratio of Spanish throw verbs based on two subtables (both labelled ‚Äútype/token ratio‚Äù): one for echar and lanzar (Table 5 from Van Hulle & Enghels 2024a: 227) and the other for arrojar, disparar and tirar (Table 8 from Van Hulle & Enghels 2024a: 232), which are reproduced in this chapter as Table¬†16.10.\n\n\n\n\nTable¬†16.10: Type/token ratio of Spanish ‚Äòthrow‚Äô verbs over time based on Van Hulle & Enghels (2024a)\n\n\n\n\n\n\nAUX\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n\narrojar\n-\n-\n-\n0.8\n0.72\n0.81\n0.84\n0.91\n0.96\n\n\ndisparar\n-\n-\n-\n-\n0.67\n-\n1\n1.00\n1.00\n\n\nechar\n0.25\n0.2\n0.15\n0.08\n0.13\n0.43\n0.04\n0.04\n0.04\n\n\nlanzar\n-\n-\n-\n-\n-\n-\n0.8\n0.75\n0.43\n\n\ntirar\n-\n-\n-\n-\n-\n-\n-\n1.00\n0.57\n\n\n\n\n\n\n\n\nTo calculate type/token ratios, we first create two matching wide tables, one for the token frequencies, and another for the type frequencies. We can use the wide table labelled token.data from Table¬†16.3, and the wide table labelled types.wide from Table¬†16.9 as they are ordered in exactly the same way (you can check this by comparing their structures using the str() function).\nFirst, we create a new data frame using the data.frame() function. This data frame will take its first column from token.data, which contains the auxiliary verbs (AUX). We access this column using token.data[, 1].4\nNext, we calculate the type/token ratio. This is done by dividing the numeric values in types.wide (i.e., all columns except the first) by the corresponding values in token.data. To this end, we use the notation types.wide[, -1] / token.data[, -1]. The [, -1] indicates that we take all columns except the first one. We exclude the first column because it contains non-numeric values (the AUX column).\nFinally, we combine these components into our new data frame. We include the AUX column as the first column by selecting it with the command token.data[, 1]. To ensure that the column names remain unchanged, we set check.names = FALSE in the data.frame() function. This prevents R from altering the original column names, keeping them exactly as they are in token.data. We also round() the values of all numeric columns to just two decimals.\n\ndata.frame(token.data[, 1], \n           types.wide[, -1] / token.data[, -1],\n           check.names = FALSE) |&gt; \n    mutate(across(where(is.numeric), round, digits = 2))\n\n       AUX   13  14   15   16   17   18   19   20   21\n1  arrojar  NaN NaN  NaN 0.80 0.72 0.81 0.84 0.91 0.96\n2 disparar  NaN NaN  NaN  NaN 0.67  NaN 1.00 1.00 1.00\n3    echar 0.25 0.2 0.15 0.08 0.13 0.42 0.04 0.04 0.04\n4   lanzar  NaN NaN  NaN  NaN  NaN  NaN 0.80 0.76 0.43\n5    tirar  NaN NaN 1.00  NaN  NaN 0.88  NaN 1.00 0.57\n\n\nOur table contains a lot NaN values. In these cells of the table, the number of tokens was zero and, as a consequence, the number of types was also zero. As it is mathematically impossible to divide zero by zero, R returns NaN values instead. To replace these NaN values to dashes (\"-\") to match the formatting of the published tables, we use the base R function is.na().\n\ntype.token1 &lt;- data.frame(token.data[, 1], \n\n                          types.wide[, -1] / token.data[, -1],\n\n                          check.names = FALSE) |&gt; \n    mutate(across(where(is.numeric), round, digits = 2))\n\ntype.token1[is.na(type.token1)] &lt;- \"-\"\n\nThe result is saved as type.token1 and is displayed below as Table¬†16.11.\n\n\n\nTable¬†16.11: Type/token ratio of Spanish ‚Äòthrow‚Äô verbs over time based on data\n\n\n\n       AUX   13  14   15   16   17   18   19   20   21\n1  arrojar    -   -    -  0.8 0.72 0.81 0.84 0.91 0.96\n2 disparar    -   -    -    - 0.67    -    1 1.00 1.00\n3    echar 0.25 0.2 0.15 0.08 0.13 0.42 0.04 0.04 0.04\n4   lanzar    -   -    -    -    -    -  0.8 0.76 0.43\n5    tirar    -   -    1    -    - 0.88    - 1.00 0.57\n\n\n\n\nComparing Table¬†16.10 and Table¬†16.11, we find some minor discrepancies between the type/token ratios presented in the published paper and those calculated on the basis of the TROLLing data. The type/token ratio of the verb tirar in the 15th and 18th centuries, are reported as 0 and 0 in the published paper (see also Table¬†16.10), but as 1 and 0.88 in Table¬†16.11. These differences correspond to the discrepancies already identified when calculating the token frequencies (see Section 16.4.1).\nThe other (very minor) discrepancies involve the type/token ratio of lanzar for the 20th century, reported as 0.75 in Table¬†16.10 but as 0.76 in Table¬†16.11 and echar in the 18th century, reported as 0.43 (see Table¬†16.10), while Table¬†16.11 displays it as 0.42. These differences arise from the fact that Van Hulle & Enghels (2024a) presumably did not use R for their calculations. The type/token ratio of echar in the 18th century is actually 0.4250, which is rounded as 0.43 by Van Hulle & Enghels (2024a), but as 0.42 by R (see Table¬†16.11). This somewhat confusing rounding behaviour is explained in the help file of the round() function:\n\n‚ÄúNote that for rounding off a 5, the IEC 60559 standard (see also ‚ÄòIEEE 754‚Äô) is expected to be used, ‚Äògo to the even digit‚Äô. Therefore round(0.5) is 0 and round(-1.5) is -2. However, this is dependent on OS services and on representation error (since e.g.¬†0.15 is not represented exactly, the rounding rule applies to the represented number and not to the printed number, and so round(0.15, 1) could be either 0.1 or 0.2).‚Äù\n\n\n\n16.7.2 Visualising the type/token ratios as a line graph\nFor the visualisation of the type/token ratios (Figure¬†16.4), we create a new data frame. We start by merging the data frames verbs.investigated and verb.types using the left_join() function, ensuring that the Century and AUX variables are aligned. This results in a single data frame, which we save as type_token.\n\ntype_token &lt;- left_join(verbs.investigated, verb.types, \n                        by = c(\"Century\", \"AUX\"))\n\nNext, we calculate the type/token ratios by adding a new column, TypeTokenRatio, to the type_token data frame using the mutate() function, which applies the type/token ratio formula to each row. Finally, we use the arrange() function to sort the data by Century and AUX organizing the results chronologically by Century and alphabetically by the verb type (AUX).\n\ntype.token.ratio &lt;- type_token  |&gt; \n  mutate(TypeTokenRatio = Types / n) |&gt; \n  arrange(Century, AUX)\n\nThe output is a table with 26 rows and five columns: AUX, Century, n, and Types and TypeTokenRatio. We can display the first six rows of the table using the head() function.\n\nhead(type.token.ratio)\n\n      AUX Century   n Types TypeTokenRatio\n1   echar      13  32     8     0.25000000\n2   echar      14  15     3     0.20000000\n3   echar      15 101    15     0.14851485\n4   tirar      15   2     2     1.00000000\n5 arrojar      16  20    16     0.80000000\n6   echar      16 153    12     0.07843137\n\n\nNow we can use the type.token.ratio data frame with the same ggplot code that we used to create Figure¬†16.2 and Figure¬†16.3, allowing us to visualize the type/token ratios of Spanish ‚Äòthrow‚Äô verbs over time as Figure¬†16.4.\n\n\nShow the R code to generate the table below.\nggplot(type.token.ratio, \n       aes(x = Century, \n           y = TypeTokenRatio, \n           color = AUX, \n           group = AUX)) +\n  geom_point() +  # Scatterplot points\n  geom_line() +   # Connect points with lines\n  scale_x_continuous(breaks = 13:21) + \n  labs(title = \"The productivity of Spanish 'throw' verbs over time\",\n       x = \"Century\",\n       y = \"type/token ratio\",\n       color = \"Verbs\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure¬†16.4: type/token ratio of Spanish ‚Äòthrow‚Äô Verbs Over Time\n\n\n\n\n\nAccording to Van Hulle & Enghels (2024a), the verb lanzar is considered the ‚Äúmost productive auxiliary‚Äù due to its high type/token ratio values, despite only appearing from the 19th century onward, and because it ‚Äúwas able to incorporate a more varied set of infinitives‚Äù (Van Hulle & Enghels 2024a: 228). In contrast, the type/token ratio for echar is comparably low. However, as Van Hulle & Enghels (2024a: 228) state:\n\n‚Äú[‚Ä¶] the type/token ratio for the micro-construction with echar is quite stable until it considerably drops from the 19th century on (n=0.04). This means that, although speakers used the construction more frequently, this was mainly done with a limited group of infinitives [‚Ä¶]‚Äù\n\nAdditionally, the verbs disparar and tirar have a type/token ratio of one for the 19th and 21st centuries, and the 15th and 20th centuries, respectively. This is due to the high hapax value, i.e., ‚Äúthe number of types that appear only once in a text or corpus‚Äù for the respective verbs (Van Hulle & Enghels 2024a: 226). In the cases of disparar and tirar, each hapax refers to only one occurrence.\nThe above plot (Figure¬†16.4) shows more clearly that arrojar, not lanzar, is actually the most semantically productive verb. When compared with echar, as the authors of the published paper have done, lanzar does indeed appear more semantically productive. However, as Van Hulle & Enghels (2024a) note, ‚Äútype/token ratio measures the realized productivity.‚Äù Based on this measure, arrojar is even more productive than lanzar, as this graphic (Figure¬†16.4) clearly illustrates. Van Hulle & Enghels (2024a) do not provide such a visualization, but this chapter has shown that it can aid in interpreting the realized productivity measured by the type/token ratio.\n\n\n\n\n\n\nTipQuiz time!\n\n\n\nQ9. What issue arises when interpreting the productivity of Spanish ‚Äòthrow‚Äô verbs over time on the basis of Figure¬†16.4?\n\n\n\n\n\nSome lines suggest a linear increase or decrease in productivity over several centuries when there were, in fact, zero occurrences of that verb in one of these centuries.\n\n\n\n\nThe graph incorrectly suggests that there were zero occurrences of some verbs in earlier centuries.\n\n\n\n\nThe graph incorrectly suggests that type/token ratios cannot go beyond 1.00.\n\n\n\n\nThe graph incorrectly suggests a decrease in productivity for all verbs.\n\n\n\n\nThe graph makes it difficult to compare the productivity of verbs across each century.\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n Q10. Based on the type/token ratios displayed in Figure¬†16.4, which ‚Äòthrow‚Äô verb appears to be the least productive one?\n\n\n\n\n\narrojar\n\n\n\n\nechar\n\n\n\n\nlanzar\n\n\n\n\ndisparar\n\n\n\n\ntirar\n\n\n\n\n\n\n\n\n Q11. Based on the type/token ratios displayed in Figure¬†16.4, which ‚Äòthrow‚Äô verb appears to be the most productive one since the 19th century?\n\n\n\n\n\nlanzar\n\n\n\n\narrojar\n\n\n\n\nechar\n\n\n\n\ndisparar\n\n\n\n\ntirar",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>'Throw' verbs in Spanish: Rep`R`oducing the results of a corpus linguistics study</span>"
    ]
  },
  {
    "objectID": "B_CaseStudies/Poppy/ThrowVerbs.html#conclusion",
    "href": "B_CaseStudies/Poppy/ThrowVerbs.html#conclusion",
    "title": "16¬† ‚ÄòThrow‚Äô verbs in Spanish: RepRoducing the results of a corpus linguistics study",
    "section": "16.8 Conclusion",
    "text": "16.8 Conclusion\nYou have successfully completed 0 out of 11 quiz questions in this chapter.\nThis chapter attempted to reproduce the results of a corpus linguistics study that explores the evolution of five throw verbs in Peninsular Spanish (echar, lanzar, disparar, tirar, and arrojar) into aspectual auxiliaries in inchoative constructions that express the beginning of an event. The authors of the original study, Van Hulle & Enghels (2024a), used historical and contemporary data to analyse the development and usage of these verbs, making their research data openly accessible. As part of this chapter, we identified some discrepancies between the results we obtained on the basis of the authors‚Äô data (Van Hulle & Enghels 2024b) and those published in the 2024 study, indicating that the version of the dataset uploaded onto TROLLing does not exactly match the one used for the published results.\nWe have also created some new data visualizations based on the authors‚Äô uploaded data, which uncover patterns in the evolution of Spanish inchoative constructions that might not be immediately apparent through the examination of the tabular results alone. These visualizations underscore the effectiveness of graphical representation as a tool for understanding linguistic shifts over time‚Äîan approach not employed in the original study.\n\n\n\n\n\n\nNoteHow to cite this chapter\n\n\n\n\n\nThis is a case study chapter of the web version of the textbook ‚ÄúData Analysis for the Language Sciences: A very gentle introduction to statistics and data visualisation in R‚Äù by Elen Le Foll.\nPlease cite the current version of this chapter as:\n\n\nSiahaan, Poppy. 2024. ‚ÄòThrow‚Äô verbs in Spanish: Reproducing the results of a corpus linguistics study. In Elen Le Foll (Ed.), Data Analysis for the Language Sciences: A very gentle introduction to statistics and data visualisation in R. Open Educational Resource. https://elenlefoll.github.io/RstatsTextbook/ (accessed DATE).",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>'Throw' verbs in Spanish: Rep`R`oducing the results of a corpus linguistics study</span>"
    ]
  },
  {
    "objectID": "B_CaseStudies/Poppy/ThrowVerbs.html#references",
    "href": "B_CaseStudies/Poppy/ThrowVerbs.html#references",
    "title": "16¬† ‚ÄòThrow‚Äô verbs in Spanish: RepRoducing the results of a corpus linguistics study",
    "section": "References",
    "text": "References\n[1] S. T. Gries and N. C. Ellis. ‚ÄúStatistical Measures for Usage-Based Linguistics‚Äù. In: Language Learning 65.S1 (2015). _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/lang.12119, p.¬†228‚Äì255. ISSN: 1467-9922. DOI: 10.1111/lang.12119. https://onlinelibrary.wiley.com/doi/abs/10.1111/lang.12119.\n[2] K. Pfadenhauer and E. Wiesinger, ed.¬†Romance motion verbs in language change: Grammar, lexicon, discourse. De Gruyter, Jul.¬†2024. ISBN: 978-3-11-124814-1. DOI: 10.1515/9783111248141. https://www.degruyter.com/document/doi/10.1515/9783111248141/html.\n[3] A. Stefanowitsch and S. Flach. ‚ÄúThe corpus-based perspective on entrenchment‚Äù. In: Entrenchment and the psychology of language learning: How we reorganize and adapt linguistic knowledge. Ed. by H. Schmid. De Gruyter, 2017, p.¬†101‚Äì127. ISBN: 978-3-11-034130-0 978-3-11-034142-3. DOI: 10.1037/15969-006. https://content.apa.org/books/15969-006.\n[4] S. Van Hulle and R. Enghels. Replication Data for: ‚ÄúThe category of throw verbs as productive source of the Spanish inchoative construction. DataverseNO, V1.‚Äù. 2024. DOI: 10.18710/TR2PWJ. https://dataverse.no/dataset.xhtml?persistentId=doi:10.18710/TR2PWJ.\n[5] S. Van Hulle and R. Enghels. ‚ÄúThe category of throw verbs as productive source of the Spanish inchoative construction‚Äù. In: Romance motion verbs in language change. Ed. by K. Pfadenhauer and E. Wiesinger. De Gruyter, Jul.¬†2024, p.¬†213‚Äì240. ISBN: 978-3-11-124814-1. DOI: 10.1515/9783111248141-009. https://www.degruyter.com/document/doi/10.1515/9783111248141-009/html.\n\nPackages used in this chapter\n\n\nR version 4.5.0 (2025-04-11)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.4.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/Brussels\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] knitcitations_1.0.12 xfun_0.53            lubridate_1.9.4     \n [4] forcats_1.0.0        stringr_1.5.1        dplyr_1.1.4         \n [7] purrr_1.0.4          readr_2.1.5          tidyr_1.3.1         \n[10] tibble_3.2.1         ggplot2_3.5.2        tidyverse_2.0.0     \n[13] here_1.0.1           kableExtra_1.4.0     checkdown_0.0.13    \n[16] webexercises_1.1.0  \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.6       htmlwidgets_1.6.4  tzdb_0.5.0         vctrs_0.6.5       \n [5] tools_4.5.0        generics_0.1.4     RefManageR_1.4.0   pkgconfig_2.0.3   \n [9] RColorBrewer_1.1-3 lifecycle_1.0.4    compiler_4.5.0     farver_2.1.2      \n[13] textshaping_1.0.1  codetools_0.2-20   litedown_0.7       htmltools_0.5.8.1 \n[17] yaml_2.3.10        pillar_1.10.2      commonmark_2.0.0   tidyselect_1.2.1  \n[21] digest_0.6.37      stringi_1.8.7      labeling_0.4.3     bibtex_0.5.1      \n[25] rprojroot_2.1.1    fastmap_1.2.0      grid_4.5.0         cli_3.6.5         \n[29] magrittr_2.0.3     utf8_1.2.5         withr_3.0.2        scales_1.4.0      \n[33] backports_1.5.0    timechange_0.3.0   rmarkdown_2.29     httr_1.4.7        \n[37] hms_1.1.3          evaluate_1.0.3     knitr_1.50         viridisLite_0.4.2 \n[41] markdown_2.0       rlang_1.1.6        Rcpp_1.0.14        glue_1.8.0        \n[45] xml2_1.3.8         svglite_2.2.1      rstudioapi_0.17.1  jsonlite_2.0.0    \n[49] R6_2.6.1           plyr_1.8.9         systemfonts_1.2.3 \n\n\n\n\nPackage references\n[1] G. Grolemund and H. Wickham. ‚ÄúDates and Times Made Easy with lubridate‚Äù. In: Journal of Statistical Software 40.3 (2011), pp. 1-25. https://www.jstatsoft.org/v40/i03/.\n[2] G. Moroz. checkdown: Check-Fields and Check-Boxes for rmarkdown. R package version 0.0.12. 2023. https://agricolamz.github.io/checkdown/.\n[3] G. Moroz. Create check-fields and check-boxes with checkdown. 2020. https://CRAN.R-project.org/package=checkdown.\n[4] K. M√ºller. here: A Simpler Way to Find Your Files. R package version 1.0.1. 2020. https://here.r-lib.org/.\n[5] K. M√ºller and H. Wickham. tibble: Simple Data Frames. R package version 3.2.1. 2023. https://tibble.tidyverse.org/.\n[6] R Core Team. R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing. Vienna, Austria, 2024. https://www.R-project.org/.\n[7] V. Spinu, G. Grolemund, and H. Wickham. lubridate: Make Dealing with Dates a Little Easier. R package version 1.9.3. 2023. https://lubridate.tidyverse.org.\n[8] H. Wickham. forcats: Tools for Working with Categorical Variables (Factors). R package version 1.0.0. 2023. https://forcats.tidyverse.org/.\n[9] H. Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2016. ISBN: 978-3-319-24277-4. https://ggplot2.tidyverse.org.\n[10] H. Wickham. stringr: Simple, Consistent Wrappers for Common String Operations. R package version 1.5.1. 2023. https://stringr.tidyverse.org.\n[11] H. Wickham. tidyverse: Easily Install and Load the Tidyverse. R package version 2.0.0. 2023. https://tidyverse.tidyverse.org.\n[12] H. Wickham, M. Averick, J. Bryan, et al.¬†‚ÄúWelcome to the tidyverse‚Äù. In: Journal of Open Source Software 4.43 (2019), p.¬†1686. DOI: 10.21105/joss.01686.\n[13] H. Wickham, W. Chang, L. Henry, et al.¬†ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. R package version 3.5.1. 2024. https://ggplot2.tidyverse.org.\n[14] H. Wickham, R. Fran√ßois, L. Henry, et al.¬†dplyr: A Grammar of Data Manipulation. R package version 1.1.4. 2023. https://dplyr.tidyverse.org.\n[15] H. Wickham and L. Henry. purrr: Functional Programming Tools. R package version 1.0.2. 2023. https://purrr.tidyverse.org/.\n[16] H. Wickham, J. Hester, and J. Bryan. readr: Read Rectangular Text Data. R package version 2.1.5. 2024. https://readr.tidyverse.org.\n[17] H. Wickham, D. Vaughan, and M. Girlich. tidyr: Tidy Messy Data. R package version 1.3.1. 2024. https://tidyr.tidyverse.org.\n[18] Y. Xie. Dynamic Documents with R and knitr. 2nd. ISBN 978-1498716963. Boca Raton, Florida: Chapman and Hall/CRC, 2015. https://yihui.org/knitr/.\n[19] Y. Xie. ‚Äúknitr: A Comprehensive Tool for Reproducible Research in R‚Äù. In: Implementing Reproducible Computational Research. Ed. by V. Stodden, F. Leisch and R. D. Peng. ISBN 978-1466561595. Chapman and Hall/CRC, 2014.\n[20] Y. Xie. knitr: A General-Purpose Package for Dynamic Report Generation in R. R package version 1.47. 2024. https://yihui.org/knitr/.\n[21] Y. Xie. xfun: Supporting Functions for Packages Maintained by Yihui Xie. R package version 0.45. 2024. https://github.com/yihui/xfun.\n[22] H. Zhu. kableExtra: Construct Complex Table with kable and Pipe Syntax. R package version 1.4.0. 2024. http://haozhu233.github.io/kableExtra/.\n\n\n\n\nGries, Stefan Th. & Nick C. Ellis. 2015. Statistical measures for usage-based linguistics. Language Learning 65(S1). 228‚Äì255. https://doi.org/10.1111/lang.12119.\n\n\nPfadenhauer, Katrin & Evelyn Wiesinger (eds.). 2024. Romance motion verbs in language change: Grammar, lexicon, discourse. De Gruyter. https://doi.org/10.1515/9783111248141.\n\n\nStefanowitsch, Anatol & Susanne Flach. 2017. The corpus-based perspective on entrenchment. In Hans-J√∂rg Schmid (ed.), Entrenchment and the psychology of language learning: How we reorganize and adapt linguistic knowledge, 101‚Äì127. De Gruyter. https://doi.org/10.1037/15969-006.\n\n\nVan Hulle, Sven & Renata Enghels. 2024a. The category of throw verbs as productive source of the spanish inchoative construction. In Katrin Pfadenhauer & Evelyn Wiesinger (eds.), Romance motion verbs in language change, 213‚Äì240. De Gruyter. https://doi.org/10.1515/9783111248141-009.\n\n\nVan Hulle, Sven & Renata Enghels. 2024b. TROLLing replication data for: ‚ÄúThe category of throw verbs as productive source of the spanish inchoative construction. DataverseNO, V1.‚Äù https://doi.org/10.18710/TR2PWJ.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>'Throw' verbs in Spanish: Rep`R`oducing the results of a corpus linguistics study</span>"
    ]
  },
  {
    "objectID": "B_CaseStudies/Poppy/ThrowVerbs.html#footnotes",
    "href": "B_CaseStudies/Poppy/ThrowVerbs.html#footnotes",
    "title": "16¬† ‚ÄòThrow‚Äô verbs in Spanish: RepRoducing the results of a corpus linguistics study",
    "section": "",
    "text": "We contacted the first and corresponding author of the paper. They responded and confirmed that these discrepancies were likely due to small changes that were made to the dataset that was ultimately used in the analyses published in Van Hulle & Enghels (2024a). These changes were deemed necessary when either additional occurrences of inchoative constructions were found in the corpora, or false positives (i.e.¬†occurrences of ‚Äòthrow‚Äô verbs that did not enter such constructions) were later identified in the dataset. The author did not provide us with the final dataset that was used in the reported analyses.‚Ü©Ô∏é\nThe normalized frequencies of echar and lanzar are found in Table 5 (Van Hulle & Enghels 2024a: 227), whilst those for arrojar, disparar and tirar are displayed in Table 8 (Van Hulle & Enghels 2024a: 232). Note that in Tables 5 and 8 (Van Hulle & Enghels 2024a: 232) all values are rounded off to two decimal places except the normalized frequency of disparar in the 21st which is reported as ‚Äú0.0008‚Äù.‚Ü©Ô∏é\nNote that we cannot copy the word counts directly from the paper, as the authors use the continental European format with the dot (.) as the thousand-separator and the comma (,) as a decimal point (e.g., 7.829.566 for the 13th century). In R, however, the dot is interpreted as a decimal separator so entering 7.829.566 will generate an error:\n\n7.829.566\n\nunexpected numeric constant in \"7.829.566\"\n‚Ü©Ô∏é\nRemember that, in base R, the notation [x, y] allows us to specify rows and columns in a data frame, where x refers to the row and y refers to the column (see Section 7.3). For example, token.data[, 1] means we are selecting all rows from the first column of token.data.‚Ü©Ô∏é",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>'Throw' verbs in Spanish: Rep`R`oducing the results of a corpus linguistics study</span>"
    ]
  },
  {
    "objectID": "B_CaseStudies/Rose-Gina/Emojis.html",
    "href": "B_CaseStudies/Rose-Gina/Emojis.html",
    "title": "17¬† The semantics of emojis: ExploRing the results of an experimental study",
    "section": "",
    "text": "Chapter overview\nThis case-study chapter will guide you through the steps to reproduce selected results from a published experimental linguistics study (Fricke, Grosz & Scheffler 2024) using R.\nThe chapter will walk you through how to:\nWe will work with the original raw data from:",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>The semantics of emojis: Explo`R`ing the results of an experimental study</span>"
    ]
  },
  {
    "objectID": "B_CaseStudies/Rose-Gina/Emojis.html#sec-introducing-the-study",
    "href": "B_CaseStudies/Rose-Gina/Emojis.html#sec-introducing-the-study",
    "title": "17¬† The semantics of emojis: ExploRing the results of an experimental study",
    "section": "17.1 Introducing the study üôÇ",
    "text": "17.1 Introducing the study üôÇ\nFace emojis are frequently used in text messages. They represent facial expressions and often make fundamental contributions to the subtext of a text message. A few studies have investigated the relationship between emojis and the emotions that they depict (Fugate & Franco 2021; Maier 2023; Pfeifer, Armstrong & Lai 2022). However, as emojis are a relatively recent phenomenon, there is still a lot to be discovered. In this chapter, we will look into a study by Fricke, Grosz & Scheffler (2024).\n\n17.1.1 Deconstructing emojis into Action Units\nFricke, Grosz & Scheffler (2024) compared ‚Äúvisually similar face emojis‚Äù using an emoji annotation system developed by Fugate & Franco (2021). This system is based on the Facial Action Coding System (FACS) for human faces (Ekman & Friesen 1978), which is an inventory of facial muscle movements that humans can make (such as raising the inner eyebrows or pulling down the corners of the lips). Fugate & Franco (2021) adapted FACS for emojis. The facial features of emojis, like eyebrows arched and eyes wide, are called Action Units (AUs). For convenience, AUs are assigned numbers, allowing them to be easily referenced. As you can see in Figure¬†17.1, each emoji consists of several AUs.\n\n\n\n\n\n\nFigure¬†17.1: Emoji pairs and their AU codes (from Fricke, Grosz & Scheffler 2024: 5, CC-BY)\n\n\n\nFricke, Grosz & Scheffler (2024) defined two different types of emoji pairs: In the AU+ condition, the pairs of emojis are similar, but are assigned a different set of AUs. The emoji pairs in the AU- condition are also similar, but their AUs are identical. Fricke, Grosz & Scheffler (2024) deliberately selected emoji pairs that were as visually similar as possible to each other, while ensuring that the two emojis either differed by exactly one Action Unit (AU+) or had no differences in Action Units (AU-).\nAUs capture the facial expressions of emojis and, as such, can assist linguists in accurately describing them. However, only expressions that can be consciously changed by humans receive labels. For example, the AU difference between üòÉ and üòÜ captures the fact that the former emoji has open eyes, while the latter has closed eyes. Since humans can choose whether to open or close their eyes, this is an AU+ pair. If the subtle difference between emojis is not manipulable by humans, as in üòÑ and üòÅ, the emojis are described by identical AUs (AU-).\n\n\n17.1.2 The experiment\n\n\n\n\n\n\nNoteHow did the experiment work?\n\n\n\nThree AU+ and three AU- emoji pairs were created (see Figure¬†17.1). Each pair was assigned two contexts, with each context corresponding to the prominent usage of one emoji, but not the other. For example, the contexts of the first pair are happiness and (cheeky) laughter. The contexts were assigned based on https://emojipedia.org and a previous norming study (Scheffler & Nenchev 2024).\nFour single-sentence narratives were created for each of the contexts (see Figure¬†17.2, translated from German below (translation Fricke, Grosz & Scheffler 2024: 6)).\n\n\nAlex writes to his best friend Stefan:\nI just learned that my cousin‚Äôs dog has his own advent calendar.\nAlex is amused. Which of the emojis matches the message better? üòÑüòÅ\n\nAlex writes to his best friend Stefan:\nI just learned that I won 500 Euro in the lottery.\nAlex is overjoyed. Which of the emojis matches the message better? üòÑüòÅ\n\n\n\n\n\n\n\n\nFigure¬†17.2: Example of a test item in the experiment (from Fricke, Grosz & Scheffler 2024: 6, CC-BY)\n\n\n\nThese short narratives were divided up into into four experimental lists of 12 items. Each list also contained 12 filler items, so that each participant saw 24 items. The participants were then asked to help choose the emoji that best matched the context. Each participant saw each emoji pair twice. It was measured how often participants chose the context-matching emoji versus the non-matching emoji.\n\n\nFricke, Grosz & Scheffler (2024)‚Äôs central research question was: Do AU differences lead to differences in meaning between the two emojis of a pair? In line with the pictorial approach by Maier (2023) the authors hypothesized that visual differences between emojis which correspond to human facial features (AU+) would be more semantically relevant than those that do not (AU-). However, they noted that if no evidence were found to support this hypothesis, it would align with Grosz et al. (2023)‚Äôs lexicalist approach. This approach suggests that visual differences between emojis and their correspondence to human facial features are less significant, placing emphasis instead on the intrinsic meaning of the emoji and its constituent parts.\n\n\n\n\n\n\nTipQuiz time!\n\n\n\nRead the abstract of the study:\n\nFricke, L., Grosz, P. G., & Scheffler, T. (2024). Semantic differences in visually similar face emojis. Language and Cognition, 1‚Äì15. https://doi.org/10.1017/langcog.2024.12\n\nQ1. According to the abstract, what were the results of Fricke, Grosz & Scheffler (2024)‚Äôs experiment?\n\n\n\nParticipants chose the context-matching emoji more often in the AU- condition than in the AU+ condition.\n\n\n\n\nParticipants chose the context-matching emoji more often in the AU+ condition than in the AU- condition.\n\n\n\n\nThere were no significant differences between the two conditions.\n\n\n\n\nFor both types of pairs, the context-matching emoji was preferred over the non-matching one.\n\n\n\n\n\n\n¬†\nQ2. The actual results of the experiment were different from what Fricke, Grosz & Scheffler (2024) had expected. According to authors‚Äô research hypothesis (visual differences between emojis which correspond to human facial features are more semantically relevant than those that do not), which of these experimental results were expected?\n\n\n\nFor the AU+ pairs, the pattern will be more random.\n\n\n\n\nFor both types of pairs, the context-matching emoji will be preferred over the non-matching one.\n\n\n\n\nParticipants will choose the context-matching emoji more often in the AU+ condition.\n\n\n\n\nFor the AU- pairs, the pattern will be more random.\n\n\n\n\nParticipants will choose the context-matching emoji more often in the AU- condition.\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>The semantics of emojis: Explo`R`ing the results of an experimental study</span>"
    ]
  },
  {
    "objectID": "B_CaseStudies/Rose-Gina/Emojis.html#sec-gender-understanding",
    "href": "B_CaseStudies/Rose-Gina/Emojis.html#sec-gender-understanding",
    "title": "17¬† The semantics of emojis: ExploRing the results of an experimental study",
    "section": "17.2 Exploring the relationship between gender and emoji understanding",
    "text": "17.2 Exploring the relationship between gender and emoji understanding\nFricke, Grosz & Scheffler (2024) asked participants about their gender, their attitude towards emojis, how often they use emojis on WhatsApp and how well they think they understand emojis. The authors visualised the distribution of men and women for emoji use and emoji attitude as barplots.\n\n\n\n\n\n\n\n\n\n\n\n(a) Emoji use by gender\n\n\n\n\n\n\n\n\n\n\n\n(b) Attitude towards emojis by gender\n\n\n\n\n\n\n\nFigure¬†17.3: Barplots from Fricke, Grosz & Scheffler (2024: 9-10, CC-BY)\n\n\n\nThe plots in Figure¬†17.3 show that women use emojis more often and have a more positive attitude towards emojis than men. We want to find out whether women also reported a higher level of emoji understanding than men. Our analysis will involve three steps:\n\nCalculating the frequencies of the genders in the data\n\nCalculating the frequencies of the different levels of emoji understanding for each gender\n\nVisualising the frequencies in a barplot similar to the plots above.\n\n\n17.2.1 ImpoRting the data\nFricke, Grosz & Scheffler (2024) have made their data and analysis code publicly available on the OSF repository (see Section 1.1). You can access these materials at https://osf.io/k2t9p/. There, the data is stored in the file raw_data.csv. To follow the steps of this chapter, you will need to download this file.\n\n\n\n\n\n\nWarningSession set-up\n\n\n\nTo run the code of this chapter, you will need the following packages. Make sure that they are installed and loaded before starting.\n\nlibrary(here)\nlibrary(tidyverse)\n#install.packages(\"patchwork\")\nlibrary(patchwork)\n#install.packages(\"ragg\")\nlibrary(ragg)\n\n\n\nWe import the authors‚Äô raw data using the read.csv() and here() functions. You will need to adjust the file path to match the folder structure of your computer (see Section 6.5).\n\nraw_data &lt;- read.csv(file = here(\"B_CaseStudies\", \"Rose-Gina\", \"data\", \"raw_data.csv\"))\n\nAs specified by Fricke, Grosz & Scheffler (2024: 8), we filter out participants who exceed the maximum age of 35 years for all following analyses. We do this by using the filter() function and store the result in a new data frame called df.\n\ndf &lt;- raw_data |&gt; \n  filter(age &lt;= 35)\n\n\n\n17.2.2 Gender frequency analysis\nLet‚Äôs first get a general overview: How many men, women, and non-binary people participated in the study?\nThe relevant variable in the data set is called gender. However, you will see that the names of the gender groups are in German. To figure out what the labels of the different gender groups are, we use the count() function:\n\ndf |&gt; \n  count(gender)\n\n    gender    n\n1   divers   72\n2 m√§nnlich 2616\n3 weiblich 1128\n\n\nBefore we start analysing, we should translate the labels (levels) of the categories into English. Using a combination of mutate() and recode(), we translate m√§nnlich to men, weiblich to women, and divers to non-binary.1\n\ndf &lt;- df |&gt; \n  mutate(gender = recode(gender, \n                         \"m√§nnlich\" = \"men\", \n                         \"weiblich\" = \"women\", \n                         \"divers\" = \"non-binary\"))\n\ndf |&gt; \n  count(gender)\n\n      gender    n\n1        men 2616\n2 non-binary   72\n3      women 1128\n\n\nNow that gender variable have English labels, we want to determine how many male, female, and non-binary subjects participated. We have used the table() function, which determines the number of occurrences of the different genders in the data. But in this case counting the occurrences is not straightforward. The data frame contains 24 rows for each subject, as each participant saw 24 items (see Section 17.1.2). So, if we were to simply count the occurrences of men, women, and non-binary in the data with count(), we would end up with 24 times the values of the frequencies.\nTo determine the actual gender distribution, we need to count the occurrences according to the subjects‚Äô unique IDs. To do this, we apply the distinct() function to keep only unique occurrences (to be precise, the first unique occurrence) of each submission_id. The argument .keep_all is set to TRUE, which means that all other variables in the data frame are kept and not deleted.\n\ndf |&gt; \n  distinct(submission_id, .keep_all = TRUE) |&gt; \n  count(gender)\n\n      gender   n\n1        men 109\n2 non-binary   3\n3      women  47\n\n\nThe mode (see Section 8.1.3) of the gender variable in the dataset is men, as you can see from the output. The gender distribution is very uneven: 109 men, 47 women, and 3 non-binary people participated in the study. If we are not careful, this imbalance can lead to misleading data visualisations.\n\n\n\n\n\n\nTipQuiz time!\n\n\n\nQ3. Which of these problems are likely to occur if we plot emoji understanding by gender in a barplot with unequal gender groups?\n\n\n\nWe will not be able to use ggplot to visualise the data.\n\n\n\n\nThe barplot is likely to be too wide for portrait publishing formats.\n\n\n\n\nThe differences in emoji understanding between gender groups may look bigger or smaller than they actually are.\n\n\n\n\nReaders may misinterpret the barplot as a histogram.\n\n\n\n\nThe y-axis of the plot may become distorted and therefore inaccurate.\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n\nTo solve this problem, we will use the same strategy as Fricke, Grosz & Scheffler (2024). We will use relative rather than absolute frequencies to make sure that the numbers for the different genders are comparable. This means that we will calculate the percentages of emoji understanding within each gender group, treating the total number of male, female, and non-binary participants separately as 100%, rather than counting all subjects together as 100%. In this way, we can see, for example, what percentage of men, women, and non-binary participants reported a very good emoji understanding and compare the numbers across groups.\n\n\n17.2.3 How well do the different genders understand emojis?\nNext, we calculate the relative frequencies of the different levels of emoji understanding for each gender.\nThe variable we are interested in is called emoji_understanding. Just like with gender, we first have to do some data wrangling. We use the count() function to get the labels:\n\ndf |&gt; \n  count(emoji_understanding)\n\n  emoji_understanding    n\n1            eher gut  912\n2                 gut 1344\n3         mittelm√§√üig   48\n4            sehr gut 1512\n\n\nWe translate mittelm√§√üig to moderate, eher gut to rather good, gut to good, and sehr gut to very good:\n\ndf &lt;- df |&gt; \n    mutate(emoji_understanding = recode(emoji_understanding,\n                                        \"mittelm√§√üig\" = \"moderate\",\n                                        \"eher gut\" = \"rather good\",\n                                        \"gut\" = \"good\",\n                                        \"sehr gut\" = \"very good\"))\ndf |&gt; \n  count(emoji_understanding)\n\n  emoji_understanding    n\n1                good 1344\n2            moderate   48\n3         rather good  912\n4           very good 1512\n\n\nThe levels are still in the wrong order. We need to rearrange them in an ascending order from moderate to very good. To do this, we define a vector c(\"moderate\", \"rather good\", \"good\", \"very good\"). Using the factor() function, we encode this vector as a factor:\n\ndf &lt;- df |&gt; \n    mutate(emoji_understanding = factor(emoji_understanding,\n                                        levels = c(\"moderate\",\n                                                   \"rather good\",\n                                                   \"good\",\n                                                   \"very good\")))\n\ndf |&gt; \n  count(emoji_understanding)\n\n  emoji_understanding    n\n1            moderate   48\n2         rather good  912\n3                good 1344\n4           very good 1512\n\n\nThe levels now look good, so we can determine the frequencies for the different gender groups within emoji_understanding. We could do this by simply cross-tabulating gender with emoji understanding (see Section 8.1.3). But since we know that the sizes of the gender subsets are very unequal, we also want to calculate the relative frequencies to make the numbers comparable. There is an easy way to calculate relative frequencies using the proportions() function (see Section 8.2.1). However, we need to make two additional considerations:\n\nOur aim is to calculate proportions within groups and not across the whole data.\nWe want to create a comprehensive visualisation that includes both groups of men and women in a single barplot.\n\nTo achieve both, we have to first group our data, using the powerful combination of group_by() and count(). We create a new data frame gender_understanding_count and again keep only each participant‚Äôs unique submission_id as above. We group the data by gender and count the frequencies for the different genders within the emoji_understanding factor:\n\ndf |&gt;\n  distinct(submission_id, .keep_all = TRUE) |&gt;\n  group_by(gender) |&gt; \n  count(gender, emoji_understanding)\n\n# A tibble: 10 √ó 3\n# Groups:   gender [3]\n   gender     emoji_understanding     n\n   &lt;chr&gt;      &lt;fct&gt;               &lt;int&gt;\n 1 men        moderate                1\n 2 men        rather good            25\n 3 men        good                   41\n 4 men        very good              42\n 5 non-binary rather good             1\n 6 non-binary good                    2\n 7 women      moderate                1\n 8 women      rather good            12\n 9 women      good                   13\n10 women      very good              21\n\n\nIn this table, n was calculated by the count() function and represents the number of occurrences for each combination of gender and emoji_understanding. Next, we use mutate() to add a column with the relative frequencies, which we calculate with the formula proportions(n) * 100 to obtain percentages.\n\ngender_understanding_count &lt;- df |&gt;\n  distinct(submission_id, .keep_all = TRUE) |&gt;\n  group_by(gender) |&gt; \n  count(gender, emoji_understanding) |&gt; \n  mutate(percentage = proportions(n) * 100)\n\ngender_understanding_count\n\n# A tibble: 10 √ó 4\n# Groups:   gender [3]\n   gender     emoji_understanding     n percentage\n   &lt;chr&gt;      &lt;fct&gt;               &lt;int&gt;      &lt;dbl&gt;\n 1 men        moderate                1      0.917\n 2 men        rather good            25     22.9  \n 3 men        good                   41     37.6  \n 4 men        very good              42     38.5  \n 5 non-binary rather good             1     33.3  \n 6 non-binary good                    2     66.7  \n 7 women      moderate                1      2.13 \n 8 women      rather good            12     25.5  \n 9 women      good                   13     27.7  \n10 women      very good              21     44.7  \n\n\nThis tabular presentation of the data already shows us that non-binary participants reported either a rather good or good understanding of emojis. A higher percentage of women (44.7%) reported a very good emoji understanding compared to men (38.5%). But let‚Äôs create our barplot to see the distribution more clearly.\n\n\n17.2.4 Data visualisation üìä\nAs mentioned above, we will visualise the relative rather than the absolute frequencies to make sure that the numbers for the different genders are comparable. In line with Fricke, Grosz & Scheffler (2024: 9), we also exclude the three non-binary participants. To this end, we use the filter() function combined with the != operator (see Section 5.5).\n\ngender_understanding_count &lt;- gender_understanding_count |&gt; \n  filter(gender != \"non-binary\")\n\nWe use ggplot() to create a barplot with the emoji_understanding categories on the x-axis and the relative frequencies that we calculated on the y-axis. The bars are coloured according to gender. We also add a title and axis labels. Finally, we remove the white space between the bottom of the bars with an additional scale_y_continuous(expand = c(0,0)) layer and change the colours to make our plot look nicer. The hexadecimal color values chosen here are from the colour-blind friendly palette ‚ÄúSet2‚Äù from the package {RColorBrewer}(Neuwirth 2022). Since we only need two colours, we chose to insert them manually to avoid having to install an additional package.\n\nggplot(gender_understanding_count, \n       aes(x = emoji_understanding, \n           y = percentage, \n           fill = gender)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  scale_y_continuous(expand = c(0,0)) +\n  labs(title = \"Self-reported Emoji Understanding by Gender\",\n       x = \"Emoji understanding\",\n       y = \"Percent\") +\n  scale_fill_manual(values = c(\"#8DA0CB\", \"#FC8D62\")) +\n  theme_classic()\n\n\n\n\n\n\n\n\nAs you can see from the barplot, the gender distribution for emoji understanding is much more even than for emoji use and emoji attitude (see Figure¬†17.3).\n\n\n\n\n\n\nTipQuiz time!\n\n\n\nQ4. How do you interpret this plot?\n\n\n\nProportionally more women than men reported a very good emoji understanding.\n\n\n\n\nAround half of all participants reported a rather good understanding of emojis.\n\n\n\n\nProportionally more women than men reported a moderate emoji understanding.\n\n\n\n\nWomen reported a lower level of emoji understanding than men.\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n\nWhen comparing our barplot to Figure¬†17.3, it is interesting to note that, whilst women reported more frequent use of emojis and a more positive attitude towards emojis, they did not report a higher understanding of emojis. It is possible that some women were more modest in rating their understanding of emojis than men, which could indicate a gender confidence gap. Reporting a good understanding likely requires more confidence compared to emoji use or attitude towards emoji.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>The semantics of emojis: Explo`R`ing the results of an experimental study</span>"
    ]
  },
  {
    "objectID": "B_CaseStudies/Rose-Gina/Emojis.html#sec-au-plot",
    "href": "B_CaseStudies/Rose-Gina/Emojis.html#sec-au-plot",
    "title": "17¬† The semantics of emojis: ExploRing the results of an experimental study",
    "section": "17.3 Comparing matching rates between AU conditions",
    "text": "17.3 Comparing matching rates between AU conditions\nWe will now turn to exploring the central research question of Fricke, Grosz & Scheffler (2024): Do AU differences lead to differences in meaning between the two emojis of a pair? As explained in Section 17.1.1, AUs are numbers which correspond to human-like facial features. In emoji pairs of the AU+ condition, the visual difference between the emojis is reflected in a number difference, e.g.¬†grinning face with big eyes üòÉ (AU: 5 + 12 + 25 + 26) and grinning squinting face üòÜ (AU: 12 + 25 + 26 + 43). In the AU- condition, the visual difference does not correspond to an AU difference, e.g.¬†grinning face with smiling eyes üòÑ and beaming face with smiling eyes üòÅ both have the same AUs (12 + 25 + 26 + 63).\nStep by step, we will build an informative plot which will include all the information needed to answer this question. This plot will display how many times each emoji was chosen in its presumed corresponding context.\nTo achieve this, we need to create a variable that tells us when each participant responded with the matching emoji. In the following, we will create this variable based on the raw data from Fricke, Grosz & Scheffler (2024).\n\n17.3.1 Preprocessing the data\nFirst, we need a variable that includes the experimental conditions of each trial. The variable name tells us whether a trial consisted of an emoji pair with an AU difference (AU+) or not (AU-), or a filler. Trials with AU+ differences include ‚ÄúAU‚Äù in the trial name, those with no AU difference begin with ‚ÄúN‚Äù, and the fillers with ‚Äúfiller‚Äù.\n\ndf |&gt; \n  distinct(name)\n\n\n\n              name\n1     filler-party\n2         AU-23-L1\n3   filler-heizung\n4         AU-40-L1\n5  filler-schlafen\n6     filler-krank\n7          N-04-L1\n8   filler-friseur\n9          N-37-L1\n10   filler-zombie\n\n\nWe use a combination of mutate(), case_when() and str_detect() to construct a new variable (AU_difference) that captures the type of trial that we are dealing with. The command essentially says: look for the string ‚ÄúAU‚Äù in the column name, and in all cases where you find it (case_when()), insert the value ‚ÄúAU+‚Äù in a new column called AU_difference. We follow this procedure for the other trial conditions, too. If neither ‚ÄúAU‚Äù, ‚ÄúN‚Äù or ‚Äúfiller‚Äù is detected, nothing (NULL) is inserted in AU_difference.\n\ndf &lt;- df |&gt; \n  mutate(AU_difference = case_when(str_detect(name, \"AU\") ~ \"AU+\",\n                                   str_detect(name, \"N\") ~ \"AU-\",\n                                   str_detect(name, \"filler\") ~ \"filler\",\n                                   .default = NULL))\n\nWe use select() to compare the two columns and check that everything worked.\n\ndf |&gt; \n  slice(1:10) |&gt; \n  select(name, AU_difference)\n\n              name AU_difference\n1     filler-party        filler\n2         AU-23-L1           AU+\n3   filler-heizung        filler\n4         AU-40-L1           AU+\n5  filler-schlafen        filler\n6     filler-krank        filler\n7          N-04-L1           AU-\n8   filler-friseur        filler\n9          N-37-L1           AU-\n10   filler-zombie        filler\n\n\nThis looks promising. Since we are only interested in the experimental items, we now filter out all filler trials.\n\ndf &lt;- df |&gt;\n  filter(AU_difference != \"filler\")\n\nWe will now create another variable called context. The column of this variable will contain the context descriptions used by Fricke, Grosz & Scheffler (2024: 5) in Figure¬†17.1. Again, we combine mutate(), case_when() and str_detect(): In the question column, we look for context-characteristic strings, and add the context descriptions whenever we have a match. Again, we check the output with table().\n\ndf &lt;- df |&gt; \n  mutate(context = case_when(str_detect(question, \"freut sich\") ~ \"happiness\",\n                             str_detect(question, \"lacht\") ~ \"(cheeky) laughter\",\n\n                             str_detect(question, \"macht sich Sorgen\") ~ \"concern\",\n                             str_detect(question, \"ist √ºberrascht\") ~ \"surprise\",\n                             str_detect(question, \"ist etwas genervt\") ~ \"mild irritation\",\n                             str_detect(question, \"√§rgert sich\") ~ \"annoyance\",\n                             str_detect(question, \"am√ºsiert sich\") ~ \"amusement\",\n                             str_detect(question, \"ist √ºbergl√ºcklich\") ~ \"(intense) happiness\",\n                             str_detect(question, \"ist entt√§uscht\") ~ \"mild disappointment\",\n                             str_detect(question, \"ist entt√§uscht\") ~ \"moderate disappointment\",\n                             str_detect(question, \"ist gut gelaunt\") ~ \"happiness2\",\n                             str_detect(question, \"ist verlegen\") ~ \"bashfulness\",\n                                   .default = NULL))\n\n\ntable(df$context)\n\n\n  (cheeky) laughter (intense) happiness           amusement           annoyance \n                159                 159                 159                 159 \n        bashfulness             concern           happiness          happiness2 \n                159                 159                 159                 159 \nmild disappointment     mild irritation            surprise \n                318                 159                 159 \n\n\n\n\n\n\n\n\nTipQuiz time!\n\n\n\nQ5. Which problems become apparent when checking the content of our new context variable using the table() function?\n\n\n\nThere are too many occurrences of matches per context than can reasonably be assumed.\n\n\n\n\nThe context descriptions were not correctly assigned in the case of matching strings.\n\n\n\n\nAll contexts have 159 occurrences, except for mild disappointment which occurs 318 times.\n\n\n\n\nThere are fewer contexts in the output than we coded for.\n\n\n\n\nThere are more contexts in the output than we coded for.\n\n\n\n\n\n\n\n\nThe contexts mild disappointment and moderate disappointment have created some issues: In the question column, both are described as ist entt√§uscht (‚Äòis disappointed‚Äô). Except for their encoding in the name column, these contexts appear to be identical. At this point, we have no choice but to look for additional disambiguating information in Fricke, Grosz & Scheffler (2024)‚Äôs analysis script, which you can access at https://osf.io/k8dtp. The relevant information can be found in lines 522 and 523 (see Figure¬†17.4).\n\n\n\n\n\n\nFigure¬†17.4: Screenshot from the authors‚Äô analysis script available at https://osf.io/k8dtp\n\n\n\nThe emoji üôÅ (mild disappointment) is coded as N-36 and ‚òπÔ∏è (moderate disappointment) as N-37. We use this information to assign these two contexts to our context variable.\n\ndf &lt;- df |&gt; \n  mutate(context = case_when(\n                             str_detect(name, \"N-36\") ~ \"mild disappointment\",\n                             str_detect(name, \"N-37\") ~ \"moderate disappointment\",\n                                   .default = context))\n\ntable(df$context)\n\n\n      (cheeky) laughter     (intense) happiness               amusement \n                    159                     159                     159 \n              annoyance             bashfulness                 concern \n                    159                     159                     159 \n              happiness              happiness2     mild disappointment \n                    159                     159                     159 \n        mild irritation moderate disappointment                surprise \n                    159                     159                     159 \n\n\nFinally, we add the critical variable that describes whether there is a match between the chosen emojis and the contexts: if the emoji and the context agree, the variable will have the value match. Otherwise, the value will be no match.\n\ndf &lt;- df |&gt; \n  mutate(\n  match = case_when(\n    context == \"happiness\" & response == \"grinning_face_with_big_eyes\" ~ \"match\",\n    context == \"(cheeky) laughter\" & response == \"grinning_squinting_face\" ~ \"match\",\n    context == \"concern\" & response == \"hushed_face\" ~ \"match\",\n    context == \"surprise\" & response == \"astonished_face\" ~ \"match\",\n    context == \"mild irritation\" & response == \"neutral_face\" ~ \"match\",\n    context == \"annoyance\" & response == \"expressionless_face\" ~ \"match\",\n    context == \"amusement\" & response == \"grinning_face_with_smiling_eyes\" ~ \"match\",\n    context == \"(intense) happiness\" & response == \"beaming_face_with_smiling_eyes\" ~ \"match\",\n    context == \"mild disappointment\" & response == \"slightly_frowning_face\" ~ \"match\",\n    context == \"moderate disappointment\" & response == \"frowning_face\" ~ \"match\",\n    context == \"happiness2\" & response == \"smiling_face_with_smiling_eyes\" ~ \"match\",\n    context == \"bashfulness\" & response == \"smiling_face\" ~ \"match\",\n    .default = \"no match\"))\n\n\n\n17.3.2 Building the plots\nWe will now build our plots to visualise the matching rates per emoji pair. In a new table called data_AU, we group the data by contexts. The command count(match) counts matches and non-matches for each context and stores them in a new column called n. We add the column percent to store the rounded percentage of matches and non-matches for each context-pair.\n\ndata_AU &lt;- df |&gt; \n  group_by(context) |&gt; \n  count(match) |&gt;\n  mutate(percent = round(proportions(n)*100, 2))\n\nUsing the View() function, we take a look at our data.\n\n\n\n\n\n\nFigure¬†17.5: The first 14 columns of the data frame data_AU as visualised using the View() function in RStudio\n\n\n\nWe plot the first emoji pair of the AU+ condition üòØ üò≤ with their respective contexts concern and surprise.\n\nplot_concern_surprise &lt;- data_AU |&gt; \n  filter(context == \"concern\" | context == \"surprise\") |&gt;\n  ggplot(aes(x = context, y = percent, fill = match)) +\n  geom_col() +\n  scale_x_discrete(limits = c(\"concern\", \"surprise\")) +\n  scale_y_continuous(expand = c(0,0)) +\n  scale_fill_manual(values = c(\"#66C2A5\", \"#E78AC3\")) +\n  geom_text(aes(label = percent), position = position_stack(vjust = 0.5)) +\n  labs(title = \"üòØ üò≤\", x = \"\") +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5, size = 20), \n        legend.title=element_blank())\n\nThe code above creates a barplot and stores it in plot_concern_surprise. Here‚Äôs what each line of code does:\n\nBegin by assigning a clear name for the plot and call the data to be plotted.\nFilter the contexts, such that only rows of the contexts concern or (|) surprise are plotted.\nCreate a ggplot object with the context values on the x-axis and percentages on the y-axis. Colours are filled corresponding to the match values.\nDisplay the data as a barplot. By default, geom_bar() counts how many times match and no match occur. However, as we have already calculated and stored the values in the column percent, we use geom_col() to be able to use the data as is.\nThe context values, which are displayed on the x-axis, are discrete. With this command, we set and order the contexts.\nUse the ‚Äúexpand‚Äù argument of the scale_y_continuous() function to remove the white space between the bars and the x-axis.\nAdjust colours with values from ‚ÄúSet2‚Äù from the {RColorBrewer} package (see Section 17.2.4).\nAnnotate the percentages of matching rates by adding them as text and placing them inside the plot, in the middle of the corresponding bars.\nAdd the corresponding emojis at the top of the plot and remove the superfluous x-axis label ‚Äúcontext‚Äù.\nAdd a theme, in this case theme_classic().\nPlots are left-aligned by default. Since we want the emojis to be displayed on top of their corresponding context bars, we move the title to the centre of the plot. To ensure that the emojis are easily interpretable, we also increase the font size to 20 points.\nFinally, we remove the title of the legend because the match and no match values are self-explanatory.\n\nLet‚Äôs take a look at our plot. It‚Äôs looking great, but we don‚Äôt need just one plot, we need six: one for each emoji pair. We could write it all out for each emoji pair, but since the code is identical (except for the contexts and the emojis), it is much more efficient to define a function to do this.\n\nplot_concern_surprise\n\n\n\n\n\n\n\n\nNoteDefining our own functions\n\n\n\nFunctions are reusable code snippets that perform specific tasks. So far, we have only used built-in R functions (see Section 7.4) and functions from add-on packages such {dplyr} from the tidyverse (see Section 9.1), but we have not defined our own functions.\nDefining our own functions can help us make our code more efficient and organised. As a rule of thumb, whenever writing new code seems redundant (i.e., when you find yourself copying and pasting entire sections of code), it is best to define a function for that task. This is will ensure that the task is always performed in the same way and, if you find that you need to amend the code to perform the task, you will only need to make the change once, within the function assigned to this task.\nThe basic structure of a function is function(argument). Looks familiar? Accordingly, we define a function the following way: function(parameters){function body}\nHere are the steps:\n\nWe define a function using the keyword function. After this keyword, we write a list of parameters in brackets. Parameters act as placeholders for the function‚Äôs arguments.\nWe then write code in the function body and enclose it in curly brackets. The function body tells the function what it is meant to do when called upon.\nWe assign our function a name using the assignment operator (&lt;-). This name will be used to call up the function. To avoid conflicts (see Section 9.2), we choose a name that is not already assigned to a built-in function.\n\n\n\nIn our case, the process of defining a plotting function is straightforward:\n\nWe start with the keyword function() and state that our function should take contexts as its first argument and emojis as its second argument, as only these change with each plot.\nWe then simply paste the code that we just wrote for the above barplot inside the curly braces, replacing the specific contexts and emojis with the parameters of our function.\nWe name our function plot_AU_matches to make clear what it does: it plots AU matches.\n\n\nplot_AU_matches &lt;- function(contexts, emojis) {\n  data_AU |&gt; \n    filter(context %in% contexts) |&gt; \n    ggplot(aes(x = context, y = percent, fill = match)) +\n    geom_col() +\n    scale_x_discrete(limits = contexts) +\n    scale_y_continuous(expand = c(0,0)) +\n    scale_fill_manual(values = c(\"#66C2A5\", \"#E78AC3\")) +\n    geom_text(aes(label = percent), position = position_stack(vjust = 0.5)) +\n    labs (x=\"\", y = \"percent\", title = emojis) +\n    theme_classic() +\n    theme(plot.title = element_text(hjust = 0.5, size = 20), \n          legend.title=element_blank())\n}\n\nWe the apply this function to all contexts and emoji pairs by filling them in as the arguments.\n\nplot_concern_surprise &lt;- plot_AU_matches(c(\"concern\", \"surprise\"), \"üòØ üò≤\")\nplot_happiness_cheeky &lt;- plot_AU_matches(c(\"happiness\", \"(cheeky) laughter\"), \"üòÉ üòÜ\")\nplot_mild_irr_annoyance &lt;- plot_AU_matches(c(\"mild irritation\", \"annoyance\"), \"üòê üòë\")\nplot_mild_disapp_mod_dissap &lt;- plot_AU_matches(c(\"mild disappointment\", \"moderate disappointment\"), \"üôÅÔ∏è ‚òπÔ∏è\")\nplot_amusement_int_happiness &lt;- plot_AU_matches(c(\"amusement\", \"(intense) happiness\"), \"üòÑ üòÅ\")\nplot_happiness2_bashfulness &lt;- plot_AU_matches(c(\"happiness2\", \"bashfulness\"), \"üòä ‚ò∫Ô∏è\")\n\n\n\n\n\n\n\nNoteHow to insert emojis in R and render plots with emojis üò∞\n\n\n\n\n\nThere are various ways to insert emojis in R. The easiest is to use the emoji keyboard (see Figure¬†17.6 (a)). To open it on MacOS, use the keyboard shortcut Crtl + ‚åò Cmd + Space or üåê fn + e and on Windows ‚äû Win + . (period). The emoji keyboard is also available in RStudio, if you go to the ‚ÄúEdit‚Äù drop-down menu and click on ‚ÄúEmojis & Symbols‚Äù. Alternatively, there are emoji libraries for R, for example {emo(ji)} (Wickham, Fran√ßois & D‚ÄôAgostino McGowan 2024).\nAs we want to display emojis within plots, we need to pay even more attention to graphics. Emojis as part of plots created by ggplot cannot be displayed by default. Additional problems can occur when rendering a Quarto or RMarkdown document to HTML.\nIf displaying emojis as part of plots in RStudio does not work for you, you will need to use the high-quality graphics library ‚ÄúAGG‚Äù (‚ÄúAnti-Grain Geometry‚Äù) or ‚ÄúCairo‚Äù as your graphics backend in RStudio. To do this, head to the ‚ÄúTools‚Äù drop-down menu and click on ‚ÄúGlobal Options‚Äù. Then, go to the ‚ÄúGraphics‚Äù tab and select the ‚ÄúAGG‚Äù or ‚ÄúCairo‚Äù option (see Figure¬†17.6 (b)).\n\n\n\n\n\n\n\n\n\n\n\n(a) The emoji keyboard\n\n\n\n\n\n\n\n\n\n\n\n(b) Recommended graphics backend in RStudio\n\n\n\n\n\n\n\nFigure¬†17.6: Tools for inserting and displaying emojis in RStudio\n\n\n\nIf you are using AGG as your graphics backend, you can use the {ragg} package (Pedersen & Shemanarev 2024) to correctly render your Quarto/RMarkdown document to HTML with all the emojis in the plots. This package provides graphic devices based on AGG and includes advanced text rendering, with support for emojis. To use {ragg} in combinination with the {knitr} engine, first install the package and then add the following setup command at the beginning of your document:\n\ninstall.packages(\"ragg\")\n\nknitr::opts_chunk$set(dev = \"ragg_png\")\n\nIf AGG does not work for you, you can use Cairo. Cairo comes preinstalled with R so you don‚Äôt need to install it yourself. The set-up command for your Quarto/RMarkdown document is:\n\nknitr::opts_chunk$set(dev.args = list(png = list(type = \"cairo\")))\n\n\n\n\n\n\n17.3.3 Assembling plots with {patchwork}\nBy applying our newly created function plot_AU_matches() to all emoji pairs and contexts, we have created one barplot for each emoji pair. We will now use the {patchwork} package (Pedersen 2024) to assemble the plots into one figure. As the name suggests, {patchwork} enables us to patch several plots together and arrange them as we wish. The basic operator to combine plots in {patchwork} is the + operator. Additionally, plots can be combined:\n\nHorizontally using | and\nVertically using /.\n\nBrackets can be used to combine horizontal and vertical arrangements.\n\n\n\n\n\n\nFigure¬†17.7: Patchwork artwork by Allison Horst (CC-BY)\n\n\n\nIn line with the research question of Fricke, Grosz & Scheffler (2024), we want to compare the matching rates of emojis and contexts in the AU+ condition with the matching rates in the AU- condition. Our goal is therefore to create a plot that looks similar to Figure¬†17.8 created by Fricke, Grosz & Scheffler (2024).\n\n\n\n\n\n\nFigure¬†17.8: Plot of individual emoji pairs that compares AU conditions (Fricke, Grosz & Scheffler 2024: 11, CC-BY)\n\n\n\nFirst, let us plan the layout of our combined plot with some placeholder names. Our combined plot will have two columns and three rows: In both column1 and column2, three plots are stacked vertically on top of each other. These are the plots of the AU+ and the AU- condition, respectively. We then place these patchworks next to each other (horizontally) for comparison.\ncolumn1 &lt;- p1 / p2 / p3\n\ncolumn2 &lt;- p4 / p5 / p6\n\ncolumns_combined &lt;- column1 | column2\nWe follow the logic above to create our combined plot, choosing informative names for our subplots. To run this code, you will need to have the {patchwork} package installed and the library loaded.\n\n#install.packages(\"patchwork\")\n#library(patchwork)\n\n#AU+ condition:\nAU_plus_patch &lt;-\n  plot_concern_surprise / plot_happiness_cheeky / plot_mild_irr_annoyance\n\n#AU- condition:\nAU_minus_patch &lt;-\n  plot_mild_disapp_mod_dissap / plot_amusement_int_happiness / plot_happiness2_bashfulness\n\nTo further specify the layout, we use the plot_layout() function from {patchwork}. By setting the ‚Äúguide‚Äù argument to ‚Äúcollect‚Äù, legends that are identical within each patchwork are merged into one. We place the legends at the bottom of each subplot.\n\n#AU+ condition:\nAU_plus_patch &lt;- AU_plus_patch +\n  plot_layout(guides = \"collect\") & theme(legend.position = \"bottom\")\n\n#AU- condition:\nAU_minus_patch &lt;- AU_minus_patch +\n  plot_layout(guides = \"collect\") & theme(legend.position = \"bottom\")\n\nSince AU_plus_patch and AU_minus_patch are to be combined in one plot, we need to add titles to keep them apart. Technically, it is possible (and recommended!) to use the plot_annotation() function of the {patchwork} package for this. However, annotations made with this function are only shown at the highest nesting level. As we will be building a double-nested plot, any annotations we do on the ‚Äúblocks-of-three‚Äù-level will not be displayed. We can work around this by using the function wrap_elements(). This fixates the blocks in their current position and allows us to add titles using ggtitles() instead.\n\nAU_plus_patch &lt;- wrap_elements(plot = AU_plus_patch) +\n  ggtitle(\"[AU+] condition\")\n\nAU_minus_patch &lt;- wrap_elements(plot = AU_minus_patch) +\n  ggtitle(\"[AU-] condition\")\n\nFinally, we put both columns together to get our final plot.\n\nAU_plus_patch | AU_minus_patch\n\n\n\n\n\n\n\nFigure¬†17.9: Plot combining all six plots into one figure\n\n\n\nFigure¬†17.9 contains information on the matching rates of all emoji pairs with their contexts. It contains the same information as Figure¬†17.8 from Fricke, Grosz & Scheffler (2024), but does not look exactly the same. Which look do you think is easiest to interpret?\n\n\n\n\n\n\nNoteAlternative ways of dealing with the legend\n\n\n\n\n\nYou probably will have noticed that Figure¬†17.9 contains two identical legends. This is the trade-off we take by using the wrap_elements() function: We have fixated the patchworks in their state with their legends, which means that the legends cannot be merged later. There are a couple of other options that will produce different outcomes, however, none is going to be perfect: Since we do not want to delete the legend completely, one option would be to keep the legends of all six plots, as in Figure¬†17.10 (a). Another option would be to keep the legend of one block and delete the other. However, as you can see in Figure¬†17.10 (b), this makes the bars take up the space of the legend, and the bars in one block become wider than in the other one.\n\n\n\n\n\n\n\n\n\n\n\n(a) The combined plot with six legends\n\n\n\n\n\n\n\n\n\n\n\n(b) The combined plot with one legend\n\n\n\n\n\n\n\nFigure¬†17.10: Options for the legend placement\n\n\n\n\n\n\n\n\n17.3.4 Interpreting the plot\nBy looking and interpreting Figure¬†17.9, we can now finally answer the research question: Do AU differences lead to differences in meaning between the two emojis of a pair?\nBased on the descriptive statistics visualised in Figure¬†17.9, the answer is no, seemingly not. The AU difference does not seem to be critical when deciding which emoji to use in a specific context. The original study also concluded that the matching emoji was ‚Äúgenerally preferred with matching rates above chance level‚Äù (Fricke, Grosz & Scheffler 2024: 11), both in the AU+ and in the AU- condition. Now, was all this work for nothing?\nNo, not at all! We can still draw some interesting inferences from the plot we created. For example, we see that minor visual differences between emojis do appear to affect the understanding and selection of emojis in different contexts: By slightly varying the contexts, participants were made to choose emojis with different facial features. Matching rates were quite similar within emoji pairs and, notably, also largely consistent across emoji pairs, regardless of their AU status. Hence, there is much more to explore in future linguistics studies on the semantics of emoji in text messages!\nThe following quiz questions are about the interpretation of Figure¬†17.9. The questions should help you make sense of the information displayed.\n\n\n\n\n\n\nTipQuiz time!\n\n\n\nQ6. In which context pair did participants choose the matching emojis most often?\n\n\n\nmild disappointment and moderate disappointment\n\n\n\n\nhappiness and (cheeky) laughter\n\n\n\n\nhappiness2 and bashfulness \n\n\n\n\namusement and (intense) happiness\n\n\n\n\nconcern and surprise\n\n\n\n\nmild irritation and annoyance\n\n\n\n\n\n\n Q7. How can similar matching rates across emoji pairs (that is, across sub-plots) be interpreted?\n\n\n\nFor these pairs, the least number of participants chose the matching emoji.\n\n\n\n\nFor these pairs, participants had the most difficulty choosing one emoji over the other.\n\n\n\n\nFor these pairs, a very similar number of participants chose the matching emoji.\n\n\n\n\nFor these pairs, half the participants preferred one emoji and half preferred the other.\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n Q8. Which interpretations of the lower left plot are correct? Select all that apply.\n\n\n\nThe emoji üòê was selected for its matching context considerably less often than all the other emojis.\n\n\n\n\nThe disparate matching rates of the üòêüòë pair prove that, in general, AU differences affect participants' preferences.\n\n\n\n\nOnly in the üòêüòë pair does the matching rate for one emoji exceed chance level, while the matching rate for the other falls below chance.\n\n\n\n\nThere is a major difference between the matching rates of the annoyance and happiness contexts.\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n Q9. What are plausible reasons for the striking results presented in the lower left barplot? Select all that apply.\n\n\n\nUnlike what was anticipated, most participants used the emojis üòê and üòë in very different contexts.\n\n\n\n\nThe stories created for mild irritation and annoyance triggered a similar reaction.\n\n\n\n\nThe participants did not realize there was a difference between üòê and üòë.\n\n\n\n\nThe stories of the mild irritation context were perceived as more annoying than the authors had anticipated.\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n\nOverall, Figure¬†17.9 shows that there was indeed a preference for context-matching emojis. Importantly, participants preferred the context-matching emoji in both the AU+ condition and the AU- condition: The overall matching rate of AU- pairs is very similar to that of AU+ pairs. This means that whether or not emoji features coincided with human facial features did not (significantly) affect the participants‚Äô decision for one emoji or the other.\nThe findings do not support Fricke, Grosz & Scheffler (2024)‚Äôs experimental hypothesis, which was based on the pictorial approach. Instead, as Fricke, Grosz & Scheffler (2024: 12) observe, the results suggest that visually similar emojis can convey different meanings, even when they correspond to the same human facial expressions.\nThis observation closely aligns with the predictions of the lexicalist approach proposed by Grosz et al. (2023). However, the authors caution that it does not provide definitive evidence for the lexicalist approach (Fricke, Grosz & Scheffler 2024: 12).",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>The semantics of emojis: Explo`R`ing the results of an experimental study</span>"
    ]
  },
  {
    "objectID": "B_CaseStudies/Rose-Gina/Emojis.html#sec-conclusion",
    "href": "B_CaseStudies/Rose-Gina/Emojis.html#sec-conclusion",
    "title": "17¬† The semantics of emojis: ExploRing the results of an experimental study",
    "section": "17.4 Conclusion",
    "text": "17.4 Conclusion\nYou have successfully completed 0 out of 9 quiz questions in this chapter.\nYou are now a pro in handling (stacked) barplots! You can build, customise, arrange, and interpret them. Barplots are powerful for visualising categorical data, offering a straightforward way to compare frequencies and make patterns apparent. However, they do have their limitations. For instance, they are not ideal for displaying continuous data. Building and assembling plots can be quite fiddly and it can take some trial-and-error to make the plot look like what you had imagined. But there is a solution for (almost) everything and hopefully, the beautiful plot you create in the process will be worth the effort.\nThis chapter‚Äôs analysis revealed gender-specific differences in emoji understanding, potentially indicating a gender confidence gap between men and women. On average, however, both genders reported at least a good understanding of emojis. The visualisations have been adjusted for the gender imbalance in the data, demonstrating the importance of accounting for differences in group sizes.\nIn this chapter, we have created an informative figure that answers the experiment‚Äôs research question by combining multiple plots. The question whether Action Unit (AU) differences are critical for emoji preference was answered in the negative. However, we have made several other discoveries along the way: As noted by Fricke, Grosz & Scheffler (2024), we have found that small changes of emojis‚Äô facial features do affect choice patterns.\nEmojis, it turns out, contain lots of information, and there is a science behind them ü§ì. While experimentally measuring why we prefer certain emojis over other ones represents a real challenge, Fricke, Grosz & Scheffler (2024) provide valuable insights into this fascinating area of study. As the authors shared their data and code, we were able to successfully reproduce their results, as well as create new informative figures on the basis of their data.\n\n\n\n\n\n\nNoteHow to cite this chapter\n\n\n\n\n\nThis is a case study chapter of the web version of the textbook ‚ÄúData Analysis for the Language Sciences: A very gentle introduction to statistics and data visualisation in R‚Äù by Elen Le Foll.\nPlease cite the current version of this chapter as:\n\n\nH√∂rsting, Rose and Gina Reinhard. 2024. The semantics of emojis: ExploRing the results of an experimental study. In Elen Le Foll (Ed.), Data Analysis for the Language Sciences: A very gentle introduction to statistics and data visualisation in R. Open Educational Resource. https://elenlefoll.github.io/RstatsTextbook/ (accessed DATE).",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>The semantics of emojis: Explo`R`ing the results of an experimental study</span>"
    ]
  },
  {
    "objectID": "B_CaseStudies/Rose-Gina/Emojis.html#references",
    "href": "B_CaseStudies/Rose-Gina/Emojis.html#references",
    "title": "17¬† The semantics of emojis: ExploRing the results of an experimental study",
    "section": "References",
    "text": "References\nFricke, Lea, Patrick G Grosz, and Tatjana Scheffler. 2024. Semantic Differences in Visually Similar Face Emojis. Language and Cognition. Cambridge University Press 1‚Äì15. https://doi.org/10.1017/langcog.2024.12.\nFugate, Jennifer MB & Courtny L Franco. 2021. Implications for Emotion: Using Anatomically Based Facial Coding to Compare Emoji Faces Across Platforms. Frontiers in Psychology. Frontiers Media SA 12. 605928. https://doi.org/10.3389/fpsyg.2021.605928.\nGrosz, Patrick Georg, Gabriel Greenberg, Christian De Leon & Elsi Kaiser. 2023. A semantics of face emoji in discourse. Linguistics and Philosophy. Springer 46(4). 905-957. https://doi.org/10.1007/s10988-022-09369-8\nMaier, Emar. 2023. Emojis as Pictures. Ergo 10. https://doi.org/10.3998/ergo.4641.\nNeuwirth, Erich. 2022. Package ‚ÄúRColorBrewer‚Äù. ColorBrewer Palettes 991. https://cran.r-project.org/web/packages/RColorBrewer/RColorBrewer.pdf.\nPedersen, Thomas Lin. 2024. Patchwork: The Composer of Plots. https://patchwork.data-imaginist.com.\nPedersen, Thomas Lin & Maxim Shemanarev. 2024. Ragg: Graphic Devices Based on AGG. https://ragg.r-lib.org.\nPfeifer, Valeria A, Emma L Armstrong & Vicky Tzuyin Lai. 2022. Do all facial emojis communicate emotion? The impact of facial emojis on perceived sender emotion and text processing. Computers in Human Behavior. Elsevier 126. 107016. https://doi.org/10.1016/j.chb.2021.107016.\nScheffler, Tatjana & Ivan Nenchev. 2024. Affective, semantic, frequency, and descriptive norms for 107 face emojis. Behavior Research Methods. Springer 1‚Äì22. https://doi.org/10.3758/s13428-024-02444-x.\nWickham, Hadley, Romain Fran√ßois & Lucy D‚ÄôAgostino McGowan. 2024. Emo: Easily Insert ‚Äôemoji‚Äô. https://github.com/hadley/emo.\n\nPackages used in this chapter\n\n\nR version 4.5.0 (2025-04-11)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.4.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/Brussels\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] knitcitations_1.0.12 ragg_1.4.0           patchwork_1.3.0     \n [4] lubridate_1.9.4      forcats_1.0.0        stringr_1.5.1       \n [7] dplyr_1.1.4          purrr_1.0.4          readr_2.1.5         \n[10] tidyr_1.3.1          tibble_3.2.1         ggplot2_3.5.2       \n[13] tidyverse_2.0.0      here_1.0.1           checkdown_0.0.13    \n[16] webexercises_1.1.0  \n\nloaded via a namespace (and not attached):\n [1] utf8_1.2.5         generics_0.1.4     xml2_1.3.8         stringi_1.8.7     \n [5] hms_1.1.3          digest_0.6.37      magrittr_2.0.3     evaluate_1.0.3    \n [9] grid_4.5.0         timechange_0.3.0   RColorBrewer_1.1-3 fastmap_1.2.0     \n[13] plyr_1.8.9         rprojroot_2.1.1    jsonlite_2.0.0     backports_1.5.0   \n[17] httr_1.4.7         scales_1.4.0       codetools_0.2-20   bibtex_0.5.1      \n[21] textshaping_1.0.1  cli_3.6.5          rlang_1.1.6        litedown_0.7      \n[25] commonmark_2.0.0   withr_3.0.2        yaml_2.3.10        tools_4.5.0       \n[29] tzdb_0.5.0         vctrs_0.6.5        R6_2.6.1           lifecycle_1.0.4   \n[33] RefManageR_1.4.0   htmlwidgets_1.6.4  pkgconfig_2.0.3    pillar_1.10.2     \n[37] gtable_0.3.6       Rcpp_1.0.14        glue_1.8.0         systemfonts_1.2.3 \n[41] xfun_0.53          tidyselect_1.2.1   rstudioapi_0.17.1  knitr_1.50        \n[45] farver_2.1.2       htmltools_0.5.8.1  labeling_0.4.3     rmarkdown_2.29    \n[49] compiler_4.5.0     markdown_2.0      \n\n\n\n\nPackage references\n[1] D. Barr and L. DeBruine. webexercises: Create Interactive Web Exercises in R Markdown (Formerly webex). R package version 1.1.0. 2023. https://github.com/psyteachr/webexercises.\n[2] C. Boettiger. knitcitations: Citations for Knitr Markdown Files. R package version 1.0.12. 2021. https://github.com/cboettig/knitcitations.\n[3] G. Grolemund and H. Wickham. ‚ÄúDates and Times Made Easy with lubridate‚Äù. In: Journal of Statistical Software 40.3 (2011), pp. 1-25. https://www.jstatsoft.org/v40/i03/.\n[4] G. Moroz. checkdown: Check-Fields and Check-Boxes for rmarkdown. R package version 0.0.12. 2023. https://agricolamz.github.io/checkdown/.\n[5] G. Moroz. Create check-fields and check-boxes with checkdown. 2020. https://CRAN.R-project.org/package=checkdown.\n[6] K. M√ºller. here: A Simpler Way to Find Your Files. R package version 1.0.1. 2020. https://here.r-lib.org/.\n[7] K. M√ºller and H. Wickham. tibble: Simple Data Frames. R package version 3.2.1. 2023. https://tibble.tidyverse.org/.\n[8] T. L. Pedersen. patchwork: The Composer of Plots. R package version 1.2.0. 2024. https://patchwork.data-imaginist.com.\n[9] T. L. Pedersen and M. Shemanarev. ragg: Graphic Devices Based on AGG. R package version 1.3.3. 2024. https://ragg.r-lib.org.\n[10] R Core Team. R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing. Vienna, Austria, 2024. https://www.R-project.org/.\n[11] V. Spinu, G. Grolemund, and H. Wickham. lubridate: Make Dealing with Dates a Little Easier. R package version 1.9.3. 2023. https://lubridate.tidyverse.org.\n[12] H. Wickham. forcats: Tools for Working with Categorical Variables (Factors). R package version 1.0.0. 2023. https://forcats.tidyverse.org/.\n[13] H. Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2016. ISBN: 978-3-319-24277-4. https://ggplot2.tidyverse.org.\n[14] H. Wickham. stringr: Simple, Consistent Wrappers for Common String Operations. R package version 1.5.1. 2023. https://stringr.tidyverse.org.\n[15] H. Wickham. tidyverse: Easily Install and Load the Tidyverse. R package version 2.0.0. 2023. https://tidyverse.tidyverse.org.\n[16] H. Wickham, M. Averick, J. Bryan, et al.¬†‚ÄúWelcome to the tidyverse‚Äù. In: Journal of Open Source Software 4.43 (2019), p.¬†1686. DOI: 10.21105/joss.01686.\n[17] H. Wickham, W. Chang, L. Henry, et al.¬†ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. R package version 3.5.1. 2024. https://ggplot2.tidyverse.org.\n[18] H. Wickham, R. Fran√ßois, L. Henry, et al.¬†dplyr: A Grammar of Data Manipulation. R package version 1.1.4. 2023. https://dplyr.tidyverse.org.\n[19] H. Wickham and L. Henry. purrr: Functional Programming Tools. R package version 1.0.2. 2023. https://purrr.tidyverse.org/.\n[20] H. Wickham, J. Hester, and J. Bryan. readr: Read Rectangular Text Data. R package version 2.1.5. 2024. https://readr.tidyverse.org.\n[21] H. Wickham, D. Vaughan, and M. Girlich. tidyr: Tidy Messy Data. R package version 1.3.1. 2024. https://tidyr.tidyverse.org.\n[22] Y. Xie. Dynamic Documents with R and knitr. 2nd. ISBN 978-1498716963. Boca Raton, Florida: Chapman and Hall/CRC, 2015. https://yihui.org/knitr/.\n[23] Y. Xie. ‚Äúknitr: A Comprehensive Tool for Reproducible Research in R‚Äù. In: Implementing Reproducible Computational Research. Ed. by V. Stodden, F. Leisch and R. D. Peng. ISBN 978-1466561595. Chapman and Hall/CRC, 2014.\n[24] Y. Xie. knitr: A General-Purpose Package for Dynamic Report Generation in R. R package version 1.47. 2024. https://yihui.org/knitr/.\n\n\n\n\nEkman, Paul & Wallace V Friesen. 1978. Facial action coding system. Environmental Psychology & Nonverbal Behavior.\n\n\nFricke, Lea, Patrick G Grosz & Tatjana Scheffler. 2024. Semantic differences in visually similar face emojis. Language and Cognition. Cambridge University Press 1‚Äì15. https://doi.org/10.1017/langcog.2024.12.\n\n\nFugate, Jennifer MB & Courtny L Franco. 2021. Implications for emotion: Using anatomically based facial coding to compare emoji faces across platforms. Frontiers in Psychology. Frontiers Media SA 12. 605928. https://doi.org/10.3389/fpsyg.2021.605928.\n\n\nGrosz, Patrick Georg, Gabriel Greenberg, Christian De Leon & Elsi Kaiser. 2023. A semantics of face emoji in discourse. Linguistics and Philosophy. Springer 46(4). 905‚Äì957. https://doi.org/10.1007/s10988-022-09369-8.\n\n\nMaier, Emar. 2023. Emojis as pictures. Ergo 10. https://doi.org/10.3998/ergo.4641.\n\n\nNeuwirth, Erich. 2022. Package ‚ÄúRColorBrewer.‚Äù ColorBrewer palettes 991. https://cran.r-project.org/web/packages/RColorBrewer/RColorBrewer.pdf.\n\n\nPedersen, Thomas Lin. 2024. Patchwork: The Composer of Plots. https://patchwork.data-imaginist.com.\n\n\nPedersen, Thomas Lin & Maxim Shemanarev. 2024. Ragg: Graphic devices based on AGG. https://ragg.r-lib.org.\n\n\nPfeifer, Valeria A, Emma L Armstrong & Vicky Tzuyin Lai. 2022. Do all facial emojis communicate emotion? The impact of facial emojis on perceived sender emotion and text processing. Computers in Human Behavior. Elsevier 126. 107016. https://doi.org/10.1016/j.chb.2021.107016.\n\n\nScheffler, Tatjana & Ivan Nenchev. 2024. Affective, semantic, frequency, and descriptive norms for 107 face emojis. Behavior Research Methods. Springer 1‚Äì22. https://doi.org/10.3758/s13428-024-02444-x.\n\n\nWickham, Hadley, Romain Fran√ßois & Lucy D‚ÄôAgostino McGowan. 2024. Emo: Easily insert ‚Äôemoji‚Äô. https://github.com/hadley/emo.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>The semantics of emojis: Explo`R`ing the results of an experimental study</span>"
    ]
  },
  {
    "objectID": "B_CaseStudies/Rose-Gina/Emojis.html#footnotes",
    "href": "B_CaseStudies/Rose-Gina/Emojis.html#footnotes",
    "title": "17¬† The semantics of emojis: ExploRing the results of an experimental study",
    "section": "",
    "text": "We decided to translate ‚Äòdivers‚Äô as ‚Äònon-binary‚Äô, as this is the English term that Fricke, Grosz & Scheffler (2024) used in their paper.‚Ü©Ô∏é",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>The semantics of emojis: Explo`R`ing the results of an experimental study</span>"
    ]
  },
  {
    "objectID": "B_CaseStudies/Dela/Perdon.html",
    "href": "B_CaseStudies/Dela/Perdon.html",
    "title": "18¬† The gRammaticalization of ‚Äòperd√≥n‚Äô in Spanish varieties",
    "section": "",
    "text": "18.1 Introducing the study\nThis chapter will guide you through the steps to reproduce the results of a published corpus linguistics study (Jansegers, Melis & Arrington 2024) using R. It will walk you through how to:\nWhether you are new to R or seeking to sharpen your analytical skills, this chapter provides a guided, reproducible introduction to data-driven linguistic analysis using Spanish apology markers as a case study.\nIn this chapter, we reproduce the results of a corpus linguistics study (Jansegers, Melis & Arrington 2024), which investigates the grammaticalization of the Spanish apology marker perd√≥n in two regional varieties: Mexican and Peninsular Spanish. Grammaticalization refers to the historical process by which lexical items such as verbs or nouns evolve into grammatical or discourse-related elements (Hopper & Traugott 2003; Heine & Kuteva 2002). In this case, the verb perdonar (‚Äòto pardon‚Äô) has undergone semantic and functional shifts, becoming the discourse marker perd√≥n, particularly in the context of low-imposition offenses.\nThe original study (Jansegers, Melis & Arrington 2024) analyzes corpus data to explore how apology markers (perd√≥n, lo siento, disculpa) vary according to factors such as offense type, speaker responsibility, and Spanish variety. Their methodology draws on frameworks from grammaticalization theory (Hopper & Traugott 2003) and politeness theory (Brown & Levinson 1987).\nOur goal is to reproduce the analyses of Jansegers, Melis & Arrington (2024) using the same dataset and statistical procedures, and visualizations in R. This will allow us to compare our outputs to those reported in the original study and assess the consistency of results across offense types and regional varieties.\nThis reproduction focuses on the following research question:",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>The g`R`ammaticalization of 'perd√≥n' in Spanish varieties</span>"
    ]
  },
  {
    "objectID": "B_CaseStudies/Dela/Perdon.html#sec-Intro",
    "href": "B_CaseStudies/Dela/Perdon.html#sec-Intro",
    "title": "18¬† The gRammaticalization of ‚Äòperd√≥n‚Äô in Spanish varieties",
    "section": "",
    "text": "Does perd√≥n function differently in Mexican and Peninsular Spanish?",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>The g`R`ammaticalization of 'perd√≥n' in Spanish varieties</span>"
    ]
  },
  {
    "objectID": "B_CaseStudies/Dela/Perdon.html#sec-retrieving",
    "href": "B_CaseStudies/Dela/Perdon.html#sec-retrieving",
    "title": "18¬† The gRammaticalization of ‚Äòperd√≥n‚Äô in Spanish varieties",
    "section": "18.2 Retrieving the data",
    "text": "18.2 Retrieving the data\nWe begin by retrieving the original dataset used in the study by Jansegers, Melis & Arrington (2024). The researchers compiled corpus data from two distinct varieties of Spanish:\n\nMexican Spanish\nPeninsular Spanish\n\nThese datasets are made up of real-life language interactions, including transcripts of conversations, online discussions, and formal writing. Each part of the corpus has been carefully annotated to highlight apology phrases like perd√≥n, disculpa, and lo siento showing how they are used in different contexts.\nIn their open-access publication in Languages, the authors included the following Data Availability Statement (Jansegers, Melis & Arrington 2024: 19):\n\nData Availability Statement\nThe research data of this study is available in TROLLing. Jansegers, Marlies; Melis, Chantal; Arrington B√°ez, Jennie Elenor, 2023, Replication Data for: Diverging grammaticalization patterns across Spanish varieties: the case of perd√≥n in Mexican and Peninsular Spanish, https://doi.org/10.18710/IEXVVN (accessed on 1 December 2023), DataverseNO.\n\nTo obtain the dataset, follow these straightforward steps:\n\nFollow the link to the data repository:\nhttps://doi.org/10.18710/IEXVVN\nLocate the dataset:\nLocate the dataset file named Data_perdon_20231221.csv.\nDownload the dataset:\nSave the CSV file (Data_perdon_20231221.csv) to your local project directory.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>The g`R`ammaticalization of 'perd√≥n' in Spanish varieties</span>"
    ]
  },
  {
    "objectID": "B_CaseStudies/Dela/Perdon.html#sec-dataprocessing",
    "href": "B_CaseStudies/Dela/Perdon.html#sec-dataprocessing",
    "title": "18¬† The gRammaticalization of ‚Äòperd√≥n‚Äô in Spanish varieties",
    "section": "18.3 Preprocessing the data",
    "text": "18.3 Preprocessing the data\nTo get started, you‚Äôll want to load (and if necessary, first install) the R libraries that we will need for this reproduction project.\n\nlibrary(here) # Useful for file path management.\nlibrary(ggmosaic) # Visualizes relationships in data through mosaic plots in {ggplot2}.\nlibrary(kableExtra) # To display nice-looking tables.\nlibrary(skimr) # Provides summaries of dataset variables for quick insights into distributions and missing data.\nlibrary(tidyverse) # A collection of core data science packages, including {ggplot2} and {dplyr} for data visualisation, data wrangling, and data import.\n\nOnce the necessary packages are loaded, we can read in the dataset and check its contents. Note that the dataset is semi-colon separated.\n\n# Import the dataset\ndata &lt;- read.csv(here(\"B_CaseStudies\", \"Dela\", \"Data_perdon_20231221.csv\"), sep = \";\")\n\n# Display the first few rows\nhead(data)\n\n  variety  corpus      form           offense_type face_affected\n1      MX PRESEEA disculpar inappropriate_behavior      positive\n2      MX PRESEEA disculpar     lack_consideration      positive\n3      MX PRESEEA disculpar      censored_language      positive\n4      MX PRESEEA disculpar     temporal_territory      negative\n5      MX PRESEEA disculpar           slips_tongue      positive\n6      MX PRESEEA disculpar           slips_tongue      positive\n  face_orientation\n1          speaker\n2           hearer\n3          speaker\n4           hearer\n5          speaker\n6          speaker\n\n\n\nThe dataset is stored as a CSV file, so we use read.csv().\nsep = \";\" ensures the correct delimiter is used (European-style CSVs).\nhead(data) helps us quickly preview the dataset.\n\nNow that we have imported the data , it‚Äôs crucial to understand the structure of the dataset to ensure all variables are correctly formatted.\n\n# Check the structure of the dataset\nstr(data)\n\n'data.frame':   769 obs. of  6 variables:\n $ variety         : chr  \"MX\" \"MX\" \"MX\" \"MX\" ...\n $ corpus          : chr  \"PRESEEA\" \"PRESEEA\" \"PRESEEA\" \"PRESEEA\" ...\n $ form            : chr  \"disculpar\" \"disculpar\" \"disculpar\" \"disculpar\" ...\n $ offense_type    : chr  \"inappropriate_behavior\" \"lack_consideration\" \"censored_language\" \"temporal_territory\" ...\n $ face_affected   : chr  \"positive\" \"positive\" \"positive\" \"negative\" ...\n $ face_orientation: chr  \"speaker\" \"hearer\" \"speaker\" \"hearer\" ...\n\n\nIn this step, we need to check for missing values before we start the analysis.\n\n# Check for missing values\nsum(is.na(data))\n\n[1] 0\n\n\nTo further understand the dataset, let‚Äôs generate summary descriptive statistics.\n\n# Get an overview of the dataset\nskim(data)\n\n\nData summary\n\n\nName\ndata\n\n\nNumber of rows\n769\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nvariety\n0\n1\n2\n2\n0\n2\n0\n\n\ncorpus\n0\n1\n3\n10\n0\n9\n0\n\n\nform\n0\n1\n6\n9\n0\n4\n0\n\n\noffense_type\n0\n1\n10\n22\n0\n14\n0\n\n\nface_affected\n0\n1\n8\n8\n0\n2\n0\n\n\nface_orientation\n0\n1\n6\n7\n0\n2\n0\n\n\n\n\n\n\n\n\n\n\n\nTipüìù Quiz Time!\n\n\n\n[Q1] When importing the dataset, why do we specify sep = \";\" in read.csv()?\n\n\n\n\n\nBecause the CSV uses semicolons as the separator character\n\n\n\n\nTo automatically remove rows with missing values\n\n\n\n\nTo indicate that this dataset uses semicolons instead of decimal points\n\n\n\n\nTo convert columns contain texts into factors\n\n\n\n\nTo speed up the import of large datasets like this one\n\n\n\n\n\n\n\n\nüê≠ Click for a hint!\n\n\n\n\n[Q2] What is the purpose of using skim(data) after importing the dataset?\n\n\n\n\n\nTo quickly summarize variable types, missing values, and distributions\n\n\n\n\nTo delete redundant columns automatically\n\n\n\n\nTo detect and replace missing values with averages\n\n\n\n\nTo visualize the data with {ggplot2}\n\n\n\n\n\n\n\n\nüê≠ Click for a hint!",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>The g`R`ammaticalization of 'perd√≥n' in Spanish varieties</span>"
    ]
  },
  {
    "objectID": "B_CaseStudies/Dela/Perdon.html#sec-review",
    "href": "B_CaseStudies/Dela/Perdon.html#sec-review",
    "title": "18¬† The gRammaticalization of ‚Äòperd√≥n‚Äô in Spanish varieties",
    "section": "18.4 Reviewing the variables",
    "text": "18.4 Reviewing the variables\nThe dataset consists primarily of categorical variables, each of which represents a set of predefined categories. Understanding these variables and the levels that they can take is essential before beginning analysis:\n\nvariety: Refers to the regional variety of Spanish in which the apology occurs.\n\nLevels: MX (Mexican) and SP (Peninsular).\n\ncorpus: Indicates the specific corpus or sub-corpus from which the example was drawn.\n\nLevels: PRESEEA, CSCM, CHBC, CME, Ameresco, CORMA, VALESCO, C-ORAL-ROM, and CORLEC.\n\nform: The type of apology marker used in the utterance.\n\nLevels: perd√≥n, disculpar, perdonar, and lo siento\n\noffense_type: Categorizes the nature or source of the offense prompting the apology.\n\nLevels: inappropriate_behavior, criticism_disagreement, obligation, damage_belongings, etc. (14 specific types in total)\n\nface_affected: Describes whose face (i.e., social identity or image) is being addressed or repaired by the apology.\n\nLevels: positive, negative.\n\nface_orientation: Refers to whether the apology is oriented toward preserving the speaker‚Äôs own face or that of another.\n\nLevels: speaker, hearer.\n\n\nSince many of our variables are categorical (variety, form, offense_type, etc.), we convert them to factors to facilitate the analysis in R.\n\ndata_filtered &lt;- data |&gt;\n  mutate(across(c(form, variety, offense_type, face_affected, face_orientation), factor))\n\nNow that our dataset is well-structured, we can move forward to descriptive statistics and visualization Section 18.5.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>The g`R`ammaticalization of 'perd√≥n' in Spanish varieties</span>"
    ]
  },
  {
    "objectID": "B_CaseStudies/Dela/Perdon.html#sec-DelaPloting",
    "href": "B_CaseStudies/Dela/Perdon.html#sec-DelaPloting",
    "title": "18¬† The gRammaticalization of ‚Äòperd√≥n‚Äô in Spanish varieties",
    "section": "18.5 Plotting the results",
    "text": "18.5 Plotting the results\nFirst, we take a look at frequency of the the different forms of apology markers across both Spanish varieties.\n\n# Count frequency of apology markers\napology_counts &lt;- data |&gt;\n  count(variety, form) |&gt;\n  arrange(desc(n))\n\napology_counts |&gt; \n  kable()\n\n\n\n\nvariety\nform\nn\n\n\n\n\nMX\nperd√≥n\n299\n\n\nSP\nperd√≥n\n201\n\n\nSP\nperdonar\n158\n\n\nMX\ndisculpar\n49\n\n\nSP\nlo siento\n35\n\n\nSP\ndisculpar\n12\n\n\nMX\nperdonar\n11\n\n\nMX\nlo siento\n4\n\n\n\n\n\nTo better visualize the contrast, we also create a grouped bar chart.\n\n\nShow figure code\nggplot(apology_counts, aes(x = form, \n                           y = n, \n                           fill = variety)) +\n  geom_col(position = \"dodge\") +\n  labs(\n    title = \"Frequency of Apology Markers by Spanish Variety\",\n    x = \"Apology Marker\",\n    y = \"Count\",\n    fill = \"Variety\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure¬†18.1: Frequency of Apology Markers\n\n\n\n\n\nThis chart confirms the trends reported by Jansegers, Melis & Arrington (2024): perd√≥n is the most frequent marker, especially in Mexican Spanish, while Peninsular Spanish shows a broader mix of forms. These findings align with politeness theory (Brown & Levinson 1987), which explains cultural variation in face strategies, and grammaticalization frameworks (Hopper & Traugott 2003; Heine & Kuteva 2002), which describe how frequent lexical items become discourse markers.\nWhile Figure¬†18.1 earlier provided an overall frequency comparison of apology markers across Spanish varieties, mosaic plots allow for a more detailed visualization. To visualize how apology marker usage differs by Spanish variety, we use geom_mosaic() from the {ggmosaic} package. This plot shows the distribution of the different apology forms across Mexican and Peninsular Spanish, with bar widths reflecting the relative frequencies within each variety.\n\n\nShow figure code\nggplot(data_filtered) +\n  geom_mosaic(aes(x = variety, \n                  fill = form)) +\n  labs(\n    title = \"Mosaic Plot: Apology Marker Distribution by Variety\",\n    x = \"Spanish Variety\",\n    y = \"Proportion of Apology Markers\",\n    fill = \"Apology Marker\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure¬†18.2: Mosaic Plot: Apology Marker Distribution by Variety\n\n\n\n\n\n\n\n\n\n\n\nTipüìù Quiz Time!\n\n\n\n[Q3] What does the height (length) of the bars represent in Figure¬†18.2?\n\n\n\n\n\nThe number of apology marker types across the two Spanish varieties\n\n\n\n\nThe relative frequency of each apology marker within each Spanish variety\n\n\n\n\nThe number of face-orientation contexts in both Mexican and Peninsular Spanish\n\n\n\n\nThe number of corpus observations in each Spanish variety\n\n\n\n\n\n\n\n\nüê≠ Click for a hint!",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>The g`R`ammaticalization of 'perd√≥n' in Spanish varieties</span>"
    ]
  },
  {
    "objectID": "B_CaseStudies/Dela/Perdon.html#reproducing-the-tables",
    "href": "B_CaseStudies/Dela/Perdon.html#reproducing-the-tables",
    "title": "18¬† The gRammaticalization of ‚Äòperd√≥n‚Äô in Spanish varieties",
    "section": "18.6 Reproducing the tables",
    "text": "18.6 Reproducing the tables\nIn this section, we reproduce the main descriptive statistics from Jansegers, Melis & Arrington (2024) regarding the distribution of explicit apology markers across Spanish varieties and face orientations.\nWe begin by reconstructing the contingency table comparing Mexican and Peninsular Spanish in Table¬†18.1. We then break down the use of each form in relation to face-orientation contexts for both Mexican (Table¬†18.2) and Peninsular (Table¬†18.3) speakers. Frequencies and row-wise percentages are included to match the original analysis.\n\n18.6.1 Shared methodology\nAll tables were generated using the following functions:\n\ncount() to obtain for raw frequencies per combination of characteristics.\nproportions() √ó 100 for percentages within each group.\npaste0() to combine counts and percentages.\npivot_wider() to reshape for display comparable to the original publication.\n\nnames_from = form uses the unique apology forms to create new columns.\nvalues_from = n_pct fills those columns with the combined ‚Äúcount (percent)‚Äù strings.\n\n\nWe only varied the grouping/filtering depending on the table‚Äôs focus.\n\n\n18.6.2 Apology forms by variety\nTo begin, we count the number of each apology form in Mexican and Peninsular Spanish, and then calculate row-wise percentages using group_by() and proportions() to show relative frequency across varieties.\n\n\nShow table code\napology_table &lt;- data |&gt;\n  count(variety, form) |&gt;\n  group_by(variety) |&gt;\n  mutate(percentage = round(proportions(n) * 100, 0)) |&gt;\n  mutate(n_pct = paste0(n, \" (\", percentage, \"%)\")) |&gt;\n  select(variety, form, n_pct) |&gt;\n  pivot_wider(names_from = variety, values_from = n_pct)\n\napology_table |&gt; \n  kable()\n\n\n\n\nTable¬†18.1: Apology Marker Distribution by Variety\n\n\n\n\n\n\nform\nMX\nSP\n\n\n\n\ndisculpar\n49 (13%)\n12 (3%)\n\n\nlo siento\n4 (1%)\n35 (9%)\n\n\nperdonar\n11 (3%)\n158 (39%)\n\n\nperd√≥n\n299 (82%)\n201 (50%)\n\n\n\n\n\n\n\n\nTable¬†18.1 is a reproduction of Table 2 from Jansegers, Melis & Arrington (2024). The results show a clear preference for perd√≥n in Mexico, while perdonar is more common in Spain. This contrast reflects broader regional differences in formality and apology strategies.\nThe percentages didn‚Äôt exactly match the original article due to rounding difference. While 49 / 363 equals 13.49%, R‚Äôs round() function rounds it down to 13%, whereas the original paper rounds it up to 14%.\nThis minor discrepancy affects only one cell in the table and does not impact the overall distribution pattern.\n\n\n18.6.3 Conducting a Chi-square test\nBuilding on the findings summarised in Table¬†18.1, we now test whether the observed differences in apology form usage between Mexican and Peninsular Spanish are statistically significant at an alpha level of 0.05. In line with the original study (Jansegers, Melis & Arrington 2024), we applied a Chi-square test to test this.\n\n# Create a contingency table\napology_matrix &lt;- table(data$variety, data$form)\n\n# Run the Chi-square test\nchisq.test(apology_matrix)\n\n\n    Pearson's Chi-squared test\n\ndata:  apology_matrix\nX-squared = 192.35, df = 3, p-value &lt; 2.2e-16\n\n\nThe Chi-Square test that there is a statistically significant association between Spanish variety and apology marker (œá¬≤ = 192.35, df = 3, p &lt; .001), matching the results in Jansegers, Melis & Arrington (2024).\n\n\n18.6.4 Apology forms by face orientation (Mexico)\nTo produce Table 3 from Jansegers, Melis & Arrington (2024), we extend the procedure used to generate Table¬†18.1 by introducing a new composite variable, face_group, which merges face_orientation and face_affected into three analytically meaningful categories:\n\nPositive face S (speaker‚Äôs face affected),\nNegative face H (hearer‚Äôs negative face),\nPositive face H (hearer‚Äôs positive face).\n\nThese reflects the speaker vs.¬†hearer face-threat distinctions used by Jansegers, Melis & Arrington (2024).\n\n\nShow table code\nmexico_forms &lt;- data |&gt;\n  filter(variety == \"MX\") |&gt;\n  mutate(face_group = case_when(\n    face_orientation == \"speaker\" ~ \"Positive face S\",\n    face_orientation == \"hearer\" & face_affected == \"negative\" ~ \"Negative face H\",\n    face_orientation == \"hearer\" & face_affected == \"positive\" ~ \"Positive face H\"\n  )) |&gt;\n  count(face_group, form) |&gt;\n  group_by(face_group) |&gt;\n  mutate(pct = round(100 * n / sum(n))) |&gt;\n  mutate(n_pct = paste0(n, \" (\", pct, \"%)\")) |&gt;\n  select(face_group, form, n_pct) |&gt;\n  pivot_wider(names_from = form, values_from = n_pct)\n\nmexico_forms |&gt; \n  kable()\n\n\n\n\nTable¬†18.2: Apology Forms by Face Orientation (Mexico), Table 3\n\n\n\n\n\n\nface_group\ndisculpar\nlo siento\nperdonar\nperd√≥n\n\n\n\n\nNegative face H\n22 (18%)\n1 (1%)\n6 (5%)\n90 (76%)\n\n\nPositive face H\n12 (40%)\n3 (10%)\n2 (7%)\n13 (43%)\n\n\nPositive face S\n15 (7%)\nNA\n3 (1%)\n196 (92%)\n\n\n\n\n\n\n\n\nOur output confirms the study‚Äôs finding that perd√≥n is strongly favored when the speaker‚Äôs own face is at stake. All percentages and frequencies match those in the original study exactly, without rounding differences.\n\n\n18.6.5 Apology forms by face orientation (Spain)\nWe recreate Table 4 from the original study by grouping the Peninsular Spanish data by face orientation and face affected.\n\n\nShow table code\nspain_forms &lt;- data |&gt;\n  filter(variety == \"SP\") |&gt;\n  mutate(face_group = case_when(\n    face_orientation == \"speaker\" ~ \"Positive face S\",\n    face_orientation == \"hearer\" & face_affected == \"negative\" ~ \"Negative face H\",\n    face_orientation == \"hearer\" & face_affected == \"positive\" ~ \"Positive face H\"\n  )) |&gt;\n  count(face_group, form) |&gt;\n  group_by(face_group) |&gt;\n  mutate(pct = round(100 * n / sum(n))) |&gt;\n  mutate(n_pct = paste0(n, \" (\", pct, \"%)\")) |&gt;\n  select(face_group, form, n_pct) |&gt;\n  pivot_wider(names_from = form, values_from = n_pct)\n\nspain_forms |&gt; \n  kable()\n\n\n\n\nTable¬†18.3: Apology Forms by Face Orientation (Spain), Table 4\n\n\n\n\n\n\nface_group\ndisculpar\nlo siento\nperdonar\nperd√≥n\n\n\n\n\nNegative face H\n7 (4%)\n15 (9%)\n88 (50%)\n66 (38%)\n\n\nPositive face H\n1 (2%)\n14 (22%)\n39 (61%)\n10 (16%)\n\n\nPositive face S\n4 (2%)\n6 (4%)\n31 (19%)\n125 (75%)\n\n\n\n\n\n\n\n\nThe results match those reported in the original study (Jansegers, Melis & Arrington 2024): the verb perdonar is common in hearer-oriented contexts, while perd√≥n appears more in speaker-oriented ones. The small discrepancies observed such as 36% instead of 37% for perd√≥n under Negative face H, or 17% instead of 16% under Positive face H are limited to 1 percentage point and likely stem from rounding differences in the percentage calculations (see ?round()). These very minor differences do not affect the interpretation of the results.\n\n\n\n\n\n\nTipQuiz Time!\n\n\n\nüìù Let‚Äôs check your understanding of the data wrangling steps necessary to generate these tables!\n[Q4] What does the paste0() function do?\n\n\n\n\n\nIt combines count values and percentages into a single vector.\n\n\n\n\nIt formats columns for easy export to Word.\n\n\n\n\nIt calculates percentages.\n\n\n\n\nIt combines two or more vectors into one and separates these vectors with the number 0.\n\n\n\n\n\n\n\n\nüê≠ Click for a hint!\n\n\n\n\n[Q5] Why did we use pivot_wider() in our table generation steps?\n\n\n\n\n\nTo convert the long-format counts into a wide-format table with forms as columns.\n\n\n\n\nTo make the plot display the x-axis in alphabetical order.\n\n\n\n\nTo automatically rename the apology marker variables.\n\n\n\n\nTo filter out rows with zero frequency.\n\n\n\n\n\n\n\n\nüê≠ Click for a hint!",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>The g`R`ammaticalization of 'perd√≥n' in Spanish varieties</span>"
    ]
  },
  {
    "objectID": "B_CaseStudies/Dela/Perdon.html#conclusion",
    "href": "B_CaseStudies/Dela/Perdon.html#conclusion",
    "title": "18¬† The gRammaticalization of ‚Äòperd√≥n‚Äô in Spanish varieties",
    "section": "18.7 Conclusion",
    "text": "18.7 Conclusion\nThis project set out to reproduce the main findings of Jansegers, Melis & Arrington (2024), which explored the grammaticalization of perd√≥n in Mexican and Peninsular Spanish. We were able to reproduce the findings of the original authors: in this corpus data, perd√≥n is far more frequent in Mexican Spanish, while Peninsular Spanish speakers make use of a broader range of apology forms, including perdonar and lo siento. The frequency tables and percentage distributions that we reproduced from the author‚Äôs original data closely mirror those published in the original study.\nBeyond reproducing the numerical results, we also matched the study‚Äôs visualizations using bar plots (Figure¬†18.1) and mosaic plots (Figure¬†18.2), which clearly show how apology strategies vary by variety and face orientation. These visuals help to illustrate the claim that perd√≥n has undergone grammaticalization‚Äîespecially in Mexican Spanish into a high-frequency discourse marker used to manage politeness.\nOverall, the reproduction validates both the descriptive and statistical results of the original study, demonstrating how perd√≥n‚Äôs usage reflects broader sociopragmatic and grammatical trends across Spanish varieties. Minor formatting or rounding differences aside, our results strongly align with the published findings and reinforce the value of data sharing in corpus-based linguistic research.\n\n18.7.1 Suggestions for future research\nWhile this project successfully reproduces and extends the findings of Jansegers, Melis & Arrington (2024), further studies could enrich the analysis by incorporating additional Spanish varieties beyond Mexico and Spain, or by examining sociolinguistic variables such as speaker age, gender, and social status (see, e.g., Hern√°ndez-Campoy & Conde-Silvestre 2012). Longitudinal research could also explore whether the grammaticalization of perd√≥n continues to evolve over time.\n\n\n\n\n\n\nNoteüìå Implementing Citation Management with .bib in Quarto\n\n\n\n\n\nTo ensure standardized citation formatting, this project employs a BibTeX (.bib) file, referenced within the YAML metadata. The .bib file (e.g., final.bib) is specified under the bibliography field, allowing Quarto to format in-text citations and automatically generate a reference list.\nAdditionally, a Citation Style Language (CSL) file (apa.csl) is included to enforce APA-style referencing. This setup enables seamless integration of bibliographic data while maintaining consistency across the document.\nIn-text citations are implemented using the @key notation, which Quarto automatically resolves into properly formatted references during rendering.\nFor more information on how to insert bibliographic references in Quarto documents, see Section 14.9.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>The g`R`ammaticalization of 'perd√≥n' in Spanish varieties</span>"
    ]
  },
  {
    "objectID": "B_CaseStudies/Dela/Perdon.html#packages-used-in-this-chapter",
    "href": "B_CaseStudies/Dela/Perdon.html#packages-used-in-this-chapter",
    "title": "18¬† The gRammaticalization of ‚Äòperd√≥n‚Äô in Spanish varieties",
    "section": "Packages used in this chapter",
    "text": "Packages used in this chapter\n\n\n[1] H. Jeppson, H. Hofmann, and D. Cook. _ggmosaic: Mosaic Plots in the\nggplot2 Framework_. R package version 0.3.3. 2021.\n&lt;https://github.com/haleyjeppson/ggmosaic&gt;.\n\n[2] K. M√ºller and H. Wickham. _tibble: Simple Data Frames_. R package\nversion 3.2.1. 2023. &lt;https://tibble.tidyverse.org&gt;.\n\n[3] B. Ripley. _nnet: Feed-Forward Neural Networks and Multinomial\nLog-Linear Models_. R package version 7.3-20. 2025.\n&lt;http://www.stats.ox.ac.uk/pub/MASS4/&gt;.\n\n[4] V. Spinu, G. Grolemund, and H. Wickham. _lubridate: Make Dealing\nwith Dates a Little Easier_. R package version 1.9.3. 2023.\n&lt;https://lubridate.tidyverse.org&gt;.\n\n[5] E. Waring, M. Quinn, A. McNamara, et al. _skimr: Compact and\nFlexible Summaries of Data_. R package version 2.1.5. 2022.\n&lt;https://docs.ropensci.org/skimr/&gt;.\n\n[6] H. Wickham. _forcats: Tools for Working with Categorical Variables\n(Factors)_. R package version 1.0.0,\n&lt;https://github.com/tidyverse/forcats&gt;. 2023.\n&lt;https://forcats.tidyverse.org&gt;.\n\n[7] H. Wickham. _tidyverse: Easily Install and Load the Tidyverse_. R\npackage version 2.0.0, &lt;https://github.com/tidyverse/tidyverse&gt;. 2023.\n&lt;https://tidyverse.tidyverse.org&gt;.\n\n[8] H. Wickham, W. Chang, L. Henry, et al. _ggplot2: Create Elegant\nData Visualisations Using the Grammar of Graphics_. R package version\n3.5.1, &lt;https://github.com/tidyverse/ggplot2&gt;. 2024.\n&lt;https://ggplot2.tidyverse.org&gt;.\n\n[9] H. Wickham, R. Fran√ßois, L. Henry, et al. _dplyr: A Grammar of Data\nManipulation_. R package version 1.1.4. 2023.\n&lt;https://dplyr.tidyverse.org&gt;.\n\n[10] H. Wickham and L. Henry. _purrr: Functional Programming Tools_. R\npackage version 1.0.2. 2023. &lt;https://purrr.tidyverse.org&gt;.\n\n[11] H. Wickham, J. Hester, and J. Bryan. _readr: Read Rectangular Text\nData_. R package version 2.1.5. 2024. &lt;https://readr.tidyverse.org&gt;.\n\n[12] H. Wickham, D. Vaughan, and M. Girlich. _tidyr: Tidy Messy Data_.\nR package version 1.3.1. 2024. &lt;https://tidyr.tidyverse.org&gt;.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>The g`R`ammaticalization of 'perd√≥n' in Spanish varieties</span>"
    ]
  },
  {
    "objectID": "B_CaseStudies/Dela/Perdon.html#references",
    "href": "B_CaseStudies/Dela/Perdon.html#references",
    "title": "18¬† The gRammaticalization of ‚Äòperd√≥n‚Äô in Spanish varieties",
    "section": "References",
    "text": "References\n[1] P. Brown and S. C. Levinson. Politeness: Some Universals in Language Usage. Cambridge University Press, 1987. https://www.cambridge.org/core/books/politeness-some-universals-in-language-usage/.\n[2] B. Heine and T. Kuteva. World Lexicon of Grammaticalization. Cambridge University Press, 2002. https://doi.org/10.1017/CBO9780511613463.\n[3] J. M. Hern√°ndez-Campoy and J. C. Conde-Silvestre. The Handbook of Historical Sociolinguistics. Wiley-Blackwell, 2012. https://doi.org/10.1017/S0047404515000688.\n[4] P. Hopper and E. C. Traugott. Grammaticalization. Cambridge University Press, 2003. https://doi.org/10.1017/CBO9781139165525.\n[5] M. Jansegers, E. Melis, and G. Arrington. ‚ÄúThe Grammaticalization of Perd√≥n in Spanish Varieties‚Äù. In: Journal of Pragmatics 192 (2024), pp.¬†15-32. https://doi.org/10.3390/languages9010013.\n[6] E. C. Traugott and R. B. Dasher. Regularity in Semantic Change. Cambridge University Press, 2001. https://doi.org/10.1017/CBO9780511486500.\n[7] H. Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer, 2016. https://ggplot2-book.org/.\n\n\n\n\nBrown, Penelope & Stephen C. Levinson. 1987. Politeness: Some universals in language usage. Cambridge University Press. https://www.cambridge.org/core/books/politeness-some-universals-in-language-usage/.\n\n\nHeine, Bernd & Tania Kuteva. 2002. World lexicon of grammaticalization. Cambridge University Press. https://doi.org/10.1017/CBO9780511613463.\n\n\nHern√°ndez-Campoy, Juan Manuel & Juan Camilo Conde-Silvestre. 2012. The handbook of historical sociolinguistics. Wiley-Blackwell. https://doi.org/10.1017/S0047404515000688.\n\n\nHopper, Paul & Elizabeth Closs Traugott. 2003. Grammaticalization. Cambridge University Press. https://doi.org/10.1017/CBO9781139165525.\n\n\nJansegers, Marlies, Estela Melis & G. Arrington. 2024. The grammaticalization of perd√≥n in spanish varieties. Journal of Pragmatics 192. 15‚Äì32. https://doi.org/10.3390/languages9010013.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>The g`R`ammaticalization of 'perd√≥n' in Spanish varieties</span>"
    ]
  },
  {
    "objectID": "B_CaseStudies/Hannah-Beyhan/Retrieval.html",
    "href": "B_CaseStudies/Hannah-Beyhan/Retrieval.html",
    "title": "19¬† LeaRning Direction Effects in Retrieval Practice on EFL Vocabulary Learning",
    "section": "",
    "text": "Chapter overview\nThis chapter aims to reproduce the analyses published in:\nBased on the original data from Terai, Yamashita & Pasich (2021), we show how R can be used in Second Language Acquisition (SLA) research. This chapter will walk you through how to:\nBy the end of this chapter, you will have experience of fitting and interpreting mixed-effects logistics regression models in R on experimental data.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Lea`R`ning Direction Effects in Retrieval Practice on EFL Vocabulary Learning</span>"
    ]
  },
  {
    "objectID": "B_CaseStudies/Hannah-Beyhan/Retrieval.html#introducing-the-study",
    "href": "B_CaseStudies/Hannah-Beyhan/Retrieval.html#introducing-the-study",
    "title": "19¬† LeaRning Direction Effects in Retrieval Practice on EFL Vocabulary Learning",
    "section": "19.1 Introducing the study",
    "text": "19.1 Introducing the study\nIn this chapter, we attempt to reproduce the results of an SLA study by Terai, Yamashita & Pasich (2021). The study focuses on learning directions in pair-associated vocabulary learning: L2 to L1, where L2 words are used as stimuli and responses are given in L1 vs.¬†L1 to L2, where it is the other way around. As opposed to studying practices where both a target and the answer are simultaneously presented, pair-associated learning involves retrieval. Retrieval is defined as the process of accessing stored information and plays a crucial role in retaining a learned word in memory (Terai, Yamashita & Pasich 2021: 1116-1117). Previous findings regarding the efficacy of the two types of learning directions had been inconsistent. The study that we will reproduce focuses on the relationship between L2 proficiency and the effectiveness of the two learning directions in paired-associate learning in L2 vocabulary acquisition. It aims to answer two research questions:\n\nWhich learning direction is more effective for vocabulary acquisition, L1 to L2 learning or L2 to L1 learning?\nHow does L2 proficiency influence the effects of L2 to L1 and L1 to L2 learning?\n\nTo answer these questions, Terai, Yamashita & Pasich (2021) designed an experiment in which Japanese EFL (English as a Foreign Language) learners studied word pairs and then completed retrieval practice tasks in different conditions. After learning, students were tested on their ability to produce the target vocabulary items in both Japanese (their L1) and English (their L2). Terai, Yamashita & Pasich (2021) formulated two hypotheses:\n\nThere is no significant difference between the two learning directions.\nThe effect of the learning direction depends on the learner‚Äôs L2 proficiency (Terai, Yamashita & Pasich 2021: 1121-1122).",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Lea`R`ning Direction Effects in Retrieval Practice on EFL Vocabulary Learning</span>"
    ]
  },
  {
    "objectID": "B_CaseStudies/Hannah-Beyhan/Retrieval.html#retrieving-the-original-data",
    "href": "B_CaseStudies/Hannah-Beyhan/Retrieval.html#retrieving-the-original-data",
    "title": "19¬† LeaRning Direction Effects in Retrieval Practice on EFL Vocabulary Learning",
    "section": "19.2 Retrieving the original data",
    "text": "19.2 Retrieving the original data\nWe will use the study‚Äôs original data for our reproduction, which the authors have made openly accessible on IRIS, an open-access database for language research data and materials.\nNot only have the authors made their research data available, they have also shared their R code. Our reproduction will examine and run this R code to investigate and interpret the statistical analyses and results presented in the original publication.\nThe dataset (dataset1_ssla_20210313.csv) contains data for each participant and item, including the following key variables:\n\nAnswer (whether the participant gave the correct answer (1 = correct, 0 = incorrect))\nTest (distinguishing the two test types: L1 Production and L2 Production)\nCondition (distinguishing the two learning directions: L2 -&gt; L1 or L1 -&gt; L2)\n\nBut more on that later.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Lea`R`ning Direction Effects in Retrieval Practice on EFL Vocabulary Learning</span>"
    ]
  },
  {
    "objectID": "B_CaseStudies/Hannah-Beyhan/Retrieval.html#set-up-and-data-import",
    "href": "B_CaseStudies/Hannah-Beyhan/Retrieval.html#set-up-and-data-import",
    "title": "19¬† LeaRning Direction Effects in Retrieval Practice on EFL Vocabulary Learning",
    "section": "19.3 Set-up and Data import",
    "text": "19.3 Set-up and Data import\nBefore importing the dataset and starting on our project, we need to load all the packages that we will need. You may have to first install some of these packages using the install.packages() command in your console.\n\nlibrary(here) \nlibrary(tidyverse)\n\nNext, we import the data. In contrast to Terai, Yamashita & Pasich (2021)‚Äôs code, we used the {here} package for importing the data. This package creates paths relative to the top-level directory and therefore makes it easier to reference files in a reproducible manner (see Section 6.5).\n\nVocabularyA_L1production &lt;- read.csv(file = here(\"B_CaseStudies\", \"Hannah-Beyhan\", \"data\", \"vocaA_L1pro_20210313.csv\"))\nVocabularyA_L2production &lt;- read.csv(file = here(\"B_CaseStudies\", \"Hannah-Beyhan\", \"data\", \"vocaA_L2pro_20210313.csv\"))\nVocabularyB_L1production &lt;- read.csv(file = here(\"B_CaseStudies\", \"Hannah-Beyhan\", \"data\", \"vocaB_L1pro_20210313.csv\"))\nVocabularyB_L2production &lt;- read.csv(file = here(\"B_CaseStudies\", \"Hannah-Beyhan\", \"data\", \"vocaB_L2pro_20210313.csv\"))\n\n# Loading the dataset\ndat &lt;- read.csv(file = here(\"B_CaseStudies\", \"Hannah-Beyhan\", \"data\", \"dataset1_ssla_20210313.csv\"), header=T)\n\nAs already mentioned, the dataset contains different information on items and participants. The other four files contain the vocabulary scores for each item, for both L1 and L2 production tests and both Vocabulary A and Vocabulary B items. Every cell contains either 1 (correct) or 0 (incorrect).",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Lea`R`ning Direction Effects in Retrieval Practice on EFL Vocabulary Learning</span>"
    ]
  },
  {
    "objectID": "B_CaseStudies/Hannah-Beyhan/Retrieval.html#descriptive-statistics",
    "href": "B_CaseStudies/Hannah-Beyhan/Retrieval.html#descriptive-statistics",
    "title": "19¬† LeaRning Direction Effects in Retrieval Practice on EFL Vocabulary Learning",
    "section": "19.4 Descriptive statistics",
    "text": "19.4 Descriptive statistics\nBefore we dive into the descriptive statistics conducted in Terai, Yamashita & Pasich (2021), we need to explain that one part of it will not be reproduced. While descriptive statistics about the participants (age, years of learning, etc.) are mentioned in the original paper (Terai, Yamashita & Pasich 2021: 1122-1123), we were not able to find the data to reproduce these findings. That said, it is common for participant data not to be shared to protect their identities. Therefore, in this section, we will focus on reproducing the descriptive statistics of the target items (40 English words used in the study), test types and reliability testing. We‚Äôll start with the target items, which are found in Table 3 in the paper (Terai, Yamashita & Pasich 2021: 1123).\nMost of the numbers reproduced here are statistics already introduced in the textbook: mean, median, maximum and minimum, as well as the standard deviation (SD) (see Chapter 8). In addition, Terai, Yamashita & Pasich (2021) report skewness and kurtosis. Skewness and kurtosis are both measures of the shape and distribution of a variable. Skewness measures the asymmetry of a distribution. Simply put, skewness describes the proportion of a distribution which differs from a completely symmetrical shape (Parajuli 2023):\n\nSkew = 0 -&gt; perfectly symmetrical.\nNegative Skew -&gt; longer ‚Äútail‚Äù on the left.\nPositive Skew -&gt; longer ‚Äútail‚Äù on the right.\n\nKurtosis is a measure of peaks of a distribution. Basically, it is a description of how the peaks compare to a normal distribution (Parajuli 2023):\n\nKurtosis = 0 -&gt; similar to a normal distribution.\nPositive kurtosis -&gt; sharper peak (more concentrated in the center).\nNegative kurtosis -&gt; flatter peak. (Parajuli 2023)\n\n\n19.4.1 Reproduction of Table 3\nIn this section, our aim is to reproduce Table 3 from Terai, Yamashita & Pasich (2021): 1123, which displays the descriptive statistics of the target items of the study. Unfortunately, the code for all the tables shown in Terai, Yamashita & Pasich (2021) was not included in the shared R code. But as we have the data in the form of a .csv file, this is not a problem! We have the organized data ready for us to analyze. Thus, we will learn how to reproduce the data of Table 3 step-by-step.\nIn the following chunk, we load some necessary libraries and use the trimws() function, which removes space from the beginning and end of the text. This helps to prevent issues like ‚Äúberth‚Äù vs.¬†‚Äúberth‚Äù being treated as two different word forms.\n\n# Load libraries (packages need to first be installed, if they aren't already)\n\nlibrary(moments) #for skewness and kurtosis\nlibrary(knitr) #for nice tables\nlibrary(kableExtra) #for nicer tables\n\ndat$ItemID &lt;- trimws(dat$ItemID)\n\nIn the paper by Terai, Yamashita & Pasich (2021) we can see that the variables for the English (L2-related variables) words they used are: Frequency, Syllables, and Letters. For Japanese (L1-related variables): Frequency, Letters, and Mora (syllables) as well as Familiarity (Fami A). Fami (A) refers to familiarity ratings taken from the large database by Amano & Kondo (2000) as the authors explain (Terai, Yamashita & Pasich 2021: 1123). These ratings showed how familiar each Japanese word is to the general population. The second familiarity ratings (Fami (B)), represents the participants‚Äô own familiarity ratings collected for this study. We were not able to reproduce Fami (B) as the data was not made public.\n\n\nShow the R code to generate the table below.\nvar_info &lt;- data.frame(\n  Column = c(\"ItemID\", \"L2Frequency\", \"Syllables\", \"Alphabet\", \"L1Frequency\", \"WordLength\", \"mora\", \"Familiarity\"),\n  Description = c(\n    \"English target word (40 items)\",\n    \"Frequency of the word in English (COCA: Corpus of Contemporary American English)\",\n    \"Number of syllables in the English word\",\n    \"Number of letters in the English word ('Letters' in the L2 table)\",\n    \"Frequency of the word in Japanese (Amano & Kondo, 1999)\",\n    \"Number of letters in the Japanese orthographic form\",\n    \"Number of mora (Japanese syllable-like units)\",\n    \"Participants Familiarity ratings from current study (Fami (B))\"\n  )\n)\n\nkable(var_info) %&gt;%\n  kable_styling(full_width = FALSE, position = \"center\")\n\n\n\n\nTable¬†19.1: Description of the Dataset Variables\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nItemID\nEnglish target word (40 items)\n\n\nL2Frequency\nFrequency of the word in English (COCA: Corpus of Contemporary American English)\n\n\nSyllables\nNumber of syllables in the English word\n\n\nAlphabet\nNumber of letters in the English word ('Letters' in the L2 table)\n\n\nL1Frequency\nFrequency of the word in Japanese (Amano & Kondo, 1999)\n\n\nWordLength\nNumber of letters in the Japanese orthographic form\n\n\nmora\nNumber of mora (Japanese syllable-like units)\n\n\nFamiliarity\nParticipants Familiarity ratings from current study (Fami (B))\n\n\n\n\n\n\n\n\nBasically, what we are doing next is reshaping our data from participant-focused to item-focused. The raw dataset has multiple rows per word because each participant contributed several responses and thus has their own row per word. For example, the word ‚Äúweasel‚Äù appears many times, once for each participant. But to compute descriptive statistics of the items, we only want one row per word.\nTo solve this issue we use the functions group_by() and summarise() to turn all those rows into just a single one per word. With group_by(ItemID), we group all rows together that contain the same word. Combining the summarise() and first() functions then creates our new dataset by keeping only the first mention of each word‚Äôs frequency, number of syllables, etc. Only for the familiarity ratings do we calculate the mean() across all participants as word familiarity differs across the participants. The last argument .groups = \"drop\" removes the grouping structure so that the resulting dataset is no longer grouped by ItemID.\n\nitems &lt;- dat |&gt;\n  group_by(ItemID) |&gt;\n  summarise(\n    L2Frequency = first(L2Frequency),\n    Syllables = first(Syllables),\n    Alphabet = first(Alphabet),\n    L1Frequency = first(L1Frequency),\n    WordLength = first(WordLength),\n    mora = first(mora),\n    familiarity = mean(familiarity, na.rm = TRUE), \n    .groups = \"drop\"\n  )\n\nWe want to do a quick check before we continue: Did we correctly import all 40 target words from the study (as described in Terai, Yamashita & Pasich 2021: 1123) into our items dataframe?\n\nnrow(items)\n\n[1] 40\n\n\nWith this command we can also see what kind of words the authors were examining:\n\nhead(items, 5)\n\n# A tibble: 5 √ó 8\n  ItemID L2Frequency Syllables Alphabet L1Frequency WordLength  mora familiarity\n  &lt;chr&gt;        &lt;int&gt;     &lt;int&gt;    &lt;int&gt;       &lt;int&gt;      &lt;int&gt; &lt;int&gt;       &lt;dbl&gt;\n1 alcove         932         2        6         178          4     4        5.53\n2 azalea         450         3        6         230          3     3        5.81\n3 badger         818         2        6           6          4     4        3.94\n4 berth         1851         1        5         245          4     4        5.53\n5 billow         231         2        6         362          4     4        5.16\n\n\nIn order to avoid repeating code (and because we are lazy), we created our own function that computes all the descriptive statistics (mean, SD, median, min, max, skew, kurtosis) we need for each variable. We write our own function to do so (which you can think of as a ‚Äúsmall machine‚Äù) that we will name quick_stats() and that will do the calculations for us. quick_stats &lt;- funcion(x) creates this function with the input ‚Äòx‚Äô which represents whichever variable we want to analyze. Inside of our function, we use data.frame() to organize our statistics before we begin the calculations. Each function within our function takes our input variable ‚Äòx‚Äô and computes the statistics. The functions skewness() and kurtosis() come from the {moments} package that we loaded earlier.\n\nquick_stats &lt;- function(x) {\n  data.frame(\n    Mean = mean(x, na.rm = TRUE),\n    SD = sd(x, na.rm = TRUE),\n    Median = median(x, na.rm = TRUE), \n    Minimum = min(x, na.rm = TRUE),\n    Maximum = max(x, na.rm = TRUE),\n    Skew = moments::skewness(x, na.rm = TRUE),\n    Kurtosis = moments::kurtosis(x, na.rm = TRUE)\n    )\n}\n\nNow that we have most of what we need for the tables, we can start to build them up! We need to create two subtables.\nThe code below does this with two important functions: rbind() which stacks the rows we want to create on top of each other to build the final table which we will print in the next step. And cbind() which creates columns that stick together side by side. For example cbind(Variable = \"Frequency\", desc(items$L2Frequency) creates a column with the name ‚ÄúVariable‚Äù with the value ‚ÄúFrequency‚Äù next to the summary of the output of quick_stats(). With our helper function desc() we then insert the statistics from our data, in this case it is the ‚ÄúL2Frequency‚Äù. Basically, we are creating here three rows stacked on top of each other for tab_L2, and four rows for tab_L1. As you see we only have to run our helper function quick_stats() seven times!\n\ntab_L2 &lt;- rbind(\n  cbind(Variable = \"Frequency\", quick_stats(items$L2Frequency)),\n  cbind(Variable = \"Syllables\", quick_stats(items$Syllables)),\n  cbind(Variable = \"Letters\", quick_stats(items$Alphabet))\n)\n\ntab_L1 &lt;- rbind(\n  cbind(Variable = \"Frequency\", quick_stats(items$L1Frequency)),\n  cbind(Variable = \"Letters\", quick_stats(items$WordLength)),\n  cbind(Variable = \"Mora (syllables)\", quick_stats(items$mora)),\n  cbind(Variable = \"Fami (A)\", quick_stats(items$familiarity))\n)\n\nPerfect, the next and easiest part of building up our two subtables is captioning them and then finally printing them. Here we need the kable() and the kable_classic() function. We use these functions to make the tables look easy to read. The kable() function is quite nice because it helps to make the R code output look a little more professional. The function comes from the {knitr} package. kable(tab_L2, ...) takes our tab_2 data frame and converts it into a formatted table which we then caption with the argument caption = \"...\". With the argument align = \"lrrrrrr\" we control the way the text is aligned in each column. the ‚Äòl‚Äô means left-aligned, while every added ‚Äúr‚Äù means the remaining columns should be right-aligned. With the argument digits = 2 we display all numeric values to a maximum of two decimal places.\nThe kable_classics() function comes from the {kableExtra} package we loaded earlier. It is also another function to tidy up our table style. The command full_width = FALSE makes sure that the table does not stretch too wide. When you run this code, you will see a nice table with clear structure and formatting which will make it easier to read and interpret, it also looks ready to be published.\n\nkable(tab_L2,\n      align = \"lrrrrrrr\",\n      digits = 2) |&gt;\n  kable_classic(full_width = FALSE)\n\n\n\nTable¬†19.2: Table 3a. Descriptive statistics for L2 (English) item properties.\n\n\n\n\n\n\nVariable\nMean\nSD\nMedian\nMinimum\nMaximum\nSkew\nKurtosis\n\n\n\n\nFrequency\n1025.33\n851.93\n813.5\n51\n3930\n1.51\n5.21\n\n\nSyllables\n2.00\n0.91\n2.0\n1\n5\n0.84\n4.06\n\n\nLetters\n6.22\n1.75\n6.0\n3\n12\n0.97\n4.52\n\n\n\n\n\n\n\n\n\nkable(tab_L1,\n      align = \"lrrrrrrr\",\n      digits = 2) |&gt;\n  kable_classic(full_width = FALSE)\n\n\n\nTable¬†19.3: Table 3b. Descriptive statistics for L1 (Japanese) item properties.\n\n\n\n\n\n\nVariable\nMean\nSD\nMedian\nMinimum\nMaximum\nSkew\nKurtosis\n\n\n\n\nFrequency\n596.90\n885.49\n275.00\n6.00\n5109.00\n3.59\n18.10\n\n\nLetters\n3.67\n1.07\n4.00\n1.00\n6.00\n0.17\n3.48\n\n\nMora (syllables)\n3.52\n0.96\n4.00\n1.00\n6.00\n0.19\n4.29\n\n\nFami (A)\n5.22\n0.64\n5.36\n3.72\n6.38\n-0.63\n2.83\n\n\n\n\n\n\n\n\nOur reproduced tables Table¬†19.2 and Table¬†19.3 now closely match Table 3 (Terai, Yamashita & Pasich 2021: 1123)! The L2 frequency has a mean of about 1025 with a quite large SD (~852), which shows us that some English words were quite frequent while others were much rarer. The skewness is positive (~1.51), although slightly different than the one reported in the paper (~1.57). The difference is only small and still shows that the distribution has a ‚Äúlong tail‚Äù of very high-frequency words. English words are, on average, 2 syllables, and range from 1 to 5 syllables with only a slight positive skewness which means that most words are short and only a few are longer. Again we note a small difference in our skewness metric compared to the one reported in the original study. On average, words are 6.2 letters long, again with a positive skewness. For the L1 variables, the frequency distribution also shows a strong positive skew, which means that few Japanese words are very common, but many are quite rare. Fami (A) averages about 5.2 on a 7-point scale, with a pretty low SD, suggesting that the words were mostly familiar, meaning participants generally recognized these Japanese words easily.\nOur skewness values match the original paper fairly closely, with only small differences. However, our kurtosis values differ quite a lot from the published results. This may seem like a huge issue at first, but is actually quite normal. Kurtosis calculations can vary between packages, for example the {moments} package we used calculates excess kurtosis, while the authors of the original paper might have used a different method. We tried several methods to reproduce the kurtosis values from the original publication, but failed to produce the values reported in the original study. We calculated both the excess kurtosis and the pearson kurtosis (see code chunk below), but no numbers match the ones in the paper.\n\n# Instead of mora any other variable of Table 3 can be added to calculate kurtosis\nexcess_kurt &lt;- kurtosis(dat$mora, na.rm = TRUE) \npearson_kurt &lt;- excess_kurt + 3\nround(c(excess = excess_kurt, pearson = pearson_kurt), 3)\n\n excess pearson \n  4.285   7.285 \n\n\nThere are several possibilities why this is the case. There might be an outlier that may have been removed, or maybe the authors used another method. The issue with the differences in our numbers and the ones by Terai, Yamashita & Pasich (2021) is that the differences are so big the graphs are going to look quite different, especially for the Fami (A) variable. In the original paper, the kurtosis shows -0.02 which would be a slightly flatter graph, while our kurtosis number 4.29 would be sharper. This is why, in an extension, we have added some graphs to show the importance of skewness and kurtosis and what exactly these numbers show.\n\n\n\n\n\n\nNoteExtension: Visualizations of Table 3\n\n\n\nTable 3 from the original study displays a summary of many descriptive statistics (mean, SD, range, skewness, kurtosis) for the 40 target words, but these numbers are much easier to understand when they are visualized. Below we want to turn some key variables of Table 3 into quick histograms to visually connect the ideas of skewness and kurtosis to the actual items used in this study.\n\n# The {tidyverse} library needs to be loaded if you have not done it before.\n\nitems &lt;- dat |&gt;\n  group_by(ItemID) |&gt;\n  summarise(\n    L2Frequency  = first(L2Frequency),\n    Syllables    = first(Syllables),\n    Letters_L2   = first(Alphabet),\n    L1Frequency  = first(L1Frequency),\n    Letters_L1   = first(WordLength),\n    Mora         = first(mora),\n    Familiarity  = mean(familiarity, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\nWe will start with a graph of L2 Frequencies. To do this we use the ggplot() command. The first line of code here ggplot(data = items, aes(x = L2Frequency)) tells ggplot to basically use the ‚Äúitems‚Äù dataset and put the L2 frequencies of each word onto the x-axis. geom_density(...) creates the actual density plot. To fully understand where the numbers in the table come from, we add reference lines: geom_vline(xintercept = mean(...)) for the mean value, and geom_vline(xintercept = median(...), linetype = \"dashed\"...) for a dashed line at the median.\nDo not be too confused by the numbers on the y-axis. The values can be read as a kind of probability, a higher peak means more words are around that frequency, while a lower peak means fewer words around that frequency.\n\nggplot(items, aes(x = L2Frequency)) +\n  geom_density(fill = \"lightblue\") +\n  geom_vline(xintercept = mean(items$L2Frequency), linewidth = 0.7) +\n  geom_vline(xintercept = median(items$L2Frequency), linetype = \"dashed\", linewidth = 0.7) +\n  scale_y_continuous(labels = scales::comma) +\n  scale_x_continuous(labels = scales::comma) +\n  labs(x = \"L2 frequency (COCA)\",\n       y = \"Probability\") +\ntheme_minimal()\n\n\n\n\n\n\n\nFigure¬†19.1: Distribution of L2 word frequencies used in the study\n\n\n\n\n\nIn Figure¬†19.1 we can clearly see a right-skewed distribution which we saw in the table with the positive skewness values (~1.51). As you may recall, a normally distributed graph would look like a symmetric bell curve. By contrast, here, we see that most words have lower or medium frequency with only a few really high-frequency words (see the long right tail). The mean sits to the right of the median which is another characteristic of positive skew. Our kurtosis value (~5.21) shows us the sharpness or peakedness of our curve which you can see quite nicely when you compare the graph for L2 frequency to the one for L1 frequency (see Figure¬†19.2).\n\nggplot(items, aes(x = L1Frequency)) +\n  geom_density(fill = \"lightblue\") +\n  geom_vline(xintercept = mean(items$L1Frequency), linewidth = 0.7) +\n  geom_vline(xintercept = median(items$L1Frequency), linetype = \"dashed\", linewidth = 0.7) +\n  scale_x_continuous(labels = scales::comma) +\n  scale_y_continuous(labels = scales::comma) +\n  labs(x = \"L1 frequency\",\n       y = \"Probability\",\n       caption = \"Solid line = mean; dashed line = median\") +\ntheme_minimal()\n\n\n\n\n\n\n\nFigure¬†19.2: Distribution of L1 word frequencies used in the study\n\n\n\n\n\nFigure¬†19.2 shows that a right-skew that is even stronger than for L2 word frequencies (skew ~3.59). This shows once again that most words are lower-frequency, there are only very few high-frequency words. Here, with a kurtosis value of ~18.10 we can see that with higher positive numbers the peak becomes sharper and sharper.\n\n\n\n\n19.4.2 Descriptive statistics of the tests and reliability testing\nIn this section, we will attempt to reproduce the authors‚Äô descriptive statistics regarding the two types of post-tests and calculate Cronbach‚Äôs Œ± to estimate their reliability.\n\n19.4.2.1 Reliability analysis\nTo conduct reliability analysis, the {psych} package needs to be installed and loaded.\n\nlibrary(psych)\n\nTo calculate Cronbach‚Äôs Œ±, Terai, Yamashita & Pasich (2021) applied the alpha() function to the vocabulary scores of both the L1 and L2 production sets. Below, we only show the code of the reliability analysis for Vocabulary A scores of the L1 production dataset, as an example:\n\nalpha(VocabularyA_L1production[,-1], warnings=FALSE)\n\n\n\nShow code for reliability analysis for the rest of the dataset\n# Vocabulary A (L2 production)\n\nalpha(VocabularyA_L2production[,-1], warnings = FALSE)\n\n# Vocabulary B (L1 production)\n\nalpha(VocabularyB_L1production[,-1], warnings = FALSE)\n\n# Vocabulary B (L2 production)\n\nalpha(VocabularyB_L2production[,-1], warnings = FALSE)\n\n\nThese reliability analyses compute Cronbach‚Äôs Œ±, a measure of internal consistency of tests. This measure indicates whether responses are consistent across items. Before interpreting these results, we will get to the descriptive statistics of the two types of post-tests.\n\n\n19.4.2.2 Accuracy\nTo investigate accuracy, Terai, Yamashita & Pasich (2021) processed the collected results for each participant ‚Äî their ID, how many items they answered, and how many they got right ‚Äî and visualized them as boxplots. In the original code, the authors loaded the dataset for this section again and stored it under a different object name dat.acc. We skipped that step and simply used dat instead as it is already available as an R object in our local environment.\nThe following code chunk, which analyses data pertaining to L2 -&gt; L1 direction of the L1 production set, processes the results for each participant ‚Äî their ID, answered items, correct answers ‚Äî and puts them into a clean table. Therefore, it gives us the core numbers necessary to describe and analyze learners‚Äô vocabularly retrieval accuracy.\n\n# L2 ‚Üí L1 (L1 production test)\n\nz &lt;- unique(dat$ID)\n\nScore &lt;- c()\nIDes &lt;- c()\ntry &lt;- c()\n\nfor (i in z){\n  dat |&gt; \n    dplyr::filter(ID == i, Condition == \"L2L1\", Test == \"L1 Production\") |&gt; \n    dplyr::select(Answer) -&gt; acc_recT\n  a &lt;- as.vector(unlist(acc_recT))\n  b &lt;- sum(a)\n  c &lt;- length(a)\n  Score &lt;- c(Score, b)\n  IDes &lt;- c(IDes, i)\n  try &lt;- c(try, c)\n}\n\naccu_L2L1_L1Pro &lt;- data.frame(IDes, try, Score)\nnames(accu_L2L1_L1Pro) &lt;- c(\"ID\", \"Try\", \"Score\")\naccu_L2L1_L1Pro\n\nWe want to take the time to explain this code chunk in detail. The first line takes unique IDs from dat and saves it as z, z therefore being a vector of unique IDs. This part was slightly changed in comparison to Terai, Yamashita & Pasich (2021)‚Äôs code, which used three lines of code for this step. Further, a for loop is used to iterate over elements of this vector and assigning the IDs to the variable i. Inside this loop, for each i (a unique ID) the data is filtered for that participant under certain conditions, and then the Answer column is selected. The result is stored as the object acc_recT, which is further converted into a vector a. With b and c, the processed answers are computed and lastly, these results are appended into three vectors: Score, IDes, and try. These three vectors are combined into a final data frame accu_L2L1_L1Pro, where each vector becomes a column in a table, each row corresponding to one participant. These columns are renamed and the table is printed. If we want to properly see the table outside of the console, we can use the View() function:\n\nView(accu_L2L1_L1Pro)\n\nFor L1 -&gt; L2 of the L1 production set, we follow the same steps.\n\n\nShow code for L1 -&gt; L2 (L1 production test)\n# L1 ‚Üí L2 (L1 production test)\n\nz &lt;- unique(dat$ID)\n\nScore &lt;- c()\nIDes &lt;- c()\ntry &lt;- c()\nfor (i in z){\n  dat |&gt; \n    dplyr::filter(ID == i, Condition ==\"L1L2\", Test == \"L1 Production\") |&gt; \n    dplyr::select(Answer)-&gt; acc_recT\n  a &lt;- as.vector(unlist(acc_recT))\n  b &lt;- sum(a)\n  c &lt;- length(a)\n  Score &lt;- c(Score, b)\n  IDes &lt;- c(IDes, i)\n  try &lt;- c(try, c)\n}\n\naccu_L1L2_L1Pro &lt;- data.frame(IDes, try, Score)\nnames(accu_L1L2_L1Pro) &lt;- c(\"ID\", \"Try\", \"Score\")\naccu_L1L2_L1Pro\n\n\nWhile accuracy did not play a very big role in the published paper, the authors did a bit more with it in their code. Aside from using the describe() function on the final data frames, which returns a rich set of descriptive statistics, they visualized the results as boxplots. Since we found them to be quite nice for understanding the descriptive statistics of the tests, we decided to include these in this chapter, too.\n\n\nShow use of describe() function on Score column to provide descriptive statistics for L1 production\n# L2 ‚Üí L1 learning\ndescribe(accu_L2L1_L1Pro$Score)\n\n# L1 ‚Üí L2 learning\ndescribe(accu_L1L2_L1Pro$Score)\n\n\nTo visualize descriptive statistics for the L1 production set, the authors generated boxplots comparing the two learning directions. For this to work, the {beeswarm} package has to be installed and loaded. The authors assigned a data frame L1pro that contains the scores from both learning directions in the L1 production test. After changing the names of the columns to how they shall appear on the x-axis, the authors created side-by-side boxplots for the two learning conditions. Finally, they added the beeswarm() function specifying add = T, which lets individual scores appear as jittered dots so that they don‚Äôt overlap.\n\n# Plot (L1 production)\n\nlibrary(beeswarm)\n\nL1pro &lt;- data.frame(accu_L2L1_L1Pro$Score, accu_L1L2_L1Pro$Score)\nnames(L1pro) &lt;- c(\"L2 ‚Üí L1 learning\", \"L1 ‚Üí L2 learning\")\nboxplot(L1pro, col = \"grey91\", outline = T)\nbeeswarm(L1pro, add = T)\n\n\n\n\n\n\n\nFigure¬†19.3: L1 Production Test scores across the two learning directions\n\n\n\n\n\nBoxplots are a way to visualize both the central tendency (median) of a variable and the spread around this central tendency (IQR) (see Section 8.3.2). The median is represented by the thick line inside the box, while the box represents interquartile range, meaning the range of the middle 50% of the data. The whiskers outside the box extend to the highest and smallest values, and the jittered dots represent individual data points. As we can see here, the median is similar in both learning directions. L2 ‚Üí L1 displays slightly greater variability and a higher concentration of upper outliers. Overall, performance appears comparable between the two groups.\nThe same steps were applied to the L2 production set, using the L2 scores.\n\n\nShow code for L2 ‚Üí L1 (L2 production test)\n# L2 ‚Üí L1 (L2 production test)\n\nz &lt;- unique(dat$ID)\n\nScore &lt;- c()\nIDes &lt;- c()\ntry &lt;- c()\nfor (i in z){\n  dat |&gt; \n    dplyr::filter(ID == i, Condition == \"L2L1\", Test == \"L2 Production\") |&gt; \n    dplyr::select(Answer) -&gt; acc_proT\n  a &lt;- as.vector(unlist(acc_proT))\n  b &lt;- sum(a)\n  c &lt;- length(a)\n  Score &lt;- c(Score, b)\n  IDes &lt;- c(IDes, i)\n  try &lt;- c(try, c)\n}\n\naccu_L2L1_L2Pro &lt;- data.frame(IDes, try, Score)\nnames(accu_L2L1_L2Pro) &lt;- c(\"ID\", \"Try\", \"Score\")\naccu_L2L1_L2Pro\n\n\n\n\nShow code for L1 ‚Üí L2 (L2 production test)\n# L1 ‚Üí L2 (L2 production test)\n\nz &lt;- unique(dat$ID)\n\nScore &lt;- c()\nIDes &lt;- c()\ntry &lt;- c()\nfor (i in z){\n  dat |&gt; \n    dplyr::filter(ID == i, Condition == \"L1L2\", Test == \"L2 Production\") |&gt; \n    dplyr::select(Answer)-&gt; acc_proT\n  a &lt;- as.vector(unlist(acc_proT))\n  b &lt;- sum(a)\n  c &lt;- length(a)\n  Score &lt;- c(Score, b)\n  IDes &lt;- c(IDes, i)\n  try &lt;- c(try, c)\n}\n\naccu_L1L2_L2Pro &lt;- data.frame(IDes,try, Score)\nnames(accu_L1L2_L2Pro) &lt;- c(\"ID\", \"Try\", \"Score\")\naccu_L1L2_L2Pro\n\n\n\n\nShow use of describe() function on Score column to provide descriptive statistics for L2 production\n# L2 -&gt; L1 learning\n\ndescribe(accu_L2L1_L2Pro$Score)\n\n# L1 -&gt; L2 learning\n\ndescribe(accu_L1L2_L2Pro$Score)\n\n\nAs with L1, Terai, Yamashita & Pasich (2021) created similar boxplots comparing the two learning directions in L2 production using the beeswarm package.\n\n\nShow R code to generate boxplots\n# Plot (L2 production)\n\nL2pro &lt;- data.frame(accu_L2L1_L2Pro$Score, accu_L1L2_L2Pro$Score)\nnames(L2pro) &lt;- c(\"L2 ‚Üí L1 learning\", \"L1 ‚Üí L2 learning\")\nboxplot(L2pro, col = \"grey91\", outline = T)\nbeeswarm(L2pro, add = T)\n\n\n\n\n\n\n\n\nFigure¬†19.4: L2 Production Test scores across the two learning directions\n\n\n\n\n\nMedian scores in the L2 production test are similar for both learning directions (L2 ‚Üí L1 and L1 ‚Üí L2), suggesting comparable performance in both groups. L1 ‚Üí L2 learning shows a slightly higher median. In the L2 ‚Üí L1 group, values below the median are more spread out, indicating more variety. Comparing the L1 and L2 production tests, L1 production scores appear to be generally higher and more consistent across both learning directions.\n\n\n19.4.2.3 Reproducing Table 4 and 5\nAfter conducting reliability testing and descriptive statistics for the tests, we want to display the results in tables like Tables 4 and 5 from the original publication (Terai, Yamashita & Pasich 2021: 1126). As with the target items, there was no code accessible for creating these tables. But with the available data, we can find a way to reproduce them anyway!\nFirst, we want to create a table that summarizes results of descriptive statistics of tests, similar to Table 4 (Terai, Yamashita & Pasich 2021: 1126). The {knitr} and {kableExtra} packages have to be loaded to proceed. As a first step, we assign a variable that contains the scores for both datasets (L1 and L2 production) and use the describe() function to get all the important measures.\n\ndescriptive_stats_tests &lt;- data.frame(L1pro, L2pro) |&gt; \n  describe()\n\nTo reproduce the original table, the first two columns (vars and n) are removed. Also, the rows are renamed to be more readable.\n\ndescriptive_stats_tests_trimmed &lt;- descriptive_stats_tests |&gt; \n  select(-n, -vars, -trimmed, -mad, -range, -se)\n\nrownames(descriptive_stats_tests_trimmed) &lt;- c(\"L2 ‚Üí L1 (L1pro)\", \"L1 ‚Üí L2 (L1pro)\", \"L2 ‚Üí L1 (L2pro)\", \"L1 ‚Üí L2 (L2pro)\")\n\nNow we want to display the results in a clean, formatted table using the kable() function.\n\nkable(descriptive_stats_tests_trimmed, \n      digits = 2) |&gt; \n  pack_rows(index = c(\"L1 production test\" = 2, \"L2 production test\" = 2)) |&gt; \n  kable_styling(full_width = FALSE, bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nTable¬†19.4: Descriptive Statistics for the L1 and L2 production test scores\n\n\n\n\n\n\n\nmean\nsd\nmedian\nmin\nmax\nskew\nkurtosis\n\n\n\n\nL1 production test\n\n\nL2 ‚Üí L1 (L1pro)\n9.71\n4.16\n9\n4\n18\n0.44\n-0.92\n\n\nL1 ‚Üí L2 (L1pro)\n9.00\n3.85\n9\n0\n16\n-0.15\n-0.30\n\n\nL2 production test\n\n\nL2 ‚Üí L1 (L2pro)\n5.64\n3.49\n5\n0\n13\n0.40\n-0.92\n\n\nL1 ‚Üí L2 (L2pro)\n6.39\n3.52\n6\n0\n13\n-0.01\n-0.80\n\n\n\n\n\n\n\n\nWe have created Table¬†19.4, a table that summarizes the descriptive statistics of the two types of post-tests, where the measures of scores in the two production tests can be easily compared. In the L1 production test, scores were shown to be generally higher than in L2 production test.\nNow we want to do the same with the results of reliability testing, namely the alpha coefficients. To display these results, we want to create a table similar to Table 5 in the paper (Terai, Yamashita & Pasich 2021: 1126). To accomplish this, we first need to apply the alpha() function to calculate Cronbach‚Äôs alpha and 95% confidence intervals for two vocabulary tests (A and B) at two proficiency levels (L1 and L2).\n\nalpha_A_L1 &lt;- alpha(VocabularyA_L1production[,-1], warnings = FALSE)\nalpha_A_L2 &lt;- alpha(VocabularyA_L2production[,-1], warnings = FALSE)\nalpha_B_L1 &lt;- alpha(VocabularyB_L1production[,-1], warnings = FALSE)\nalpha_B_L2 &lt;- alpha(VocabularyB_L2production[,-1], warnings = FALSE)\n\nThen we use sprintf() to create a summary table showing the reliability estimates and 95% confidence intervals (CIs) for each test and level, and format strings for the table.\n\nalpha_table &lt;- data.frame(\n  Vocabulary = c(\"Vocabulary A\", \"Vocabulary B\"),\n  \n  Alpha_L1 = c(\n    sprintf(\"%.2f\", alpha_A_L1$total$raw_alpha),\n    sprintf(\"%.2f\", alpha_B_L1$total$raw_alpha)\n  ),\n  CI_L1 = c(\n    sprintf(\"[%.2f - %.2f]\", alpha_A_L1$feldt$lower.ci, alpha_A_L1$feldt$upper.ci),\n    sprintf(\"[%.2f - %.2f]\", alpha_B_L1$feldt$lower.ci, alpha_B_L1$feldt$upper.ci)\n  ),\n  \n  Alpha_L2 = c(\n    sprintf(\"%.2f\", alpha_A_L2$total$raw_alpha),\n    sprintf(\"%.2f\", alpha_B_L2$total$raw_alpha)\n  ),\n  CI_L2 = c(\n    sprintf(\"[%.2f - %.2f]\", alpha_A_L2$feldt$lower.ci, alpha_A_L2$feldt$upper.ci),\n    sprintf(\"[%.2f - %.2f]\", alpha_B_L2$feldt$lower.ci, alpha_B_L2$feldt$upper.ci)\n  ),\n  \n  stringsAsFactors = FALSE\n)\n\nTo rename the column headers, we use the colnames() function, using an empty string for ‚ÄúVocabulary‚Äù.\n\ncolnames(alpha_table) &lt;- c(\"\", \"Alpha\", \"95% CI\", \"Alpha\", \"95% CI\")\n\nLastly, we want to display the results in a clean, formatted table similarly to the authors, for which we use the kable() function.\n\nalpha_table |&gt; \n  kable(align = \"lcccc\") %&gt;%\n  add_header_above(c(\" \" = 1, \"L1 production\" = 2, \"L2 production\" = 2))\n\n\n\nTable¬†19.5: Alpha coefficients for L1 and L2 production test\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nL1 production\n\n\nL2 production\n\n\n\n\nAlpha\n95% CI\nAlpha\n95% CI\n\n\n\n\nVocabulary A\n0.84\n[0.73 - 0.91]\n0.82\n[0.71 - 0.91]\n\n\nVocabulary B\n0.74\n[0.58 - 0.86]\n0.73\n[0.56 - 0.86]\n\n\n\n\n\n\n\n\nIf we compare Table¬†19.5 about alpha coefficients with the one in the paper (Terai, Yamashita & Pasich 2021: 1126), we notice a difference in confidence intervals, specifically in the hundredths place. If we look at the output of alpha(), we see that it outputs out two kinds of confidence intervals: Feldt and Duhachek. Reading up in the help file of the ?alpha() function, it becomes clear that alpha.ci (used to access CIs from the alpha function) computes CIs following the Feldt procedure, which is based on the mean covariance, while Duhachek‚Äôs procedure considers the variance of the covariances. In the help file it is stated that, in March 2022m alpha.ci was updated to follow the Feldt procedure. Since the original publication was published in 2021, this might explain the deviating CIs here. If one would like to look at Duhachek‚Äôs CI instead, it can either be seen in the output of alpha(), or one might install an earlier version of the package. Since we want to use the current {psych} package for our project and the differences don‚Äôt change the main outcome, we decided to stick with Feldt‚Äôs CI and simply wanted to point out why we observe these differences here.\nAs mentioned before, Cronbach‚Äôs alpha indicates whether responses are consistent between items, and ranges between 0 and 1, a higher value meaning higher reliability. As we can see in the results of the reliability analysis, alpha coefficients range from .73 to .84, showing adequate reliability for all the tests.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Lea`R`ning Direction Effects in Retrieval Practice on EFL Vocabulary Learning</span>"
    ]
  },
  {
    "objectID": "B_CaseStudies/Hannah-Beyhan/Retrieval.html#generalized-linear-mixed-effects-models",
    "href": "B_CaseStudies/Hannah-Beyhan/Retrieval.html#generalized-linear-mixed-effects-models",
    "title": "19¬† LeaRning Direction Effects in Retrieval Practice on EFL Vocabulary Learning",
    "section": "19.5 Generalized linear mixed-effects models",
    "text": "19.5 Generalized linear mixed-effects models\nTerai, Yamashita & Pasich (2021) used generalized linear mixed-effect models (GLMMs), which are an extension of linear mixed models (see Chapter 13). GLMM analysis was employed to examine three predictor variables: Learning Condition (L2 to L1 and L1 to L2), Test Type (L1 production and L2 production), and Vocabulary Size (L2 Proficiency), as well as the interaction between two variables. They tried to predict Accuracy (the outcome variable) as a function of these predictor variables.\nThree GLMMs were chosen for this analysis, the first to analyze the relationship between learning conditions and production tests (RQ1) (Terai, Yamashita & Pasich 2021: 1125). The second and third models analyze the effects of the two learning directions (L2 to L1 and L1 to L2) based on the results of the production tests (RQ2). They are pretty similar, except that the third and last model also make use of L2 production test scores. The first model (RQ 1) will be elaborated in more detail.\n\n19.5.1 Effects of Learning Condition (RQ1)\nAs we recall from the introduction, the first research question of the study was: Which learning direction is more effective for vocabulary acquisition, L1 to L2 learning or L2 to L1 learning? Therefore, the first model was built to analyze the relationship between the outcomes of the production tests and the learning conditions. It contained Learning Condition and Test Type as predictor variables, as well as the interaction of the two variables. Random effects (Subject and Item) were included, and production test answers were used as the outcome variable (Terai, Yamashita & Pasich 2021: 1125).\nBefore getting started on the model, several packages have to be installed and loaded:\n\nlibrary(lme4) # package for mixed models\nlibrary(effects) # package for plotting model effects\nlibrary(emmeans) # package for post-hoc comparisons\nlibrary(phia) # similar to {emmeans}\n\nAlso, the data needs to be set up for modeling. Following the code in Terai, Yamashita & Pasich (2021), the test type (Test) and the learning direction (Condition) variables are converted to factor variables as they represent categories rather than numerical values. Converting the variables into factors now will also be useful for the statistical analysis such as an ANOVA (Analysis of Variance), which will be conducted later.\n\ndat$Test &lt;- as.factor(dat$Test)\ndat$Condition &lt;- as.factor(dat$Condition)\n\nBefore fitting the models, Terai, Yamashita & Pasich (2021) also apply contrast coding, also called treatment coding (Chambers & Hastie 1992). While it may look confusing and maybe intimidating, it is actually quite an important step. Basically, the study aims to compare two groups (L1 -&gt; L2 vs.¬†L2 -&gt; L1). Terai, Yamashita & Pasich (2021) use simple coding, where the first Condition (L1 -&gt; L2) gets coded as -0.5 and the second Condition (L2 -&gt; L1) as +0.5. This makes interpretations easier when there are interactions since the intercept represents the overall mean across the two conditions (Barlaz 2022).\n\nc &lt;- contr.treatment(2)\nmy.coding &lt;- matrix(rep(1/2, 2, ncol = 1))\nmy.simple &lt;- c-my.coding\nmy.simple\n\n     2\n1 -0.5\n2  0.5\n\ncontrasts(dat$Test) &lt;- my.simple\ncontrasts(dat$Condition) &lt;- my.simple\n\nAlso, the scale() function is applied to the Vocab column to create a standardized version of the variable (by subtracting the mean and dividing by the standard deviation). All these steps set up the dataset for statistical modeling.\n\ndat$s.vocab &lt;- scale(dat$Vocab)\n\n\n19.5.1.1 Model 1 without interaction\nTerai, Yamashita & Pasich (2021) fit a logistic mixed-effects regression model predicting learners‚Äô vocabulary production accuracy (Answer) as a function of Test, Condition, and their interaction with the glmer() function. We want to go through the process step-by-step, and connect the authors‚Äô original code with the interpretation of the model in the paper.\nThe following code chunk shows how Terai et al.¬†fit an initial model without any interactions (fit1). Subsequently, the authors calculated the AIC (Akike Information Criterion) for their model. We will explain more about this criterion later.\n\n# Model without interaction\nfit1 &lt;- glmer(Answer ~ Test + Condition + (1|ID) + (1|ItemID), \n              family = binomial, \n              data = dat, \n              glmerControl(optimizer = \"bobyqa\"))\n\n# Produce result summary\nsummary(fit1)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Answer ~ Test + Condition + (1 | ID) + (1 | ItemID)\n   Data: dat\nControl: glmerControl(optimizer = \"bobyqa\")\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   2429.8    2458.1   -1209.9    2419.8      2145 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.4028 -0.6501 -0.3270  0.7222  5.6099 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n ItemID (Intercept) 0.9467   0.9730  \n ID     (Intercept) 0.8176   0.9042  \nNumber of obs: 2150, groups:  ItemID, 40; ID, 28\n\nFixed effects:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.53260    0.23603  -2.256    0.024 *  \nTest2       -0.97215    0.10458  -9.296   &lt;2e-16 ***\nCondition2  -0.02364    0.10247  -0.231    0.818    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n           (Intr) Test2\nTest2      0.022       \nCondition2 0.002  0.003\n\n# Calculating AIC\nAIC(fit1)\n\n[1] 2429.771\n\n\nSince it is so important for our study, we want to take a closer look at this step. Terai, Yamashita & Pasich (2021) fit a GLLM using the glmer() function. In the model formula, both fixed effects (Test answers and learning condition) and random effects (ID and ItemID) are specified. Specifying (1 | ID) allows for a random intercept for each level of the variable ID (participants) and (1 | ItemID) does the same for each level of the variable ItemID (test items). This accounts for repeated measures and inter-individual and inter-item variability. By specifying family = binomial, we can tell the glmer() function that the outcome variable is binary (correct answer = 1, incorrect answer = 0). The last argument specifies an optimizing algorithm, which helps the model to converge. The authors used the summary() function to examine the output of the model.\nWithout the interaction, the model assumes that the effects of Test and Condition are additive and do not depend on each other in any way. The AIC is a measure used to estimate the fit of a model to the data in comparison to another one, with a lower AIC indicating better fit. Therefore, we will get back to this after fitting the model with interaction to compare the performance of the two models.\nIf we look at the output of the model summary(), it includes scaled residuals. Residuals are the differences between observed and predicted values (see Chapter 13). This section shows summary statistics of the model‚Äôs residuals after scaling: Minimum, first quartile (25th percentile), median (50th percentile), third quartile (75th percentile), and maximum. These values give an impression of how well the model fits the data. The Random effects section computes the estimated variability in the intercept across subjects (ID) and items (ItemID), accounting for differences in overall accuracy between participants and between items.\n\n\n19.5.1.2 Model 1 with interaction\nNext, the authors fit a model similar to the first one, except that it also models an interaction between Test and Condition, thus investigating if the effect of one predictor variable depends on the level of the other.\nFor this model fit1.1, the glmer() function was used as well, but with a change in the formula: the predictors are connected with * instead of +. In addition to the model‚Äôs coefficient estimates and the AIC, Terai, Yamashita & Pasich (2021) also calculated confidence intervals for the model using the function confint().\n\n# Model with interaction\nfit1.1 &lt;- glmer(Answer ~ Test * Condition + (1|ID) + (1|ItemID), \n                family = binomial, \n                data = dat, \n                glmerControl(optimizer = \"bobyqa\"))\nsummary(fit1.1)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Answer ~ Test * Condition + (1 | ID) + (1 | ItemID)\n   Data: dat\nControl: glmerControl(optimizer = \"bobyqa\")\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   2427.1    2461.1   -1207.6    2415.1      2144 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.2386 -0.6340 -0.3341  0.7195  5.3153 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n ItemID (Intercept) 0.9518   0.9756  \n ID     (Intercept) 0.8222   0.9067  \nNumber of obs: 2150, groups:  ItemID, 40; ID, 28\n\nFixed effects:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)      -0.53392    0.23667  -2.256   0.0241 *  \nTest2            -0.97597    0.10477  -9.315   &lt;2e-16 ***\nCondition2       -0.03763    0.10283  -0.366   0.7144    \nTest2:Condition2 -0.44597    0.20557  -2.169   0.0300 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Test2 Cndtn2\nTest2       0.022              \nCondition2  0.005  0.019       \nTst2:Cndtn2 0.005  0.029 0.063 \n\n# Computing confidence intervals\nconfint(fit1.1)\n\n                      2.5 %      97.5 %\n.sig01            0.7629507  1.27445594\n.sig02            0.6849006  1.24190402\n(Intercept)      -1.0087438 -0.06326542\nTest2            -1.1843879 -0.77093946\nCondition2       -0.2405335  0.16501237\nTest2:Condition2 -0.8521078 -0.04121350\n\n# Setting number of digits and calculating AIC\nAIC(fit1.1)\n\n[1] 2427.102\n\n\nNow after calculating AICs for both versions of the model, it becomes apparent that the model with interaction has a lower AIC (with interaction: 2427.102; without interaction: 2429.771), indicating that the model fits the data better.\nThe authors state that the results show a significant main effect of Test Type (Œ≤¬†= -0.976, SE = 0.105, z = -9.315, p &lt; .001) (Terai, Yamashita & Pasich 2021: 1126). We want to find these results in the output of our code. When we look at the summary output of fit1.1 under Fixed effects, the numbers can be found in the ‚ÄúTest2‚Äù row. They are all rounded up to three digits for better readability. The negative estimate tells us that accuracy in Test 2 is predicted to be lower than in Test 1. The p-value can be extracted from the column marked Pr(&gt;|z|), where it is displayed in the form of &lt;2e-16, since it is such a small number. It tells us that the effect (of switching between test types) is significant. The z-value tells us how many standard deviations away a value is from the mean.\nMoving on to Learning Condition, the authors found no statistically significant main effect: Condition2: Œ≤¬†= -0.038, SE = 0.103, z = -0.366, p = .714) (Terai, Yamashita & Pasich 2021: 1126). In this case, the p-value is &gt; 0.05, meaning that the effect of learning condition is not significantly different from no effect. The main effect estimate of Condition2 is quite small (close to 0), telling us the predicted odds don‚Äôt change much when moving from one learning condition to the other. However, the interaction of Test Type and Learning Condition (see row ‚ÄúTest2:Condition2‚Äù) was shown to be significant (Œ≤¬†= -0.446, SE = 0.206, z = -2.169, p = .030) (Terai, Yamashita & Pasich 2021: 1126). The interaction term suggests that the effect of Test on accuracy depends on the Condition, or the other way around. The estimate tells us that, when participants take the L2 production test after learning in the L2 -&gt; L1 direction, the accuracy is even lower than when only one of them is. The p-value of .030 indicates that this observed interaction effect is unlikely to be observed in a sample of this size if the interaction effect were actually null.\n\n\n19.5.1.3 Model comparison\nTerai, Yamashita & Pasich (2021) conducted further analysis and comparisons of the two models, using the function anova(), which is an analysis of variance (a statistical method which compares the variance across the means of different groups), and testInteractions(), which makes it possible to perform simple main effects analysis. First, the authors conducted a comparison of the models with and without the interaction term using the anova() function.\n\nanova(fit1, fit1.1)\n\nData: dat\nModels:\nfit1: Answer ~ Test + Condition + (1 | ID) + (1 | ItemID)\nfit1.1: Answer ~ Test * Condition + (1 | ID) + (1 | ItemID)\n       npar    AIC    BIC  logLik -2*log(L) Chisq Df Pr(&gt;Chisq)  \nfit1      5 2429.8 2458.1 -1209.9    2419.8                      \nfit1.1    6 2427.1 2461.1 -1207.5    2415.1 4.669  1    0.03071 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTerai, Yamashita & Pasich (2021) compared the two models with the anova() function to test if the interaction term improved the model. The test outputs a p-value of 0.03071. Being smaller than 0.05, the effect is statistically significant, which is also symbolized by the * specified in the significance codes in the output. It tells us that fit1.1 (model with interaction) fits the data significantly better than fit1 (without interaction).\n\n\n19.5.1.4 Pairwise comparisons on the effects of Condition and Test\nAfter comparing the models and testing the interaction, Terai, Yamashita & Pasich (2021) conducted pairwise comparisons on the effects of Condition and Test.\nFor this, the authors used the emmeans() function, which computes estimated marginal means. First, they conducted pairwise comparisons of Condition levels within each level of Test, and assigned it to a variable a. This object is then printed.\n\n# Computing estimated marginal means using the `emmeans()` function\n\na &lt;- emmeans(fit1.1, pairwise ~ Condition|Test, adjust = \"tukey\")\na\n\n$emmeans\nTest = L1 Production:\n Condition  emmean    SE  df asymp.LCL asymp.UCL\n L1L2      -0.1386 0.251 Inf    -0.631     0.354\n L2L1       0.0467 0.251 Inf    -0.446     0.540\n\nTest = L2 Production:\n Condition  emmean    SE  df asymp.LCL asymp.UCL\n L1L2      -0.8916 0.254 Inf    -1.389    -0.394\n L2L1      -1.1522 0.256 Inf    -1.654    -0.651\n\nResults are given on the logit (not the response) scale. \nConfidence level used: 0.95 \n\n$contrasts\nTest = L1 Production:\n contrast    estimate    SE  df z.ratio p.value\n L1L2 - L2L1   -0.185 0.141 Inf  -1.317  0.1878\n\nTest = L2 Production:\n contrast    estimate    SE  df z.ratio p.value\n L1L2 - L2L1    0.261 0.150 Inf   1.739  0.0821\n\nResults are given on the log odds ratio (not the response) scale. \n\n\nAdditionally, Terai, Yamashita & Pasich (2021) calculated confidence intervals and effect sizes for this object a.\n\n# Computing confidence intervals for emmeans object a\n\nconfint(a, parm, level = 0.95)\n\n$emmeans\nTest = L1 Production:\n Condition  emmean    SE  df asymp.LCL asymp.UCL\n L1L2      -0.1386 0.251 Inf    -0.631     0.354\n L2L1       0.0467 0.251 Inf    -0.446     0.540\n\nTest = L2 Production:\n Condition  emmean    SE  df asymp.LCL asymp.UCL\n L1L2      -0.8916 0.254 Inf    -1.389    -0.394\n L2L1      -1.1522 0.256 Inf    -1.654    -0.651\n\nResults are given on the logit (not the response) scale. \nConfidence level used: 0.95 \n\n$contrasts\nTest = L1 Production:\n contrast    estimate    SE  df asymp.LCL asymp.UCL\n L1L2 - L2L1   -0.185 0.141 Inf   -0.4612    0.0905\n\nTest = L2 Production:\n contrast    estimate    SE  df asymp.LCL asymp.UCL\n L1L2 - L2L1    0.261 0.150 Inf   -0.0332    0.5544\n\nResults are given on the log odds ratio (not the response) scale. \nConfidence level used: 0.95 \n\n# Calculating effect sizes\n\neff_size(a, sigma = sigma(fit1.1), edf = Inf)\n\nTest = L1 Production:\n contrast      effect.size    SE  df asymp.LCL asymp.UCL\n (L1L2 - L2L1)      -0.185 0.141 Inf   -0.4612    0.0905\n\nTest = L2 Production:\n contrast      effect.size    SE  df asymp.LCL asymp.UCL\n (L1L2 - L2L1)       0.261 0.150 Inf   -0.0332    0.5544\n\nsigma used for effect sizes: 1 \nConfidence level used: 0.95 \n\n\nThe same procedure was applied in the other direction: While with variable a, emmeans() was told to do pairwise comparisons of Condition levels within each level of Test, Terai, Yamashita & Pasich (2021) also computed all comparisons of Test levels within each level of Condition, and saved this object as the variable b.\n\n# Computing EMMs\n\nb &lt;- emmeans(fit1.1, pairwise ~ Test|Condition, adjust = \"tukey\")\nb\n\n$emmeans\nCondition = L1L2:\n Test           emmean    SE  df asymp.LCL asymp.UCL\n L1 Production -0.1386 0.251 Inf    -0.631     0.354\n L2 Production -0.8916 0.254 Inf    -1.389    -0.394\n\nCondition = L2L1:\n Test           emmean    SE  df asymp.LCL asymp.UCL\n L1 Production  0.0467 0.251 Inf    -0.446     0.540\n L2 Production -1.1522 0.256 Inf    -1.654    -0.651\n\nResults are given on the logit (not the response) scale. \nConfidence level used: 0.95 \n\n$contrasts\nCondition = L1L2:\n contrast                      estimate    SE  df z.ratio p.value\n L1 Production - L2 Production    0.753 0.145 Inf   5.206 &lt;0.0001\n\nCondition = L2L1:\n contrast                      estimate    SE  df z.ratio p.value\n L1 Production - L2 Production    1.199 0.149 Inf   8.054 &lt;0.0001\n\nResults are given on the log odds ratio (not the response) scale. \n\n# Computing confidence intervals for emmeans object b\n  \nconfint(b, parm, level = 0.95)\n\n$emmeans\nCondition = L1L2:\n Test           emmean    SE  df asymp.LCL asymp.UCL\n L1 Production -0.1386 0.251 Inf    -0.631     0.354\n L2 Production -0.8916 0.254 Inf    -1.389    -0.394\n\nCondition = L2L1:\n Test           emmean    SE  df asymp.LCL asymp.UCL\n L1 Production  0.0467 0.251 Inf    -0.446     0.540\n L2 Production -1.1522 0.256 Inf    -1.654    -0.651\n\nResults are given on the logit (not the response) scale. \nConfidence level used: 0.95 \n\n$contrasts\nCondition = L1L2:\n contrast                      estimate    SE  df asymp.LCL asymp.UCL\n L1 Production - L2 Production    0.753 0.145 Inf     0.469      1.04\n\nCondition = L2L1:\n contrast                      estimate    SE  df asymp.LCL asymp.UCL\n L1 Production - L2 Production    1.199 0.149 Inf     0.907      1.49\n\nResults are given on the log odds ratio (not the response) scale. \nConfidence level used: 0.95 \n\n# Calculating effect sizes\n\neff_size(b, sigma = sigma(fit1.1), edf = Inf)\n\nCondition = L1L2:\n contrast                        effect.size    SE  df asymp.LCL asymp.UCL\n (L1 Production - L2 Production)       0.753 0.145 Inf     0.469      1.04\n\nCondition = L2L1:\n contrast                        effect.size    SE  df asymp.LCL asymp.UCL\n (L1 Production - L2 Production)       1.199 0.149 Inf     0.907      1.49\n\nsigma used for effect sizes: 1 \nConfidence level used: 0.95 \n\n\nThe analysis conducted in the code chunk above provides the most important information for the interpretation of this model as reported in Terai, Yamashita & Pasich (2021). We want to go through the authors‚Äô statements and connect them with the code we reproduced. In the paper, the authors claim there was a statistically significant difference between the scores of the two tests, suggesting that L1 production test scores were higher than L2 production test scores in both learning conditions (L2 to L1 learning: p¬†&lt;¬†.001, d = 1.20, 95% CI [0.91, 1.49]; L1 to L2 learning: p¬†&lt; .001, d = 0.75, [0.47, 1.04]) (Terai, Yamashita & Pasich 2021: 1126). Now, we want to take a close look at these numbers and what they mean. We can find the numbers stated here in the object b output by the emmeans() function. The p-values show up when printing b, and lie¬†&lt; .001 for both learning conditions. A small p-value tells us that an effect is unlikely due to chance, but does not inform us about how large the effect is. This is where effect size comes into play: The function eff_size() computes the estimated effect sizes for both conditions, which are referred to as d in the paper. They both demonstrate a large (larger for L2 to L1) effect size, and indicate that the difference (different levels of Test within each Condition) is not just statistically significant, but also sizeable. Lastly, the confint() function applied to b computes the 95% confidence intervals for both learning directions, which means that, if the study were to be repeated many times, in 95% of the repetitions, the 95% confidence interval would contain the true standardized effect size.\nThe authors also mention that results showed no main effects of Learning Condition (L1 production test: p = .188, d = -0.19, 95% CI [-0.46, 0.09]; L2 production test: p = .082, d = 0.26, [-0.03, 0.55]) (Terai, Yamashita & Pasich 2021: 1126). This can be derived when we look at object a, which compares learning conditions separately for L1 Production and L2 Production. Printing a returns p-values which lie above 0.05 in both production tests and indicate no statistical significance of the differences between learning conditions. The effect sizes show a small negative effect for L1 Production, and a small positive effect for L2 Production. The confidence intervals in both cases include 0 (since it lies between -0.03 and 0.55, for example), telling us that no effect (d = 0) is a possibility given the data. To sum up, all these values do not support a main effect of Learning Condition, as the authors make clear in their paper.\n\n\n19.5.1.5 Visualizing Model 1\nFinally, the authors presented the results of this model in a plot that visualizes all important information regarding research question 1 (Terai, Yamashita & Pasich 2021: 1127). In the following code chunk, you can see how they set graphical parameters and then created a base R plot.\n\npar(pty = \"s\") # sets the shape: square\npar(pin =c(10, 10)) # sets the plot size\npar(\"ps\") # font size\n\n\nplot(allEffects(fit1.1), multiline = T, ci.style = \"bands\", xlab = \"Test Type\", ylab = \"Accuracy Rate\",\n       main = \"\", lty = c(1,2), rescale.axis = F, ylim = c(0, 1),\n       colors = \"black\", asp = 1)\n\n\n\n\n\n\n\nFigure¬†19.5: Interaction between Test Type and Learning Condition on accuracy rates\n\n\n\n\n\nFigure¬†19.5 shows the interaction between Test Type and Learning Condition on accuracy rates, based on the generalized linear mixed-effects model (fit1.1). It is an effect plot generated with the allEffects() function. The solid line represents the L1 to L2 learning condition; the dotted line represents the L2 to L1 learning condition. The y-axis represents accuracy rates on both the L1 and L2 production tests. The x-axis represents the two test types. We can see that the accuracy rates differ across Test Types and Learning Conditions, with a clear interaction between the two: The non-parallel lines indicate that the effect of one variable changes depends on the level of the other variable. The distance between the two lines is relatively small at each test type, which reflects the results that revealed no significant difference in learning effects between L2 to L1 learning and L1 to L2 learning.\nGenerally, Terai, Yamashita & Pasich (2021) used base R plots in their original code. While there is nothing wrong with that, one could also use {ggplot2}, which provides many options to customise the plot. Also, it makes it easy to see what is plotted and how specific elements are modified, which can contribute to the reproducibility of the code. If you want to learn more about using {ggplot2}, check out Chapter 10.\n\n\n\n19.5.2 Effects of Learning Directions and L2 Proficiency (RQ2)\nTerai, Yamashita & Pasich (2021) hypothesized that the effectiveness of learning direction or word pairs might be dependent on the learners‚Äô proficiency in English. In other words, lower-proficiency learners might benefit more from practicing in the L2 -&gt; L1 direction, whilst higher-proficiency learners might benefit more from the L1 -&gt; L2 direction.\nTo answer and test this question, the authors once again fit two separate generalized linear mixed models (GLMMs) for the L1 and L2 production tests. Before we work through the authors‚Äô code step-by-step, we need to know what we are working with here: Our outcome variable is the accuracy on production tests and our predictor variables are the learning direction (L1 -&gt; L2 vs.¬†L2 -&gt; L1) and participants‚Äô vocabulary size (which is used as a proxy for English proficiency in this study). The random effects are Participant ID and Item ID so that the model can account for individual differences across participants and the property of individual vocabulary items.\n\n19.5.2.1 L1 Production test\nWe begin by focusing only on the L1 production data. In the script they provided, the authors loaded the data again for this part and stored it as another object. However, we will continue to use dat to reproduce the code, instead. The authors used the function filter(), which only keeps the rows for the L1 Production test. This data is filtered because, while each participant took both L1 and L2 production tests, the authors found different patterns for both test types, which is why they chose to analyze them separately (Terai, Yamashita & Pasich 2021: 1126). This is the part where participants of the study had to produce the L1 (Japanese) word when shown the L2 (English) word.\n\ndat.L1 &lt;- filter(dat, Test == \"L1 Production\")\n\nNext, the learning directions (Condition) were converted to a factor. Terai, Yamashita & Pasich (2021) converted the categories (L1 -&gt; L2 vs.¬†L2 -&gt; L1) into two factor levels so that R treats it as a categorical binary variable.\n\n# Setting Condition (Learning direction) as a factor\n\ndat.L1$Condition &lt;- as.factor(dat.L1$Condition)\n\nHere, too, the authors make use of contrast coding once more and standardised vocabulary scores:\n\nc &lt;- contr.treatment(2)\nmy.coding &lt;- matrix(rep(1/2, 2, ncol = 1))\nmy.simple &lt;- c-my.coding\n\ncontrasts(dat.L1$Condition) &lt;- my.simple\n\ndat.L1$s.vocab &lt;- scale(dat.L1$Vocab)\n\n\n\n19.5.2.2 Model without interaction\nThis model without interaction assumes that learning direction and L2 proficiency each have independent effects on test accuracy.\n\n1fit2 &lt;- glmer(Answer ~ s.vocab + Condition +\n2                (1|ID) + (1|ItemID),\n3            family = binomial,\n            data = dat.L1, \n            glmerControl(optimizer = \"bobyqa\"))\n\nsummary(fit2)\n\n\n1\n\nThe model formula includes main effects of learning condition and proficiency plus their interaction.\n\n2\n\nThe model formula also includes random intercepts for participants and items (to account for repeated measures).\n\n3\n\nThis is necessary because our outcome is binary (correct/incorrect).\n\n\n\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Answer ~ s.vocab + Condition + (1 | ID) + (1 | ItemID)\n   Data: dat.L1\nControl: glmerControl(optimizer = \"bobyqa\")\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   1316.3    1341.2    -653.1    1306.3      1070 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.7382 -0.7161 -0.2849  0.7304  3.1811 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n ItemID (Intercept) 0.9085   0.9531  \n ID     (Intercept) 0.8452   0.9194  \nNumber of obs: 1075, groups:  ItemID, 40; ID, 28\n\nFixed effects:\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)  -0.0468     0.2409  -0.194    0.846\ns.vocab       0.1047     0.1846   0.567    0.571\nCondition2    0.1874     0.1406   1.333    0.183\n\nCorrelation of Fixed Effects:\n           (Intr) s.vocb\ns.vocab    -0.025       \nCondition2  0.002  0.006\n\n\nFurther, Terai, Yamashita & Pasich (2021) computed the model‚Äôs AIC.\n\nAIC(fit2) \n\n[1] 1316.26\n\n\nThe model predicts the probability of a correct response (Answer). The random intercepts (1|ID) and (1|ItemID) account for individual differences between participants and for the unique difficulty of individual vocabulary items. The summary output of this model without interaction shows that s.vocab is not a significant predictor of retrieval accuracy with p = .57, which suggests that English proficiency by itself does not make a statistically significant contribution to the prediction of participants‚Äô L1 Production accuracy. The model also suggests that the test Condition does not make a statistically significant contribution to the predictive accuracy of the model (p = .18), meaning that learning direction alone also does not explain differences. So, if we look at each predictor separately, there seems to be no clear effect.\n\n\n19.5.2.3 Model with Interaction\nWhat if English proficiency only matters in one of the two learning directions? To figure this out, Terai, Yamashita & Pasich (2021) added an interaction term to the model: s.vocab * Condition.\n\nfit2.1 &lt;- glmer(Answer ~ s.vocab * Condition + (1|ID) + (1|ItemID),\n              family = binomial, \n              data = dat.L1, \n              glmerControl(optimizer = \"bobyqa\"))\n\nsummary(fit2.1)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Answer ~ s.vocab * Condition + (1 | ID) + (1 | ItemID)\n   Data: dat.L1\nControl: glmerControl(optimizer = \"bobyqa\")\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   1311.6    1341.4    -649.8    1299.6      1069 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.3577 -0.7070 -0.2723  0.7294  3.0368 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n ItemID (Intercept) 0.9314   0.9651  \n ID     (Intercept) 0.8583   0.9265  \nNumber of obs: 1075, groups:  ItemID, 40; ID, 28\n\nFixed effects:\n                   Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)        -0.04872    0.24313  -0.200  0.84117   \ns.vocab             0.10336    0.18596   0.556  0.57834   \nCondition2          0.18358    0.14125   1.300  0.19368   \ns.vocab:Condition2 -0.36734    0.14148  -2.596  0.00942 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) s.vocb Cndtn2\ns.vocab     -0.026              \nCondition2   0.000  0.001       \ns.vcb:Cndt2  0.001 -0.003  0.006\n\nconfint(fit2.1)\n\n                         2.5 %      97.5 %\n.sig01              0.73079653  1.28734674\n.sig02              0.67541189  1.29854303\n(Intercept)        -0.53661195  0.43865905\ns.vocab            -0.27346324  0.48467861\nCondition2         -0.09542201  0.46370242\ns.vocab:Condition2 -0.64914673 -0.08900216\n\nAIC(fit2.1)\n\n[1] 1311.563\n\n\nAs Terai, Yamashita & Pasich (2021) stated in their paper, the interaction of Learning Condition and Vocabulary Size is significant with a coefficient estimate of about -0.37, SE = 0.141, z = -2.596, p = .009, which means that the effect of vocabulary proficiency depends on the learning direction.\n\n\n19.5.2.4 Visualization\nVisualizing model predictions is a very useful way to better understand complex models like this one. Terai, Yamashita & Pasich (2021) visualized the model fit2.1 using the following base R plot.\n\nplot(allEffects(fit2.1), multiline = T, ci.style = \"bands\",\n    xlab = \"Vocabulary Size\", ylab = \"Accuracy Rate\",\n    main = \"\", lty = c(1, 2),\n    rescale.axis = F, ylim = c(0, 1), colors = \"black\", asp = 1)\n\n\n\n\n\n\n\nFigure¬†19.6: Predicted L1 Production accuracy across vocabulary sizes and learning directions\n\n\n\n\n\nFigure¬†19.6 shows a solid line representing the L1 -&gt; L2 learning condition, and a dotted line representing the L2 -&gt; L1 learning condition. The y-axis shows L1 production accuracy rates of the test, and the x-axis the scaled scores of the vocabulary size tests (Terai, Yamashita & Pasich 2021: 1128). Looking at Figure¬†19.6 you can see a nearly flat line for L2 -&gt; L1 and a positive, steeper line for L1 -&gt; L2 which cross at intermediate proficiency (~5.419 as reported in the paper, p.¬†1127). The crossover point at ~5.419 suggests that for learners in the L1 -&gt; L2 direction a score of over ~5.419 on the test for vocabulary size was more beneficial than for L2 -&gt; L1 learners. The fact that the line for L2 -&gt; L1 learning is nearly flat shows that proficiency in L2 did not have a big influence. Basically, the plot tells us that, for lower proficiency learners, L2 -&gt; L1 is better, and for higher proficiency learners, L1 -&gt; L2 is more beneficial (Terai, Yamashita & Pasich 2021: 1128).\n\n\n19.5.2.5 L2 Production Test\nTerai, Yamashita & Pasich (2021) repeated all the same steps for the L2 production test, now predicting accuracy for when the participants had to produce English words after learning Japanese-English pairs.\n\n# Filtering the data we need\ndat.L2 &lt;- filter(dat, Test == \"L2 Production\")\n\n# Setting Condition as factors\ndat.L2$Condition &lt;- as.factor(dat.L2$Condition)\n\nc &lt;- contr.treatment(2)\nmy.coding &lt;- matrix(rep(1/2, 2, ncol = 1))\nmy.simple &lt;- c-my.coding\n\ncontrasts(dat.L2$Condition) &lt;- my.simple \n\n# Scaling Vocab scores\ndat.L2$s.vocab &lt;- scale(dat.L2$Vocab)\n\n\n# Model without interaction\nfit3 &lt;- glmer(Answer ~ s.vocab+Condition + (1|ID) + (1|ItemID),\n              family = binomial, \n              data = dat.L2, \n              glmerControl(optimizer = \"bobyqa\"))\nsummary(fit3)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Answer ~ s.vocab + Condition + (1 | ID) + (1 | ItemID)\n   Data: dat.L2\nControl: glmerControl(optimizer = \"bobyqa\")\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   1146.5    1171.4    -568.3    1136.5      1070 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.2217 -0.5586 -0.3314  0.6289  5.0300 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n ItemID (Intercept) 1.5602   1.249   \n ID     (Intercept) 0.8818   0.939   \nNumber of obs: 1075, groups:  ItemID, 40; ID, 28\n\nFixed effects:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -1.1134     0.2799  -3.978 6.95e-05 ***\ns.vocab       0.2293     0.1913   1.199   0.2306    \nCondition2   -0.2759     0.1538  -1.793   0.0729 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n           (Intr) s.vocb\ns.vocab    -0.037       \nCondition2  0.017 -0.001\n\nAIC(fit3)\n\n[1] 1146.519\n\n\n\n# Model with interaction\nfit3.1 &lt;- glmer(Answer ~ s.vocab * Condition + (1|ID) + (1|ItemID),\n                family = binomial,\n                data = dat.L2, \n                glmerControl(optimizer = \"bobyqa\"))\nsummary(fit3.1)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Answer ~ s.vocab * Condition + (1 | ID) + (1 | ItemID)\n   Data: dat.L2\nControl: glmerControl(optimizer = \"bobyqa\")\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   1145.3    1175.2    -566.7    1133.3      1069 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.6092 -0.5624 -0.3263  0.6291  4.8839 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n ItemID (Intercept) 1.5769   1.2557  \n ID     (Intercept) 0.8932   0.9451  \nNumber of obs: 1075, groups:  ItemID, 40; ID, 28\n\nFixed effects:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)         -1.1196     0.2815  -3.978 6.96e-05 ***\ns.vocab              0.2270     0.1925   1.179   0.2382    \nCondition2          -0.2623     0.1544  -1.699   0.0893 .  \ns.vocab:Condition2  -0.2780     0.1536  -1.809   0.0704 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) s.vocb Cndtn2\ns.vocab     -0.037              \nCondition2   0.014  0.012       \ns.vcb:Cndt2  0.019  0.003 -0.041\n\nconfint(fit3.1)\n\n                        2.5 %      97.5 %\n.sig01              0.9458295  1.69438382\n.sig02              0.6790572  1.33827015\n(Intercept)        -1.6992600 -0.56667480\ns.vocab            -0.1627638  0.62538806\nCondition2         -0.5720030  0.04444102\ns.vocab:Condition2 -0.5864708  0.02637148\n\nAIC(fit3.1)\n\n[1] 1145.316\n\n\nAs for the first research question, the authors calculated AICs for both versions of the model, with and without interaction. We can see that the model with interaction has a lower AIC (1145.32) than the one without interaction (1146.52), which ‚Äìas we remember‚Äì means that the model with the interaction is a better fit. Now, taking a look at the output of the summary() function, we see that the Vocabulary Size coefficient estimate is -0.227 (SE = 0.193, z = 1.179, p = .238). Our p-value is greater than 0.05, which means the effect is not statistically significant and proficiency alone does predict accuracy on the L2 production test. We see something nearly identical for the Learning condition with an estimate of -0.262 (SE = 0.154, z = -1.699, p = 0.089). This means that the performance is not predicted to significantly differ between the two learning directions. In a final step, the interaction term s.vocab * Condition was considered, which was also not statistically significant (Œ≤¬†= -0.278, SE = 0.154, z = -1.810, p = .070).\n\n\n19.5.2.6 Visualization\n\nplot(allEffects(fit3.1), multiline = T, ci.style = \"bands\", xlab = \"Vocabulary Size\", ylab = \"Accuracy Rate\",\n       main = \"\", lty = c(1, 2), rescale.axis = F, ylim = c(0, 1), colors = \"black\", asp = 1)\n\n\n\n\n\n\n\nFigure¬†19.7: Predicted L2 Production accuracy across vocabulary size and learning direction\n\n\n\n\n\nJust as the summary statistics of the model above suggested, Figure¬†19.7 shows that, while there was no significance of the results for learning direction or L2 proficiency, the trend bears similartiy to the results of our L1 production test (flatter L2 -&gt; L1 slope, steeper L1 -&gt; L2 slope). So, for lower-proficiency learners, L2 -&gt; L1 leads to higher performance than L1 -&gt; L2 learning. Furthermore, the authors explained that L2 -&gt; L1 learning is much less influenced by L2 proficiency (Terai, Yamashita & Pasich 2021: 1128), which is clearly illustrated by the nearly flat dashed line in the graphs.\n\n\n\n19.5.3 Interpretation & Take-home message\nGenerally, for the L1 production test, Terai, Yamashita & Pasich (2021)‚Äôs results showed that lower-proficiency learners benefited more from the L2 -&gt; L1 learning direction, while higher-proficiency learners benefited more from the L1 -&gt; L2 learning direction. For the L2 production test, a similar trend was observed, but it was not statistically significant, which calls for replication and further research. Overall, vocabulary proficiency shapes how effectively retrieval works: the optimal learning direction is not a one-size-fits-all matter, but may rather be dependent on learner proficiency. However, it is also worth highlighting that only a single language pair was tested in this study. Assuming that the above-mentioned results are generalisable L1-L2 effects may be a bit far-fetched.\n\n\n19.5.4 Checking multicollinearity\nNow, before interpreting these results too confidently, one should also check whether the predictors are highly correlated with one another, which would be a problem called multicollinearity (see Section 13.8) that might affect the results of our GLMMs.\nWhy does this matter? If the predictors are very strongly correlated, the model might have trouble to keep apart their unique contributions. It might then inflate standard errors and thus make estimates unreliable (Pennsylvania State University 2018).\nA common diagnostic here is the Variance Inflation Factor (VIF). A VIF close to 1 means predictors are not collinear. Here are some rules: VIF = 1: perfect independence, VIF &gt; 5: potential concern, VIF &gt; 10: serious multicollinearity (Pennsylvania State University 2018).\n\n19.5.4.1 Calculating VIFs for the Models\nTerai, Yamashita & Pasich (2021) used a helper function vif.mer() to calculate VIFs for mixed models, which can be directly imported from GitHub using the source() function.\n\nsource(\"https://raw.githubusercontent.com/aufrank/R-hacks/master/mer-utils.R\")\n\nvif.mer(fit1.1)\n\n           Test2       Condition2 Test2:Condition2 \n        1.001121         1.004295         1.004777 \n\nvif.mer(fit2.1)\n\n           s.vocab         Condition2 s.vocab:Condition2 \n          1.000008           1.000044           1.000049 \n\nvif.mer(fit3.1)\n\n           s.vocab         Condition2 s.vocab:Condition2 \n          1.000152           1.001814           1.001678 \n\n\nAll the VIF values are obviously quite close to 1. This means that the predictors (s.vocab, Condition, and their interaction) are not correlated with each other in problematic ways. We can therefore be confident that the estimates of the interaction effects that were observed in the GLMMS are not due to multicollinearity.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Lea`R`ning Direction Effects in Retrieval Practice on EFL Vocabulary Learning</span>"
    ]
  },
  {
    "objectID": "B_CaseStudies/Hannah-Beyhan/Retrieval.html#conclusion",
    "href": "B_CaseStudies/Hannah-Beyhan/Retrieval.html#conclusion",
    "title": "19¬† LeaRning Direction Effects in Retrieval Practice on EFL Vocabulary Learning",
    "section": "19.6 Conclusion",
    "text": "19.6 Conclusion\nYou are now familiar with the interpretation of generalized linear mixed models! You have learned how to fit them, interpret their main effects and interactions, as well as checking for multicollinearity using the VIF measure. GLMMs are powerful tools for predicting binary outcomes such as true-false accuracy measures. They allow us to account for repeated measures across participants and items such as we encountered in this study by Terai, Yamashita & Pasich (2021). It is completely normal that at times GLMMs can feel intimidating: coding contrasts and interpreting the outcomes are not always straightforward. But there is always a solution. The insights we can gain from a nicely fitted model make all this effort worthwhile.\nOwing to the fact that Terai, Yamashita & Pasich (2021) published their dataset and R code along with the publication, we were able to reproduce their findings and gain hands-on experience with modeling. This chapter does not include the reproduction and explanation of every part of the code that the authors shared, as we focused on the analyses discussed in Terai, Yamashita & Pasich (2021). In the reproduced code, we only changed minor things, like not reloading the same dataset several times under a different name. This sort of project not only helps with our understanding of the study, but it also sharpens our skills as future SLA researchers.\n\n\n\n\n\n\nNoteHow to cite this chapter\n\n\n\n\n\nThis is a case study chapter of the web version of the textbook ‚ÄúData Analysis for the Language Sciences: A very gentle introduction to statistics and data visualisation in R‚Äù by Elen Le Foll.\nPlease cite the current version of this chapter as:\n\n\nBoxleitner, Hannah and Beyhan Aliy. 2025. Learning Direction Effects in Retrieval Practice on EFL Vocabulary Learning. In Elen Le Foll (Ed.), Data Analysis for the Language Sciences: A very gentle introduction to statistics and data visualisation in R. Open Educational Resource. https://elenlefoll.github.io/RstatsTextbook/ (accessed DATE).",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Lea`R`ning Direction Effects in Retrieval Practice on EFL Vocabulary Learning</span>"
    ]
  },
  {
    "objectID": "B_CaseStudies/Hannah-Beyhan/Retrieval.html#references",
    "href": "B_CaseStudies/Hannah-Beyhan/Retrieval.html#references",
    "title": "19¬† LeaRning Direction Effects in Retrieval Practice on EFL Vocabulary Learning",
    "section": "References",
    "text": "References\n[1] S. Amano and T. Kondo. Nihongo-no-Goitokusei Hindo [Lexical properties of Japanese: Frequency]. Sanseido, 2000.\n[2] M. Barlaz. Contrast coding in R. 2022. https://marissabarlaz.github.io/portfolio/contrastcoding/#contrast-coding.\n[3] B. M. Bolker. ‚Äú13 Linear and generalized linear mixed models‚Äù. In: Ecological Statistics: Contemporary theory and application. Ed. by G. A. Fox, S. Negrete-Yankelevich and V. J. Sosa. Oxford Academic, 2015. DOI: 10.1093/acprof:oso/9780199672547.003.0014.\n[4] J. M. Chambers and T. J. Hastie. Statistical models in S. New York: Routledge, 1992.\n[5] S. Parajuli. Chapter 7 Skewness and Kurtosis. 2023. https://bookdown.org/subashparajuli/quant-r/skewness-and-kurtosis.html.\n[6] Pennsylvania State University. 10.7 - Detecting Multicollinearity Using Variance Inflation Factors | STAT 462. 2018. https://online.stat.psu.edu/stat462/node/180/.\n[7] M. Terai, J. Yamashita, and K. E. Pasich. ‚ÄúEffects of Learning Direction in Retrival Practice on EFL Vocabulary Learning‚Äù. In: Studies in Second Language Acquisition 43.5 (2021), pp.¬†1116-1137. DOI: https://doi.org/10.1017/S0272263121000346.\n\nPackage references\n[1] D. Bates, M. M√§chler, B. Bolker, et al.¬†‚ÄúFitting Linear Mixed-Effects Models Using lme4‚Äù. In: Journal of Statistical Software 67.1 (2015), pp.¬†1-48. DOI: 10.18637/jss.v067.i01.\n[2] D. Bates, M. Maechler, B. Bolker, et al.¬†lme4: Linear Mixed-Effects Models using Eigen and S4. R package version 1.1-37. 2025. https://github.com/lme4/lme4/.\n[3] D. Bates, M. Maechler, and M. Jagan. Matrix: Sparse and Dense Matrix Classes and Methods. R package version 1.7-3. 2025. https://Matrix.R-forge.R-project.org.\n[4] C. Boettiger. knitcitations: Citations for Knitr Markdown Files. R package version 1.0.12. 2021. https://github.com/cboettig/knitcitations.\n[5] P. Breheny and W. Burchett. visreg: Visualization of Regression Models. R package version 2.7.0. 2020. http://pbreheny.github.io/visreg.\n[6] P. Breheny and W. Burchett. ‚ÄúVisualization of Regression Models Using visreg‚Äù. In: The R Journal 9.2 (2017), pp.¬†56-71.\n[7] G. Cs√°rdi, J. Hester, H. Wickham, et al.¬†remotes: R Package Installation from Remote Repositories, Including GitHub. R package version 2.5.0. 2024. https://remotes.r-lib.org.\n[8] H. De Rosario-Martinez. phia: Post-Hoc Interaction Analysis. R package version 0.3-2. 2025. https://github.com/heliosdrm/phia.\n[9] A. Eklund and J. Trimble. beeswarm: The Bee Swarm Plot, an Alternative to Stripchart. R package version 0.4.0. 2021. https://github.com/aroneklund/beeswarm.\n[10] S. A. file. paletteer: Comprehensive Collection of Color Palettes. R package version 1.6.0. 2024. https://github.com/EmilHvitfeldt/paletteer.\n[11] J. Fox. ‚ÄúEffect Displays in R for Generalised Linear Models‚Äù. In: Journal of Statistical Software 8.15 (2003), pp.¬†1-27. DOI: 10.18637/jss.v008.i15.\n[12] J. Fox and J. Hong. ‚ÄúEffect Displays in R for Multinomial and Proportional-Odds Logit Models: Extensions to the effects Package‚Äù. In: Journal of Statistical Software 32.1 (2009), pp.¬†1-24. DOI: 10.18637/jss.v032.i01.\n[13] J. Fox and S. Weisberg. An R Companion to Applied Regression. Third. Thousand Oaks CA: Sage, 2019. https://www.john-fox.ca/Companion/.\n[14] J. Fox and S. Weisberg. An R Companion to Applied Regression. 3rd. Thousand Oaks CA: Sage, 2019. https://socialsciences.mcmaster.ca/jfox/Books/Companion/index.html.\n[15] J. Fox and S. Weisberg. ‚ÄúVisualizing Fit and Lack of Fit in Complex Regression Models with Predictor Effect Plots and Partial Residuals‚Äù. In: Journal of Statistical Software 87.9 (2018), pp. 1-27. DOI: 10.18637/jss.v087.i09.\n[16] J. Fox, S. Weisberg, and B. Price. car: Companion to Applied Regression. R package version 3.1-3. 2024. https://r-forge.r-project.org/projects/car/.\n[17] J. Fox, S. Weisberg, and B. Price. carData: Companion to Applied Regression Data Sets. R package version 3.0-5. 2022. https://r-forge.r-project.org/projects/car/.\n[18] J. Fox, S. Weisberg, B. Price, et al.¬†effects: Effect Displays for Linear, Generalized Linear, and Other Models. R package version 4.2-2. 2022. https://www.r-project.org.\n[19] G. Grolemund and H. Wickham. ‚ÄúDates and Times Made Easy with lubridate‚Äù. In: Journal of Statistical Software 40.3 (2011), pp. 1-25. https://www.jstatsoft.org/v40/i03/.\n[20] L. Komsta and F. Novomestky. moments: Moments, Cumulants, Skewness, Kurtosis and Related Tests. R package version 0.14.1. 2022. https://www.r-project.org.\n[21] R. V. Lenth. emmeans: Estimated Marginal Means, aka Least-Squares Means. R package version 1.11.1. 2025. https://rvlenth.github.io/emmeans/.\n[22] K. M√ºller. here: A Simpler Way to Find Your Files. R package version 1.0.1. 2020. https://here.r-lib.org/.\n[23] K. M√ºller and H. Wickham. tibble: Simple Data Frames. R package version 3.3.0. 2025. https://tibble.tidyverse.org/.\n[24] R Core Team. R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing. Vienna, Austria, 2025. https://www.R-project.org/.\n[25] W. Revelle. psych: Procedures for Psychological, Psychometric, and Personality Research. R package version 2.5.3. 2025. https://personality-project.org/r/psych/.\n[26] V. Spinu, G. Grolemund, and H. Wickham. lubridate: Make Dealing with Dates a Little Easier. R package version 1.9.4. 2024. https://lubridate.tidyverse.org.\n[27] H. Wickham. forcats: Tools for Working with Categorical Variables (Factors). R package version 1.0.1. 2025. https://forcats.tidyverse.org/.\n[28] H. Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2016. ISBN: 978-3-319-24277-4. https://ggplot2.tidyverse.org.\n[29] H. Wickham. stringr: Simple, Consistent Wrappers for Common String Operations. R package version 1.6.0. 2025. https://stringr.tidyverse.org.\n[30] H. Wickham. tidyverse: Easily Install and Load the Tidyverse. R package version 2.0.0. 2023. https://tidyverse.tidyverse.org.\n[31] H. Wickham, M. Averick, J. Bryan, et al.¬†‚ÄúWelcome to the tidyverse‚Äù. In: Journal of Open Source Software 4.43 (2019), p.¬†1686. DOI: 10.21105/joss.01686.\n[32] H. Wickham, W. Chang, L. Henry, et al.¬†ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. R package version 4.0.1. 2025. https://ggplot2.tidyverse.org.\n[33] H. Wickham, R. Fran√ßois, L. Henry, et al.¬†dplyr: A Grammar of Data Manipulation. R package version 1.1.4. 2023. https://dplyr.tidyverse.org.\n[34] H. Wickham and L. Henry. purrr: Functional Programming Tools. R package version 1.2.0. 2025. https://purrr.tidyverse.org/.\n[35] H. Wickham, J. Hester, and J. Bryan. readr: Read Rectangular Text Data. R package version 2.1.5. 2024. https://readr.tidyverse.org.\n[36] H. Wickham, D. Vaughan, and M. Girlich. tidyr: Tidy Messy Data. R package version 1.3.1. 2024. https://tidyr.tidyverse.org.\n[37] Y. Xie. Dynamic Documents with R and knitr. 2nd. ISBN 978-1498716963. Boca Raton, Florida: Chapman and Hall/CRC, 2015. https://yihui.org/knitr/.\n[38] Y. Xie. ‚Äúknitr: A Comprehensive Tool for Reproducible Research in R‚Äù. In: Implementing Reproducible Computational Research. Ed. by V. Stodden, F. Leisch and R. D. Peng. ISBN 978-1466561595. Chapman and Hall/CRC, 2014.\n[39] Y. Xie. knitr: A General-Purpose Package for Dynamic Report Generation in R. R package version 1.50. 2025. https://yihui.org/knitr/.\n[40] H. Zhu. kableExtra: Construct Complex Table with kable and Pipe Syntax. R package version 1.4.0. 2024. http://haozhu233.github.io/kableExtra/.\n\n\n\n\nAmano, S. & T. Kondo. 2000. Nihongo-no-goitokusei hindo [lexical properties of japanese: frequency]. Sanseido.\n\n\nBarlaz, M. 2022. Contrast coding in r. https://marissabarlaz.github.io/portfolio/contrastcoding/#contrast-coding.\n\n\nChambers, John M. & Trevor J. Hastie. 1992. Statistical models in s. New York: Routledge.\n\n\nParajuli, Subash. 2023. Chapter 7 skewness and kurtosis. https://bookdown.org/subashparajuli/quant-r/skewness-and-kurtosis.html.\n\n\nPennsylvania State University. 2018. 10.7 - detecting multicollinearity using variance inflation factors | STAT 462. https://online.stat.psu.edu/stat462/node/180/.\n\n\nTerai, Masato, Junko Yamashita & Kelly E. Pasich. 2021. Effects of learning direction in retrival practice on EFL vocabulary learning. Studies in Second Language Acquisition 43(5). 1116‚Äì1137. https://doi.org/https://doi.org/10.1017/S0272263121000346.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Lea`R`ning Direction Effects in Retrieval Practice on EFL Vocabulary Learning</span>"
    ]
  }
]