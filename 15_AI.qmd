{{< include emoji_script.md >}}

# What's next? AI-assisted resea`R`ch? {#sec-AI}

If you are reading the last chapter of what is by no means a short textbook, you are likely someone who is eager to acquire new skills and knowledge. As such, you are well aware that learning is a process that requires intrinsic motivation, time, and a great deal of effort and patience. Many tech companies, by contrast, are keen for us to believe that this tiring, time-consuming experience of human learning is no longer necessary: So-called 'artificial intelligence' (AI) systems are designed to spare us the burden of thinking for ourselves. If the answers to our (research) questions are just a few, effortless prompts away, it is legitimate to ask: Is it still worth devoting so much time and effort to acquire and further develop knowledge and skills in data analysis, programming, and statistics?

This concluding chapter explains why we can answer this question with a decisive "yes". To this end, we consider the role of AI in research and learning before recapping what we have learnt so far and where to go from there.

## On the technology behind AI ü§ñ

Contrary to popular belief, 'artificial intelligence' is not a technology. It is, and always has been, a marketing term used to sell (or secure research funding for) a wide, diverse, and shifting array of ideas, research projects, systems, and technologies (see @fig-VennAI from @guestUncriticalAdoptionAI2025; also @vanrooijReclaimingAITheoretical2024). Despite marketing claims, these systems and technologies are not 'intelligent'; they cannot think, understand, or reason [see e.g. @quattrociocchiEpistemologicalFaultLines2025]. This is not to say that they do not have valuable applications, but rather that we need to critically evaluate promotional claims and anthropomorphising language (e.g. AI *says* / *thinks* / *knows* / *understands*).

![A cartoon set theoretic view of terms commonly used when discussing the superset AI from @guestUncriticalAdoptionAI2025: Figure¬†1 ([CC BY 4.0](https://creativecommons.org/licenses/by/4.0/legalcode))](images/GuestEA_2025_sets_AI.png){#fig-VennAI width="400"}

Popular, commercial chat-based AI products such as ChatGPT, Claude, CoPilot, and Gemini are powered by Large Language Models (LLMs). LLMs are statistical models fitted to huge amounts of training data to generate a probable response given a prompt based on statistical patterns found in the training data, reinforcement learning from human feedback, as well as additional opaque (usually profit-driven) criteria [see e.g. @benderAIConHow2025]. At first sight, the first, underlying principle of next-token prediction is comparable to the autocomplete functions from our phones and Google search bar.

![Autocomplete function on web search query (google.com on 8 February 2026)](images/LLMs_fe55f270.gif){#fig-autocomplete fig-alt="A looping GIF of a screen capture showing someone typing \"LLMs are not\" into Google and various automated completions pop up like \"LLMs are not ai\"." width="600"}

It goes without saying that modern LLMs are much more powerful predictive text generators than the models that once offered to help us draft our text messages. This is due to four key factors:

1.  The development of a novel algorithmic architecture known as a **transformer** that is based on attention mechanisms [@vaswaniAttentionAllYou2017]

2.  The availability and largely unethical, if not outright illegal [see e.g. @samuelsonGenerativeAIMeets2023; @lucchiChatGPTCaseStudy2024] scraping of **huge amounts of training data** from the internet, including (academic) books and articles, but also vast amounts of blog posts, social media data, forum discussions, Wikipedia articles, and YouTube videos.

3.  The availability of (relatively) cheap, large-scale **computational power** [which, however, still comes at a high environmental cost, see e.g. @luccioniMisinformationOmissionNeed2025; @schonExpertAssessmentSystemic2025].

4.  **Reinforcement learning from human feedback**, a process whereby crowdsourced human workers mostly from low-income countries [see e.g. @perrigoExclusive$2Hour2023] provide extensive feedback on LLM outputs to fine-tune models for what humans want to obtain when they query a model [see @benderAIConHow2025: Ch¬†3].

Despite its name, OpenAI ‚Äî the main company behind ChatGPT ‚Äî does not develop [open-source](@sec-OpenSource) LLMs, nor is the company a not-for-profit initiative. So why have OpenAI and other large tech firms been offering their AI products for free or at prices well below actual running (let alone development) costs? Two reasons are worth considering. First, because one major bottleneck to improving current LLMs is access to new data. Human-generated data is highly valuable and chatbot users are providing lots of it. AI companies are harvesting this data to train the next generation of LLMs. Second, because free or cheap access to AI products encourage us to become reliant on them for all kinds of work-related tasks and personal activities. As they become ubiquitous to our everyday lives, we are naturally inclined to rely on and trust their outputs. It is only a matter of time until subscriptions prices are hiked up and/or promoted contents are (more or less transparently) integrated in model outputs [see e.g. @m√ºhlhoff2025: 91-97].

:::: {.content-visible when-format="html"}
::: {.callout-tip collapse="false"}
#### Your turn! {.unnumbered}

[**Q15.1**]{style="color:green;"} Which of the following analogies have been used by different scholars to refer to Large Language Models (LLMs)?

```{r}
#| echo: false
#| label: "Q15.1"

library(checkdown)

check_question(c("Stochastic parrots",
                           "Automated plagiarism",
                           "Spicy autocomplete",
                           "Bullshit",
                           "Synthetic text extruding machines",
                           "Kitsch"),
               options = c("Stochastic parrots",
                           "Automated plagiarism",
                           "Spicy autocomplete",
                           "Ask-Einstein-anything engine",
                           "Bullshit",
                           "Magic research wand",
                           "Synthetic text extruding machines",
                           "Kitsch"),
button_label = "Check answer",
q_id = "Q15.1",
type = "check",
right = "That's right, well done! Which metaphor(s) do you like best?
<br>
<br>
The term 'stocastic parrot' is probably the best known LLM metaphor. It was introduced by computational linguist [Emily M. Bender and colleagues in 2021](https://doi.org/10.1145/3442188.3445922) to characterise LLMs as systems that mimic text without true understanding, highlighting their limitations in processing meaning. This metaphor emphasises that LLMs generate outputs based on statistical patterns in their training data, similar to how parrots mimic sounds without comprehension. 
<br>
<br>
In 2022, the cognitive scientist Iris Van Rooij published a [blog post](https://irisvanrooijcogsci.com/2022/12/29/against-automated-plagiarism/) in which she succictly explains why she believes that LLMs cannot legitimately be used for academic writing because they essentially automate plagiarism. 
<br>
<br>
The 'spicy autocomplete' metaphor is difficult to trace back to one or more specific author(s). It suggests that LLMs are just fancy versions of a smart phone‚Äôs predictive text, which predict the next word based on what came before, except that LLMs add some randomness, i.e. spice, to the output. This framing also implies that LLMs are pattern-matching algorithms without real understanding or problem-solving ability (see [Gro√ü 2024](https://publikationen.soziologie.de/index.php/soziologie/en/article/view/1798)).
<br>
<br>
In 2024, Hicks, Humphries & Slater published a paper in the journal 'Ethics and Information Technology' entitled '[ChatGPT is bullshit](https://doi.org/10.1007/s10676-024-09775-5)', in which they argue that the output of LLMs is best understood as 'bullshit' in the philosophical sense described by [Frankfurt (2005)](https://en.wikipedia.org/wiki/On_Bullshit) because LLMs are indifferent to the truth of their outputs.
<br>
<br>
'Synthetic text extruding machines' is a term that Emily M. Bender and Alex Hanna like to use, e.g. in their 2025 book entitled '[The AI Con: How to Fight Big Tech's Hype and Create the Future We Want](https://thecon.ai/)'. They describe the process of LLM-generated texts by explaining that, \"[l]ike an industrial plastic process, language corpora are forced through complicated machinery to produce a product that looks like communicative language, but without any intent or thinking mind behind it.\"
<br>
<br>
As an alternative to well-established metaphors such as the ones listed above, classical philologist Gyburg Uhlmann proposed 'kitsch' as a new metaphor to describe the output of LLMs. She argues that 'kitsch' \"is particularly suitable for analytically illuminating a previously neglected feature of LLM-based images and texts: their tendency to produce homogeneous and average content, which is [...] leading to the equalisation of language, style and argument\" ([Uhlmann 2025](https://arxiv.org/pdf/2509.16794)). 
<br>
<br>
",
wrong = "Not quite. You will need to select all the analogies that have (rightly or wrongly) been used to described LLMs/AI to get this question right. For a tip, check the hint below.
<br>
<br>")
check_hint("Six of the above analogies have been used to describe LLMs by academics and scholars. Check out these publications by [Emily M. Bender et al.](https://doi.org/10.1145/3442188.3445922), [Richard Gro√ü](https://publikationen.soziologie.de/index.php/soziologie/en/article/view/1798), [Emily M. Bender & Alex Hanna](https://thecon.ai/), [Michael Townsen Hicks et al.](https://doi.org/10.1007/s10676-024-09775-5), [Iris Van Rooij](https://irisvanrooijcogsci.com/2022/12/29/against-automated-plagiarism/), and [Gyburg Uhlmann](https://arxiv.org/pdf/2509.16794) to find out more.", 
           hint_title = "üê≠ Click on the mouse for a hint.")

```
:::
::::

## On the value of critical thinking

In educational contexts, the challenges brought about by AI have been compared to the introduction of pocket calculators in the 1960s. The comparison is an interesting one. Although calculators are now widely available and perform school-level arithmetic to perfection, we still first teach school pupils to do mathematics without a calculator because we know that this is necessary to develop an *understanding* of what a calculator does. Following this analogy, we should still teach and learn data analysis, statistics, and programming, even if AI were to generate error-free solutions to these tasks. The crux of the problem is how to convince ourselves that it is worth learning to do things the hard way given how convenient and effortless AI products seem. More on that in @sec-HumanLearning but, for now, let's return to our calculator analogy and consider another, crucial issue: the reliability of LLM outputs.

A calculator is a deterministic system that will always provide the same, correct answer to a mathematical operation. By contrast, an LLM is a stochastic model that is fitted to generate probable outputs. These are generated by highly complex black-box algorithms that are based on (often illegally and/or unethically acquired) training data, reinforcement learning from human feedback, and additional company-internal fine-tuning, as well as a degree of randomness. This means that, unlike calculators, their outputs are irreproducible and therefore unreliable.

A marketing term that tech companies have been pushing to describe this inherent lack of reliability is "hallucination". However, as LLMs have no way of representing what is true or false, it is misleading to speak of LLMs "hallucinating". LLMs are more likely to generate outputs that are truthful when they have been trained a lot of reliable data on the subject but, by definition, they cannot evaluate their sources ‚Äî which can range from high-quality, peer-reviewed academic journals to satiric Reddit comments written by cheeky teenagers ‚Äî and, crucially, have no *understanding* of their contents.

Given that their outputs are both irreproducible and unreliable, commercial AI tools are unsuitable for most research-related activities. For writing, LLMs not only output texts without any form of fact-checking, they also automate plagiarism as they fail to credit the authors whose texts they were trained on. In this context, it is worth noting that any bibliographic references output by LLMs are also randomly generated text strings. They may or may not correspond to real sources. When the sources exist, they may or may not contain (some of) the information regurgitated by the LLMs. We have no way of knowing from the model output.

The same goes for literature reviews: having no access to the training data, this is not a task that we can responsibly delegate to an LLM. To make matters worse, AI systems are known to perpetuate and exacerbate biases [e.g. the 'Matthew effect', see @pooleyMatthewEffectAI2025]. High-quality research requires us to put in the intellectual effort of searching, reading, and critically evaluating the literature *ourselves*. Like writing, this is an integral part of the research process.

:::: {.content-visible when-format="html"}
::: {.callout-tip collapse="false"}
#### Your turn! {.unnumbered}

[**Q15.2**]{style="color:green;"} Read Jeff Pooley's short paper '[The Matthew Effect in AI Summary](https://web.archive.org/web/20251225173407/https://www.jeffpooley.com/2025/11/the-matthew-effect-in-ai-summary/)'. What does the Matthew Effect refer to?

```{r}
#| echo: false
#| label: "Q15.2"

check_question("It describes how successful researchers receive more recognition and opportunities, leading to more success.",
               options = c("It describes how successful researchers receive more recognition and opportunities, leading to more success.",
                           "It refers to the practice of LLM-assisted plagiarism in academic writing.",
                           "It describes how successful researchers tend to be harsher in peer reviewing processes in order to remain successful themselves.",
                           "It refers to the positive impact of open-access publishing on research dissemination."),
button_label = "Check answer",
q_id = "Q15.2",
random_answer_order = TRUE,
type = "radio",
right = "That's right!",
wrong = "No, that's not it. Re-read the article linked in the question and/or check the hint.")
check_hint("The Matthew effect is named after the following passage from the Bible: \"For to every one who has will more be given, and he will have abundance; but from him who has not, even what he has will be taken away‚Äù (Matthew 25:29)\".", hint_title = "üê≠ Click on the mouse for a hint.")

```

¬†

[**Q15.3**]{style="color:green;"} Which term describes the phenomenon whereby the contributions of marginalised female scientists are overlooked or attributed to their male colleagues?

```{r}
#| echo: false
#| label: "Q15.3"

check_question("The Matilda effect",
               options = c("The Matilda effect",
                           "The Margaret Rossiter effect",
                           "The Dunning-Kruger effect",
                           "The Harriet effect"),
button_label = "Check answer",
q_id = "Q15.3",
random_answer_order = TRUE,
type = "radio",
right = "That's right!",
wrong = "Incorrect.")
check_hint("This effect is named after Matilda Joslyn Gage, a 19th-century women's rights activist.", hint_title = "üê≠ Click on the mouse for a hint.")

```

¬†

[**Q15.4**]{style="color:green;"} According to @pooleyMatthewEffectAI2025, which of the following biases in academia are likely to be aggravated by the use of Large Language Models (LLMs) to summarise academic literature and write research articles?

```{r}
#| echo: false
#| label: "Q15.4"

check_question(c("Racial biases",
                           "Gender biases",
                           "Language biases, particularly favouring English",
                           "Inequalities favouring central, influential regions over less influential, so-called \"peripheral\" regions"),
               options = c("Racial biases",
                           "Gender biases",
                           "Language biases, particularly favouring English",
                           "Inequalities favouring central, influential regions over less influential, so-called \"peripheral\" regions"),
               button_label = "Check answer",
q_id = "Q15.4",
random_answer_order = TRUE,
type = "check",
right = "That's right!",
wrong = "That's a start but, sadly, there are more biases.")
check_hint("Jeff Pooley discusses how LLMs can exacerbate existing biases in academia.", hint_title = "üê≠ Click on the mouse for a hint.")

```
:::
::::

Writing code for data (pre)processing, analysis, visualisation, and statistical modelling is also increasingly becoming part and parcel of research processes in the language sciences. Whilst it may be tempting to outsource (part of) the data analysis process to a machine learning algorithm, an LLM or an AI agent, time spent examining our data (e.g. cleaning, wrangling, and visualising them) allows us to gain in-depth knowledge of our data with which we can spot issues in complex statistical analyses further down the road. It is much easier to critically interpret the outputs of complex models if we have a good intuitive sense of what is plausible based on our data. This is not something that we can responsibly delegate to even the most sophisticated AI.

![A [xkcd comic](https://xkcd.com/1838) depicting a pile of algebra as a machine learning algorithm (xkcd [CC BY-NC 2.5](https://xkcd.com/license.html))](https://imgs.xkcd.com/comics/machine_learning.png){fig-alt="Comic showing a stick person holding a canoe paddle at his side and standing on top of a \"big pile of linear algebra\" containing a funnel labeled \"data\" and box labeled \"answers\". A second stick person stands to the left side of the panel. Person 2: This is your machine learning system?  Person 1: Yup! You pour the data into this big pile of linear algebra, then collect the answers on the other side.  Person 2: What if the answers are wrong?  Person 2: Just stir the pile until they start looking right." width="319"}

## On the value of human learning {#sec-HumanLearning}

All this is not to say that AI products are *never* useful. Many people report successfully using AI for designing experiments, writing, and coding ‚Äî including for (academic) research. Here, two factors are worth considering. First, the less we know about a domain, the more we tend to overestimate the quality of LLM outputs in this domain. When it comes to learning how to code and do statistics, political scientist, `R` package developer, and educator Andrew Heiss[^15_ai-1] [-@heissDataVisualizationCan2024] goes as far as saying that "using ChatGPT and other LLMs when *learning* `R` is actually really detrimental to learning, especially if you just copy/paste directly from what it spits out." He goes on to explain that:

[^15_ai-1]: I highly recommend Andrew Heiss' [blog](https://www.andrewheiss.com/blog/) and his beautiful [teaching resources](https://www.andrewheiss.com/teaching/). Fun fact for language students and linguists: Andrew majored in Arabic and Italian and didn't learn about statistics or `R` until he started his second master's!

> Using ChatGPT with R requires a good baseline knowledge of `R` to actually be useful. A good analogy for this is with recipes. ChatGPT is really confident at spitting out plausible-looking recipes. A few months ago, for fun, I asked it to give me a cookie recipe. I got back something with flour, eggs, sugar, and all other standard-looking ingredients, but it also said to include 3/4 cup of baking powder. That's wild and obviously wrong, but I only knew that because I've made cookies before [@heissDataVisualizationCan2024: n.p.].

This begs the question as to how novices can reach the level of expertise necessary to be able to reliably assess LLM outputs in a specific domain. Researchers are often short on time and LLMs are sold to us as a convenient way to take shortcuts. Indeed, AI products are designed to give us the illusion that we are more efficient and productive when we use and trust them. The risk is that, once we have become dependent, we are no longer able to compare how long we would have needed to complete the same task had we not relied on a third-party AI system to do so.

::::: {.content-visible when-format="html"}
:::: {.callout-tip collapse="false"}
#### Your turn! {.unnumbered}

Read through Google's "AI overview" displayed in @fig-AIcheese. You can click on the images to enlarge them.

::: {#fig-Reddit layout-ncol="2"}
![Google AI overview to the query "cheese not sticking to pizza" from 2024](images/google_cheese.jpg){#fig-AIcheese fig-alt="Google query: 'cheese not sticking to pizza' The AI Overview reads: Here are some tips to help cheese stick to pizza: Mix about 1/8 cup of non-toxic glue, like Elmer's school glue, into the sauce to add some tackiness. Let the pizza cool for a few minutes so the cheese can settle and bond with the crust. Cook the cheese until it just melts, but don't overcook it."}

![Satirical [Reddit post](https://www.reddit.com/r/Pizza/comments/1a19s0/comment/c8t7bbp/) from 2013](images/reddit_cheese.png){#fig-RedditCheese fig-alt="Reddit post by user \"fucksmith\" dating from 13y ago. The post reads: To get the cheese to stick I recommend mixing about 1/8 cup of Elmer's glue Q in with the sauce. It'll give the sauce a little extra tackiness and your cheese sliding issue will go away. It'll also add a little unique flavor. I like Elmer's school glue, but any glue will work as long as it's non-toxic. The post has 197 replies."}

Screenshots from [r/Pizza](https://www.reddit.com/r/Pizza/comments/1a19s0/comment/l58bvc1/) illustrating the infamous 'pizza glue' AI fail [see also @notopoulosGoogleAISaid2024]
:::

[**Q15.5**]{style="color:green;"} Among a number of sensible-sounding suggestions, we find a recommendation for adding non-toxic glue to pizza sauce. This mention is thought to have come from an old Reddit post (see @fig-RedditCheese). Which aspect(s) of the AI overview point to this theory?

```{r}
#| echo: false
#| label: "Q15.5"

check_question(c("The AI overview mentions the same type of glue as in the Reddit post.",
                           "The AI overview uses the same, in the context of cooking, unusual word as fucksmith: 'tackiness'.",

               "The AI overview mentions the same quantity of glue as in the Reddit post."),
               options = c("The format of the AI overview is similar to that of the Reddit post.",
                           "The AI overview's mention of non-toxic glue to the sauce is likely a hallucination.",
                           "The AI overview mentions the same quantity of glue as in the Reddit post.",
                           "Reddit is usually one of the most reliable sources for recipes and cooking tips on the Internet.",
                           "The AI overview mentions the same type of glue as in the Reddit post.",
                           "The AI overview uses the same, in the context of cooking, unusual word as fucksmith: 'tackiness'."),
button_label = "Check answer",
q_id = "Q15.5",
random_answer_order = TRUE,
type = "check",
right = "That's right!",
wrong = "No, not quite.")
```
::::
:::::

When it comes to programming, the use of LLMs may seem less risky than when conducting literature reviews, writing prose, or analysing data. After all, we can test that generated code runs as expected. If it does not (which is often the case although we might not immediately spot this), we can debug the code with the support of one or more LLMs until it does. Whilst this might feel like the path of least resistance, data scientist, `R` developer, and educator, Mine √áetinkaya-Rundel, explains how debugging someone else's code (e.g., what an LLM produced) is considerably harder than debugging our own code [@chowEpisode4Mine]. This is because when it's our own code, we had an idea, followed a certain example, or a specific strategy and this knowledge helps us debug in a systematic way. Crucially, each problem or error is an opportunity to learn. Relying on LLM-generated code to solve these errors robs us of these opportunities.

> AI can't learn from its mistakes‚Äîit doesn't understand why something failed. It just pattern-matches from training data [@stetskovGreatSoftwareQuality2025: n.p.].

Research on the mid- to long-term impact of AI usage on cognitive abilities such as writing, coding, and critical thinking is still in its infancy; however, a number of studies point towards a very genuine risk of **deskilling** [see e.g. @ferdman2025] in many domains of use [e.g. medicine, see @natali2025]. In the context of AI-assisted coding, researchers from Anthropic (the company behind the Claude family of AI products) conducted a pre-registered [@tamkin2026] experiment in which 52 (mostly junior) software developers completed a programming task using a Python library that they were not familiar with and were subsequently tested for their understanding of the code. @tamkin2026 compared:

a.  how quickly the programmers completed the task with and without AI assistance and
b.  whether using AI made them less, more, or equally likely to understand the code they had just written.

![Difference in means (and 95% CI error bars) of overall task time and test scores between the treatment (AI Assistant) and control (No AI) groups (n¬†=¬†52) [Source: @tamkin2026: Fig.¬†1]](images/Shen_Tamkin2026_Fig1.png){#fig-ShenTamkin fig-alt="Two plots: the first shows the average time taken to complete the experimental task. The average with AI was 23 minutes, and without AI nearly 25. But the confidence intervals overlap considerably. The second plot shows average test scores. Here the difference is much clearer: the AI group scores 50% on average, whilst the non-AI control group scores around 65%."}

On average, the participants assigned to the AI assistance group completed the programming task about two minutes faster than those who worked without AI ‚Äî a difference which was not statistically significant (*p*¬†=¬†0.391). There was, however, a significant difference in their code comprehension test scores: the AI group averaged 50% on the test, compared to 67% in the group that did not have access to AI (Cohen's¬†*d*¬†=¬†0.738, *p*¬†=¬†0.01). @tamkin2026 also report that the largest gap in scores between the two groups was on debugging questions. They attribute the gains in skill development of the group that did not use AI to "the process of encountering and subsequently resolving errors independently" [@tamkin2026: 2-3].

> Senior developers don't grow out of thin air \[...¬†They\] develop intuition through thousands of small failures [@stetskovGreatSoftwareQuality2025: n.p.].

:::: {.content-visible when-format="html"}
::: {.callout-tip collapse="false"}
#### Your turn! {.unnumbered}

As part of an exploratory (not pre-registered) analysis, @tamkin2026 from Anthropic (see @sec-HumanLearning) decomposed the quiz scores into sub-areas and question types (see @fig-TamkinShenFig8). Each question in the quiz belonged to exactly one task (e.g., Task 1 or Task 2) and exactly one question type (e.g., Conceptual, Debugging, or Code Reading). @fig-TamkinShenFig8 shows that, for both tasks, the control (no AI) group performed better than the AI group.

[![Score breakdown by questions type relating to each task and skill area [Source: @tamkin2026: Fig.¬†8]](images/Shen_Tamkin2026_Fig8.png){#fig-TamkinShenFig8 fig-alt="A plot showing participants' average scores in each type of quiz questions together with 95% CI error bars. Across all subareas of the quiz (task 1, task 2, conceptual, debugging, and code reading), the control, no AI group performed better on average than the treatment, AI group. However, the 95% CI intervals of the two groups always overlap to a smaller or larger degree. The difference in average scores is largest for the debugging questions and smallest for the code reading."}](http://arxiv.org/abs/2601.20245)

[**Q15.6**]{style="color:green;"} Looking at the results displayed in @fig-TamkinShenFig8, which question type shows the largest difference in average quiz scores between the treatment and control groups?

```{r}
#| echo: false
#| label: "Q15.6"

check_question("Debugging",
               options = c("Conceptual", "Debugging", "Code Reading"),
               button_label = "Check answer",
               q_id = "Q15.6",
               type = "radio",
               right = "That's right! The debugging questions show the largest difference between the treatment (AI) and the control (no AI) groups.",
               wrong = "No, that's not it. Look for the question type with the greatest vertical distance between the treatment (AI) and control (no AI) group averages (represented as dots).")
```

¬†

[**Q15.7**]{style="color:green;"} Why might the control group have, on average, performed better on debugging questions compared to the AI group?

```{r}
#| echo: false
#| label: "Q15.7"

check_question("The control group had no AI assistance, so they encountered more errors during the task and gained more experience debugging.",
               options = c("The treatment group had more practice with debugging during the experimental task.",
                           "The control group had no AI assistance, so they encountered more errors during the task and gained more experience debugging.",
                           "The debugging questions were easier than the rest, so both groups scored high, and the treatment group only scored slightly higher by chance.",
                           "The AI tool was specifically designed to help with debugging, giving the treatment group a clear advantage."),
               button_label = "Check answer",
               q_id = "Q15.7",
               random_answer_order = TRUE,
               type = "check",
               right = "That's right! The authors of the study conclude that, without relying on \"AI\" help, the control group made more errors and developed stronger debugging skills through experience.",
               wrong = "Not quite. Consider how not having access to \"AI\" assistance might have led to more hands-on debugging practice.")
check_hint("The authors of the study only mention one of these as a possible reason. Which do you think is mostly likely?", hint_title = "üê≠ Click on the mouse for a hint.")

```

¬†

[**Q15.8**]{style="color:green;"} What do the results displayed in @fig-TamkinShenFig8 suggest about the impact of AI assistance on code reading skills?

```{r}
#| echo: false
#| label: "Q15.8"

check_question("Both groups had similar exposure to reading code through the task, so the AI and non-AI groups performed similarly.",
               options = c("The AI tool significantly improved code reading performance for the treatment group.",
                           "The control group had more experience reading code, so they scored considerably higher.",
                           "Both groups had similar exposure to reading code through the task, so the AI and non-AI groups performed similarly.",
                           "The confidence intervals corresponding to the code reading questions overlap too much to be able to draw any conclusions."),
               button_label = "Check answer",
               q_id = "Q15.8",
               random_answer_order = TRUE,
               type = "radio",
               right = "Indeed, the authors of the study write that the small gap suggests similar exposure and learning in code reading across groups.",
               wrong = "No, the small gap actually implies that AI assistance did not provide a major advantage in this area. Why might that be?")
```
:::
::::

## On the value of the commons

It is worth pointing out that the programmers in the control, no AI group in the aforementioned Anthropic study [@tamkin2026] did not write code with no help whatsoever. Instead, they had access to regular web searching and code documentation. It is a common misconception that learning to code involves memorising lots of functions and commands. In practice, programming always involves looking things up. Up until very recently, researchers would typically search the web to find answers to their coding problems instead of prompting an AI product. These searches led us to interesting forum discussions, blog posts by fellow researchers and developers, and helpful documentation files (e.g. in the form of so-called '[vignettes](https://r-pkgs.org/vignettes.html)'). If our web search did not help us solve our problem, we prepared a **reprex,** short for minimal **repr**oducible **ex**ample [see @wickhamDataScienceImport2023: [Ch. 8](https://r4ds.hadley.nz/workflow-help.html) on getting help], posted it on a dedicated forum and community members would typically provide helpful answers within days or even hours.

All of these contents were human-generated. Not all were 100% reliable, but high-quality answers to problems on [StackOverflow](https://stackoverflow.com/questions), for example, were upvoted by readers and errors in the documentation of open-source packages were quickly identified and corrected by the community of users. The same principle was true of statistical questions. Today, this strategy is still viable but, unfortunately, there are a lot of AI-generated webpages ('AI slop') that one first needs to ignore.

:::: {.content-visible when-format="html"}
::: {.callout-tip collapse="false"}
#### Your turn! {.unnumbered}

[**Q15.9**]{style="color:green;"} What is [Stack Exchange](https://stats.stackexchange.com/)?

```{r}
#| echo: false
#| label: "Q15.9"

check_question("A network of websites where people can ask and answer questions on a wide range of topics, the most popular of which is StackOverflow for programming questions.",
               options = c("A network of websites where people can ask and answer questions on a wide range of topics, the most popular of which is StackOverflow for programming questions.",
                           "An open-source social media platform for sharing photos and videos about research and programming.",
                           "A free cloud-based service for hosting and managing data and code repositories.",
                           "An online game for learning about programming and statistics, among other topics.",
                           "An open-source tool for managing project tasks and workflows that is widely used in academic research."), 
               button_label = "Check answer", 
               q_id = "Q15.9", 
               random_answer_order = TRUE, 
               type = "radio", 
               right = "That's right!", 
               wrong = "No, not quite.") 
check_hint("Find out more by heading to <https://stats.stackexchange.com/>.", hint_title = "üê≠ Click on the mouse for a hint.")

```

¬†

[**Q15.10**]{style="color:green;"} Open this [archived version](https://web.archive.org/web/20260209093908/https://stats.stackexchange.com/questions/11609/clarification-on-interpreting-confidence-intervals) of a Q&A about confidence intervals hosted on Cross Validated, Stack Exchange's statistics forum. In which year did Eliott originally ask their question?

```{r}
#| echo: false
#| label: Q15.10

check_question("2011",
button_label = "Check answer",
q_id = "Q15.10",
random_answer_order = TRUE,
right = "That's right!",
wrong = "No, not quite.")
check_hint("Visit the linked Q&A page on Cross Validated and scroll to the bottom of the first post to find out a) when the question was last edited and b) when it was first asked.", hint_title = "üê≠ Click on the mouse for a hint.")
```

¬†

[**Q15.11**]{style="color:green;"} As of 9 February 2026 when the page was archived, how many Cross Validated members had upvoted the top answer?

```{r}
#| echo: false
#| label: Q15.11

check_question("32 members",
               options = c("28 members",
                           "30 members",
                           "32 members",
                           "34 members"),
button_label = "Check answer",
q_id = "Q15.11",
random_answer_order = TRUE,
type = "radio",
right = "That's right!",
wrong = "No, not quite.")
check_hint("Look for the number next to the up arrow in the top-left corner of the answer.", hint_title = "üê≠ Click on the mouse for a hint.")
```

¬†

In @sec-ttest, we conducted the following *t*-test to find out whether the observed difference between the L1 and L2 speakers' non-verbal IQ 'Blocks' test scores was significant or not:

```{r}
#| echo: false
library(here)
Dabrowska.data <- readRDS(file = here("data", "processed", "combined_L1_L2_data.rds"))
```

```{r}
t.test(formula = Blocks ~ Group, 
       data = Dabrowska.data)
```

The output of the `t.test()` command above was fed into an LLM (DeepSeek R1 Distill Llama 70B) with a prompt requesting a short paragraph interpreting these results. The model's response is printed below:

> The Welch two‚Äësample t‚Äëtest shows a significant difference between the groups (t‚ÄØ=‚ÄØ‚Äë4.41, df‚ÄØ‚âà‚ÄØ152.5, p‚ÄØ=‚ÄØ1.96‚ÄØ√ó‚ÄØ10‚Åª‚Åµ). Group‚ÄØL1 has a mean of 13.84 blocks, while group L2‚Äôs mean is 17.46 blocks, a mean difference of ‚Äë3.62 blocks. The 95‚ÄØ% confidence interval for this difference (‚Äë5.24 to ‚Äë1.99) means we can be 95‚ÄØ% confident that the true average in L2 exceeds that in L1 by roughly 2 to 5 blocks. Because the interval does not include zero, the null hypothesis of equal means is rejected.

[**Q15.12**]{style="color:green;"} Aside from the fact that "blocks" is not the correct measurement unit, which major statistical error does the LLM's interpretation of the *t*-test output include?

```{r}
#| echo: false
#| label: Q15.12

check_question("The confidence interval is incorrectly interpreted as a probability statement about the true mean difference across the full population.",
               options = c("The LLM failed to recognise that the t-test was inappropriate because the data are not normally distributed.",
                           "The confidence interval is incorrectly interpreted as a probability statement about the true mean difference across the full population.",
                           "The p-value is incorrectly interpreted and is in fact too large to reject the null hypothesis.",
                           "The mean difference should be positive, not negative, since L2 speakers have a higher mean.",
                           "The t-test output makes a statement about the alternative hypothesis, not the null hypothesis."),
               button_label = "Check answer",
               q_id = "Q15.12",
               random_answer_order = TRUE,
               type = "radio",
               right = "That's right! The 95% confidence interval does not mean that there's a 95% probability that the true difference lies in that range. This is a frequent misconception of confidence intervals that is debunked in the Stack Exchange post that you looked at for earlier quiz questions. Rather, it means that, if we repeated the study many times, 95% of such intervals would contain the true difference.",
               wrong = "No, that's not it. Consider what confidence intervals represent. This is explained in the Stack Exchange post examined as part of the quiz questions above.")
```
:::
::::

In addition to the flood of AI slop making it difficult to find reliable information, human content creators are finding that their intellectual property is being scraped without consent to be used as training data for commercial AI products. In the era of [Open Scholarship](@sec-OpenScholarship), researchers, software developers, and educators have been sharing their work with the world for the benefit of scientific progress, typically with only authorship attribution as a reward. However, this may change as AI companies appropriate their work and LLMs automate plagiarism [@rooijAutomatedPlagiarism2022].

This may sound like an entirely dystopian situation but, up until fairly recently, researchers had to pay to use programming languages for statistical analyses and scientific computing. FORTAN compilers, MatLab, S, and SPPS were (and still are) proprietary software which were inaccessible to many researchers and students. It is no exaggeration to say that open-source, community-led programming languages such as `R`, Python, and Julia revolutionised data analysis, making state-of-the-art methods accessible to far more people. However, as high-quality open-access resources become rarer due the contamination of AI slop and illegal scraping, we may be returning to an era of restricted access to scientific computing. Hence, it is worth remembering that it is communities of humans who have been developing programming languages such as `R` and Python, their many extensions such as the {tidyverse} packages, open-source software such as *R Studio*, and high-quality documentation and Open Educational Resources.

These communities depend on collaboration, interactions, and mutual support. Yet, we have entered an age where human interactions are marketed as unproductive, time-consuming, and burdensome. We are told that they can easily be replaced by more efficient and "objective" chatbots. Aside from the fact that LLMs are known to be prone to all kinds of very serious biases and that their efficiency is far from proven [see e.g. @AIVsHuman on how AI code creates more problems than human-generated code], we should not loose sight of the value of subjective, human interactions. This is not to say that LLMs are never useful, but rather that I also strongly encourage you to (also) devote time to learning from reliable, human-generated resources (see e.g. [Next-step resources](https://elenlefoll.github.io/RstatsTextbook/A_FurtherResources.html)), join a course with other human beings to continue your learning journey, and/or find a learning buddy to discuss and solve problems together. This textbook was entirely human-generated and benefited greatly from countless rounds of revisions thanks to interactions with and feedback from (human!) students and colleagues (see [Acknowledgements](https://elenlefoll.github.io/RstatsTextbook/#acknowledgements)). Of course, it would have been quicker to write the textbook without asking for feedback or to get an LLM to generate first drafts of sections, code, and/or quiz questions. But in research, teaching, and learning, **quality matters more than velocity**.

### Check your progress `r emoji("star2")` {.unnumbered}

Congratulations, you have reached the end of this textbook! In this concluding chapter, you have answered [`r checkdown::insert_score()` out of 12 questions]{style="color:green;"} correctly.

::: callout-note
#### More food for thought üçèüçé

-   Bergstrom, Carl T. & Jevin D. West. Modern-Day Oracles or Bullshit Machines: How to thrive in a ChatGPT world. Online course. <https://thebullshitmachines.com>. \[Open Educational Resource\].

-   Dingemanse, Mark. 2024. Generative AI and Research Integrity. OSF. <https://doi.org/10.31219/osf.io/2c48n>. \[Open Access\].

-   Guest, Olivia, Marcela Suarez, Barbara M√ºller, Edwin van Meerkerk, Arnoud Oude Groote Beverborg, Ronald de Haan, Andrea Reyes Elizondo, et al. 2025. Against the Uncritical Adoption of ‚ÄúAI‚Äù Technologies in Academia. Zenodo. <https://zenodo.org/records/17065099>. \[Open Access\].

    ‚û°Ô∏è See also Olivia Guest's curated list of readings on critical AI literacy: <https://olivia.science/ai/> \[Open Access\].

-   M√ºhlhoff, Rainer. 2025. *The ethics of AI: Power, critique, responsibility*. Bristol: Bristol University Press. <https://doi.org/10.51952/9781529249262>. \[[Open Access](https://bristoluniversitypressdigital.com/downloadpdf/monobook-oa/book/9781529249262/9781529249262.pdf)\].

    ‚û°Ô∏è See also Rainer M√ºhlhoff's *Introduction to the Ethics of AI 2025* lecture videos. [https://rainermuehlhoff.de/en/EoAI2025/](#0){.uri}. \[Open Educational Resource\].
:::

::: {.callout-caution collapse="false"}
## Using LLMs in `R` ‚ú®

Once you have developed a sound understanding of how LLMs and AI products function and have critically reflected on their potential, limitations, and societal impact (see recommendations above), you may want to explore working with non-commercial, open-source LLMs [see @liesenfeldOpeningChatGPTTracking2023 and the [European Open Source AI Index](https://osai-index.eu/)] in `R`.

An excellent starting point to this end is Luis D. Verde Arregoitia's curated list of `R` packages and other resources: '[Large Language Model tools for R](https://luisdva.github.io/llmsr-book/)' (also available in [Spanish](https://luisdva.github.io/llmsr-book//es/index.es.html)). I also highly recommend reading the "[Read first](https://luisdva.github.io/llmsr-book/before-start.html)" and "[Further reading](https://luisdva.github.io/llmsr-book/courses.html)" sections.

![Selection of hex logos of LLM-related `R` packages from @arregoitiaLargeLanguageModel2026](images/LLMs4Rhex.png){#fig-hexLLM4R width="421"}
:::

## What's next? üß≠

This textbook has taken you on a journey: first introducing Open Scholarship (@sec-OpenScholarship), which forms the backbone of this textbook's approach to doing science, then consolidating knowledge about file formats, file naming, and project organisation (@sec-DataFormats and @sec-DataManagement), which are often major hurdles for successful data analysis pipelines. Having installed and set up `R` and *RStudio* (@sec-Installing), you took your first steps in learning to code in `R` (@sec-GettingStaRted). Next, you learnt to import real research data into an `R` project (@sec-ImportingData). From @sec-VaRiablesAndFunctions onwards, you learnt how to analyse data in `R` using descriptive and inferential statistics and data visualisations. This entailed wrangling data to prepare them for such analyses, as well as developing an understanding of key statistical concepts such as measures of central tendency and variability, distributions, effect sizes, confidence intervals, and *p*-values. @sec-SLR introduced statistical modelling with simple linear regression and @sec-MLR expanded this concept to multiple predictor variables and interactions between predictors. Having mastered this foundational knowledge, you are now ready to tackle more advanced statistical methods such as mixed-effects and non-linear regression models, logistic and other classification models, machine learning algorithms, and much more (see [Next-step resources](https://elenlefoll.github.io/RstatsTextbook/A_FurtherResources.html)). In @sec-LiteRateProgramming, you learnt how to apply literate programming skills to conduct and publish reproducible research in Quarto. Finally, in this concluding chapter, you reflected on the values of critical thinking, human learning, and of nurturing communities in the day and age of AI.

> Learning statistical theory and practice are inseparable from scientific reasoning [@vasishthHowEmbraceVariation2021: 1312].

I personally believe that it will always be worth investing in learning complex matters, developing critical thinking skills, and building meaningful human relationships. No matter how powerful and efficient future AI products may be, the skills that will be valued in the future will not be "advanced" prompting techniques, but rather social skills, critical (statistical) literacy, and computational thinking. Crucially, these are also the skills that are essential to make the most of AI tools.

The resources listed in the [Appendix](https://elenlefoll.github.io/RstatsTextbook/A_FurtherResources.html) are an excellent starting point for continuing your learning journey. In addition, the [online version](https://elenlefoll.github.io/RstatsTextbook) of this textbook features an expanding collection of case study chapters co-authored by students from my 'Introduction to Data Analysis in `R`' class. Each chapter attempts to computationally reproduce a published a linguistics study using the authors' original data. The student authors document all the steps necessary to reproduce the results and discuss the success (or not!) of the reproduction. Attempting to reproduce the steps outlined in a case study that interests you, or that covers a method you'd like to learn more about, is a great way to consolidate and expand the skills and knowledge you have acquired so far.

![Happy lea`R`ning!Ô∏è (artwork by [Allison Horst](https://allisonhorst.com/r-packages-functions) [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/))](images/AHorst_RFlowers.png){fig-alt="Two cute little monsters decorating the R logo with hearts, flowers, and smilies."}

::: {.content-visible when-format="docx"}
# Further resources/ideas {.unnumbered}

-   <https://posit.co/blog/introducing-gander/>

-   <https://air.albert-rapp.de/>

-   <https://www.seascapemodels.org/AI-assistants-for-scientific-coding/>

-   <https://www.numberanalytics.com/blog/priors-categorical-data-guide> (AI may actually get worse over time!)

-   <https://mikelovesrobots.substack.com/p/wheres-the-shovelware-why-ai-coding>

-   <https://www.ru.nl/en/research/research-news/opposing-the-inevitability-of-ai-at-universities-is-possible-and-necessary>

-   <https://research-and-innovation.ec.europa.eu/document/2b6cf7e5-36ac-41cb-aab5-0d32050143dc_en>

-   <https://www.dfg.de/resource/blob/289676/89c03e7a7a8a024093602995974832f9/230921-statement-executive-committee-ki-ai-data.pdf>
:::
