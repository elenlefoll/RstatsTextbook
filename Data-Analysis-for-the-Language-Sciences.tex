% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{5}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother
\usepackage{fancyvrb}

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother

\ifLuaTeX
  \usepackage{luacolor}
  \usepackage[soul]{lua-ul}
\else
  \usepackage{soul}
\fi

% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}



\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 


\usepackage{booktabs}
\usepackage{caption}
\usepackage{longtable}
\usepackage{colortbl}
\usepackage{array}
\usepackage{anyfontsize}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{emoji}
\usepackage{fvextra}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
\DefineVerbatimEnvironment{OutputCode}{Verbatim}{breaklines,commandchars=\\\{\}}
\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tikz}{}{\usepackage{tikz}}
\makeatother
        \newcommand*\circled[1]{\tikz[baseline=(char.base)]{
          \node[shape=circle,draw,inner sep=1pt] (char) {{\scriptsize#1}};}}  
                  
\newcounter{quartocalloutnteno}
\newcommand{\quartocalloutnte}[1]{\refstepcounter{quartocalloutnteno}\label{#1}}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\VerbatimFootnotes % allow verbatim text in footnotes
\hypersetup{
  pdftitle={Data Analysis for the Language Sciences},
  pdfauthor={Elen Le Foll},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Data Analysis for the Language Sciences}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{A very gentle introduction to statistics and data
visualisation in \texttt{R}}
\author{Elen Le Foll}
\date{1 March 2025}
\begin{document}
\maketitle

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}

\bookmarksetup{startatroot}

\chapter*{Preface}\label{preface}
\addcontentsline{toc}{chapter}{Preface}

\markboth{Preface}{Preface}

\section*{What is this book about?}\label{what-is-this-book-about}
\addcontentsline{toc}{section}{What is this book about?}

\markright{What is this book about?}

This textbook is intended as a hands-on introduction to data management,
statistics, and data visualisation for students and researchers in the
language sciences. It relies exclusively on freely accessible,
open-source tools, focusing primarily on the programming language and
environment \texttt{R}.

It is often claimed that learning \texttt{R} is ``not for everyone'', or
that it has ``a steep learning curve''. This textbook aims to prove that
the opposite is true. There are many reasons why it is worth investing
the time and effort to learn how to do research in \texttt{R}, and it is
no more difficult than learning any other new skill. In fact, the
results of a recent study suggests that language aptitude is a much
stronger predictor of programming aptitude than numeracy (i.e., ``being
good at numbers'') (Prat et al. 2020). So if you have successfully
learnt a foreign language in the past, there is no reason why you
shouldn't succeed in learning a programming language!

\begin{quote}
Learning R is like learning a foreign language. If you enjoy learning
languages, then `R' is just another one. {[}\ldots{]} You have to learn
vocabulary, grammar and syntax. Similar to learning a new language,
programming languages also have steep learning curves and require quite
some commitment. (Dauber 2024)
\end{quote}

The rationale for this textbook is based on my personal observations, in
both teaching and consulting, that many `introductory' textbooks to
statistics and/or \texttt{R} are not suitable for many humanities
students and researchers, who typically have little to no prior
programming experience and for whom the word ``statistics'' often evokes
little more than unpleasant memories of school mathematics. It is worth
stressing that is not a matter of generation (I have observed this
phenomenon across all age groups), intelligence (I have taught people
far more intelligent than me), or an innate inability to deal with
numbers and/or computers (although these are beliefs that, sadly, some
have deeply internalised). Instead, I am convinced that, for many
people, it is simply a matter of finding a sturdy, first stepping stone
and gathering up the courage to step on it to begin this learning
journey.

The aim of this textbook is by no means to replace any of the brilliant,
existing textbooks aimed at imparting statistical literacy for
linguistics research, but rather to provide a stepping stone to be able
to access these wonderful resources.\footnote{A (work-in-progress) list
  of next-step resources can be found in the
  \href{https://elenlefoll.github.io/RstatsTextbook/A_FurtherResources.html}{Appendix}.}

\section*{Who is this book for?}\label{who-is-this-book-for}
\addcontentsline{toc}{section}{Who is this book for?}

\markright{Who is this book for?}

The target audience for this book are students and researchers in the
language sciences, including (applied) linguistics, (first and second)
language teaching, and language education research. All examples are
taken from these research areas. Ultimately, however, this textbook may
be of use to anyone who feels they could benefit from a maximally
accessible stepping stone, whichever discipline they come from.

This textbook is intended to be read linearly, chapter by chapter. Apart
from the first introductory chapter, all other chapters will require
several hours of commitment. They include quizzes and short practical
tasks. Completing these tasks is essential to genuinely assimilate the
textbook's contents. That's because the best way to learn a new skill is
to try things out. So, with this in mind, let's get cracking!

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/AHorst_RLearnersWeBelieve.png}}

}

\caption{\label{fig-WeBelieveinYou}Artwork encouraging beginner
\texttt{R} learners by
\href{https://allisonhorst.com/allison-horst}{@allison\_horst}.}

\end{figure}%

\section*{About the author}\label{about-the-author}
\addcontentsline{toc}{section}{About the author}

\markright{About the author}

I started learning about statistics and \texttt{R} in 2017 when I
realised that it would be important for me to conduct the kind of
quantitative analyses that I wanted to do as part of my PhD in applied
linguistics/English language teaching (Le Foll 2022). I had no previous
experience in either and there were no such courses at my university.
Even though I mostly learnt by myself, it would be incorrect to say that
I am self-taught: I learnt from some of the resources listed in appendix
of
\href{https://elenlefoll.github.io/RstatsTextbook/A_FurtherResources.html}{next-step
resources}, attended bootcamps and summer schools, read countless posts
on StackOverflow and various blogs, and exchanged with like-minded
people on social media
(\href{https://mastodon.social/tags/rstats}{\#Rstats},
\href{https://mastodon.social/tags/dataviz}{\#dataviz},
\href{https://mastodon.social/tags/tidytuesday}{\#TidyTuesday}). This
why it is probably fairer to say that I am community-taught.

I now like to describe myself as an ``advanced beginner'' in \texttt{R}
and statistics. I am not a programmer, nor a statistician, but rather an
applied linguist and committed educator. I enjoy teaching data literacy,
statistics, and data visualisation to current and future generations of
linguists, language education scholars, and teachers. I teach regular
methods courses at the University of Cologne that are attended not just
by M.A.~and M.Ed. students, but also by some doctoral and post-doctoral
researcher colleagues. In addition, I teach workshops for both doctoral
and post-doctoral researchers at other institutions on a freelance
basis.

This textbook was partly designed on the basis of materials that I have
developed for these courses and workshops. Publishing these materials is
my way to contribute to the wonderful community of people who have
helped me on my lea\texttt{R}ning journey. ðŸ¤—

\begin{figure}[H]

{\centering \includegraphics[width=4.35417in,height=\textheight,keepaspectratio]{images/CL2017_6.jpeg}

}

\caption[Me back in 2017, proudly presenting at my first international
conference.]{Me back in 2017, proudly presenting at my first international
conference.\footnotemark{}}

\end{figure}%
\footnotetext{I chose this picture because I vividly remember two
  professors pointing out that I had written ``\emph{p}~=~0.00'' on my
  poster (which I had copied-and-pasted from the output of the
  statistics software that I had used) and laughing among themselves
  ---but well within earshot--- at how stupid that was. Learning
  quantitative data analysis skills certainly requires a lot of effort
  on the part of the learner, but it also requires an academic culture
  that strives to \emph{include} rather than \emph{exclude}. This
  textbook explicitly aims for an inclusive approach to teaching the
  basics of data analysis in \texttt{R} and I have included this photo
  as a reminder to always persevere, whether in the face of seemingly
  insurmountable error message or snarky remarks! For those of you who
  are curious, a \emph{p}-value can never equal exactly zero. But
  \emph{p}-values can be extremely close to zero so that the value may
  be rounded off to 0.00. In this case, however, it is standard practice
  to report \emph{p} \textless~0.001.}

\section*{Acknowledgements}\label{acknowledgements}
\addcontentsline{toc}{section}{Acknowledgements}

\markright{Acknowledgements}

This textbook has benefited greatly from the generous, critical feedback
I have received from both novice and expert users of \texttt{R}
throughout this project. Many thanks to my colleagues Nick Bearman,
Marie Flesch, Ben Golub, Fritjof Lammers, Akira Murakami, and Sonja
EisenbeiÃŸ for their friendly critical peer review and to my (former)
students at the
\href{https://romanistik.phil-fak.uni-koeln.de/}{University of Cologne},
Jan Hollmann, Rose HÃ¶rsting, Vishar Kavehamoli, Marie KlÃ¼nter, Vijaya
Lakshmi, Paula Raabe, Gina Reinhard, Matteo Schmelzer, Poppy Siahaan,
Veronika Strobl, Clara Stumm, Katja Wiesner, Ali YÄ±ldÄ±z, and Isabel
Zimmer, for their highly valuable learner feedback on earlier drafts of
various chapters of this textbook.

Special thanks also go out to the researchers whose works are used as
case studies in this textbook, in particular Sarah Schimke and Ewa
DÄ…browska, and to \href{https://allisonhorst.com/}{Allison Horst} whose
beautiful and witty artworks illustrate many of the chapters of this
textbook (e.g., Figure~\ref{fig-WeBelieveinYou}).

In addition, I would like to thank everyone who has contributed to my
own data analysis learning journey. At the risk of forgetting someone, I
would like to extend special thanks to Vaclav Brezina, Guillaume
Desagulier, Stephanie Evert, Stefan Gries, DaniÃ«l Lakens, Natalia
Levshina, Luke Tudge, Bodo Winter, the
\href{https://rladies.org/}{RLadies} Stack group, the \texttt{R} package
developers and maintainers of all the packages that I use, as well as
the many generous contributors to Stack Overflow and to the
\href{https://fediscience.org/tags/rstats}{\#Rstats} community on social
media.

\section*{Get in touch! ðŸ“©}\label{get-in-touch}
\addcontentsline{toc}{section}{Get in touch! ðŸ“©}

\markright{Get in touch! ðŸ“©}

If (parts of) this textbook helped you on your lea\texttt{R}ning journey
or for your teaching, do drop me a line to let me know!

If you've spotted an error or if you have any other suggestion to
improve this resource, I would love to hear from you.
\href{mailto:elefoll@uni-koeln.de}{âœ‰ï¸}

\section*{A word about the license ðŸ”“}\label{a-word-about-the-license}
\addcontentsline{toc}{section}{A word about the license ðŸ”“}

\markright{A word about the license ðŸ”“}

The online version of this textbook is published under a
\href{https://creativecommons.org/licenses/by-nc-sa/4.0/}{CC BY-NC-SA}
license, which means that these materials can be shared and adapted for
free as long as the original source is cited, the use is non-commerical,
and any adaptations are shared under the same or a compatible license.

I have chosen this license because I explicitly object to tech companies
scraping my work and reselling (mashed-up versions of) it as ``AI''. If
you work for a for-profit educational institution or are a freelance
trainer who would like to use (parts of) this textbook, please send me a
brief e-mail to explain your context of (re-)use; I will most likely be
very happy to grant you permission to do so.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{\textbf{How to cite this textbook}}, titlerule=0mm, leftrule=.75mm]

Please cite the current version of the web version of the textbook as:

\begin{quote}
Le Foll, Elen. 2025. Data Analysis for the Language Sciences: A very
gentle introduction to statistics and data visualisation in R. Open
Educational Resource. \url{https://elenlefoll.github.io/RstatsTextbook/}
(accessed DATE).
\end{quote}

To cite a specific passage, please quote the corresponding chapter or
section number(s). Note that the case-study chapters include their how
``how to cite'' text boxes.

\end{tcolorbox}

\bookmarksetup{startatroot}

\chapter{Open Scholarship}\label{sec-OpenScholarship}

This book aims to provide a stepping stone for students and scholars of
traditionally less quantitative and computational disciplines to gather
first (hopefully positive!) experiences with statistical and
computational approaches to working with empirical data\footnote{Empirical
  data are based on what is experienced or observed rather than on
  theory alone.}. The underlying belief is that these methods ought to
be accessible to all, regardless of their academic background or
personal circumstances. To this end, this book embraces the principles
of Open Scholarship.

\begin{quote}
{[}Open Scholarship{]} reflects the idea that knowledge of all kinds
should be openly shared, transparent, rigorous, reproducible,
replicable, accumulative, and inclusive (allowing for all knowledge
systems). Open scholarship includes all scholarly activities that are
not solely limited to research such as teaching and pedagogy. (Parsons
et al. 2022)
\end{quote}

\subsubsection*{\texorpdfstring{\textbf{Chapter
overview}}{Chapter overview}}\label{chapter-overview}
\addcontentsline{toc}{subsubsection}{\textbf{Chapter overview}}

In this chapter, you will learn about the relevance of Open Scholarship
in learning how to manage, manipulate, analyse, and visualise research
data. In doing so, the following aspects of Open Scholarship will be
introduced:

\begin{itemize}
\tightlist
\item
  Open Science
\item
  Open Source Software
\item
  Open Education
\item
  Open Educational Resources (OERs)
\end{itemize}

\section{Open Science}\label{sec-OpenScience}

Open Science is a major component of Open Scholarship and the two terms
are frequently used synonymously. Open Scholarship, however, is broader
in that it includes all kinds of knowledge, whereas Open Science focuses
on what is conventionally considered ``scientific knowledge''. Open
Science covers many different aspects including:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1700}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.5600}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1300}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1300}}@{}}
\toprule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Open materials} & Giving free, unrestricted, public access to
research materials in a way that allows others to replicate the results
of published studies and to conduct new studies based on these existing
materials.

Materials may include questionnaire items, all kinds of experimental
stimuli, annotation schemes, inclusion and exclusion criteria, etc. (see
Task 2 in Section~\ref{sec-Sharing}). &
\pandocbounded{\includegraphics[keepaspectratio]{images/OpenMaterials.png}}
& \\
\textbf{Open data} & Giving free, unrestricted, public access to
scientific data, whenever ethically and legally possible (see
Berez-Kroeker et al. 2022). An important principle of Open Science is
the sharing of FAIR data; that is data that are Findable, Accessible,
Interoperable, and Reusable.

In Section~\ref{sec-DataLanguageSciences}, we will see that studies in
the language sciences can involve many different types of data including
texts, tables, images, and videos. &
\pandocbounded{\includegraphics[keepaspectratio]{images/OpenData.png}}
& \\
\textbf{Open code} & Making computer code freely and publicly available
with appropriate documentation to make research methods and data
analyses transparent.

Open code can include source code for custom software and packages, code
for stimuli generation, data collection and processing, statistical
analysis, and data visualisation. Sharing code allows for
collaborations, while sharing both code and data allows others to
reproduce published results. &
\pandocbounded{\includegraphics[keepaspectratio]{images/OpenCode.png}}
& \\
\textbf{Open access} & Giving free, unrestricted, public access to
scientific outputs, foremost publications. Contrary to a frequent
misunderstanding, authors or their institutions do not necessarily have
to pay article processing fees
(\href{https://forrt.org/glossary/english/article_processing_charge/}{APC}s)
to publish their work in open access. Publishing open access can instead
involve uploading a pre-copyedit version of a publication on a public
repository (see Section~\ref{sec-Sharing}) or publishing in a so-called
`diamond' (typically non-profit) open access publication outlet (see
section on
\href{https://book.the-turing-way.org/reproducible-research/open/open-access.html}{Open
Access} in The Turing Way Community 2022). &
\pandocbounded{\includegraphics[keepaspectratio]{images/OpenAccess.jpg}}
& \\
\end{longtable}

Sharing research data allows us to \textbf{reproduce} the analyses
reported in research publications based on the authors' original data
and to test whether different analysis methods would have led to
different conclusions. Sharing research materials and code means that we
can \textbf{replicate} studies to check the robustness of published
results and/or their generalisability across different populations. For
example, if a journal article reports on the effectiveness of a new
language teaching method based on a study conducted at a British
university, we can test whether the same effect can be observed when
replicating the study at a Nigerian university or an Indonesian
secondary school.

Open Science advocates argue that scientific knowledge ``{[}should{]},
where appropriate, be openly accessible, transparent, rigorous,
reproducible, replicable, accumulative, and inclusive, all which are
considered fundamental features of the scientific endeavour'' (Parsons
et al. 2022). This corresponds to an ideal that, although probably
impossible to fully achieve, is nonetheless worth striving for at all
times.

\begin{quote}
Open science consists of principles and behaviors that promote
transparent, credible, reproducible, and accessible science. (Parsons et
al. 2022)
\end{quote}

To conduct open science, a sound understanding of data management and of
effective data analysis workflows is crucial. This textbook aims to
provide a gentle, practical introduction to these foundational skills
using examples from the language sciences. Published as an Open
Educational Resource (see Section~\ref{sec-OpenEducation}), it showcases
linguistics and Second Language Acquisition (SLA) publications that
include open data, open code and/or materials and teaches data analysis
using exclusively open-source software and programming languages (see
Section~\ref{sec-OpenSource}).

\section{Open Source}\label{sec-OpenSource}

In line with its aim to provide an accessible introduction to statistics
and data visualisation, this textbook relies exclusively on open-source
software and programming languages, foremost LibreOffice Calc,
\texttt{R} and RStudio. Open source refers to software whose source code
is available under a license that grants anyone the rights to study,
modify, and distribute the software to anyone and for any purpose. If we
think of a software application as a cake, the source code is like its
recipe. It contains the list of ingredients and the steps to bake the
cake. Open source means that the recipe is publicly available. You can
access it, read it, and use it to bake the cake. You can also modify it
to add your own twist, such as adding a new ingredient or making it
vegan, and share it with others. In the context of software, this allows
many people to collaborate, make improvements, and share their versions,
resulting in better and more diverse software (see
Figure~\ref{fig-OpenSoftware}).

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/TuringWay_open-development.jpg}}

}

\caption{\label{fig-OpenSoftware}Open software development (CC-BY 4.0
Scriberia with \href{https://book.the-turing-way.org/}{The Turing Way}
community, DOI:
\href{https://doi.org/10.5281/zenodo.3332807}{10.5281/zenodo.3332807})}

\end{figure}%

Using open-source software in this introductory textbook means that
anyone\footnote{Provided that they have access to the internet and a
  functioning personal computer.} can download, install and use the
required software at no cost. However, it is very important to note that
not all free software (also called `freeware') is open source.

~

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Going further}, titlerule=0mm, leftrule=.75mm]

In this introductory textbook, we have simplified things considerably.
To be considered \textbf{open source}, software distributions actually
have to comply with ten criteria. You can read up on them here:

\begin{itemize}
\tightlist
\item
  \url{https://opensource.org/osd}
\end{itemize}

To find out more about the benefits of open-source software in the
context of research, I recommend reading:

\begin{itemize}
\tightlist
\item
  \url{https://book.the-turing-way.org/reproducible-research/open/open-source}
\end{itemize}

\end{tcolorbox}

\section{Open Education}\label{sec-OpenEducation}

Open Education is a key component of Open Scholarship (see
Chapter~\ref{sec-OpenScholarship}). Open Education aims to stimulate
collaborative teaching and learning and to provide high-quality Open
Educational Resources (OERs) that are accessible for all.

As illustrated in Figure~\ref{fig-OER}, OERs are licensed in such a way
that everyone has the right to engage in ``5 Rs'' when using OERs. The 5
Rs of OERs are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{R}etain - the right to make, own, and control copies of the
  content (e.g.~download, duplicate, and store copies of an OER).
\item
  \textbf{R}euse - the right to use the content in a range of ways
  (e.g.~as teaching materials on a course, as part of a website, or in a
  video).
\item
  \textbf{R}evise - the right to adapt, adjust, modify, or alter the
  content itself (e.g.~translate the content into another language,
  create a version for a different programming language).
\item
  \textbf{R}emix - the right to combine the original or revised content
  with other open materials to create something new.
\item
  \textbf{R}edistribute - the right to share copies of the original
  content, any revisions, and remixes with others (e.g.~give a copy of
  the content to a friend).
\end{enumerate}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/oer.jpg}}

}

\caption{\label{fig-OER}OER sketch note CC-BY 4.0
\href{https://leko.th-nuernberg.de/portal_digitale_lehre/praxisbeispiele/lehrmaterialien-teilen-fuer-flexibles-lernen/}{Yvonne
Stry}}

\end{figure}%

OERs may be published under different licenses and, in engaging in the 5
Rs, the exact terms of an OER's license must be respected. For example,
the web-based version of this textbook is published as an OER under the
Creative Commons license
\href{https://creativecommons.org/licenses/by-nc-sa/4.0/}{\texttt{CC\ BY-NC-SA}}.
This means that anyone can engage in the 5 Rs with it (i.e.~users are
free to read and use, edit, remix, and expand upon the textbook) as long
as:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  the original author and source is mentioned (hence you should specify
  who this resource is
  \href{https://creativecommons.org/licenses/by-nc-sa/4.0/}{\texttt{BY}}),
\item
  any derived version is not made into a commercial product
  (\href{https://creativecommons.org/licenses/by-nc-sa/4.0/}{\texttt{NC}}
  stands for \emph{non-commercial}), and that
\item
  any derived versions of this textbook (e.g.~a translated version or a
  version adapted for history scholars) are also shared with this same
  license
  (\href{https://creativecommons.org/licenses/by-nc-sa/4.0/}{\texttt{SA}}
  stands for \emph{share alike}).
\end{enumerate}

In line with the principles of Open Education, all of the datasets used
as case studies in this textbook have been published in open access. We
will analyse real data from published research studies in the fields of
applied linguistics and language education to learn about data
management, statistics, and data visualisation.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Going further}, titlerule=0mm, leftrule=.75mm]

There are thousands of high-quality \textbf{Open Educational Resources}
(OERs) out there, yet few people are aware of them. OER databases are
good starting points to start exploring OERs, e.g.:

\begin{itemize}
\tightlist
\item
  \url{https://oercommons.org/}
\item
  \url{https://www.twillo.de/oer/web/}
\end{itemize}

An appendix of
\href{https://elenlefoll.github.io/RstatsTextbook/A_FurtherResources.html}{next-step
resources} also lists recommended \textbf{next-step OERs} on data
management, data analysis in \texttt{R}, statistics, data visualisation,
Open Science, and reproducibility.

If you want to \textbf{share} your own research materials, data, or OER
but you're unsure about which license to use, this handy
\href{https://chooser-beta.creativecommons.org/}{license chooser tool}
is a great starting point. In addition, librarians are usually very
happy to advise students and researchers on these topics.

\end{tcolorbox}

\section*{Check your progress ðŸŒŸ}\label{check-your-progress}
\addcontentsline{toc}{section}{Check your progress ðŸŒŸ}

\markright{Check your progress ðŸŒŸ}

You have successfully completed { out of 6 questions} in this chapter.

Are you confident that you can\ldots?

\begin{itemize}
\tightlist
\item[$\square$]
  Explain the basic principles of Open Science
  (Section~\ref{sec-OpenScience})
\item[$\square$]
  Find out if software and programming languages are open-source or
  proprietary (Section~\ref{sec-OpenSource})
\item[$\square$]
  Install open-source software on your own computer
  (Section~\ref{sec-OpenSource})
\item[$\square$]
  Explain the basic principles of Open Education
  (Section~\ref{sec-OpenEducation})
\item[$\square$]
  Work out if you can reuse something that is licensed with a Creative
  Commons license (Section~\ref{sec-OpenEducation}).
\end{itemize}

The next two chapters are devoted to research data management. While you
may be keen to get cracking with data analysis in \texttt{R}, it is
crucial that we first ensure that we understand what kind of research
data we are dealing with, how and where they are saved, under which
name, etc. otherwise nothing will work! Or at least not for very
long\ldots{} ðŸ˜”

\bookmarksetup{startatroot}

\chapter{Data files and formats}\label{sec-DataFormats}

\subsubsection*{\texorpdfstring{\textbf{Chapter
overview}}{Chapter overview}}\label{chapter-overview-1}
\addcontentsline{toc}{subsubsection}{\textbf{Chapter overview}}

This chapter first considers what data means in the context of language
research, before turning to how these data are formatted and stored. You
will learn about:

\begin{itemize}
\tightlist
\item
  Different types of data used in language research
\item
  Computer data formats and file extensions
\item
  Sharing and accessing research data and materials
\item
  Working with delimiter-separated values (DSV) files
\item
  The pitfalls of spreadsheet programs such as Microsoft Excel, Numbers,
  and Google Sheets
\end{itemize}

Along the way, you will get insights into an eye-tracking study
involving cute Playmobil figures and a meta-science investigation that
highlights the utmost importance of data literacy for research.

\section{Data in the language sciences}\label{sec-DataLanguageSciences}

In this book, we are concerned with empirical research in the language
sciences, in other words, with research that is based on the analysis of
\textbf{data}. But what are data exactly? Data can be collected via
surveys, measurements, or observations. To begin with, however, these
collected datasets are ``raw''. Data only becomes \textbf{information}
once we have analysed and interpreted the data in a meaningful way.
Hence, just like uncooked pasta does not make a flavourful meal, we must
learn to ``cook'' the raw data to obtain meaningful information.

What kind of data are analysed in the language sciences? To get a rough
idea of the range of data types analysed in the language sciences, let
us take a look at the \href{https://iris-database.org}{IRIS database}.

\begin{quote}
IRIS is a collection of instruments, materials, stimuli, data, and data
coding and analysis tools used for research into languages, including
first, second, and beyond, and signed language learning,
multilingualism, language education, language use, and language
processing. Materials are freely accessible and searchable, easy to
upload (for contributions) and download (for use). (iris-database.org
2011)
\end{quote}

As such, IRIS supports Open Science and Open Scholarship (see
Chapter~\ref{sec-OpenScholarship}).

\section{Types of research data}\label{sec-ResearchData}

Given the wide range of methods used in language research, it is no
surprise that they are so many different types of research data (see
e.g. Good 2022). Although the data types listed on the IRIS search page
(see \textbf{?@fig-IRISDataTypes} for extract) are very broad and the
categories not clearly defined, the list illustrates the breadth of
research data types typically analysed in language studies.

The first data type category, ``Oral production'', for instance, can
equally refer to text transcriptions of language users' oral production,
audio, or video files. It can also refer to either raw data or to (more
or less) processed data. For example, a transcript of a conversation
could have been automatically annotated for part-of-speech, meaning that
every word would be marked for their word class
(e.g.~\texttt{This\_DT\ is\_VBZ\ not\_RB\ raw\_JJ\ text\_NN\ data\_NN\ .\_PUNC}),
or it could have been manually anonymised by adding placeholders
(e.g.~\texttt{Is\ \textless{}NAME\textgreater{}\ going\ out\ with\ \textless{}NAME\textgreater{}?})
indicating that certain words have been retracted for data protection
reasons.

The second most frequent data type category, ``Closed response format'',
includes different kinds of questionnaires and tests. Questionnaires may
ask study participants to disclose personal information relevant to the
research questions using single or multiple-choice questions, such as
what language(s) they use at home, how long they have studied a language
for, or how old they are. Tests may be designed to assess participants'
language competences (e.g.~in the form of a vocabulary or grammar test),
as well as other aspects relevant to the research questions being
investigated (e.g.~short-term memory or baseline reaction times).

In this book, we will focus on the research processes that take place
after the data have been collected. However, it is vital that we are
aware of the conditions and context in which the data we are analysing
were collected and pre-processed. It is no exaggeration to say that
these steps in the research process can entirely change the results of
the data analysis. Suppose we decide to compare the abilities of two
groups of French L2 learners. To do this, we administered a language
production test to two whole classes of secondary school students
learning French as a second language using two different teaching
methods. If one group had 15 minutes to complete the test and the other
had up to 60 minutes, the results would not be comparable.

Whilst there are many ways to ensure that as many factors as possible
are controlled for, not all \emph{can} be controlled for. What is
crucial is that all aspects of the data collection process are well
documented so that all factors, whether controlled or not, can be taken
into account when analysing the data.

In research, we usually distinguish between \textbf{primary data}, which
are the data that you collected yourself, and \textbf{secondary data},
which are data that were collected by others. Hence if you were to carry
out a new study based on data that you found on IRIS, you would be
conducting a secondary data analysis. Especially when conducting
secondary data analyses, it is crucial that we have enough information
about the data itself, i.e.~\textbf{metadata}. Metadata is crucial for
finding, sharing, evaluating, and reusing datasets (Trippel 2025). For
some data and projects, it makes sense to create separate metadata files
that contain additional or more detailed information about the collected
data. For language-related data, various metadata tools and standards
exist (see e.g. Paquot et al. 2024; Consortium 2025; Windhouwer \&
Goosen 2022; Withers 2012) and it makes sense to try to stick to these
standards as far as possible to ensure greater comparability and
comparability across different datasets (for more information on how to
develop a Data Management Plan for a linguistics study, see Kung 2022).

\section{Data formats and file extensions}\label{sec-FileExtensions}

Different data types come in different data formats. For audio files,
you may be familiar with the MP3 format, but this is by no means the
only format in which audio files can be saved. Many other audio file
formats exist, such as \textbf{Wave}form Audio File Format (WAVE) and
\textbf{F}ree \textbf{L}ossless \textbf{A}udio \textbf{C}odec (FLAC).

We can usually tell in what format a file is in by looking at its file
extension. The file extension is the suffix of the file name. It comes
at the end of the file name and is preceded by a dot. The file extension
of a WAVE file is \texttt{.wav}, whereas that of an MP3 file is
\texttt{.mp3}; hence the file \texttt{recording.wav} is a WAVE file,
whereas \texttt{recording.mp3} is an MP3 file.

Unfortunately, many modern operating systems have a tendency to hide
file extensions by default. This results in the files
\texttt{recording.wav} and \texttt{recording.mp3} both being displayed
as \texttt{recording} in File Finder/Explorer windows (compare
Figure~\ref{fig-NoExtensions} and Figure~\ref{fig-Extensions}). This is
misleading and can lead to all kinds of problems.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/FinderWindowNoExtensions.png}}

}

\subcaption{\label{fig-NoExtensions}Displaying file names without file
extensions}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/FinderWindowExtensions.png}}

}

\subcaption{\label{fig-Extensions}Displaying file names with their
extensions}

\end{minipage}%

\caption{\label{fig-FileExtensions}Demonstrating the importance of
seeing file extensions.}

\end{figure}%

To ensure that you can always see the extensions of the files on your
computer in the File Explorer (on Windows) or the File Finder (on
macOS), follow these instructions:

\begin{itemize}
\tightlist
\item
  On Windows:
  \url{https://www.howtogeek.com/205086/beginner-how-to-make-windows-show-file-extensions/}.
\item
  On macOS:
  \url{https://support.apple.com/en-gb/guide/mac-help/mchlp2304/mac}
  (select the version of your operating system at the top of the page).
\end{itemize}

\section{Sharing research data and materials}\label{sec-Sharing}

In line with the principles of Open Science (see
Chapter~\ref{sec-OpenScholarship}), it is important to ensure that both
the materials that were used to collect research data
(e.g.~questionnaire items, audio, image or video stimuli, language
aptitude tests, etc.) and the data themselves are made openly available
to the research community, whenever legally possible and ethically
responsible. Sharing materials ensures that studies can be replicated,
for example with new participants or in a different language. Sharing
research data also allows independent researchers to reproduce the
results of studies, allowing them to verify the reported results and to
conduct additional analyses that may confirm, contradict, or extend the
conclusions of the original studies.

You may be wondering how linguists and language education researchers
can make their research data and materials publicly available.
Table~\ref{tbl-RepoTable} provides a non-exhaustive list of public
repositories where researchers can upload research data and materials
(with figures collected in early June 2024\footnote{Note that, for some
  repositories, the number of entries includes other types of research
  outputs, e.g.~preprints and figures.}). Some are specific to the
language sciences, while others cater to all research disciplines. All
of the examples, tasks, and exercises in this book are based on research
data and materials that researchers have made available in open access
on one or more of these repositories.

\begin{table}

\caption{\label{tbl-RepoTable}Non-exhaustive list of public repositories
of research data and materials.}

\centering{

\begin{verbatim}
                                               Repository        Discipline
1                                                   Dryad               All
2                                                Figshare               All
3                                                     HAL               All
4                                       Harvard Dataverse               All
5                                                    IRIS       Linguistics
6                            Open Science Repository, OSF               All
7 TromsÃ¸ Repository of Language and Linguistics, TROLLing       Linguistics
8                                                   Vivil Clinical research
9                                                  Zenodo               All
  Nb. of entries Provides DOI Online since
1          60000          Yes         2008
2        8000000          Yes         2012
3        5000000           No         2001
4         160000          Yes         2006
5           3500           No         2011
6         153663          Yes         2012
7           4500          Yes         2014
8           7000          Yes         2013
9        3750000          Yes         2013
\end{verbatim}

}

\end{table}%

In the following {\textbf{Your Turn!}} sections, we will look at a study
by Schimke et al. (2018) (see Figure~\ref{fig-SchimkeTitle}), which is
an example of a publication which was awarded the Open Data and the Open
Materials badges (see Figure~\ref{fig-OpenDataOpenMaterials}). This
means that the research materials and data associated with this study
can be found in an open, online repository:

\begin{quote}
``This article has been awarded Open Materials and Open Data badges. All
materials and data are publicly accessible via the IRIS Repository at
https://www.iris-database.org/iris/app/home/detail?id=york:934337. Learn
more about the Open Practices badges from the Center for Open Science:
https://osf.io/tvyxz/wiki'' Schimke et al. (2018).
\end{quote}

The authors could have chosen to upload their materials and data to any
of the online repositories listed in Table~\ref{tbl-RepoTable} but, in
this case, they chose \href{https://iris-database.org/}{IRIS}.

\begin{figure}

\begin{minipage}{0.70\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/SchimkeEA2018Abstract.png}}

}

\subcaption{\label{fig-SchimkeTitle}Title page of the Schimke et al.
(2018)}

\end{minipage}%
%
\begin{minipage}{0.30\linewidth}

\centering{

\includegraphics[width=2.63542in,height=\textheight,keepaspectratio]{images/OpenDataOpenMaterials.jpg}

}

\subcaption{\label{fig-OpenDataOpenMaterials}The Open Data and Open
Materials badges}

\end{minipage}%

\caption{\label{fig-SchimkeEA2018}An example of a publication for which
both research materials and data have been published.}

\end{figure}%

Among other results, Schimke et al. (2018) report on two eye-tracking
experiments. One of these experiments involved Spanish-speaking
participants listening to ambiguous sentences in Spanish whilst looking
at images of Playmobil figures (see Figure~\ref{fig-Playmobil} for an
example).

\begin{figure}

\centering{

\includegraphics[width=4.16667in,height=\textheight,keepaspectratio]{images/Playmobil.png}

}

\caption{\label{fig-Playmobil}Image from Experiment 1 in Schimke et al.
(2018)}

\end{figure}%

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note \ref*{nte-Eyetracking}: How did the experiment work?}, titlerule=0mm, leftrule=.75mm]

\quartocalloutnte{nte-Eyetracking} 

In this eye-tracking experiment, participants were instructed to decide
whether the sentences they heard matched the Playmobil images or not.
Consider the following two sentences from the experiment:

\begin{quote}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{El barrendero se encontrÃ³ con el cartero antes de que recogiera
  las cartas.}\\
  {[}The street sweeper met the postman before he fetched the
  letters.{]}
\item
  \emph{El barrendero se encontrÃ³ con el cartero antes de que recogiera
  la escoba.}\\
  {[}The street sweeper met the postman before he fetched the broom.{]}
\end{enumerate}
\end{quote}

Up until the point at which either \emph{las cartas} {[}the letters{]}
or \emph{la escoba} {[}the broom{]} are heard, it is unclear who is
doing the fetching. From a grammatical point of view, it could be either
the street sweeper or the postman.

Participants were presented with Figure~\ref{fig-Playmobil} as they were
listening to either Sentence 1 or Sentence 2. At the same time, the
researchers measured how long it took for the participants to look at
the subject governing the verb \emph{recogiera}. In other words, for
Sentence 1, they were interested in how long it took participants to
focus on the postman Playmobil figure and, in Sentence 2, on the street
sweeper. Such fine measurements are made in milliseconds, i.e.~in
thousandths of seconds, using a special eye-tracking device.

\end{tcolorbox}

\section{Working with tabular data}\label{sec-TabularData}

The measurements made by the eye-tracking device in Schimke et al.
(2018)'s eye-tracking experiments were stored in the form of tables.
Table~\ref{tbl-EyeTrackingTable} is an extract of a table that contains
processed eye-tracking data from Schimke et al. (2018). It forms part of
the study's supplementary materials and can also be downloaded from the
\href{https://iris-database.org/details/lYT9m-a75tV}{IRIS database}.

In this table, each row corresponds to the data associated with one
participant's eye movements while listening to a single stimulus
sentence and looking at the corresponding Playmobil image (e.g.
Figure~\ref{fig-Playmobil}). The extract displayed as
Table~\ref{tbl-EyeTrackingTable} only shows the data associated with the
first six stimulus sentences (\texttt{items}) that participant ``s1'', a
Spanish L2 learner, listened to. The columns \texttt{crit1},
\texttt{crit2} and \texttt{crit3} contain values derived from the
measurements made using the eye-tracking device.\footnote{Details of
  what these values mean are not relevant here but, for those of you who
  are curious, they correspond to the ``log odds of looks'' that
  participant made towards one or the other Playmobil figure whilst
  listening to the experimental stimulus sentences at three time points,
  called ``critical regions''. These critical regions include the time
  window between the onset of the pronoun and 480 milliseconds after the
  onset of the disambiguating information. Schimke et al. (2018:
  768--769) explain that ``{[}a{]} positive value of the log odds
  indicates more looks to the subject than to the object antecedent,
  while a negative value indicates the reverse pattern.''} From
Table~\ref{tbl-EyeTrackingTable}, we can also see that participant
``s1'' was 19 years old when they started formally learning Spanish
(\texttt{AoO} stands for ``age of onset of formal instruction'') and
that they were 20 when the experiment was conducted.

\begin{table}

\caption{\label{tbl-EyeTrackingTable}Extract of table containing
eye-tracking data from Schimke et al.~(2018)\textquotesingle s appendix}

\centering{

\begin{verbatim}
  language subject disambiguation item      crit1       crit2      crit3 AoO
1        S      s1              1    1  0.3451355 -0.56187889  0.7036070  19
2        S      s1              2    2 -0.2679332 -1.58496250  0.1852149  19
3        S      s1              1    3 -1.1563420  0.98980423 -1.5849625  19
4        S      s1              2    4 -1.5849625 -0.08746284 -1.5849625  19
5        S      s1              1    5  1.5849625  0.18312230  1.5849625  19
6        S      s1              2    6 -0.7824086 -0.85480208 -1.1758498  19
  age
1  20
2  20
3  20
4  20
5  20
6  20
\end{verbatim}

}

\end{table}%

When working with data, tables are ubiquitous. Data stored in tables are
called \textbf{tabular data}. Hence, learning to work with tabular data
is a crucial data literacy skill.

In the language sciences, the results of most studies (whether
experimental or corpus studies) are stored in tables. For example, when
researchers conduct an online survey, the data collected by the online
survey platform (e.g.~\href{https://www.qualtrics.com}{Qualtrics},
\href{https://www.limesurvey.org/}{LimeSurvey},
\href{https://www.soscisurvey.de/}{SoSci Survey}) are automatically
stored in the form of one or more table(s). These can then be exported
from the survey platform in various tabular file formats
(e.g.~\texttt{.csv}, \texttt{.json}, \texttt{.xlsx}).

In some cases, data may be collected by analogue means, e.g.~by getting
participants to answer a paper questionnaire or collecting school
children's work on paper. However, for quantitative analysis, analogue
research data are first digitalised. Then, the data are typically stored
as text files in file formats such as \texttt{.txt} or \texttt{.csv}.

\subsection{Delimiter-separated values (DSV) files}\label{sec-DSV}

Tables can be stored in many data formats but the simplest and most
widely used in linguistic research are \textbf{text files with}
\textbf{delimiter-separated values (DSV)}. For sharing and archiving
research data, DSV files are favoured over formats specific to propriety
software such as \texttt{.xslx} (Microsoft Excel files) or
\texttt{.numbers} (Apple Numbers files). This is because DSV files can
be ``understood'' by many different programs and on all operating
systems. The fact that they are simple text files means that we will
also be able to reliably read them in the future, even if programs such
as Excel or Numbers have evolved or have been discontinued. Reliability
and compatibility are fundamental to maintaining the integrity of
research data and ensuring that data can be reused, even in the distant
future.

In DSV files, each value (e.g.~measurement or response) is separated by
a specific \textbf{separator} character. In principle, any character can
be used to separate values, but the most common separators are the comma
(\texttt{,}), tab (\texttt{\textbackslash{}t}), colon (\texttt{:}), and
semicolon (\texttt{;}). Below is the \texttt{.csv} file corresponding to
Table~\ref{tbl-RepoTable}.

\begin{verbatim}
Repository,Discipline,Nb. of entries,Provides DOI,Online since
Dryad,All,60000,Yes,2008
Figshare,All,8000000,Yes,2012
HAL,All,5000000,No,2001
Harvard Dataverse,All,160000,Yes,2006
IRIS,Linguistics,3500,No,2011
"Open Science Repository, OSF",All,153663,Yes,2012
"TromsÃ¸ Repository of Language and Linguistics,TROLLing",Linguistics,4500,Yes,2014
Vivil,Clinical research,7000,Yes,2013
Zenodo,All,3750000,Yes,2013
\end{verbatim}

As you can see, the values are separated by commas.\footnote{Note that
  the file extension \texttt{.csv} stands for ``comma-separated
  values''. Confusingly, however, DSV files are often given a
  \texttt{.csv} extension even when the separator character is not the
  comma. As a result, even though the \texttt{.tsv} extension stands for
  ``tab-separated values'', \texttt{.csv} files are frequently separated
  by a tab (\texttt{\textbackslash{}t}) rather than comma. Isn't that
  fun? ðŸ™ƒ} Additionally, some of the values are enclosed in, or
\textbf{delimited} by, double quotation marks (\texttt{"}). This
prevents any commas that may occur within an actual field value,
e.g.~the comma in the field \texttt{Open\ Science\ Repository,\ OSF},
from being interpreted as a separator character.

Given that DSV files are text files, it is possible to open them in a
free plain-text editor
(e.g.~\href{https://notepad-plus-plus.org/}{Notepad++} or
\href{http://www.barebones.com/products/bbedit/}{BBEdit}) or a
text-processing program (e.g.~Microsoft Word or LibreOffice Writer).
However, these programmes will typically display DSV files as in
Figure~\ref{fig-DSVinWord}.

\begin{figure}

\centering{

\includegraphics[width=5.85417in,height=\textheight,keepaspectratio]{images/DSVinWord.png}

}

\caption{\label{fig-DSVinWord}The \texttt{.csv} file corresponding to
Table~\ref{tbl-RepoTable} opened in Microsoft Word}

\end{figure}%

We can probably agree that what we are seeing in
Figure~\ref{fig-DSVinWord} is not a very reader-friendly way to display
tabular data! This is why DSV files are more often opened in spreadsheet
programs (e.g.~LibreOffice Calc, Google Sheets, Microsoft Excel,
Numbers) than in text-editing programs. Let's find out how in the next
section.

\subsection{Opening DSV files in LibreOffice
Calc}\label{sec-DSVLibreOffice}

There are several ways to open a DSV file in LibreOffice Calc but the
safest is to launch LibreOffice (see {\textbf{Task 1.1}} in
Section~\ref{sec-OpenSource} if you have not yet installed LibreOffice)
and, from the list of options under `Create', click on `Calc
Spreadsheet' to open up a blank spreadsheet. Then, from the `File'
drop-down menu, select `Open\ldots{}' or use the keyboard shortcut
\texttt{Ctrl/Cmd\ +\ O} and locate the DSV file that you wish to open.

On opening a DSV file in LibreOffice Calc, we get a dialogue box with
various options (see Figure~\ref{fig-DSVImportCalc}).

\begin{figure}

\centering{

\includegraphics[width=4.6875in,height=\textheight,keepaspectratio]{images/DSVImportCalc.png}

}

\caption{\label{fig-DSVImportCalc}Text import dialogue in
LibreOfficeCalc}

\end{figure}%

To correctly import this particular DSV file, it is necessary to specify
that the separator character is the comma (\texttt{,}) and that the
delimiter character is the double quotation mark (\texttt{"}) (see
selected options in Figure~\ref{fig-DSVImportCalc}). With these settings
in LibreOffice Calc, the table is rendered as in
Figure~\ref{fig-DSVinCalc}.

\begin{figure}

\centering{

\includegraphics[width=6.26042in,height=\textheight,keepaspectratio]{images/DSVinCalc.png}

}

\caption{\label{fig-DSVinCalc}CSV file opened in LibreOffice Calc}

\end{figure}%

Note that if you open a DSV file in Excel or Google Sheets, you will not
be shown such a dialogue box. Instead, these programs assume that they
can guess which separator and delimiter characters your file uses.
Whilst this may, at first, sound convenient, this is \emph{not} good
news: \emph{you} should be the one in control of how your data files are
interpreted, not the program! In the next section, you will learn why
opening DSV files such as \texttt{.csv} and \texttt{.tsv} files in
Microsoft Excel, Google Sheets, or Numbers can be very dangerous. In
some cases, these programs will `corrupt', i.e.~permanently damage, your
DSV files, which can lead to irreversible data loss!

The bad news is that, if you are using Windows or MacOS, it is very
likely that either Excel or Numbers is your default app to open DSV
files. This means that if you double click on a \texttt{.csv} and
\texttt{.tsv} file in your Finder/Explorer window, the file will likely
automatically open up in either Excel or Numbers. This is why it is
important you do \emph{not} double-click on such files to open them:
Opening a file just \emph{once} with these programs can lead to data
loss! If this happens to you with a file that you have downloaded from a
repository, your best bet is to delete your local version of the file
and download a fresh version so that you can start again from scratch.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-important-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-important-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{What if I absolutely have to open a DSV file in Excel? ðŸ˜§}, titlerule=0mm, leftrule=.75mm]

If you absolutely must open a DSV file (e.g.~a \texttt{.csv} or
\texttt{.tsv} file) in Excel (for example because you do not have
sufficient permissions to install LibreOffice on the computer that you
are using), do \emph{not} open the file by double clicking on the file
as this will automatically trigger Excel's problematic auto-formatting
behaviour (see Section~\ref{sec-ExcelWarning})! Instead, first launch
Excel and create a new blank workbook. Then navigate to the `Data' tab,
select the `Get Data' option, and then `From Text/CSV' (see
Figure~\ref{fig-ExcelGetData}). In the following dialogue, you can
specify how the data should be imported. The options are very similar to
the ones offered in LibreOffice (see above).

Note that with this method it \emph{may} be possible to prevent Excel
from automatically (and irreversibly!) applying transformations to your
data. However, sadly, this may not suffice. Read on to find out
more\ldots{}

\begin{figure}[H]

\centering{

\includegraphics[width=4.71875in,height=\textheight,keepaspectratio]{images/ExcelGetData.png}

}

\caption{\label{fig-ExcelGetData}Import data into excel}

\end{figure}%

\end{tcolorbox}

\section{A word of warning about spreadsheet programs
âš ï¸}\label{sec-ExcelWarning}

You should be aware that opening DSV files in spreadsheet programs can
corrupt the files! Once a file is corrupted, it is often not possible to
retrieve the original data so this is very bad news, indeed. Such
problems are particularly frequent when opening DSV files with Microsoft
Excel and Google Sheets. This is because the default settings in these
programs surreptitiously modify files upon opening.

These `auto-format' modifications include replacing certain values by
dates (e.g.~changing \texttt{3-4} to \texttt{March,\ 4th}) or numbers
(e.g.~changing \texttt{1.23E5} to \texttt{123000})\footnote{In
  scientific notation, ``E'' stands for ``exponent'', which refers to
  the number of times a number needs to be multiplied by 10. This
  notation is used as a shorthand way of writing very large or very
  small numbers. This is why ``1.23E5'' is interpreted by Excel as 1.23
  multiplied by 10 to the power of 5, which is to say: 1.23 multiplied
  by 100,000. This operation shifts the decimal point five places to the
  right, resulting in the number 123000.}, removing leading zeros
(e.g.~changing \texttt{001} to \texttt{1}), or misinterpreting certain
characters (e.g.~the value \texttt{-ism} will generate an error because
the hyphen is interpreted as minus sign).

Not only can these auto-format modifications lead to inaccurate data
analysis but, in the worst of cases, they can even cause data loss. The
crux of the problem is that often users do not realise what the program
has done in the background. How bad can this be? Find out by completing
the {\textbf{task}} below.

It is worth noting that, for some Windows users, these auto-formatting
issues can corrupt files that they have \emph{never} actively opened in
Excel! ðŸ¤¯ This happens when Windows applies Excel's default settings to
all CSV files, regardless of what program they are actually opened with.
To ensure that this does not happen to you, check that Excel is
\emph{definitely not} your default app to open \texttt{.csv} and
\texttt{.tsv} files (see below for instructions).

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-tip-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Opening a \texttt{.csv} or \texttt{.tsv} file in LibreOffice from a File
Finder/Explorer window}, titlerule=0mm, leftrule=.75mm]

Remember that to open a \texttt{.csv} or \texttt{.tsv} file on your
computer, should \emph{never ever} double-click on it and let the
default program open it! As we saw in Section~\ref{sec-ExcelWarning},
this can break or `corrupt' the file. To avoid accidentally
double-clicking on a \texttt{.csv} or \texttt{.tsv} file and having the
file corrupted, I recommend making either
\href{https://www.libreoffice.org/}{LibreOffice} or a plain-text editor
(e.g.~\href{https://notepad-plus-plus.org/}{Notepad++} or
\href{http://www.barebones.com/products/bbedit/}{BBEdit}) your default
application to open up such files.

On \textbf{MacOS}, you can change the default application used to open
files of any file extensions by right-clicking a file name with this
particular extension and than selecting `Get Info'
(Figure~\ref{fig-ChangeDefaultAppMac01}). In the example below, Numbers
is the default application for all \texttt{.csv} files (see
Figure~\ref{fig-ChangeDefaultAppMac02}). In the dropdown menu `Open
with:', you can then select LibreOffice (provided you have installed it
beforehand!) and finally click on `Change All\ldots{}'
(Figure~\ref{fig-ChangeDefaultAppMac03}). You will be asked to confirm
your choice.

\begin{figure}[H]

\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics[width=4.6875in,height=\textheight,keepaspectratio]{images/ChangeDefaultAppMac01.png}

}

\subcaption{\label{fig-ChangeDefaultAppMac01}~}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics[width=4.6875in,height=\textheight,keepaspectratio]{images/ChangeDefaultAppMac02.png}

}

\subcaption{\label{fig-ChangeDefaultAppMac02}~}

\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics[width=4.6875in,height=\textheight,keepaspectratio]{images/ChangeDefaultAppMac03.png}

}

\subcaption{\label{fig-ChangeDefaultAppMac03}~}

\end{minipage}%

\caption{\label{fig-DefaultExcelMac}Changing the default application for
a file extension on MacOS}

\end{figure}%

If your operating system is \textbf{Windows}, you should look in your
Windows' settings for the option `Default Apps' (see
Figure~\ref{fig-DefaultAppsWindows}).

\begin{figure}[H]

\centering{

\includegraphics[width=6.69792in,height=\textheight,keepaspectratio]{images/DefaultAppsSetting.png}

}

\caption{\label{fig-DefaultAppsWindows}Default apps in Windows settings}

\end{figure}%

In the next step, select `Choose default apps by file type'. Here, you
can search for \texttt{.csv} as a file type, and choose which program
you want to set as the default program for opening \texttt{.csv} files.
If Excel is currently your default (as in
Figure~\ref{fig-ExcelDefaultApp}), you can click on Excel and choose a
different program. LibreOffice is a sensible, open-source alternative
(see Figure~\ref{fig-DefaultAppsChangeDialog}). A plain-text editor such
as Notepad would also be fine (also listed on
Figure~\ref{fig-DefaultAppsChangeDialog}).

\begin{figure}[H]

\begin{minipage}{0.50\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/DefaultAppsExcel.png}}

}

\subcaption{\label{fig-ExcelDefaultApp}Excel as the default programme
for \texttt{.csv} files}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/DefaultAppsChangeDialog.png}}

}

\subcaption{\label{fig-DefaultAppsChangeDialog}Changing the default
programme for \texttt{.csv} files}

\end{minipage}%

\caption{\label{fig-DefaultExcelWindows}Changing the default app for
opening \texttt{.csv} files in Windows}

\end{figure}%

If it is not possible to adjust the default app settings, either due to
insufficient permissions or because you only have temporary access to
this PC, do \emph{not} to open \texttt{.csv} or \texttt{.tsv} files with
the default program. Instead, right-click on the file name and, using
the `Open with' option, select the option to open the file with
LibreOffice, if available, or else with a plain-text editor.

\end{tcolorbox}

\subsection*{Check your progress ðŸŒŸ}\label{check-your-progress-1}
\addcontentsline{toc}{subsection}{Check your progress ðŸŒŸ}

You have successfully completed { out of 14 questions} in this chapter.

Are you confident that you can\ldots?

\begin{itemize}
\tightlist
\item[$\square$]
  Distinguish different types of research data
  (Section~\ref{sec-ResearchData})
\item[$\square$]
  Find and download openly available research data and materials
  (Section~\ref{sec-Sharing})
\item[$\square$]
  Distinguish different data formats using the file extensions
  (Section~\ref{sec-FileExtensions})
\item[$\square$]
  Open delimiter-separated values (DSV) files in LibreOffice Calc
  (Section~\ref{sec-DSV}) - (Section~\ref{sec-DSVLibreOffice})
\item[$\square$]
  Explain the risks of opening DSV files in Microsoft Excel and Google
  sheets (Section~\ref{sec-ExcelWarning})
\end{itemize}

In Chapter~\ref{sec-DataManagement}, you will learn how to name, save,
and back-up research data files so as to facilitate sound data analysis.

\bookmarksetup{startatroot}

\chapter{Data management}\label{sec-DataManagement}

\subsubsection*{\texorpdfstring{\textbf{Chapter
overview}}{Chapter overview}}\label{chapter-overview-2}
\addcontentsline{toc}{subsubsection}{\textbf{Chapter overview}}

Even if you are confident that you have no trouble managing your
computer files, it is still worth taking a few minutes to read up on the
basics of data management. This is especially true if you consider
yourself a ``digital native'' as modern operating systems have made the
way that computers deal with files very opaque.

In this chapter, you will learn about:

\begin{itemize}
\tightlist
\item
  File and folder naming conventions
\item
  Absolute and relative computer paths
\item
  Solutions for backing up your files
\end{itemize}

\section{Recipes for successful data
management}\label{recipes-for-successful-data-management}

Data management is hardly a ``hot'' topic that people like to dwell on.
That's a shame because good file management is absolutely central to be
able to conduct research and poor file management has the potential to
seriously `spice things up'\ldots{} but not in a good way! ðŸŒ¶ï¸ Whether
you are working on a short course assignment, your Master's or PhD
thesis, or as part of a large research project team: research-related
files must be named appropriately and safely stored in meaningful
places.

Imagine trying to make a curry in an utterly disorganised kitchen that
contains dozens of different spices, scattered across different cabinets
and drawers, with vague or misleading labels. For example, you might
have three jars labelled ``Chilli'' and no way of knowing which is mild
``Kashmiri Chilli'' as opposed to the extra hot ``Thai Bird's Eye
Chilli''. The third might not be chilli at all, but actually a jar of
paprika that has been entirely mislabelled. Some of these spices have
been gathering dust for decades but the labels have no best-before dates
so there is no way of knowing which are still fragrant. Cooking in such
a kitchen would turn even the simplest cooking task into a tedious,
time-consuming, and error-prone chore: If you're not extremely careful,
you could easily end up serving something that is bland or, in the worst
of cases, entirely inedible! Similarly, in research, if your files are
poorly named or stored haphazardly, it will make your work far less
efficient, considerably more error-prone, and ultimately utterly
frustrating.

!{[}An inefficient and potentially dangerous workflow (artwork by
\href{https://twitter.com/allison_horst}{Allison Horst})
(\href{https://creativecommons.org/licenses/by/4.0/}{CC BY
4.0})(images/AHorst\_MessyKitchen.png)\{\#fig-MessyKitchen fig-alt=``A
frustrated looking little monster in front of a very disorganised
cooking area, with smoke and fire coming from a pot surrounded by a mess
of bowls, utensils, and scattered ingredients.''\}

But the good news is: just as a tidy, well-organized kitchen can greatly
enhance your cooking experience, good file management can streamline
your research process, help you avoid making mistakes, and reduce
stress. In the following sections, we will cook up some good practices
for file naming, data management, and project organisation. We will
start with basic recipes for naming and managing your files. See the
`Going further' boxes for tips on learning the `gourmet skills' needed
to handle more complex projects. So, let's don the chef's hat and learn
how to create a user-friendly computer workspace. And remember, as with
cooking, practice makes perfect!

\section{Naming conventions}\label{sec-FileNaming}

File names are labels. They tell us what is inside a file and helps us
identify the correct file quickly and reliably. If you had to run to the
printing shop to get your thesis printed in time for a tight deadline,
which of these sets of files would you rather have to choose from? Which
is more likely to lead you to getting the wrong version printed?

\begin{figure}

\begin{minipage}{0.50\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/FileNamesBadExample.png}}

}

\subcaption{\label{fig-FileNamesBadExample}~}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/FileNamesGoodExample.png}}

}

\subcaption{\label{fig-FileNamesGoodExample}~}

\end{minipage}%

\caption{\label{fig-filenames}Two sets of file names, one clearly better
than the other.}

\end{figure}%

Like the labels on your neatly organised spice jars, file and folder
names should be clear, concise, and easily readable. Good file and
folder names should be both human-friendly and computer-friendly.

By \textbf{human-friendly} we mean that you and any other human being
should easily be able to understand what a folder or file contains. Just
like you wouldn't want a label on a spice jar to be a random string of
numbers (e.g.~\texttt{0171}) or only include the best-before date but
nothing else (e.g.~\texttt{31\ Jan\ 2028}), you also wouldn't want to
guess what a file contains based on an ambiguous or unclear name like
\texttt{Chili}. Labels should be informative but succinct
(e.g.~\texttt{Thai\ Bird\textquotesingle{}s\ Eye\ Chilli\ 31\ Jan\ 2028}
not
\texttt{Thai\ Bird\textquotesingle{}s\ Eye\ Chilli\ bought\ on\ December\ 19,\ 2023\ whilst\ Christmas\ shopping\ with\ mum,\ note\ that\ the\ best\ before\ date\ is\ 31\ January\ 2028})!
Unless you and all your colleagues read Thai, do not be tempted to write
the part of the file name in Thai as this could also lead to
misunderstandings.

Another reason for not including Thai characters in your file name is
that it would not be \textbf{computer-friendly}. In general, computers
are not good at dealing with names that contain anything else but Latin
alphanumeric characters, e.g.~the letters \texttt{A} to \texttt{Z} and
\texttt{a} to \texttt{z} with no accents and the numbers \texttt{0} to
\texttt{9}. Hyphens (\texttt{-}) and underscores (\texttt{\_}) can also
be used, but not spaces. The dot (\texttt{.}) is reserved for the file
extension and should ideally not be used elsewhere in the file name.

Hence, whilst
\texttt{Thai\ Bird\textquotesingle{}s\ Eye\ Chilli\ 31\ Jan\ 2028} is
human-friendly, it is not computer-friendly. To make it a
computer-friendly label, we need to remove the apostrophe. Whilst spaces
are not strictly forbidden, they can cause all kinds of issues and are
therefore also best avoided. Space characters can be replaced by hyphens
(\texttt{-}) and underscores (\texttt{\_}) and the two can be combined
in a meaningful way. For example, in the label
\texttt{Thai-Birds-Eye-Chilli\_31-Jan-2028}, the \texttt{\_}
distinguishes between two different pieces of information, whilst the
\texttt{-} helps humans to parse individual words within a piece of
information. Using such patterns consistently not only helps humans to
read file names efficiently, it also means that computers can easily
`parse', i.e.~break down such names into meaningful items. This can be
very useful to search for files or automatically extract metadata from
file names.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/AHorst_InThatCase.png}}

}

\caption{\label{fig-InThatCase}To help us remember the different,
systematic ways to use letter case, hyphens, and underscores in naming
conventions, these patterns have fun names (art work by
\href{https://allisonhorst.com/allison-horst}{@allison\_horst}).}

\end{figure}%

It is fine to use both lower-case and upper-case letters in file and
folder names. However, some operating systems will treat upper-case and
lower-case letters as the same, whilst others will not. This means that
you should avoid having file names that are only distinguishable by
case.

Finally, it is worth noting that file names cannot be infinitely long!
The maximum length of a file name depends on the operating system and
the application that you use\footnote{For example, many Windows
  applications have a maximum file path length of 260 characters
  (alvinashcraft et al. 2022).} but, as a rule of thumb, if you can
display the entire file name in a reasonably sized Finder window (on
macOS) or File Explorer window (on Windows), its length is
unquestionably both human- and computer-friendly.

It is also important to ensure that file names are easily sortable. If
you have a series of files that document a process, consider beginning
each file name with a number that correspond to the order of the
process, e.g.~\texttt{01\_DataPreparation.R},
\texttt{02\_DataAnnotation.R}, \texttt{03\_AnnotationEvaluation.R}.
Left-padding the numbers with one or more \texttt{0} will mean that the
files are sorted numerically, even when files are listed alphabetically
(see Figure~\ref{fig-FileNamesLeftPaddedNumbers}).

\begin{figure}

\begin{minipage}{0.50\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/FileNamesLexicographic.png}}

}

\subcaption{\label{fig-FileNamesNaturalNumbers}File names without
additional zeros numbers}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/FileNamesPaddedNumbers.png}}

}

\subcaption{\label{fig-FileNamesLeftPaddedNumbers}File names with
left-padded numbers}

\end{minipage}%

\caption{\label{fig-NumberPadding}Why left-padding file names is good
file naming practice.}

\end{figure}%

It is sometimes useful to include the date in file names. However, many
date formats are not easily sortable (see
Figure~\ref{fig-FileNamesDateFormatUnordered}). Formatting dates using
the `YYYY-MM-DD' format as in
Figure~\ref{fig-FileNamesDateFormatOrdered} will allow you to easily
sort your files in chronological order.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/FileNamesDateFormatUnordered.png}}

}

\subcaption{\label{fig-FileNamesDateFormatUnordered}File names with a
non-ordered date format}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/FileNamesDateFormatOrdered.png}}

}

\subcaption{\label{fig-FileNamesDateFormatOrdered}File names with an
ordered date format}

\end{minipage}%

\caption{\label{fig-Dates}Why using the YYYY-MM-DD is good file naming
practice.}

\end{figure}%

YYYY-MM-DD is the internationally recommended standard for dates
(\href{https://www.iso.org/iso-8601-date-and-time-format.html}{ISO
8601}). It provides a clear, computer-readable representation of dates
that avoids the confusion that can arise from different cultures using
different day-month conventions (e.g.~12/13 can mean both 12 December
and 13, November).

Even though computers have gotten much better at dealing with folder and
file names containing spaces and special characters, using anything
other than basic Latin alphanumeric characters (\texttt{A-Z},
\texttt{a-z}, and \texttt{0-9}), \texttt{-} and \texttt{\_} in file and
folder names will - sooner or later - cause you or your colleagues some
serious issues. This is especially true when you start coding. Do not
delay getting used to using systematic, human- and computer-friendly
folder and file names! In the long run, these simple guidelines will
make your digital life much smoother and save you much time and
unnecessary stress.

\section{Folders and paths}\label{sec-FoldersPaths}

Now that you know how to name your files and folders sensibly, we can
turn to best practices for organising these files and folders. Returning
to our kitchen analogy, imagine that, over many years, you collected
hundreds of recipes from friends and family. These recipes are jotted
down on individual sheets of paper, all of which have been thoughtlessly
tossed into a large kitchen drawer called `Documents', which also
happens to contain receipts for kitchen appliances still under warranty,
takeaway brochures, and various other bits of paper. In such a kitchen,
finding Aunt Sophie's famous caramelised apple cake could take a while!
If, however, you had a dedicated kitchen drawer for recipes which
contained neatly labelled folders of different types of dishes, you
would know to look for this cake recipe in the Desserts folder. Within
the Desserts folder, you could have sub-folders for different types of
desserts (e.g.~cakes, ice creams, trifles). This would make finding Aunt
Sophie's recipe an absolute piece of cake!

Thinking about how to structure folders and sub-folders for your
projects is about creating a kind of road map that should be readily
interpretable by both humans and computers. This is where the concept of
`paths' arises. Paths, in simple terms, describe the location of a file
or a folder in a computer's filesystem. There are different types of
paths. An \textbf{absolute path} provides a complete path from the
computer's ``root folder''. If our house were our root folder, the
absolute path to Aunt Sophie's recipe would be
\texttt{"/Kitchen/Recipes/Desserts/Cakes/Apple-Cake\_Aunt-Sophie"}.
Hence, just like your home's postal address, which ideally specifies
your home's absolute location worldwide, an absolute path provides a
complete path from a computer's \textbf{root folder} to the file or
folder in question.

By contrast, a \textbf{relative path} represents the location of a file
or folder relative to another folder. Hence, if we already have the
Dessert folder open in front of us, the relative path to the apple cake
recipe would simply be \texttt{"Cakes/Apple-Cake\_Aunt-Sophie"}.
However, if we wanted to access a recipe in the Starters folder from the
Cakes folder, we would first have to go ``back up the path'' from the
Cakes folder to the Recipes drawer. This is achieved by adding
\texttt{../} to the front of the relative path,
e.g.~\texttt{"../Starters/Soups/Pea-Mint-Soup\_Barbara"}.

For example,
\texttt{"/Users/lefoll/Documents/Teaching/RstatsTextbook/ToDo.txt"} is
the absolute path from my computer's root folder to the file containing
my to-do list in relation to this textbook project. By contrast, a
``relative path'' represents the location of a file or folder relative
to another folder. Hence, if I am already in the directory
\texttt{"/Users/lefoll/Documents/Teaching/RstatsTextbook/"}, the
relative path to my to-do list is only \texttt{"ToDo.txt"}.

Note that, here, we use the term ``folder'' as a metaphor for a computer
file directory. Most modern operating systems use folder icons that look
like the kind of paper file folders that office workers use to have
piled up on their desks as a means of visually representing directories
in computer file systems.

To complicate things a little, the way file paths are written varies
depending on the computer's operating system. In Unix-based systems like
Linux and macOS, paths are written using forward slashes
(e.g.~\texttt{"/Users/elen/Documents/Teaching/RstatsTextbook/ToDo.txt"}),
whereas on Windows, paths are written using backslashes
(e.g.~\texttt{"C:\textbackslash{}Users\textbackslash{}elen\textbackslash{}Documents\textbackslash{}Teaching\textbackslash{}RstatsTextbook\textbackslash{}ToDo.txt"}).

There are many ways to find out where your files are stored on your
computer. Let us begin by opening a Finder window (on macOS) or a File
Explorer window (on Windows). Navigate to the folder which contains the
file for which you want to find the absolute path. Alternatively you
could use your computer's search function to search for the file. Once
you have found it:

\begin{itemize}
\item
  on Windows: Right-click on the file (in some older Windows versions,
  you may also need to press the ``shift'' key). Among the options
  presented to you, click on the one to copy the file path (e.g.~``Copy
  as path'' or similar in the language of your operating system).
\item
  on macOS: Right-click on the file and then press the Option/âŒ¥ key on
  your keyboard. Pressing down this key will change the options you are
  given after having right-clicked. One of these options should now be
  ``Copy \ldots{} as Pathname'' (or something equivalent in the language
  of your operating system). Click on this option.
\end{itemize}

Then, open any text-editing programme (e.g.~LibreOffice Writer,
Microsoft Word, TextEdit, or NotePad++) and use the shortcut
\texttt{Ctrl/Cmd\ +\ V} to paste your file's path in the empty document.
If you are on Windows, your path should have backslashes, whereas if you
are on Linux or macOS, your path should have forward slashes.

\section{Backing up data: `Fire safety' measures in the digital kitchen
ðŸ§¯}\label{sec-BackupData}

A basic principle of sound data management consists in keeping a copy of
\emph{all} your files in more than one place. This ensures that, should
something go awry, your research is not lost forever but instead can be
recovered and restored promptly. There are many ways things could go
wrong: laptops can get stolen or permanently damaged (laptops are not
terribly keen on hot chocolate as it turns out\ldots{} ðŸ™ˆ), computer
files can be corrupted and become unusable, you or someone else may
accidentally delete files, your computer can become infested with a
nasty virus, etc.

An effective way to protect your projects is to abide by the
\textbf{3-2-1 rule} (Schweinberger 2022). It's simple:

\begin{itemize}
\tightlist
\item
  Ensure that you have at least \textbf{three} copies of your data
  (e.g.~one that you work with on your personal computer and two back-up
  copies).
\item
  Split the backup copies between \textbf{two} different storage media
  (e.g.~a hard-drive stored in your office and online in a secure cloud
  service).
\item
  Store \textbf{one} of these copies in a secure place off-site
  (i.e.~not where your computer usually is).
\end{itemize}

One solution is to store your \textbf{three copies} on:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  your personal laptop or computer,
\item
  a backup hard drive stored in a secure location, and
\item
  a secure online repository such as the data management system provided
  by your institution, e.g.~Sciebo, ownCloud, or GitLab.
\end{enumerate}

Choosing an online repository will protect your data if your computer
malfunctions or is damaged or stolen, but remember that it can also
potentially make your data accessible to others. This is particularly
true of commercial back-up solutions such as Microsoft's OneDrive,
Google's Drive, Apple's iCloud, or Dropbox, which although convenient
and very user-friendly, should not be used to store sensitive data
(e.g.~data that may be used to identify individuals, contain financial
information, health records, location data, or proprietary research
data). Always check if your institution has its own, secure cloud
option. If not, keeping a second hard-drive copy in a separate, secure
location is likely the safest solution.

Whilst the \textbf{3-2-1 rule} stipulates that you should keep at least
three copies of each file, in an optimal scenario, each file should
exist only once at each location (e.g.~on your laptop, a separate
hard-drive, and the server of an online repository). It is quite easy to
(often unknowingly) end up with several duplicates of the same file on
any one machine but this can cause issues if, for example, you end up
updating the wrong version of the file. Avoiding and eliminating file
duplicates is therefore an important step towards proficient data
management.

\section{Conclusion}\label{conclusion}

Sound data management - comprising of both good folder and file naming
practices and the smart organisation of these folders and files - is the
foundation for efficient research workflow. Understanding and applying
these basic principles of file management will ensure that everything in
your digital `kitchen' has its place, is well labelled, and easy to
find. By ensuring that we keep our kitchens clean, tidy, and safe, we
can whip out some truly delicious dishes!

\begin{figure}

\begin{minipage}{0.50\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/AHorst_MessyKitchen.png}}

}

\subcaption{\label{fig-BadKitchen}}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/AHorst_TidyKitchen.png}}

}

\subcaption{\label{fig-GoodKitchen}}

\end{minipage}%

\caption{\label{fig-Kitchens}Artwork illustrating the kitchen workflow
metaphor by \href{https://twitter.com/allison_horst}{Allison Horst}
(\href{https://creativecommons.org/licenses/by/4.0/}{CC BY 4.0})}

\end{figure}%

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Going further}, titlerule=0mm, leftrule=.75mm]

This short online module is ideal to learn more about smarter ways to
work with files and data:

\begin{itemize}
\tightlist
\item
  The University of Queensland Library. 2023. Work with Data and Files.
  The University of Queensland.
  \url{https://uq.pressbooks.pub/digital-essentials-data-and-files/}.
  (14 May, 2024).
\end{itemize}

To go further, here are some great, open-access resources to dive deeper
into data management and data standards in linguistics and education
research:

\begin{itemize}
\item
  Berez-Kroeker, Andrea L., Bradley McDonnell, Eve Koller \& Lauren B.
  Collister. 2022. \emph{The Open Handbook of Linguistic Data
  Management}. MIT Press.
  \url{https://doi.org/10.7551/mitpress/12200.001.0001}.
\item
  Heid, Ulrich, Piotr BaÅ„ski \& Laura Herzberg (eds.). 2025.
  \emph{Harmonizing Language Data: Standards for Linguistic Resources}
  (Digital Linguistics 4). Boston: De Gruyter.
  \url{https://doi.org/10.1515/9783112208212}.
\item
  Lewis, Crystal. \emph{Data Management in Large-Scale Education
  Research}. \url{https://datamgmtinedresearch.com/}. (14 May, 2024).
\end{itemize}

\end{tcolorbox}

\subsection*{Check your progress ðŸŒŸ}\label{check-your-progress-2}
\addcontentsline{toc}{subsection}{Check your progress ðŸŒŸ}

You have successfully completed { out of 10 questions} in this chapter.

Are you confident that you can\ldots?

\begin{itemize}
\tightlist
\item[$\square$]
  Give your files and folders names that are both human- and
  computer-readable (Section~\ref{sec-FileNaming})
\item[$\square$]
  Organise your files and folders in a sensible manner
  (Section~\ref{sec-FoldersPaths})
\item[$\square$]
  Remember the 3-2-1 rule on backups (Section~\ref{sec-BackupData})
\end{itemize}

If that's the case, you are now all set to install \texttt{R} and
RStudio in Chapter~\ref{sec-InstallingR} and learn how to get started in
\texttt{R} in Chapter~\ref{sec-GettingStaRted}!

\bookmarksetup{startatroot}

\chapter{\texorpdfstring{Installing \texttt{R} and
RStudio}{Installing R and RStudio}}\label{sec-InstallingR}

\subsubsection*{\texorpdfstring{\textbf{Chapter
overview}}{Chapter overview}}\label{chapter-overview-3}
\addcontentsline{toc}{subsubsection}{\textbf{Chapter overview}}

This chapter is designed to help you get started using \texttt{R} and
\emph{RStudio}, assuming no prior use of either. We will be covering the
following topics:

\begin{itemize}
\tightlist
\item
  Why it's worth learning \texttt{R}
\item
  Downloading \texttt{R} and \emph{RStudio}
\item
  Setting up \emph{RStudio}
\item
  Using the Console in \emph{RStudio}
\item
  Installing and loading \texttt{R} packages
\item
  Accessing help files
\item
  Citing packages
\end{itemize}

If you already have some experience of using \texttt{R} and
\emph{RStudio}, please ensure that both are up-to-date before continuing
(see Section~\ref{sec-Updates}). Whilst parts of this chapter will
likely be revision, others may be the opportunity to learn some new tips
about setting up and using \texttt{R} in \emph{RStudio}, installing and
citing packages.

\section{\texorpdfstring{Why learn
\texttt{R}?}{Why learn R?}}\label{why-learn-r}

In short, because \texttt{R} can do it all! ðŸ™ƒ This statement is only a
slight exaggeration: \texttt{R} is indeed a highly \textbf{versatile}
programming language and environment that allows us to do a multitude of
tasks relevant to the language sciences. These include data handling and
processing, statistical analysis, creating effective and appealing data
visualisations, web scraping, text analysis, generating reports in
various formats, designing web pages, and interactive apps, and much,
much more! ðŸ’ª

Whilst some will claim that \texttt{R} has a steep learning curve, this
textbook aims to prove that the opposite is true! Whilst it's fair to
say that, as with all new things, it will take you a while to get the
hang of it, once you've got started, you will see that your
possibilities are (pretty much) endless and that learning how to do new
things in \texttt{R} makes for fun and very rewarding challenges. What's
more, this textbook introduces the \emph{tidyverse} approach to
programming in \texttt{R} (see Section~\ref{sec-tidyverse}), which is
particularly \textbf{accessible} to beginners. We will also use
\emph{RStudio} to access \texttt{R}, which makes things considerably
more \textbf{intuitive} and generally easier to work with.

Crucially, both \texttt{R} and the \emph{RStudio Desktop} version that
we will be using are \textbf{free} and \textbf{open source} (see
Section~\ref{sec-OpenSource}), which means that they are accessible to
all, regardless of their institutional affiliation or professional
status. This is in contrast to proprietary statistical software such as
SPSS, for which you or your university needs to buy an expensive
license. To get started in \texttt{R}, all you will need is access to
the internet, a computer (unfortunately, a tablet will probably not
suffice), and the intrinsic \textbf{motivation} to work your way through
the fundamental skills taught in this textbook.

\begin{quote}
{[}Using R is{]} like the green and environment-friendly gardening
alternative to buying plastic wrapped tomatoes in the supermarket that
have no taste anyway (\href{https://slcladal.github.io/whyr.html}{Martin
Schweinberger 2002 in `Why R?'}).

\begin{figure}

\centering{

\includegraphics[width=3.91667in,height=\textheight,keepaspectratio]{images/TomatoHarvest.jpg}

}

\caption{\label{fig-tomatoes}``\href{https://www.flickr.com/photos/47264866@N00/9455141053}{Tomato
Harvest, Yellow \& Red}'' by
\href{https://www.flickr.com/photos/47264866@N00}{OakleyOriginals}
(\href{https://creativecommons.org/licenses/by/2.0/?ref=openverse}{CC BY
2.0}).}

\end{figure}%
\end{quote}

Last but not least, in choosing to learn \texttt{R}, you are entering a
vibrant \textbf{community} of users. As an open-source programming
environment, \texttt{R} is the product of many different people's
contributions. Everyday, new packages, functions, and resources are
being developed, improved, and shared with the community. Given that
\texttt{R} has evolved into one of the most popular languages for
\textbf{scientific programming} (and has become ``the de facto standard
in the language sciences'' Winter 2020: xiii), many of these have been
created by scientists and are particularly well-suited to
\textbf{research workflows}. Moreover, the \texttt{R} community is known
for being welcoming, supportive, and inclusive. This is reflected in the
strong presence of many community-led initiatives such as
\href{https://rladies.org/}{RLadies+},
\href{https://rainbowr.netlify.app/}{RainbowR}, and
\href{https://latinr.org/}{LatinR}, that encourage under-represented
groups to participate in and contribute to the \texttt{R} community. ðŸ¤—

\begin{figure}

\centering{

\includegraphics[width=2.60417in,height=\textheight,keepaspectratio]{images/rladiesrp_logo.png}

}

\caption{\label{fig-RLadies}Logo of the
\href{https://rladiesrp.github.io/}{RLadies RibeirÃ£o Preto}, one of
\href{https://benubah.github.io/r-community-explorer/rladies.html}{many
RLadies+ chapters}.}

\end{figure}%

\subsubsection*{\texorpdfstring{\emph{``I am studying a
language/linguistics so why should I learn to code?''}
ðŸ¤”}{``I am studying a language/linguistics so why should I learn to code?'' ðŸ¤”}}\label{i-am-studying-a-languagelinguistics-so-why-should-i-learn-to-code}
\addcontentsline{toc}{subsubsection}{\emph{``I am studying a
language/linguistics so why should I learn to code?''} ðŸ¤”}

Using scripts rather than GUI software will help you make your research
\textbf{less error-prone}, more \textbf{transparent}, and
\textbf{sustainable}. Being open-source, there are no restrictions as to
who can run \texttt{R} code and older versions are available ensuring
that reproduction is feasible, even years later. As many other linguists
use \texttt{R} (see e.g. Mizumoto \& Plonsky 2016), you will be able to
\textbf{collaborate} with others and understand other researchers'
\texttt{R} code. As we will see in
Chapter~\ref{sec-LiteRateProgramming}, in \emph{RStudio}, it is also
very easy to export \texttt{R} code and share your scripts, for example
as part of an appendix to your research publication, in various formats
(including \texttt{.html} that can be opened in any browser and
\texttt{.pdf}, see Chapter~\ref{sec-LiteRateProgramming}).

In addition, learning to code in \texttt{R} provides an excellent
foundation in \textbf{data literacy} and \textbf{statistical reasoning}.
These are skills that are highly valued among employers, both in
academia and the industry. Many companies, public institutions
(e.g.~ministries, hospitals, national agencies) and NGOs hire data
scientists who often work in \texttt{R}. And, even if you end up doing
little to no coding yourself, understanding the basic principles of
programming is a highly useful skill in the modern world; it is crucial
to be able to effectively communicate and collaborate with programming
colleagues. More generally, \textbf{computational thinking}
\textbf{skills} are highly transferable (see e.g. Ye, Lai \& Wong 2022).
For instance, they can help us to structure our work processes more
systematically, identify patterns more effectively, and formulate our
thoughts more precisely.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{What about learning Python instead? ðŸ}, titlerule=0mm, leftrule=.75mm]

Some of you may be wondering whether you should be learning Python
rather than \texttt{R}. Both are widely used languages in
\textbf{scientific programming} and \textbf{data science}. At the time
of writing, there are more resources specifically aimed at linguists and
education researchers in \texttt{R} than there are in Python simply
because it is currently the most widely used language in these
disciplines. Should you wish to learn Python at a later stage (see
\href{https://elenlefoll.github.io/RstatsTextbook/A_FurtherResources.html}{Next-step
Resources}), many of the same principles that you will have learned in
this textbook will apply: it should feel somewhat like learning Italian
when you already speak Spanish or French.

\end{tcolorbox}

\section{\texorpdfstring{Installing R and
\emph{RStudio}}{Installing R and RStudio}}\label{installing-r-and-rstudio}

\subsection{\texorpdfstring{What are \texttt{R} and \emph{RStudio}? And
why do I need
both?}{What are R and RStudio? And why do I need both?}}\label{sec-IDE}

As a beginner, it's easy to confuse \texttt{R} and \emph{RStudio}, but
it's important to understand that they are two very different things.
\texttt{R} is a programming environment for statistical computing and
graphics that uses the programming language \texttt{R}. Think of it as
the engine with which we will learn to perform lots of different tasks.
\emph{RStudio}, by contrast, is a set of tools, a so-called `integrated
development environment' (IDE). It makes working in \texttt{R} much more
intuitive and efficient. If \texttt{R} is the engine of our car, you can
imagine \emph{RStudio} as our dashboard. Hence, even though we will
later on appear to only be working in \emph{RStudio}, \texttt{R} will
actually be doing the heavy-lifting, under the hood.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics[width=2.40625in,height=\textheight,keepaspectratio]{images/Rlogo.png}

}

\subcaption{\label{fig-RLogo}Logo of the programming language and
environment \texttt{R}}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics[width=1.92708in,height=1.85417in]{images/RStudio-Logo_Trademark.png}

}

\subcaption{\label{fig-RStudioLogo}Logo of the IDE \emph{RStudio}
(RStudioÂ® is a trademark of Posit Software, PBC)}

\end{minipage}%

\caption{\label{fig-Logos}Even the two logos are easy to confuse, but
remember that \texttt{R} and \emph{RStudio} are two very different
things!}

\end{figure}%

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Using other IDEs to work in \texttt{R}}, titlerule=0mm, leftrule=.75mm]

At the time of writing, \emph{RStudio} is the most widely used
Integrated Development Environment (IDE) to work in \texttt{R}. However,
it is worth noting that many other IDEs that can be used to access
\texttt{R}. These include:

\begin{itemize}
\tightlist
\item
  \href{https://jupyter.org/}{Jupyter notebook}
\item
  \href{https://code.visualstudio.com/}{Visual Studio Code}
\item
  \href{https://www.jetbrains.com/pycharm/}{PyCharm}
\item
  \href{https://eclipseide.org/}{Eclipse}
\end{itemize}

Whilst this textbook will assume that everyone is working in
\emph{RStudio}, if you are already familiar with another IDE that works
well with \texttt{R}, you are welcome to continue working in that IDE.
Each IDE has a different feel to it and offers different functions so,
ultimately, it'll be up to you to find the one that suits you best!

\end{tcolorbox}

\subsection{\texorpdfstring{Installing
\texttt{R}}{Installing R}}\label{sec-installingR}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Go to the website of the Comprehensive R Archive Network (CRAN):
  \url{https://cran.r-project.org}.
\item
  Click on the ``Download R for \ldots{}'' link that matches your
  operating system (Linux, macOS or Windows), then:

  \begin{itemize}
  \tightlist
  \item
    For \textbf{Windows}, click on the top `base' link, also marked as
    ``install R for the first time'' (Note that you should also use this
    link if you are updating your R version). On the next page, click on
    the top ``Download R'' link.
  \item
    For \textbf{MacOS}, click on either the top \texttt{.pkg} link if
    you have an Apple silicon Mac (e.g.~M1, M2, M3) or the second
    \texttt{.pkg} link, if you have an older Intel Mac.
  \item
    For \textbf{Linux}, click on your Linux distribution and then follow
    the instructions on the following pages.
  \end{itemize}
\item
  Once you have downloaded one of these \texttt{R} versions, navigate to
  the folder where you have saved it (by default, this will be your
  Downloads folder), and double click on the executable file to install
  \texttt{R}.
\item
  Follow the on-screen instructions to install \texttt{R}.
\item
  Test that \texttt{R} is correctly installed. On Windows and MacOS,
  navigate to your Applications folder and double click on the
  \texttt{R} icon. On Linux, open up \texttt{R} by typing \texttt{R} in
  your terminal. This should open up an R Console. You can type R
  commands into the Console after the command prompt
  \texttt{\textgreater{}}. Type the following R code after the command
  prompt and then press enter: \texttt{plot(1:10)}.
\end{enumerate}

\begin{figure}

\begin{minipage}{0.50\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/RConsole.png}}

}

\subcaption{\label{fig-Console}Test command in the R Console}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/TestingRPlot.png}}

}

\subcaption{\label{fig-testplot}Resulting plot (note that the
proportions of your plot may be different depending on the size of your
window)}

\end{minipage}%

\caption{\label{fig-Rtest}Testing \texttt{R}}

\end{figure}%

âœ… If you see the plot above, you have successfully installed and tested
\texttt{R} and you can go on to installing RStudio.

âš ï¸ If that's not the case, make a note of the errors produced (copy and
paste them into a text document or take a screenshot) and search for
solutions on the internet. It is very likely that many other people have
already encountered the same problem as you and that someone from the
\texttt{R} community has posted a solution online.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{What to do if you cannot get R and/or \emph{RStudio} working on your
computer}, titlerule=0mm, leftrule=.75mm]

The aim of this chapter is to install both \texttt{R} and \emph{R
Studio} on your own computer so that you can write and run your own
scripts locally (i.e.~on your own computer without the need for an
internet connection). In some cases, however, this might not be
possible. For example, because the programmes are not available for your
operating system, or because you do not have admin rights on your
computer, or because your disk is full and you cannot delete anything.
None of these situations are ideal to do research, but don't give up on
learning \texttt{R}: there is an alternative!

You can sign up to \href{https://posit.co/products/cloud/cloud/}{Posit
Cloud}. Posit Cloud will allow you to run \texttt{R} in \emph{RStudio}
in a browser (e.g.~Firefox or Chrome) without having to install anything
on your computer. Although Posit Cloud's
\href{https://posit.cloud/plans/free}{free plan} is limited, it will
suffice to learn the contents of this textbook. You will be able to
follow the textbook in exactly the same way as everyone else. However,
you will need a stable internet connection and you may find that you
need to be a bit more patient as things are likely to run a little
slower. If you decide to opt for the Posit Cloud solution, create a free
account and then go straight to Section~\ref{sec-SettingupR}.

\end{tcolorbox}

\subsection{\texorpdfstring{Installing
\emph{RStudio}}{Installing RStudio}}\label{sec-installingRStudio}

When you head over to their website, it may be confusing to you that the
company that provides \emph{RStudio}, Posit, also offers paid-for
versions of \emph{RStudio} and other paying services. Do not worry, we
will not need any of these: These are products designed for companies
and large organisations. The version of \emph{RStudio Desktop} that we
will be using, however, is free and, given that it is open source, even
if Posit decided to stop working on this product one day, others in the
\texttt{R} community would take over. Such is the beauty of
\href{https://elenlefoll.github.io/RstatsTextbook/OpenScholarship.html}{open-source
software}! ðŸ¤—

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Head over to this page
  \url{https://posit.co/download/rstudio-desktop/} to download the
  latest version of \emph{RStudio Desktop}.
\item
  As you have already installed \texttt{R}, you can jump straight to the
  section entitled ``2: Install RStudio''. The website should have
  detected which operating system your computer is running on, so that
  you can most likely simply click on the ``Download RStudio
  Desktop\ldots{}'' button. Your download should start straight away.

  \begin{itemize}
  \tightlist
  \item
    If an incorrect operating system is detected, simply scroll down the
    page to find your operating system and download the corresponding
    version of \emph{RStudio}.
  \end{itemize}
\item
  Once you have downloaded \emph{RStudio}, navigate to the folder where
  the downloaded file has been saved (by default, this will be your
  Downloads folder), and double-click on the executable file to install
  \emph{RStudio}.
\item
  Follow the on-screen instructions to install \emph{RStudio}.
\end{enumerate}

If you run into any issues that you cannot solve with existing online
posts, the \href{https://forum.posit.co/}{Posit Community forums} are a
good place to ask for help.

\section{\texorpdfstring{Setting up
\emph{RStudio}}{Setting up RStudio}}\label{sec-SettingupR}

From now on, we will only be accessing \texttt{R} through
\emph{RStudio}. When you open up \emph{RStudio} for the first time, you
might find the layout rather intimidating. The application window is
divided into several sections, which we call \textbf{panes}. Each pane
also has several \textbf{tabs}. Although it may seem overwhelming at
first, you will soon see that these different panes and tabs will
actually make life much easier.

\subsection{Global options}\label{sec-GlobalOptions}

Before we get staRted properly, we need to change some of the default
settings of \emph{RStudio}. The first set of changes that we are going
to make ensure that, each time we launch a new \texttt{R} session in
\emph{RStudio}, we start afresh.

To do so, head over to the `Tools' drop-down menu and click on `Global
Options'. Make sure that the first three boxes are unticked (see
Figure~\ref{fig-GOGeneral}). Under ``Save workspace to .RData on exit'',
select the option ``Never''. Always starting afresh is good programming
practice. It avoids any problems being carried over from previous
\texttt{R} sessions. You can think of it like cooking in a freshly
cleaned, tidy kitchen. It's much safer than preparing a meal in a messy,
possibly even contaminated kitchen!

\begin{figure}

\begin{minipage}{0.50\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/RStudioGlobalOptionsGeneral.png}}

}

\subcaption{\label{fig-GOGeneral}General tab}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/RStudioGlobalOptionsCode.png}}

}

\subcaption{\label{fig-GOCode}Code tab}

\end{minipage}%

\caption{\label{fig-GlobalOptions}\emph{RStudio}'s Global Options}

\end{figure}%

Next, under the `Global Options' tab `Code' of the `Global Options'
window, ensure that the fourth option ``Use native pipe operator'' is
ticked (see Figure~\ref{fig-GOCode}). This is a new feature in
\texttt{R} that is very useful so we will make use of it from
Section~\ref{sec-Nesting} onwards. The other options are not relevant
for now.

Finally, head over to the `Pane Layout' tab. From here, you can
rearrange the panes of your \emph{RStudio} window. To do so, click on
the ï¹€ symbols to get a drop-down menu corresponding to each pane. You
can also select which tabs you would like to see in each pane. If you
are already familiar with \emph{RStudio}, feel free to stick to your
favourite set-up. Personally, I use the panes layout below and, if you
are new to \texttt{R}, I recommend that you select this layout, too. You
can always go back to these settings to change this set-up at any stage.
Don't forget to click on `OK' at the bottom of the `Global Options' page
to save your settings. Then, the panes in your \emph{RStudio} window
should be ordered as in Figure~\ref{fig-RStudioNewLayout}.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/RStudioGlobalOptionsPanes.png}}

}

\subcaption{\label{fig-GOPanes}Panes Layout tab}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/RStudioNewLayout.png}}

}

\subcaption{\label{fig-RStudioNewLayout}Customised panes layout}

\end{minipage}%

\caption{\label{fig-Panes}Recommended \emph{RStudio} panes layout}

\end{figure}%

\subsection{Testing RStudio}\label{testing-rstudio}

It is now time we tested whether \emph{RStudio} is communicating well
with \texttt{R}. To do so, let's run the same test as in the
\texttt{R\ Console}. This time, head over to the \textbf{Console} tab in
the top right pane of your \emph{RStudio} window and, after the command
prompt \texttt{\textgreater{}}, type: \texttt{plot(1:10)} and then press
enter. You should see the same plot as earlier on (see
Figure~\ref{fig-testplot}), appearing in the \textbf{Plots} tab of the
bottom-right pane of your \emph{RStudio} window.

If you get the following error message
\texttt{Error\ in\ plot.new()\ :\ figure\ margins\ too\ large}, this is
because your bottom-right pane is hidden from view or too small for the
plot to be printed there. Click on the small two-window icon in the
bottom-right corner if it is hidden (see
Figure~\ref{fig-MinimisedPlotPane}). Or, if it is too small, click on
the dividing line between the two right-hand side panes and, whilst
still holding down the mouse button, drag up the line until it is about
halfway up. Then, re-type the command \texttt{plot(1:10)} in the Console
pane and press enter again. The plot should appear as in
Figure~\ref{fig-testplotRStudio}.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/PlotMarginsError.png}}

}

\subcaption{\label{fig-MinimisedPlotPane}Hidden (minimised) bottom-right
pane}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/TestplotRStudio.png}}

}

\subcaption{\label{fig-testplotRStudio}Now the dividing line between the
two panes is halfway up and the plot has been successfully output in the
Plots pane}

\end{minipage}%

\caption{\label{fig-testingRStudio}Testing that \emph{RStudio} is
communicating well with your \texttt{R} installation.}

\end{figure}%

\section{\texorpdfstring{Installing \texttt{R}
packages}{Installing R packages}}\label{installing-r-packages}

\subsection{What are packages?}\label{what-are-packages}

You now have a base installation of \texttt{R}. Base \texttt{R} is very
powerful and comes with many standard packages and functions that
\texttt{R} users use on a daily basis. If you click on the
\textbf{Packages} tab in the bottom-right pane and scroll down, you will
see that there are many packages available. Only a few are selected.
These are part of the base \texttt{R} installation.

You can think of base \texttt{R} as a fully functional student kitchen.
It is rather small and only has the most essential ingredients and
equipment, but it still has everything you need to cook simple,
delicious meals. Downloading and installing additional packages is like
buying more sophisticated and specialised kitchen devices (i.e.~packages
that provide additional functions) or fancier ingredients (i.e.~packages
that provide access to specific datasets).

In addition to the members of the R Core Team who develop and maintain
base \texttt{R}, thousands of \texttt{R} users develop and share
additional \texttt{R} packages every day. These enable us to vastly
increase the capacities of base \texttt{R}. Packages are a very helpful
way to bundle together a set of \textbf{functions}, \textbf{data}, and
\textbf{documentation files} so that other \texttt{R} users can easily
download these bundles and add them to their local \texttt{R}
installation.

Throughout this textbook, the names of packages will be enclosed in
curly brackets like this: \{ggplot2\}.

\subsection{Installing packages}\label{sec-InstallPackages}

To install a package, you will first need to download it from the
internet. Packages can be stored on any website, but the most
trustworthy online repository for \texttt{R} packages and the easiest to
work with is \href{https://cran.rstudio.com/index.html}{CRAN}
(Comprehensive R Archive Network). To install the \{janeaustenr\}
package from CRAN, type the following command in the Console pane and
then type enter:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"janeaustenr"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This command will take a few seconds to run (or longer depending on how
slow your internet connection is). You should then see a message in red
in the Console indicating, among other things that you can ignore, that
the package has been successfully downloaded and how big it is (here:
1.5 megabyte). The message also gives you path to where the package's
content has been saved on your computer (see
Figure~\ref{fig-PckInstalled}). You do not need to worry about any of
the other information.

\begin{figure}

\centering{

\includegraphics[width=4.6875in,height=\textheight,keepaspectratio]{images/JaneaustenrInstalled.png}

}

\caption{\label{fig-PckInstalled}Screenshot showing that the package has
been correctly installed.}

\end{figure}%

To check that the package has been successfully downloaded and
installed, head over to the Packages tab of the bottom-right pane and
scroll down to the \{janeaustenr\} package, or search for it using the
search window within this same tab. The \{janeaustenr\} package should
now be visible, which tells us that the package is installed on your
computer. Note, however, that the checkbox next to it is currently
empty. This means that the package hasn't been loaded in our current
\texttt{R} session and therefore cannot be used yet.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Solving package installation problems â€¼ï¸}, titlerule=0mm, leftrule=.75mm]

When you install or update an \texttt{R} package, you will often
encounter a message that informs you that so-called package
\textbf{dependencies}, i.e.~other packages that are required for the
new/updated package to function, also need updating. You will be asked
if you want to update these packages as part of the installation/update
process. I recommend that you update them ``All'' or at least ``CRAN
packages only''. To do so, enter either \texttt{1} or \texttt{2} in the
Console (as specified in the instructions displayed in the Console) and
press enter.

\begin{verbatim}
These packages have more recent versions available.
It is recommended to update all of them.
Which would you like to update?

1: All                            
2: CRAN packages only             
3: None                           
4: stringi (1.7.2 -> 1.8.7) [CRAN]
\end{verbatim}

RStudio may be prompt you to restart your \texttt{R} session for the
update process to be completed. RStudio should recover your session work
on launching the new session.

In some cases, you will find that the most recent version of a package
needs to be installed from source. Installing binary versions is faster
than compiling from source and it is less error-prone for beginners so,
if you get asked
\texttt{Do\ you\ want\ to\ install\ from\ sources\ the\ packages\ which\ need\ compilation?\ (Yes/no/cancel)}
and you have no specific need to install the latest source code version,
selecting ``no'' is fine.

If you get an ``unable to access index for repository'' message when
trying to install a package, this is probably because the default CRAN
mirror for your location is unavailable. This is likely to be a
temporary connection issue. In the meantime, you can enter the following
command to choose a different mirror (preferably one close to your own
location) and try the installation again:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{chooseCRANmirror}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

In some cases, \texttt{R} may be unable to install a package and the
following message will appear in your Console:

\begin{verbatim}
Warning message:
package â€˜PackageNameâ€™ is not available for this version of R
\end{verbatim}

First, check that you haven't made a typo in the package name, bearing
in mind that \texttt{R} is a case-sensitive programming language so
that, for example, attempting to install the \{JaneAustenr\} package
will fail because the package is spelt \{janeaustenr\}.

If the package name is correctly spelt, this error probably means that
the default package version on CRAN is not compatible with your current
\texttt{R} version or that the package is not available on CRAN. Check
which version of \texttt{R} you are currently running by entering
\texttt{R.version} in your Console. Compare this version number with the
required \texttt{R} version indicated on the package's info page on
CRAN. For example, Figure~\ref{fig-CRANJane} informs us that the
\{janeaustern\} package ``depends'' on a \texttt{R} version that is 3.5
or higher. All CRAN packages are listed on:
\url{https://cran.r-project.org/web/packages/available_packages_by_name.html}.

\begin{figure}[H]

\centering{

\href{https://cran.r-project.org/web/packages/janeaustenr/index.html}{\pandocbounded{\includegraphics[keepaspectratio]{images/CRAN_janeaustenr.png}}}

}

\caption{\label{fig-CRANJane}Screenshot of the top part of
\textless https://cran.r-project.org/web/packages/janeaustenr/index.html\textgreater{}
(accessed on 17 December 2025)}

\end{figure}%

If the package requires a version of \texttt{R} that is more recent than
you what you currently have, you will need to first update \texttt{R}
(see Section~\ref{sec-UpdatingR}) before you can install this package.
If you cannot update \texttt{R} or if the package requires an older
version of \texttt{R} than you currently have, you can use the
\{remotes\} package (which you will first need to install) to install a
specific package version that is compatible with your \texttt{R}
version.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"remotes"}\NormalTok{)}
\FunctionTok{library}\NormalTok{(remotes)}

\FunctionTok{install\_version}\NormalTok{(}\StringTok{"PackageName"}\NormalTok{, }\StringTok{"VersionNumber"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

You can find older package versions on the CRAN archive at
\url{https://cran.r-project.org/src/contrib/Archive/}.

Some packages or package versions are not available on CRAN. Alternative
repositories for \texttt{R} packages include Bioconductor, GitHub, and
GitLab. The latter two are also where you can typically find packages
and package versions that are currently under development. To install
packages from these repositories, read
\href{https://intro2r.com/packages.html}{Section 1.5} in Douglas et al.
(2024).

\end{tcolorbox}

\subsection{Loading packages}\label{loading-packages}

If you want to use the new kitchen device or fancy ingredient that was
delivered in the package that you installed, you first need to get it
out of the fridge or the cupboard and place it on your kitchen counter.
This is the equivalent of \textbf{loading} a package. The command to
load a package is \texttt{library()}. This is because, once they are
unpacked (i.e.~installed), packages are stored in a directory on your
computer and this directory is referred to as a ``library''.

To load the \{janeaustenr\} package, enter the following command in the
Console:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(janeaustenr)}
\end{Highlighting}
\end{Shaded}

If you correctly installed the package and have not misspelt the
command, it may look like nothing has happened, as the Console returns
nothing (see first red ellipse on Figure~\ref{fig-LoadingJane}).
However, if you go back to your Packages tab and scroll down to the
\{janeaustenr\} package, you will see that the box next to it is now
ticked (see second ellipse on Figure~\ref{fig-LoadingJane}). This means
that the package is loaded and ready to be used.

\begin{figure}

\centering{

\includegraphics[width=4.20833in,height=\textheight,keepaspectratio]{images/LoadingJaneAustern.png}

}

\caption{\label{fig-LoadingJane}Loading a library}

\end{figure}%

Note that whilst you only need to install each package once, you will
need to load it every time we want to use it in a new \texttt{R}
session. This is because (if you set RStudio up as explained in
Section~\ref{sec-GlobalOptions}), when we close \texttt{R} and start a
new \texttt{R} session, our kitchen is perfectly clean and tidy.
Everything is back in storage and the good news is that we didn't even
need to do the washing-up! ðŸ™ƒ

\subsection{Package documentation}\label{sec-PackageDocumentation}

To find out more about any package or function, use the handy
\texttt{help()} function or its shortcut \texttt{?}. For example, to
find out more about the \{janeaustenr\} package, enter the command
\texttt{help(janeaustenr)} or \texttt{?janeaustenr} in the Console. The
help file will open up in the \textbf{Help} tab of the bottom-right pane
(see Figure~\ref{fig-RStudioNewLayout}). It contains the name of the
package and a short description, as well as the name of the package
maintainer, Julia Silge, and some additional links.

One of these links takes us to the package creator's GitHub repository.
This is where we can find a source code for the package, should we want
to check how it works under the hood, or amend it in any way. Click on
this link and scroll down the package's GitHub page to consult its
\textbf{README file}. This document informs us that the package includes
plain text versions of Jane Austen's six completed, published novels and
tells us under what name they are stored within the library. For
example, to access \emph{Pride and Prejudice,} we need to load the
library object \texttt{prideprejudice}.

Pick your favourite Jane Austen novel and enter its corresponding object
name in the Console, e.g.~\texttt{emma}. The entire novel will be
printed in the Console output! You can print only a few lines by
selecting them within square brackets. For example, the command
\texttt{emma{[}20:25{]}} will print lines 20 to 25 of the object
\texttt{emma} (see Figure~\ref{fig-emma2025}).

\begin{figure}

\centering{

\includegraphics[width=4.16667in,height=\textheight,keepaspectratio]{images/Emma2025.png}

}

\caption{\label{fig-emma2025}Screenshot showing a selection of lines
from the object \texttt{emma} (note that you can adjust the size of the
Console pane to see more or less of the text at any one time).}

\end{figure}%

To find out more about a dataset or function within a package, use the
functions \texttt{help()} or \texttt{?}, e.g.~\texttt{help(emma)} or
\texttt{?emma}. In this case, the help file provides us with a short
description of this object and a link to the original source from which
the package creator obtained the novel (which is in the
\href{https://www.gutenberg.org/help/faq.html\#what-books-does-project-gutenberg-publish}{public
domain}, otherwise it would not be possible to share it in this way).

\subsection{\texorpdfstring{Citing \texttt{R}
packages}{Citing R packages}}\label{sec-CitingPackages}

When we use a package that is not part of base \texttt{R}, it is very
important to \textbf{reference} the package properly. There are two main
reasons for doing this. For a start, the people who create and maintain
these packages largely do so in their free time and they deserve full
\textbf{credit} for their incredibly valuable work and contribution to
science. Hence, whenever you use a package for your research, you should
cite it, just like you would other sources.

The help page of the \{janeaustenr\} package already informed us that
the maintainer of the package is Julia Silge. To get a full citation,
however, we should use the \texttt{citation()} function. Enter
\texttt{citation("janeaustenr")} in the Console to find out how to cite
this package.

Note that the recommended bibliographic reference also includes the
package version, which is important for reproducibility as the package
may evolve and someone wanting to reproduce your analysis (and this may
well be future you!) will need to know which version you used. This is
the second main reason why we should be diligent about citing the exact
packages that we used to ensure that others can \textbf{reproduce} our
analyses (sec-Reproducibility). In a research report, thesis, or
academic article, you could cite the \{janeaustenr\} package like this:

\begin{quote}
We used the \emph{janeaustenr} package (Silge 2022) to access Jane
Austen's six published novels in R (R Core Team 2024).
\end{quote}

Note that you can see the full references by hovering on the in-text
citation links or by going to the
\href{https://elenlefoll.github.io/RstatsTextbook/99_references.html}{References}
section of this book. Chapter~\ref{sec-LiteRateProgramming} explains how
to insert bibliographic references in documents that include \texttt{R}
code.

\section{Keeping things up to date}\label{sec-Updates}

As with all software, it is a good idea to keep your installations of
\emph{RStudio} and \texttt{R} up-to-date. New features are constantly
being added, bugs are fixed, and updates may include important security
patches.

\subsection{Updating RStudio}\label{updating-rstudio}

By default, \emph{RStudio} will let you know when a new version is
available in a pop-up window. To update \emph{RStudio} follow the same
instructions as for the first installation (see
Section~\ref{sec-installingRStudio}). When you add \emph{RStudio} to
your apps, you will get a message warning you that an older version of
this programme already exists on your computer (see
Figure~\ref{fig-UpdateRStudio}). You can safely click on the option
``Replace''. All of your previous \textbf{Global Options} settings
(@sec-SettingupR) will be transferred to your updated \emph{RStudio}
version so this should be a quick-and-easy process.

\begin{figure}

\centering{

\includegraphics[width=4.6875in,height=\textheight,keepaspectratio]{images/UpdateRStudio.png}

}

\caption{\label{fig-UpdateRStudio}Warning message on MacOS when
installing an updated version of RStudio}

\end{figure}%

You can also check which version of \emph{RStudio} you are running by
clicking on the ``Help'' menu in \emph{RStudio}'s top toolbar and then
selecting the option ``About RStudio''. In the ``Help'' drop-down menu,
you also have an option to ``Check for Updates''.

\subsection{\texorpdfstring{Updating
\texttt{R}}{Updating R}}\label{sec-UpdatingR}

Updating \texttt{R} is a little more complex because you will also need
to update all of your \texttt{R} packages, too. Some of the packages
that you use may not (yet) be available for the latest \texttt{R}
version. This is why, for beginners, I do not recommend updating
\texttt{R} in the middle of a project. That said, it is a good idea to
keep your \texttt{R} version up-to-date. To find out which version of
\texttt{R} you are currently working with, run this command in the
Console.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{R.version.string}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "R version 4.5.2 (2025-10-31)"
\end{verbatim}

Compare this version number with the number of the latest version
available on \href{https://cran.r-project.org/}{CRAN} (see
Figure~\ref{fig-CRANHomapageRVersion}). If the version that you are
running is not the same as the latest \texttt{R} version available on
CRAN, you might want to update it. As a rule of thumb, it is a good idea
to do an update if your version is more than six months old. To proceed
with the update, close \emph{RStudio} on your computer. Then, follow the
same instructions as for the first-time installation of \texttt{R} (see
Section~\ref{sec-installingR}).

\begin{figure}

\centering{

\includegraphics[width=5.29167in,height=\textheight,keepaspectratio]{images/CRAN_R_For_MacOS_Version.png}

}

\caption{\label{fig-CRANHomapageRVersion}CRAN \texttt{R} for macOS page
using the latest recommended \texttt{R} version}

\end{figure}%

\subsection{\texorpdfstring{Updating \texttt{R}
packages}{Updating R packages}}\label{updating-r-packages}

Once you have updated \texttt{R}, it is important that you also update
your installed packages. To do so, run the following command in the
Console:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{update.packages}\NormalTok{(}\AttributeTok{ask =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{checkBuilt =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Alternatively, you can also go to the \textbf{Packages} tab of
\emph{RStudio} and click on the button ``Update''. A pop-up window will
appear with a list of the packages that need updating. Click on ``Select
All'' and ``Install Updates''.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics[width=4.6875in,height=\textheight,keepaspectratio]{images/R_Studio_Packages_Update.png}

}

\subcaption{\label{fig-RStudioPackagesTabUpdateButtoon}`Update' button
in RStudio's Packages tab}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics[width=4.6875in,height=\textheight,keepaspectratio]{images/Update_Packages_Dialog.png}

}

\subcaption{\label{fig-UpdatePackagesDialog}`Update Packages' dialogue
in RStudio}

\end{minipage}%

\caption{\label{fig-UpdateGUI}Updating packages using RStudio's
Graphical User Interface (GUI)}

\end{figure}%

Note that, if you have installed a lot of packages, this updating
operation could take a while. It requires a stable internet connection
and a bit of patience. ðŸ§˜ðŸ¾

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{An easier way to update \texttt{R} using \{installr\} (for Windows only)}, titlerule=0mm, leftrule=.75mm]

The \{installr\} package simplifies updating \texttt{R} on Windows. To
install the package use the usual commands:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"installr"}\NormalTok{) }\CommentTok{\# Run this command the first time you use the package.}

\FunctionTok{library}\NormalTok{(installr) }\CommentTok{\# Run this command everytime you want to update R using this package.}
\end{Highlighting}
\end{Shaded}

Then, run the \texttt{updateR()} function, which automates the updating
process by detecting your current \texttt{R} version, comparing it with
the latest available version, and guiding you through the process of
downloading and installing the latest version.

It is also possible to customise the update process with arguments like
\texttt{updateR(update\_packages\ =\ FALSE)} to skip package updates.
For more details, check the documentation using the command
\texttt{?updateR}.

\end{tcolorbox}

\subsection*{Check your progress ðŸŒŸ}\label{check-your-progress-3}
\addcontentsline{toc}{subsection}{Check your progress ðŸŒŸ}

You have successfully completed { out of 6 questions} in this chapter.

Are you confident that you can\ldots?

\begin{itemize}
\tightlist
\item[$\square$]
  Install \texttt{R} and \emph{RStudio} (Chapter~\ref{sec-InstallingR})
\item[$\square$]
  Set up and test \emph{RStudio} (Section~\ref{sec-SettingupR})
\item[$\square$]
  Install and load \texttt{R} packages
  (Section~\ref{sec-InstallPackages})
\item[$\square$]
  Find out more about \texttt{R} packages and functions
  (Section~\ref{sec-PackageDocumentation})
\item[$\square$]
  Cite \texttt{R} packages (Section~\ref{sec-CitingPackages})
\item[$\square$]
  Update \emph{RStudio}, \texttt{R}, and \texttt{R} packages
  (Section~\ref{sec-UpdatingR})
\end{itemize}

Once you have successfully completed all the steps outlined in this
chapter, you are ready to get sta\texttt{R}ted with
\href{https://elenlefoll.github.io/RstatsTextbook/5_GettingStaRted.html}{Chapter
5}, which provides a hands-on introduction to \texttt{R} in
\emph{RStudio}. ðŸ¤“

\bookmarksetup{startatroot}

\chapter{\texorpdfstring{Getting sta\texttt{R}ted in
\texttt{R}}{Getting staRted in R}}\label{sec-GettingStaRted}

\subsubsection*{\texorpdfstring{\textbf{Chapter
overview}}{Chapter overview}}\label{chapter-overview-4}
\addcontentsline{toc}{subsubsection}{\textbf{Chapter overview}}

Now that you have installed and tested \texttt{R} and RStudio, in this
chapter, you will learn how to:

\begin{itemize}
\tightlist
\item
  Use the \texttt{R} Console.
\item
  Do basic mathematical operations in \texttt{R}.
\item
  Create and use \texttt{R} objects.
\item
  Write and save \texttt{.R} scripts.
\item
  Add comments to your scripts.
\item
  Keep your cool when errors pop up! ðŸ˜Ž
\end{itemize}

If you are already familiar with the basics of \texttt{R} and are keen
to learn more about doing statistics in \texttt{R}, you can skip most of
this chapter. That said, it's probably not a bad idea to have a go at
the quiz questions and the final task to refresh your memory.

\section{Using the Console}\label{sec-Console}

One way to write \texttt{R} code in \emph{RStudio} is to use the
Console. If you set up \emph{RStudio} as recommended
\href{https://elenlefoll.github.io/RstatsTextbook/InstallingR.html\#global-options}{here},
the Console should be in your top-right pane. You can type a line of
code immediately after the command prompt \texttt{\textgreater{}} and
press ``Enter''.

Data input is the most basic operation in \texttt{R}. Try inputting a
number by typing it out in the Console and then pressing ``Enter''.
\texttt{R} will interpret the number and return it. You can input both
integers (whole numbers, e.g.~\texttt{13}) and decimal numbers
(e.g.~\texttt{0.5}).

\begin{figure}

\centering{

\includegraphics[width=4.6875in,height=\textheight,keepaspectratio]{images/ConsoleInputNumbers.png}

}

\caption{\label{fig-ConsoleInputNumbers}Inputting numbers in the
Console}

\end{figure}%

\texttt{R} can handle not only numbers but also text data, known as
``character strings'' or just ``strings''. Strings must always be
enclosed in quotation marks. You can choose to use either double
quotation marks \texttt{"\ "} or single quotation marks
\texttt{\textquotesingle{}\ \textquotesingle{}}, but it is important to
be consistent. In this textbook, we will use double quotation marks
throughout.

Try first inputting a single word and then an entire sentence in the
Console as in Figure~\ref{fig-ConsoleInputStrings}.

\begin{figure}

\centering{

\includegraphics[width=4.6875in,height=\textheight,keepaspectratio]{images/ConsoleInputStrings.png}

}

\caption{\label{fig-ConsoleInputStrings}Inputting strings in the
Console}

\end{figure}%

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-warning-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-warning-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Warning}, titlerule=0mm, leftrule=.75mm]

Word processors such as Microsoft Word, Pages, and LibreOffice Writer
should \emph{not} be used to write, edit, or share \texttt{R} scripts.
Among other problems, such programmes frequently enable an automatic
`smart quotes' feature, which silently replaces straight quotation marks
(\texttt{"}) with curly ones (`` and ''). These correspond to different
Unicode characters, which \texttt{R} does not recognise as valid string
delimiters. Hence, curly quotes result in broken code that returns
errors, e.g.:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{text }\OtherTok{\textless{}{-}}\NormalTok{ â€œThis line of code was edited }\ControlFlowTok{in}\NormalTok{ Word.â€}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Error: unexpected input in "text <- â€œ"
\end{verbatim}

Word processors are also known to introduce problematic line breaks and
cause havoc with automatic capitalisation. To ensure reliability and
portability, programming scripts therefore should always be written and
edited in plain-text editors such as RStudio, textEdit, or Notepad++.
They should be saved and shared in plain-text formats such as
\texttt{.R} or \texttt{.txt} (see Section~\ref{sec-DSV} and
Section~\ref{sec-SavingRScripts}). This also applies to any notes that
you take from this textbook or any programming class that you attend.

\end{tcolorbox}

\section{\texorpdfstring{Doing maths in
\texttt{R}}{Doing maths in R}}\label{sec-Maths}

\texttt{R} can also be used as a very powerful calculator. The lines of
code in Figure~\ref{fig-ConsoleInputMathematicalOperations} demonstrate
mathematical operations involving addition (\texttt{+}), subtraction
(\texttt{-}), division (\texttt{/}), and multiplication (\texttt{*}).
Try out a few yourself!

\begin{figure}

\centering{

\includegraphics[width=4.6875in,height=\textheight,keepaspectratio]{images/ConsoleInputMathematicalOperations.png}

}

\caption{\label{fig-ConsoleInputMathematicalOperations}Using the
\texttt{R} Console as a calculator}

\end{figure}%

\section{\texorpdfstring{Working with \texttt{R}
objects}{Working with R objects}}\label{sec-WorkingRObjects}

So far, we have used the Console like a calculator. It's important to
understand that, just like with a standard calculator, the output of all
of our operations was not saved anywhere. If we want to store values,
sequences of values, and the results of computations for later use,
\texttt{R} allows us to store these as ``\texttt{R} objects''.

\subsection{Creating objects}\label{creating-objects}

We use the assignment operator (\texttt{\textless{}-}) to assign a value
or sequence of values to an object name.

Write out the following line to create an object called
\texttt{my.favourite.number} that contains your own favourite number.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my.favourite.number }\OtherTok{\textless{}{-}} \DecValTok{13}
\end{Highlighting}
\end{Shaded}

When you enter this line in the Console and press ``Enter'', it should
look like nothing happened: \texttt{R} does not return anything in the
Console. Instead, it saves the output in an object called
\texttt{my.favourite.number}. However, if you look in your Environment
pane, you should see that an object has appeared
(Figure~\ref{fig-ObjectCreation}).

\begin{figure}

\centering{

\includegraphics[width=5.72917in,height=\textheight,keepaspectratio]{images/ObjectCreation.png}

}

\caption{\label{fig-ObjectCreation}Created object in the Environment
pane}

\end{figure}%

To save an object containing a character string, we use quotation marks.
Create an object called \texttt{my.favourite.word} containing your
favourite word (in any written language of your choice).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my.favourite.word }\OtherTok{\textless{}{-}} \StringTok{"empathy"}
\end{Highlighting}
\end{Shaded}

Your Environment pane should now contain two objects. You can print the
content of a stored object by entering the object name in the Console
and then pressing ``Enter'' (see Figure~\ref{fig-ShowObjects}).

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-tip-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, titlerule=0mm, leftrule=.75mm]

If you're feeling lazy or simply want to avoid making a typo, you can
type just the first few letters of an object name and then press the
``Tab'' key (\texttt{â†¹} or \texttt{â‡¥}). \emph{RStudio} will then give
you a drop-down menu with possible options. Select the one you want by
clicking on it or pressing ``Enter''.

\end{tcolorbox}

\begin{figure}

\centering{

\includegraphics[width=4.6875in,height=\textheight,keepaspectratio]{images/ShowObjects.png}

}

\caption{\label{fig-ShowObjects}Calling up stored objects in the Console
to view their content}

\end{figure}%

\subsection{Object types}\label{object-types}

These two objects are of different types. We can use the
\texttt{class()} function to find out which type of object an object is.

\begin{figure}

\centering{

\includegraphics[width=4.6875in,height=\textheight,keepaspectratio]{images/ShowObjectClass.png}

}

\caption{\label{fig-ShowObjectClass}Using the \texttt{class()} function}

\end{figure}%

Here, \texttt{my.favourite.number} is a numeric object, while
\texttt{my.favourite.word} is a character object.

\subsection{Naming objects}\label{sec-NamingObjects}

Object naming conventions in \texttt{R} are fairly flexible. We can use
dots (\texttt{.}), underscores (\texttt{\_}) and capital letters to make
our object names maximally informative and easy for us humans to read.
However, spaces and other symbols are not allowed. All of these options
work:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{word2 }\OtherTok{\textless{}{-}} \StringTok{"cheerful"}
\NormalTok{my.second.word }\OtherTok{\textless{}{-}} \StringTok{"cheerful"}
\NormalTok{my\_second\_word }\OtherTok{\textless{}{-}} \StringTok{"cheerful"}
\NormalTok{MySecondWord }\OtherTok{\textless{}{-}} \StringTok{"cheerful"}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\centering{

\includegraphics[width=5.72917in,height=\textheight,keepaspectratio]{images/MultipleObjectsEnvironmentPane.png}

}

\caption{\label{fig-MultipleObjectsEnvironmentPane}Environment pane
showing all of the objects currently stored in the \texttt{R} session
environment}

\end{figure}%

Object names should not contain spaces or symbols like \texttt{!}, nor
should they contain hyphens as the hyphen is reserved for the
mathematical operator ``minus''. Digits can be used anywhere except at
the beginning of an object name. And whilst it is possible to have
special characters such as accented letters like ``Ã¨'', it is not
recommended that you use them for object names.

\subsection{Overwriting and deleting
objects}\label{overwriting-and-deleting-objects}

Object names are unique. If you create a new object with an existing
object name, it will overwrite the existing object with the new one. In
other words, you will lose the values that you saved in the original
object. Try it out by running this line and observing what happens in
your Environment pane:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{word2 }\OtherTok{\textless{}{-}} \StringTok{"surprised"}
\end{Highlighting}
\end{Shaded}

Earlier on, you created an object called \texttt{word2} which contained
the string ``cheerful''. But, by running this new line of code,
``cheerful'' has been replaced by the string ``surprised'' - with no
warning that you were about to permanently delete ``cheerful''! ðŸ˜²

The command to delete a single object from your environment is
\texttt{remove()} or \texttt{rm()}. Hence, to permanently delete the
object \texttt{MySecondWord}, you can use either of these commands:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{remove}\NormalTok{(MySecondWord)}
\FunctionTok{rm}\NormalTok{(MySecondWord)}
\end{Highlighting}
\end{Shaded}

\section{\texorpdfstring{Working with \texttt{.R}
scripts}{Working with .R scripts}}\label{sec-RScirpt}

If we shut down \emph{RStudio} right now, we will lose all of our work
so far. This is because the objects that we have created are only saved
in the environment of our current \texttt{R} session. Whilst this might
sound reckless, it is actually a good thing: In
Section~\ref{sec-GlobalOptions} we set our `Global Options' settings in
\emph{RStudio} such that, whenever we restart RStudio, we begin with a
clean slate, or a perfectly clean and tidy kitchen. We don't want any
dirty dishes or stale ingredients lying around when we enter the
kitchen! With this in mind, close \emph{RStudio} now and open it again
to start a new \texttt{R} session.

You should now have an empty history in your Console pane and an empty
Environment pane. Whilst nobody wants to start cooking in a messy
kitchen, it's also true that, if we want to remember what we did in a
previous cooking/baking session, we should write it down. The pages of
our recipe book are \texttt{.R} scripts. In the following, we will see
that writing scripts is much better than running everything from the
Console. It allows us to save and rerun our entire analysis pipeline any
time we want. It also ensures that our analyses are reproducible and
saves us time as we don't have to rewrite our code every time.
Crucially, if we made a mistake at any stage, we can go back and correct
it and rerun the entire corrected script at the click of a button.

\subsection{\texorpdfstring{Creating a new \texttt{.R}
script}{Creating a new .R script}}\label{creating-a-new-.r-script}

There are three ways to create a new \texttt{.R} script in RStudio. Pick
the one that you like best:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Navigate to the top menu item ``File'', then select ``New File'', then
  click on ``R Script''.
\item
  Click on the icon with a white page and a green plus button in the top
  left corner of the tool bar.
\item
  Use the keyboard shortcut \texttt{Shift\ +\ Ctrl/Cmd\ +\ N}.
\end{enumerate}

Whichever option you chose, \emph{RStudio} should have opened an empty
file in a fourth pane (see Figure~\ref{fig-NewScript}). This is the
``Source pane'' and it should have appeared in the top-left corner of
your \emph{RStudio} window.

\begin{figure}

\centering{

\includegraphics[width=5.80208in,height=\textheight,keepaspectratio]{images/RStudioNewRScript.png}

}

\caption{\label{fig-NewScript}\emph{RStudio} window showing a new, empty
\texttt{.R} script that has yet to be saved}

\end{figure}%

\subsection{\texorpdfstring{Running code from an \texttt{.R}
script}{Running code from an .R script}}\label{running-code-from-an-.r-script}

We can now type our code in this empty \texttt{.R} script in the Source
pane, just like we did in the Console. Type the following lines of code
in the script (see Figure~\ref{fig-NewScript2}):

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{13}\SpecialCharTok{*}\DecValTok{13}
\NormalTok{my.favourite.number }\OtherTok{\textless{}{-}} \DecValTok{13}
\NormalTok{my.favourite.word }\OtherTok{\textless{}{-}} \StringTok{"empathy"}
\end{Highlighting}
\end{Shaded}

You will have noticed that when you pressed ``Enter'' after every line,
nothing happened: Nowhere can we see the result of \texttt{13*13}, nor
have our two objects been saved to the environment as the Environment
pane remains empty (see Figure~\ref{fig-NewScript2}). Just like a recipe
for a cake is not an actual, delicious cake, but simply a set of
instructions, a script is only a text file that contains lines of code
as instructions. For these instructions to be executed, we need to send
them to the \texttt{R} Console where they will be interpreted as
\texttt{R} code.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/NewScript.png}}

}

\caption{\label{fig-NewScript2}Writing code in an \texttt{.R} script}

\end{figure}%

To send a line of code to the Console (also referred to as ``executing''
or ``running'' code), select the line that you want to execute, or place
your mouse cursor anywhere within that line and then click on the
``Run'' button (in the top-right corner of the pane, see
Figure~\ref{fig-NewScript}) or use the keyboard shortcut
\texttt{Ctrl/Cmd\ +\ Enter}. This shortcut is worth learning, as it will
save you a lot of time and effort in the long term.

Try out these two options to run the three lines of code of your script,
then check that:

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  you are seeing the result of the mathematical operation in the Console
  output and
\item
  two objects have been added to your environment.
\end{enumerate}

You can also select several lines of code and run them all with a single
click or shortcut. \texttt{R} will interpret the lines in the order that
they are listed in your script.

\subsection{\texorpdfstring{Saving an \texttt{.R}
script}{Saving an .R script}}\label{sec-SavingRScripts}

It is now very easy to rerun this script any time we want to redo this
calculation and recreate these two \texttt{R} objects. However, our
\texttt{.R} script is not yet saved! \emph{RStudio} is warning us about
this by highlighting the file name ``Untitled1*'' in red (see
Figure~\ref{fig-NewScript}). Just like with any unsaved computer file,
if we were to shut \emph{RStudio} down now, we would lose our work. So,
let us save this \texttt{.R} script locally, that is on our own
computer. To do so either:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Navigate to the top menu item ``File'' and then click on ``Save'',
\item
  Click on the save icon ðŸ’¾, or
\item
  Use the keyboard shortcut \texttt{Ctrl/Cmd+\ S}.
\end{enumerate}

Give your script a meaningful file name. Remember that file names should
be both computer-readable and human-readable. If you navigate to the
folder where you saved your \texttt{.R} script, you should see that its
file extension is \texttt{.R}. You should also see that it is a tiny
file because it contains nothing more than a few lines of text. If you
double click on an \texttt{.R} file, \emph{RStudio} should automatically
open it. However, if you wanted, you could open \texttt{.R} files with
any text-processing software, such as LibreOffice Writer or Microsoft
Word.

\subsection{Writing comments in scripts}\label{sec-Comments}

Just like in a recipe book, in addition to writing the actual
instructions, we can also write some notes, for example to remind
ourselves of why we did things in a particular way or for what occasion
we created a special dish. In programming, notes are called ``comments''
and they are typically preceded by the \texttt{\#} symbol.

Thus, if a line starts with a \texttt{\#} symbol, we say that it is
``commented out''. \emph{RStudio} helpfully displays lines that are
commented out in a different colour. These lines will not be interpreted
as code even if you send them to the Console. Write the following lines
in your script and then try to run them.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#13\^{}13}

\CommentTok{\#StringObject3 \textless{}{-} "This line has been commented out so the object will not be saved in the environment even if you try to run it."}
\end{Highlighting}
\end{Shaded}

As you can see, nothing happens. You can also add comments next to a
line of interpretable code. In this case, the code is interpreted up
until the \texttt{\#}. This can be helpful to make a note of what a line
of code does, e.g.:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sqrt}\NormalTok{(}\DecValTok{169}\NormalTok{) }\CommentTok{\# Here the sqrt() function will compute the square root of 169.}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 13
\end{verbatim}

It is good practice to comment your code when working in an \texttt{.R}
script. Comments are crucial for other people to understand what your
code does and how it achieves that. But even if you are confident that
you are the only person who will ever use your code, it is still a very
good idea to use comments to make notes documenting your intentions and
your reasoning as you write your script.

Finally, writing comments in your code as you work through the examples
in this book is a great way to reinforce what you are learning. From
this chapter onwards, I recommend that, for each chapter, you create an
\texttt{.R} script documenting what you have learnt, adding lots of
comments to help you remember how things work. This is generally more
efficient (and less error-prone!) than trying to take notes in a
separate document (e.g.~in a Microsoft Word file) or on paper.

\section{Using relational operators}\label{sec-RelationalOperators}

Now that we have saved some objects in our environment, we can use them
in calculations. Try out the following operations (and any other that
take your fancy) with your own favourite number:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my.favourite.number }\SpecialCharTok{/} \DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 6.5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my.favourite.number }\SpecialCharTok{*}\NormalTok{ my.favourite.number}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 169
\end{verbatim}

In additional to the mathematical operations that we saw in
Section~\ref{sec-Maths}, we can also use relational operators such
\texttt{\textgreater{}}, \texttt{\textless{}}, \texttt{\textless{}=},
\texttt{\textgreater{}=}, \texttt{==} and \texttt{!=} to make all kinds
of comparisons. Try out the following commands to understand how these
relational operators work and then have a go at the quiz questions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my.favourite.number }\SpecialCharTok{\textgreater{}} \DecValTok{10}
\NormalTok{my.favourite.number }\SpecialCharTok{\textless{}} \DecValTok{10}
\NormalTok{my.favourite.number }\SpecialCharTok{==} \DecValTok{25}
\NormalTok{my.favourite.number }\SpecialCharTok{\textgreater{}=} \DecValTok{13}
\NormalTok{my.favourite.number }\SpecialCharTok{\textless{}=} \SpecialCharTok{{-}}\DecValTok{13}
\NormalTok{my.favourite.number }\SpecialCharTok{!=} \DecValTok{25}
\end{Highlighting}
\end{Shaded}

The relational operators \texttt{==} and \texttt{!=} can also be used
with character objects. Find out how they work by first creating a new
character object with a word that was added to the 2025 edition of the
popular French dictionary \emph{Petit Larousse}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{New.French.Word }\OtherTok{\textless{}{-}} \StringTok{"Ã©cogeste"}
\end{Highlighting}
\end{Shaded}

Then copy these lines of code to test how these relational operators
work with string characters.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{New.French.Word }\SpecialCharTok{==} \StringTok{"Ã©cogeste"} 
\NormalTok{New.French.Word }\SpecialCharTok{!=} \StringTok{"trottinettiste"}
\end{Highlighting}
\end{Shaded}

You will have noticed that the relational operator \texttt{==} tests
whether two strings are the same and returns \texttt{TRUE} if that's the
case. In contrast, \texttt{!=} tests whether two strings are different
and will therefore return \texttt{FALSE} if they are not different.

\section{Dealing with errors ðŸ¤¬}\label{sec-Errors}

If you try to run code that \texttt{R} cannot interpret, your Console
will display an error message in red. A large part of learning to code
is really about learning how to interpret these error messages, and
making the most common errors often enough that you immediately know how
to fix them. The process of fixing programming errors is called
\textbf{debugging} and often involves an array of emotions (see
Figure~\ref{fig-DebuggingMonster}).

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/DebuggingMonsters.png}}

}

\caption{\label{fig-DebuggingMonster}The joys of debugging by
\href{https://allisonhorst.com/allison-horst}{@allison\_horst}.}

\end{figure}%

In \texttt{R}, you will regularly encounter one particular problem that
we will call the ``plus-situation''. Let's take a closer look at this
error. Copy and paste this exact line of code in your \texttt{R} Console
and hit ``Enter'' to run it:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sqrt}\NormalTok{(my.favourite.number}
\end{Highlighting}
\end{Shaded}

Notice that, in this erroneous line of code, we have (intentionally)
forgotten to include the final bracket. As a result, after you hit
``Enter'', the Console output shows a ``\texttt{+}'' instead of the
result of the mathematical operation (see
Figure~\ref{fig-ConsoleIncompleteFunction}). The ``\texttt{+}''
indicates that the line is incomplete and therefore cannot be
interpreted yet. Whenever you see a ``\texttt{+}'' at the start of a
command in the Console, \texttt{R} is asking you to complete your line
of code.

\begin{figure}

\centering{

\includegraphics[width=4.6875in,height=\textheight,keepaspectratio]{images/ConsoleUncompleteFunction.png}

}

\caption{\label{fig-ConsoleIncompleteFunction}Incomplete function in
console}

\end{figure}%

There are two ways to fix this. The first method is to complete the line
of code directly in the Console. In the above case, this means adding
the closing bracket ``\texttt{)}'' after the ``\texttt{+}'' and hitting
``Enter'' again. Now that the line has been completed, \texttt{R} is
able to interpret it as a valid \texttt{R} command and therefore outputs
the expected result.

If you are running a line of code just once, from the Console, this
first method is fine. As we have seen above, however, most of the time,
you will write your code in a script rather than in the Console. So this
first, on-the-fly method is only recommendable for lines of code that
you will genuinely only need once. These include commands to install
packages, like \texttt{install.packages("janeaustenr")}, or to consult
documentation files, e.g.~\texttt{help(janeaustenr)}.

Given that we will mostly be working in scripts to ensure that our
analyses are reproducible (see Chapter~\ref{sec-LiteRateProgramming}),
let's now generate this error from an \texttt{.R} script. To do so, copy
and paste the erroneous line of code in your \texttt{.R} script and try
to run it by either clicking on the ``Run'' icon or using the shortcut
\texttt{Ctrl/Cmd\ +\ Enter}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sqrt}\NormalTok{(my.favourite.number}
\end{Highlighting}
\end{Shaded}

Again, our incomplete line of code cannot be interpreted and this
generates a ``plus-situation'' appears in the Console. Now, correct the
error in your script by adding the missing closing bracket and try to
run the command again.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sqrt}\NormalTok{(my.favourite.number)}
\end{Highlighting}
\end{Shaded}

As shown in Figure~\ref{fig-ConsoleError}, even though we have corrected
the problem, we now get an error! ðŸ¤¯ At first sight, this does not make
sense, but look carefully at what happened in the Console: The line of
code that \texttt{R} tried to interpret (in blue) is
\texttt{sqrt(my.favourite.number\ +\ sqrt(my.favourite.number)},
i.e.~the combination of the incomplete version of the command
\emph{plus} the complete one. This is obviously nonsense and \texttt{R}
tells us so by outputting an error message (see
Figure~\ref{fig-ConsoleError})!

\begin{figure}

\centering{

\includegraphics[width=4.6875in,height=\textheight,keepaspectratio]{images/ConsoleError.png}

}

\caption{\label{fig-ConsoleError}Error message in console}

\end{figure}%

To run a new line of code, we must see the command prompt
\texttt{\textgreater{}} in the Console. So, let's generate the error
again and learn how to fix it with the second method. Add this erroneous
line to your script again and run it:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sqrt}\NormalTok{(my.favourite.number}
\end{Highlighting}
\end{Shaded}

The plus-situation arises again, but we will now solve it using the
second method. Head over to the Console and place your cursor next to
the \texttt{+}. This time, instead of completing the line by adding a
closing bracket, press the Escape key (``Esc'') on your keyboard. This
will cancel the incomplete line of code. Then, you can add the missing
\texttt{)} in your script and rerun the newly completed line of code
from the Source pane.

This second method is the one you should use when you are documenting
your code in a script. If you don't make the changes immediately in your
script, you will forget and you will run into this error again in the
future. Think of it like a pastry chef who realises that they need to
put a little more baking powder in a cake batter for the texture to be
just right, but does not make a note of that change in their recipe
book. It's quite likely that the chef will forget the next time they
bake the cake. If it is one of their assistants who prepares the batter,
they will will have no way of knowing that the chef made that change!

Learning to make sense of error messages is a very important skill that,
like all skills, takes practice. Most errors are very easy to fix, if
you keep your cool. In fact, 90\% of errors are simply typos\footnote{I
  must confess that I made this number up. I don't have any reliable
  number, but it's fair to say that it's a very large proportion!}, so
really nothing worth stressing about!

Debugging is an unavoidable part of writing code. If you're stuck and
starting to feel frustrated, the best thing you can usually do is to
take a short break!

\begin{figure}[H]

{\centering \includegraphics[width=4.6875in,height=\textheight,keepaspectratio]{images/AHorst_ErrorMonster.png}

}

\caption{Time to take a break? (artwork by
\href{https://allisonhorst.com/allison-horst}{@allison\_horst}).}

\end{figure}%

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Click here for the solutions to {\textbf{Q5.11}}}, titlerule=0mm, leftrule=.75mm]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The first error was
  \texttt{object\ \textquotesingle{}my.favorite.number\textquotesingle{}\ not\ found}.
  This means that the object \texttt{my.favorite.number} is not stored
  in your environment. If you think it is, the problem is most likely
  due to a typo. Here, \texttt{my.favorite.number} uses American English
  spelling, whereas we used British English spelling (with a ``u'') when
  we created the object. To correct the error, you need to use exactly
  the same spelling as when you created the object.
\item
  The second error is also
  \texttt{object\ \textquotesingle{}Negin\textquotesingle{}\ not\ found}.
  However, here we do not expect an object called \texttt{Negin} to be
  in the environment because what we are actually trying to do is create
  and save a new object called \texttt{Negin-Fav-Word}! The problem is
  that \texttt{R} interprets the hyphens in this object name as
  ``minus'' and therefore tries to find the object \texttt{Negin} in
  order to then subtract \texttt{Fav} and \texttt{Word} from it. To
  correct this error, you need to remove the hyphens or replace them by
  dots.
\item
  The third error is yet another \texttt{object\ not\ found\ error}. It
  is another typo: the correct object name is not in the plural form.
\item
  The fourth error is
  \texttt{Error:\ unexpected\ symbol\ in\ "Ã¶mers\_favourite\_\ number"}.
  In addition, \emph{RStudio} warned us that there were some
  ``unexpected tokens'' in this line of code. The unexpected item is the
  space between \texttt{\_} and \texttt{number}. To fix this error, you
  need to remove this space character.
\item
  The object \texttt{my.favourite..number} is not found because the name
  of the object saved in the environment does not have two consecutive
  dots. Note that the error does \emph{not} come from the fact that this
  line begins with some white space and includes multiple space
  characters after the \texttt{=} sign. These added spaces make the line
  more difficult for us humans to read, but \texttt{R} simply ignores
  them. Hence, to fix this error, what you need to do is remove one of
  the consecutive dots in the object name.

  It is also worth noting that, once you've removed the extra dot, this
  line of code replaces the value originally stored in
  \texttt{Ã¶mers\_favourite\_number} with the value stored in
  \texttt{my.favourite.number}. If you check your environment pane, you
  will see that the command has changed
  \texttt{Ã¶mers\_favourite\_number} to \texttt{13} with no warning!! In
  other words, here, the equal sign \texttt{=} behaves in the same way
  as the assignment operator \texttt{\textless{}-}.
\item
  If you tried to run this line, you will have noticed that it does not
  actually generate an error. However, you may have noticed that the
  assignment operator is in the opposite direction
  (\texttt{-\textgreater{}} instead of \texttt{\textless{}-}). This
  means that \texttt{my.favourite.number} times two is assigned to a new
  object called \texttt{half.my.fav.number}. Having observed this, you
  will hopefully want to either amend the line for it to make
  mathematical sense
  (\texttt{my.favourite.number/2\ -\textgreater{}\ half.my.fav.number})
  or change the name of the new object for it to be meaningful
  (\texttt{my.favourite.number*2\ -\textgreater{}\ twice.my.fav.number}).
\item
  Running this line will have caused you to run into a \texttt{+}
  situation in the console. As explained in Section~\ref{sec-Errors}, to
  get out of it, first take your mouse cursor to the Console pane and
  press the Escape key on your keyboard to cancel the erroneous line.
  Whilst there is no error message to help you understand where the
  problem is coming from, you should see that \emph{RStudio} helpfully
  displays a red cross icon to the left of the line; hovering over it
  displays a multi-line message. The first line is the relevant one:
  \texttt{unexpected\ token\ \textquotesingle{}s.favourite.number\ \textless{}-\ 5}.
  This tells us that apostrophes are forbidden in object names. Remove
  the \texttt{\textquotesingle{}} and the error will be fixed.
\item
  This line also causes a \texttt{+} situation. In this case, it is due
  to a missing quotation mark. To fix this error, first cancel the
  incomplete line of code by escaping it. Then, add the missing double
  quotation mark in your script and rerun the completed line.
\item
  The message \texttt{Error:\ unexpected\ symbol\ in\ "2FavNumbers"} is
  due to the fact that object names cannot start with a number. Change
  the object name to something like \texttt{TwoFavNumbers} or
  \texttt{Fav2Numbers} to fix this error.
\item
  Here, too, the error message reads: \texttt{unexpected\ symbol}.
  However, it is important to remember that the unexpected symbol is not
  within the character string, but rather has to do with the code syntax
  used to assign the Persian string to \texttt{good.luck}. In other
  words, the problem has nothing to do with the fact that the string is
  in Persian, but rather that one of the quotation marks is missing. You
  can fix the error by ensuring that the phrase is enclosed in quotation
  marks.
\end{enumerate}

\end{tcolorbox}

\subsection*{Check your progress ðŸŒŸ}\label{check-your-progress-4}
\addcontentsline{toc}{subsection}{Check your progress ðŸŒŸ}

You have successfully completed { out of 11 questions} in this chapter.

Are you confident that you can\ldots?

\begin{itemize}
\tightlist
\item[$\square$]
  Use the console in \emph{RStudio} (Section~\ref{sec-Console})
\item[$\square$]
  Do simple maths in \texttt{R} (Section~\ref{sec-Maths})
\item[$\square$]
  Create, write, overwrite, and delete \texttt{R} objects
  (Section~\ref{sec-WorkingRObjects})
\item[$\square$]
  Create and save \texttt{.R} scripts (Section~\ref{sec-RScirpt})
\item[$\square$]
  Use relational operators in \texttt{R}
  (Section~\ref{sec-RelationalOperators})
\item[$\square$]
  Keep your calm when dealing with error messages in \texttt{R}
  (Section~\ref{sec-Errors})
\end{itemize}

If that's the case, you're ready to go through
\href{https://elenlefoll.github.io/RstatsTextbook/6_ImpoRtingData.html}{Chapter
6} to learn how to import research data into \texttt{R} so that, in the
following chapters, you can analyse real-life linguistics data!

\bookmarksetup{startatroot}

\chapter{\texorpdfstring{Impo\texttt{R}ting
data}{ImpoRting data}}\label{sec-ImportingData}

\subsubsection*{\texorpdfstring{\textbf{Chapter
overview}}{Chapter overview}}\label{chapter-overview-5}
\addcontentsline{toc}{subsubsection}{\textbf{Chapter overview}}

Many introductory \texttt{R} textbooks postpone this section until much
later by relying on datasets that are directly accessible as \texttt{R}
data objects. In real life, however, research data rarely come neatly
packaged as an \texttt{R} data object. Your data will most likely be
stored in a spreadsheet table or as text files of some kind. And -let's
be honest- they will be more messy than you would like to admit, making
this chapter and the next crucial for learning to do data analysis in
\texttt{R}.

This chapter will take you through the process of:

\begin{itemize}
\tightlist
\item
  Downloading data from a real applied linguistics study
\item
  Creating a Project in RStudio
\item
  Importing a \texttt{.csv} file as an \texttt{R} object in \texttt{R}
\item
  Examining a data frame object
\end{itemize}

In future chapters, we will continue to work with these data. We will
learn how to ``clean it up'' for data analysis, before we begin to
explore it using descriptive statistics and data visualisations.

\section{Accessing data from a published study}\label{sec-AccessingData}

As we saw in Section~\ref{sec-OpenScience}, it is good practice to share
both the data and materials associated with research studies so that
others can reproduce and replicate the research.

In the following chapters, we will focus on data associated with the
following study:

\begin{quote}
DÄ…browska, Ewa. 2019. Experience, Aptitude, and Individual Differences
in Linguistic Attainment: A Comparison of Native and Nonnative Speakers.
Language Learning 69(S1). 72--100.
\url{https://doi.org/10.1111/lang.12323}.
\end{quote}

\begin{figure}

\centering{

\includegraphics[width=4.80208in,height=\textheight,keepaspectratio]{images/DabrowskaTitle.png}

}

\caption{\label{fig-DabrowskaTitle}Title page from the journal Language
Learning}

\end{figure}%

Follow the DOI\footnote{``Digital Object Identifiers (DOI) are
  alpha-numeric strings that can be assigned to any entity, including:
  publications (including preprints), materials, datasets, and feature
  films - the use of DOIs is not restricted to just scholarly or
  academic material. DOIs''provides a system for persistent and
  actionable identification and interoperable exchange of managed
  information on digital networks.'' (\url{https://doi.org/hb.html\%29}.
  There are many different DOI registration agencies that operate DOIs,
  but the two that researchers would most likely encounter are Crossref
  and Datacite.'' (Parsons et al. 2022)} link above and read the
abstract to find out what the study was about. You do not need to have
institutional or paid access to the full paper to read the abstract.

The author, Ewa DÄ…browska, has made the data used in this study
available on an open repository (see Section~\ref{sec-Sharing}). To find
out on which repository, go back to the study's DOI link and click on
the drop-down menu ``Supporting Information''. It links to a PDF file.
Click on the link and scroll to the last page which contains the
following information about the data associated with this study:

\begin{quote}
Appendix S4: Datasets

DÄ…browska, E. (2018). L1 data {[}Data set{]}. Retrieved from
\href{https://www.iris-database.org/details/9pplf-S7kw3}{https://www.iris-database.org/iris/app/home/detail?id=york:935513}

DÄ…browska, E. (2018). L2 data {[}Data set{]}. Retrieved from
\href{https://www.iris-database.org/details/L8w1U-ZDgnH}{https://www.iris-database.org/iris/app/home/detail?id=york:935514}
\end{quote}

You may have noticed that the datasets were published in 2018, whereas
the article (DÄ…browska 2019) was published in the following year. This
is very common in academic publications as it can take many months or
even years for an article or book to be published, by which time the
author(s) may have already made the data available on a repository. This
particular article was actually first published on the journal's website
on 22 October 2018 as an ``advanced online publication'', but was not
officially published until March 2019 as part of Volume 69, Issue S1 of
the journal (see \url{https://doi.org/10.1111/lang.12323}). This
explains the discrepancy between the publication date of the datasets
and the publication date of the article recorded in the bibliographic
reference.

\section{Saving and examining the data}\label{sec-SavingData}

Click on the two links listed in Appendix S4 and download the two
datasets. Note that the URL may take a few seconds to redirect and load.
Save the two datasets in an appropriate place on your computer (see
Section~\ref{sec-FoldersPaths}), as we will continue to work with these
two files in the following chapters.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{What's a good place to save these files? ðŸ¤”}, titlerule=0mm, leftrule=.75mm]

If you haven't already done so, I suggest that you create a folder in
which you save everything that you create whilst learning from this
textbook. This folder could be called something along the lines of
\texttt{DataLiteracyTextbook}, \texttt{2024\_data\_literacy}, or
\texttt{LeFoll\_2024\_DataLiteracy} (see Section~\ref{sec-FileNaming}).
Then, within this folder, I recommend that you create another folder
called \texttt{Dabrowska2019} (note how I have not included the ``Ä…''
character in the folder name as this could cause problems), and within
this folder, create another folder called \texttt{data}. This is the
folder in which you can save these two files.

\end{tcolorbox}

The file \texttt{L1\_data.csv} contains data about the study's L1
participants. It is a delimiter-separated values (DSV) file (see
Section~\ref{sec-DSV}). The first five lines of the file are printed
below. Note that this is a very wide table as it contains many columns
(you can scroll to the right to view all the columns).

\begin{verbatim}
Participant,Age,Gender,Occupation,OccupGroup,OtherLgs,Education,EduYrs,ReadEng1,ReadEng2,ReadEng3,ReadEng,Active,ObjCl,ObjRel,Passive,Postmod,Q.has,Q.is,Locative,SubCl,SubRel,GrammarR,Grammar,VocabR,Vocab,CollocR,Colloc,Blocks,ART,LgAnalysis
1,21,M,Student,PS,None,3rd year of BA,17,1,2,2,5,8,8,8,8,8,8,6,8,8,8,78,95,48,73.33333333,30,68.75,16,17,15
2,38,M,Student/Support Worker,PS,None,NVQ IV Music Performance,13,1,2,3,6,8,8,8,8,8,8,7,8,8,8,79,97.5,58,95.55555556,35,84.375,11,31,13
3,55,M,Retired,I,None,No formal (City and Guilds),11,3,3,4,10,8,8,8,8,8,7,8,8,8,8,79,97.5,58,95.55555556,31,71.875,5,38,5
4,26,F,Web designer,PS,None,BA Fine Art,17,3,3,3,9,8,8,8,8,8,8,8,8,8,8,80,100,53,84.44444444,37,90.625,20,26,15
\end{verbatim}

\section{\texorpdfstring{Using Projects in
\emph{RStudio}}{Using Projects in RStudio}}\label{sec-RProject}

One of the advantages of working with \emph{RStudio} is that it allows
us to harness the potential of \emph{RStudio} Projects. Projects help us
to keep our digital kitchen nice and tidy. In RStudio, each project has
its own directory, environment, and history which means that we can work
on multiple projects at the same time and \emph{RStudio} will keep them
completely separate. This means that we can easily switch between
cooking different dishes, say a gluten-free egg curry and vegan
pancakes, without fear of accidentally setting the wrong temperature on
the cooker or contaminating either dish.

Regardless of whether or not you're a keen multi-tasker, \emph{RStudio}
Projects are a great way to help you keep together all the data,
scripts, and outputs associated with a single project in an organised
manner. In the long run, this will make your life much, much easier. It
will also be an absolute lifesaver as soon as you need to share your
work with others (e.g.~your supervisor, colleagues, reviewers, etc.).

To create a new Project, you have two options. In RStudio, you can
select `File', then `New Project\ldots{}' (see ``a'' on
Figure~\ref{fig-RStudioNewProject}). Alternatively, you can click on the
Project button in the top-right corner of your \emph{RStudio} window and
then select `New Project\ldots{}' (see ``b'' on
Figure~\ref{fig-RStudioNewProject}).

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/RStudioNewProject.png}}

}

\caption{\label{fig-RStudioNewProject}Create a new project in RStudio}

\end{figure}%

Both options will open up a window with three options for creating a new
project:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  New Directory (which allows you to create an entirely new project for
  which you do not yet have a folder on your computer)
\item
  Existing Directory (which allows you to create a project in an
  existing folder associated with your project)
\item
  Version Control (see Bryan 2018).
\end{enumerate}

In Section~\ref{sec-SavingData}, you should have already saved the data
that we want to import in a dedicated folder on your computer. Here, a
folder is the same as a directory. Hence, you can select the second
option: `Existing Directory'.

Clicking on this option will open up a new window
(Figure~\ref{fig-RStudioNewProject}). Click on `Browse\ldots{}' to
navigate to the folder where you intend to save all your work related to
DÄ…browska (2019). If you followed my suggestions earlier on, this would
be a folder called something along the lines of \texttt{Dabrowska2019}.
Once you have selected the correct folder, select the option `Open in a
new session' and then click on `Create Project'.

\begin{figure}[H]

{\centering \includegraphics[width=5in,height=\textheight,keepaspectratio]{images/RStudioNewProjectDialogue.png}

}

\caption{New project window}

\end{figure}%

Creating an \emph{RStudio} project generates a new file in your project
folder called \texttt{Dabrowska2019.Rproj}. You can see it in the Files
pane of RStudio. Note that the extension of this newly created file is
\texttt{.Rproj}. Such \texttt{.Rproj} files store information about your
project options, which you will not need to edit. More usefully,
\texttt{.Rproj} files can be used as shortcuts for opening your
projects. To see how this works, shut down RStudio. Then, in your
computer file system (e.g.~using a File Explorer window on Windows and a
Finder window on macOS), navigate to your project folder to locate your
\texttt{.Rproj} file (see Figure~\ref{fig-ProjectFileFinder}).
Double-click on the file. This will automatically launch \emph{RStudio}
with all the correct settings for this particular project.
Alternatively, you can use the Project button in the top-right corner of
your \emph{RStudio} window to open up a project from \emph{RStudio}
itself (see Figure~\ref{fig-RStudioLaunchProject}).

\begin{figure}

\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics[width=4.58333in,height=\textheight,keepaspectratio]{images/ProjectFileFinder.png}

}

\subcaption{\label{fig-ProjectFileFinder}Launching a Project from the
File Finder}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics[width=4.58333in,height=\textheight,keepaspectratio]{images/RStudioLaunchProject.png}

}

\subcaption{\label{fig-RStudioLaunchProject}Launching a Project from
RStudio}

\end{minipage}%

\caption{\label{fig-filenames}The two options to open an RProject.}

\end{figure}%

\section{Working directories}\label{sec-WorkingDirectories}

The folder in which the \texttt{.Rproj} file was created corresponds to
your project's \textbf{working directory}. Once you have opened a
Project, you can see the path to your project's working directory at the
top of the Console pane in RStudio. The Files pane should also show the
content of this directory.

\begin{figure}[H]

{\centering \includegraphics[width=3.125in,height=\textheight,keepaspectratio]{images/WorkingDirectory.png}

}

\caption{Contents of the project folder as displayed by RStudio}

\end{figure}%

Click on the ``New Folder'' icon in your Files pane to create a new
subfolder called \texttt{analysis}. Your folder \texttt{Dabrowska2019}
should now contain an \texttt{.RProj} file and two subfolders called
\texttt{analysis} and \texttt{data}.

\section{\texorpdfstring{Importing data from a \texttt{.csv}
file}{Importing data from a .csv file}}\label{sec-ImportingDataCSV}

We will begin by creating a new \texttt{R} script in which we will write
the code necessary to import the data from DÄ…browska (2019)`s study in
\texttt{R}. To do so, from the Files pane in RStudio, click on the
analysis folder to open it and then click on the 'New Blank File' icon
in the menu bar of the Files pane and select `R Script'. This will open
a new, empty \texttt{R} script in your Source pane. It is best to always
begin by saving a newly created file. Save this empty script with a
computer- and human-friendly file name such as \texttt{1\_DataImport.R}
(Section~\ref{sec-FileNaming}). It should now appear in your analysis
folder in the Files pane.

Given that we want to import two \texttt{.csv} files, we are going to
use the function \texttt{read.csv()}. You can find out what this
function does by running the command \texttt{?read.csv} or
\texttt{help(read.csv)} in the Console to open up the documentation.
This help file contains information about several base \texttt{R}
functions used to import data. Scroll down to find the information about
the \texttt{read.csv()} function. It reads:

\begin{verbatim}
read.csv(file, header = TRUE, sep = ",", quote = "\"",
         dec = ".", fill = TRUE, comment.char = "", ...)
\end{verbatim}

This line from the documentation informs us that this function's first
argument is the path to the \texttt{file} from which we want to import
the data. It also informs us that \texttt{file} is the only argument
that does not have a default value (as it is not followed by an equal
sign and a value). In this function, \texttt{file} is therefore the only
argument that is compulsory. Hence, in theory, all we need to write to
import the data is:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L1.data }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\AttributeTok{file =} \StringTok{"data/L1\_data.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In fact, we could shorten things even further as, unless otherwise
specified, \texttt{R} will assume that the first value listed after a
function corresponds to the function's first argument which, here, is
\texttt{file}. In other words, this command and the one above are
equivalent:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L1.data }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"data/L1\_data.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The file path \texttt{"data/L1\_data.csv"} informs \texttt{R} that the
data are located in a subfolder of the project's working directory
called \texttt{data} and that, within this \texttt{data} subfolder, the
file that we want to import is called \texttt{L1\_data.csv}. Note that
the file extension must be specified (see
Section~\ref{sec-FileExtensions}). Note, also, the file path is
separated with a single forward slash \texttt{/}. In \texttt{R}, this
should work regardless of the operating system that you are using and,
in order to be able to easily share your scripts with others, it is
recommended that you use forward slashes even if you are running Windows
(Section~\ref{sec-FoldersPaths}).

Although the command above did the job, in practice, it is often safer
to spell things out further to remind ourselves of some of the default
settings of the function that we are using in case they need to be
checked or changed at a later stage. In this example, we will therefore
import the data with the following command:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L1.data }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\AttributeTok{file =} \StringTok{"data/L1\_data.csv"}\NormalTok{,}
                    \AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{,}
                    \AttributeTok{sep =} \StringTok{","}\NormalTok{,}
                    \AttributeTok{quote =} \StringTok{"}\SpecialCharTok{\textbackslash{}"}\StringTok{"}\NormalTok{,}
                    \AttributeTok{dec =} \StringTok{"."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In the command above, \texttt{header\ =\ TRUE}, explicitly tells
\texttt{R} to import the first row of the \texttt{.csv} table as column
headers rather than values. This is not strictly necessary because, as
we saw from the function's help file, \texttt{TRUE} is already set as
the default value for this argument, but it is good to remind ourselves
of how this particular dataset is organised.

The arguments \texttt{sep} and \texttt{quote} specify the characters
that, in this \texttt{.csv} file are used to separate the values on the
one hand, and delineate them, on the other (see Section~\ref{sec-DSV}).
As we saw above, DÄ…browska (2019)'s \texttt{.csv} files use the comma as
the separator and the double quotation mark as the quoting character.
Note that the \texttt{"} character needs to be preceded by a backslash
(\texttt{\textbackslash{}}) (we say it needs to be ``escaped'') because
otherwise \texttt{R} will interpret it as part of the command syntax,
which would lead to an error. Finally, the argument \texttt{dec\ =\ "."}
explicitly tells \texttt{R} that this \texttt{.csv} file uses the dot as
the decimal point. In some countries, e.g.~Germany and France, the comma
is used to represent decimal places so, if you obtain data from a German
or French colleague, this setting may need to be changed to
\texttt{dec\ =\ ","} for the data to be imported correctly.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-important-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-important-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{``Hell is empty, and all the devils are \{here\}.'' ðŸ˜ˆ}, titlerule=0mm, leftrule=.75mm]

This section title borrows a quote from The Tempest by William
Shakespeare to reflect the fact that file paths are perhaps the most
frequent source of frustration among (beginner) coders.
Section~\ref{sec-ImportingErrors} explains how to deal with the most
frequent error messages. Ultimately, however, these errors are typically
due to poor data management (see Chapter~\ref{sec-DataManagement}).
That's because the devil's in the detail (remember: no spaces, special
characters, differences between different operating systems, etc.). As a
result, even advanced users of \texttt{R} and other programming
languages frequently find that file path issues continue to plague their
work, if they fail to take file management seriously.

To make your projects more robust to such issues, I strongly recommend
working with the \href{https://here.r-lib.org/}{\{here\}} package in
addition to RProjects. You will first need to install the package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"here"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

When you load the package, it automatically runs the \texttt{here()}
function with no argument, which returns the path to your project
directory, as determined by the location of the \texttt{.RProj} file
associated with your project.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(here)}
\end{Highlighting}
\end{Shaded}

You can now use the \texttt{here()} function to build paths relative to
this directory with the following syntax:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{here}\NormalTok{(}\StringTok{"data"}\NormalTok{, }\StringTok{"L1\_data.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "/Users/lefoll/Documents/UzK/RstatsTextbook/data/L1_data.csv"
\end{verbatim}

And you can embed this path in your import command like this:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(here)}

\NormalTok{L1.data }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\AttributeTok{file =} \FunctionTok{here}\NormalTok{(}\StringTok{"data"}\NormalTok{, }\StringTok{"L1\_data.csv"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Much like wearing a helmet for extra safety
(Figure~\ref{fig-HelmetHere}), \{here\} makes the paths that you include
in your code far more robust. In other words, they are far less likely
to fail and break your code when you share your scripts with your
colleagues, or run them yourself from different directories or operating
systems. For more reasons to use \{here\}, check out the blog post
``\href{https://malco.io/articles/2018-11-05-why-should-i-use-the-here-package-when-i-m-already-using-projects}{Why
should I use the here package when I'm already using projects?}'' by
Barrett (2018).

\begin{figure}[H]

\centering{

\includegraphics[width=6.25in,height=\textheight,keepaspectratio]{images/AHorst_RProjHere.png}

}

\caption{\label{fig-HelmetHere}Although a fairly common way of working
with data in \texttt{R}, using \texttt{setwd()} (see
Section~\ref{sec-HardcodingFilePaths}) is dangerous and will, sooner or
later, cause you and/or your colleagues some nasty accidents. A
combination of using RProj and \{here\} as described above is much
safer!}

\end{figure}%

\end{tcolorbox}

\section{Import errors and issues ðŸ¥º}\label{sec-ImportingErrors}

It is crucial that you check whether your data have genuinely been
correctly imported. Here's a list of things to check (in that order):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Were you able to run the import command without producing any errors?
  If you are getting an error, remember that this is most likely due to
  a typo (see Section~\ref{sec-Errors})!

  \begin{itemize}
  \item
    If part of the error message reads ``No such file or directory'',
    this means that either the file path or the file name is incorrect.
    Carefully check the path that you specified in your import command
    (if you're struggling to find the correct path, you may want to try
    out the import method explained in Section~\ref{sec-ImportingGUI}).
    To ensure that you are not misspelling the name of the file, you can
    press the tab key on your keyboard to get \emph{RStudio} to
    auto-complete the file name for you.
  \item
    If the error message includes the statement ``could not find
    function'', this means that you have either misspelled the name of
    the function or this is not a base \texttt{R} function and you have
    forgotten to load the library to which this function belongs (see
    Section~\ref{sec-readr}).
  \item
    As usual whenever you get an error message, also check that you have
    included all of the necessary brackets and quotation marks (see
    Section~\ref{sec-Errors}).
  \end{itemize}
\item
  Has the \texttt{R} data object appeared in your Environment pane? Does
  it have the expected number of rows (observations) and columns
  (variables)? \texttt{L1.data} contains 90 observations and 31
  variables. If you are getting different numbers, this might be because
  you previously opened the \texttt{.csv} file with Excel or that your
  computer converted it to Excel format automatically. To remedy this,
  ensure that you have followed all the steps described in
  Section~\ref{sec-DSVLibreOffice}.
\item
  To view the entire table, use the function \texttt{View()} with the
  name of your data object as the first and only argument,
  e.g.~\texttt{View(L1.data)}\footnote{Note that, unlike the functions
    that we have used so far, the \texttt{View()} function begins with a
    capital letter. \texttt{R} is a case-sensitive programming language,
    which means that \texttt{view()} and \texttt{View()} are not the
    same thing!}. This will open up a new tab in your Source pane that
  displays the full table, much like in a spreadsheet programme. You can
  search and filter the table in this tab, but you cannot edit it in any
  way (and that's a good thing because, if we want to edit things, we
  want to ensure that we keep track of our changes in a script!). Browse
  through the table and check that everything ``looks healthy''. This is
  much like visually inspecting and smelling ingredients before using
  them in a recipe. It's not perfect but if something is really off, you
  should notice it. Check that each cell appears to have one and only
  one value.
\item
  Finally, use the \texttt{str()} function to view the structure of your
  data object in a more compact way. Using the command
  \texttt{str(L1.data)} will display a summary of the data.frame in the
  Console. The summary begins by informing us that this data object is a
  data.frame, that contains 90 observations and 31 variables. Then, it
  lists all of the variables, followed by the type of values stored in
  this variable (e.g.~character strings or integers) and then the first
  few values for each variable. Especially with very wide tables that
  contain a lot of variables, it is often easier to check the summary of
  the imported data with \texttt{str()} than with \texttt{View()},
  though I would always recommend taking a few seconds to do both. This
  is time well spent!
\end{enumerate}

\section{Importing tabular data in other
formats}\label{sec-ImportingTabularData}

We have seen how to load data from a \texttt{.csv} file into \texttt{R}
by creating an \texttt{R} data frame object that contains the data
extracted from a \texttt{.csv} file. But, as we saw in
Chapter~\ref{sec-DataFormats}, not all datasets are stored as
\texttt{.csv} files. Fear not: there are many import functions in
\texttt{R}, with which you can import pretty much all kinds of data
formats! This section introduces a few of the most useful ones for
research in the language sciences.

We begin with the highly versatile function \texttt{read.table()}. The
\texttt{read.csv()} is actually a variant of \texttt{read.table()}. You
recall that when we called up the help file for the former using
\texttt{?read.csv()}, we obtained a combined help file for several
functions, the first of which was \texttt{read.table()}. By specifying
the following arguments as we did earlier, we can actually use the
\texttt{read.table()} function to import our \texttt{.csv} file with
exactly the same results:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L1.data }\OtherTok{\textless{}{-}} \FunctionTok{read.table}\NormalTok{(}\AttributeTok{file =} \StringTok{"data/L1\_data.csv"}\NormalTok{,}
                    \AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{,}
                    \AttributeTok{sep =} \StringTok{","}\NormalTok{,}
                    \AttributeTok{quote =} \StringTok{"}\SpecialCharTok{\textbackslash{}"}\StringTok{"}\NormalTok{,}
                    \AttributeTok{dec =} \StringTok{"."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Tab-separated file}\label{tab-separated-file}

In {\textbf{Task 2.3}} in Section~\ref{sec-DSVLibreOffice}, you
downloaded and examined a DSV file with a \texttt{.txt} extension that
was separated by tabs: \texttt{offlinedataLearners.txt} from Schimke et
al. (2018).

If we change the separator character argument to
\texttt{\textbackslash{}t} for tab, we can also import this dataset in
\texttt{R} using the \texttt{read.table()} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{OfflineLearnerData }\OtherTok{\textless{}{-}} \FunctionTok{read.table}\NormalTok{(}\AttributeTok{file =} \StringTok{"data/offlinedataLearners.txt"}\NormalTok{,}
                                        \AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{,}
                                        \AttributeTok{sep =} \StringTok{"}\SpecialCharTok{\textbackslash{}t}\StringTok{"}\NormalTok{,}
                                        \AttributeTok{dec =} \StringTok{"."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

For the command above to work, you will first need to save the file
\texttt{offlinedataLearners.txt} to the folder specified in the path.
Otherwise, you will get an error message informing you that there is
``No such file or directory'' (see Section~\ref{sec-ImportingErrors}).

\subsection{Semi-colon-separated file}\label{semi-colon-separated-file}

Figure~\ref{fig-BusterudEAData} displays an extract of the dataset
\texttt{AJT\_raw\_scores\_L2.csv} from an experimental study by Busterud
et al. (2023). Although this DSV file has a \texttt{.csv} extension, it
is actually separated by semicolons. As you can see in
Figure~\ref{fig-BusterudEAData}, in the file
\texttt{AJT\_raw\_scores\_L2.csv}, the comma is used to show the decimal
place.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/BusterudEA2023_data_snapshot.png}}

}

\caption{\label{fig-BusterudEAData}Extract of data file
AJT\_raw\_scores\_L2.csv from}

\end{figure}%

If you look carefully, you will also see that this dataset has some
empty cells. These data can be downloaded from
\url{https://doi.org/10.18710/JBMAPT}. It is delivered with a README
text file. It is good practice to include a README file when publishing
datasets or code and, as the name suggests, it is always a good idea to
actually read README files! ðŸ™ƒ Among other things, this particular
README explains that, in this dataset: ``Missing data are represented by
empty cells.''

If you call up the help file for the \texttt{read.table()} function
again, you will see that there is an argument called
\texttt{na.strings}. The default value is \texttt{NA}. When we import
this dataset \texttt{AJT\_raw\_scores\_L2.csv} from Busterud et al.
(2023), we will therefore need to change this argument to ensure that
empty cells are recognised as missing values.

In addition to the file path, the command to import this dataset
specifies the separator character as the semicolon
(\texttt{sep\ =\ ";"}), the character used to represent decimals
(\texttt{dec\ =\ ","}), and empty cells as missing values
(\texttt{na.strings\ =\ ""}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AJT.raw.scores.L2 }\OtherTok{\textless{}{-}} \FunctionTok{read.table}\NormalTok{(}\AttributeTok{file =} \StringTok{"data/AJT\_raw\_scores\_L2.csv"}\NormalTok{,}
                          \AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{,}
                          \AttributeTok{sep =} \StringTok{";"}\NormalTok{,}
                          \AttributeTok{dec =} \StringTok{","}\NormalTok{,}
                          \AttributeTok{na.strings =} \StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Once we have run this command, we should check that the data have been
correctly imported, for example by using the \texttt{View()} function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{View}\NormalTok{(AJT.raw.scores.L2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
           ID L3 Years.of.L3 Gender L3.selfasses L3.grade L2.selfasses L2.grade
1     BKRE452  2           4      2          2.5        3            4        3
2     SHEL876  2           4      2          3.0        5            6        5
3     SVIÃ˜510  2           4      1          4.0        4            6        5
4     EHEA194  2           4      1          2.0        2            6        4
5     ERAO442  2           4      2          3.0        3            5        4
6     SEIO103  2           4      1          2.0        3            4        3
7     NMOI241  2           4      1          3.0        4            4        4
8  BBIE911/77  2           4      1           NA        3           NA        4
9     UUNO561  2           4      1          2.0        3            3        4
10    SMAO470  2           4     NA          3.0     <NA>            6       NA
11    SSID616  2           3      1          2.0        2            6        4
12    SHRI714  2           1      2          4.0        3            3        3
13    HALI620  2           1      2          5.0        6            4        4
\end{verbatim}

Here, we can see that the data have been correctly imported as a table.
The commas have been correctly converted to decimal points and the empty
cells are now labelled \texttt{NA}.

\section{Using \{readr\} from the \{tidyverse\} to import tabular
files}\label{sec-readr}

The \{tidyverse\} is a family of packages that we will use a lot in
future chapters. This family of package includes the \{readr\} package
which features some very useful functions to import data into
\texttt{R}. You can install and load the \{readr\} package either
individually or as part of the \{tidyverse\} bundle:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Install the package individually:}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"readr"}\NormalTok{)}

\CommentTok{\# Or install the full tidyverse (this will take a little longer):}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{)}

\CommentTok{\# Load the library:}
\FunctionTok{library}\NormalTok{(readr)}
\end{Highlighting}
\end{Shaded}

\subsection*{Delimiter-separated values (DSV)
files}\label{delimiter-separated-values-dsv-files}
\addcontentsline{toc}{subsection}{Delimiter-separated values (DSV)
files}

The \{readr\} package includes functions to import DSV files that are
similar, but not identical to the base \texttt{R} functions explained
above. The main difference is that the \{readr\} functions load data
into an \texttt{R} object of type ``tibble'' rather than ``data frame''.
In practice, this will not make a difference for our work in future
chapters. Hence, the following two commands can equally be used to
import \texttt{L1\_data.csv}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Import .csv file using the base R function read.csv():}
\NormalTok{L1.data }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\AttributeTok{file =} \StringTok{"data/L1\_data.csv"}\NormalTok{, }
                    \AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{, }
                    \AttributeTok{quote =} \StringTok{"}\SpecialCharTok{\textbackslash{}"}\StringTok{"}\NormalTok{)}

\FunctionTok{class}\NormalTok{(L1.data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "data.frame"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Import .csv file using the \{readr\} function read\_csv():}
\NormalTok{L1.data }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\AttributeTok{file =} \StringTok{"data/L1\_data.csv"}\NormalTok{, }
                    \AttributeTok{col\_names =} \ConstantTok{TRUE}\NormalTok{, }
                    \AttributeTok{quote =} \StringTok{"}\SpecialCharTok{\textbackslash{}"}\StringTok{"}\NormalTok{)}

\FunctionTok{class}\NormalTok{(L1.data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "spec_tbl_df" "tbl_df"      "tbl"         "data.frame" 
\end{verbatim}

Note that instead of the argument \texttt{header\ =\ TRUE}, the
\{readr\} function \texttt{read\_csv()} takes the argument
\texttt{col\_names\ =\ TRUE}, which has the same effect.

There are a few more differences between the two functions that are
worth noting:

\begin{itemize}
\item
  If the column headers in your original data file contain spaces, these
  will be automatically replaced by dots (\texttt{.}) when you import
  data using base \texttt{R} functions. By contrast, with the \{readr\}
  functions, the spaces will, by default, be retained. As we will see
  later, this behaviour can represent both an advantage and
  disadvantage, depending on what you want to do.
\item
  The \{readr\} functions are quicker and are therefore recommended if
  you are importing large datasets.
\item
  In general, the behaviour of \{readr\} functions is more consistent
  across different operating systems and locale settings (e.g.~the
  language in which your operating system is set).
\end{itemize}

Note that, just like \texttt{read.csv()} was a special case of
\texttt{read.table}, the \{readr\} function \texttt{read\_csv()} is a
special variant of the more general function \texttt{read\_delim()} that
can be used to import data from all kinds of DSV files. Check the help
file to find out all the options using \texttt{?read\_delim}.

The help file informs us that the package includes a function
specifically designed to import semi-colon separated file with the the
comma as the decimal point: \texttt{read\_csv2()}. It further states
that ``{[}t{]}his format is common in some European countries.'' If you
scroll down the help page, you will see that its usage is summarised in
the following way:

\begin{verbatim}
read_csv2(
  file,
  col_names = TRUE,
  col_types = NULL,
  col_select = NULL,
  id = NULL,
  locale = default_locale(),
  na = c("", "NA"),
  quoted_na = TRUE,
  quote = "\"",
  comment = "",
  trim_ws = TRUE,
  skip = 0,
  n_max = Inf,
  guess_max = min(1000, n_max),
  progress = show_progress(),
  name_repair = "unique",
  num_threads = readr_threads(),
  show_col_types = should_show_types(),
  skip_empty_rows = TRUE,
  lazy = should_read_lazy()
)
\end{verbatim}

This overview of the \texttt{read\_csv2} function shows all of the
arguments of the function and their default values. For instance, with
\texttt{na\ =\ c("",\ "NA")}, it tells us that, by default, both empty
cells and cells with the value \texttt{NA} will be interpreted by the
function as \texttt{NA} values.

Having checked the default values for all of the arguments of the
\texttt{read\_csv2} function, we may conclude that we can safely use
this \{readr\} function to import the file
\texttt{AJT\_raw\_scores\_L2.csv} from Busterud et al. (2023) without
changing any of these default values. Hence, all we need is:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AJT.raw.scores.L2 }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv2}\NormalTok{(}\AttributeTok{file =} \StringTok{"data/AJT\_raw\_scores\_L2.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Note that, whereas when we used the base \texttt{R} function
\texttt{read.table()} the header for the third variable in the file was
imported as \texttt{Years.of.L3}, using the \{readr\} function, the
variable is entitled \texttt{Years\ of\ L3}.

\subsection*{Fixed width files}\label{fixed-width-files}
\addcontentsline{toc}{subsection}{Fixed width files}

Fixed width files (with file extensions such as \texttt{.gz},
.\texttt{bz2} or \texttt{.xz}) are a less common type of data source in
the language sciences. In these text-based files, the values are
separated not by a specific character such as the comma or the tab, but
by a set amount of white/empty space other than a tab. Fixed width files
can be loaded using the
\href{https://readr.tidyverse.org/reference/read_fwf.html}{\texttt{read\_fwf()}}
function from \{readr\}. Fields can be specified by their widths with
\href{https://readr.tidyverse.org/reference/read_fwf.html}{\texttt{fwf\_widths()}}
or by their positions with
\href{https://readr.tidyverse.org/reference/read_fwf.html}{\texttt{fwf\_positions()}}.

\section{Importing files from spreadsheet
software}\label{sec-SpreadsheetSoftware}

If your data are currently stored in a spreadsheet software
(e.g.~LibreOffice Calc, Google Sheets, or Microsoft Excel), you can
export them to \texttt{.csv} or \texttt{.tsv}. However, if you do not
wish to do this (e.g.~because your colleague wishes to maintain the
spreadsheet format that includes formatting elements such as bold or
coloured cells), there are functions to import these file formats
directly into \texttt{R}.

\subsection{LibreOffice Calc}\label{libreoffice-calc}

For LibreOffice Calc (which you should have installed in
Section~\ref{sec-OpenSource}), you can install the \{readODS\} package
and use its \texttt{read\_ods()} function to import \texttt{.ods} files.
Details about all the options can be found here
\url{https://www.rdocumentation.org/packages/readODS/versions/2.3.0}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Install from CRAN (safest option):}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"readODS"}\NormalTok{)}

\CommentTok{\# Or install the development version from Github:}
\NormalTok{remotes}\SpecialCharTok{::}\FunctionTok{install\_github}\NormalTok{(}\StringTok{"ropensci/readODS"}\NormalTok{)}

\CommentTok{\# Load the library:}
\FunctionTok{library}\NormalTok{(readODS)}

\CommentTok{\# Import your .ods data:}
\NormalTok{MyLibreOfficeData }\OtherTok{\textless{}{-}} \FunctionTok{read\_ods}\NormalTok{(}\StringTok{"data/MyLibreOfficeTable.ods"}\NormalTok{,}
                            \AttributeTok{sheet =} \DecValTok{1}\NormalTok{,}
                            \AttributeTok{col\_names =} \ConstantTok{TRUE}\NormalTok{,}
                            \AttributeTok{na =} \ConstantTok{NA}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Microsoft Excel}\label{microsoft-excel}

Various packages can be used to import Microsoft Excel file formats, but
the simplest is \{readxl\}, which is part of the \{tidyverse\}. It
allows users to import data in both \texttt{.xlsx} and the older
\texttt{.xls} format. You can find out more about its various options
here: \url{https://readxl.tidyverse.org/}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Install from CRAN (safest option):}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"readxl"}\NormalTok{)}

\CommentTok{\# Or install the development version from Github:}
\NormalTok{remotes}\SpecialCharTok{::}\FunctionTok{install\_github}\NormalTok{(}\StringTok{"tidyverse/readxl"}\NormalTok{)}

\CommentTok{\# Load the library:}
\FunctionTok{library}\NormalTok{(readxl)}

\CommentTok{\# Import your .ods data:}
\NormalTok{MyExcelData }\OtherTok{\textless{}{-}} \FunctionTok{read\_excel}\NormalTok{(}\StringTok{"data/MyExcelSpreadsheet.xlsx"}\NormalTok{,}
                            \AttributeTok{sheet =} \DecValTok{1}\NormalTok{,}
                            \AttributeTok{col\_names =} \ConstantTok{TRUE}\NormalTok{,}
                            \AttributeTok{na =} \ConstantTok{NA}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Google Sheets}\label{google-sheets}

There are also several ways to import data from Google Sheets. The
simplest is to export your tabular data as a \texttt{.csv},
\texttt{.tsv}, \texttt{.xslx}, or \texttt{.ods} file by selecting Google
Sheet's menu option `File' \textgreater{} `Download'. Then, you can
simply import this downloaded file in \texttt{R} using the corresponding
function as described above.

However, if you want to directly import your data from Google Sheets and
be able to dynamically update the analyses that you conduct in
\texttt{R} even as the input data are amended on Google Sheets, you can
use the \{googlesheets4\} package (which is part of the \{tidyverse\}):

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Install from CRAN (safest option):}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"googlesheets4"}\NormalTok{)}

\CommentTok{\# Or install the development version from Github:}
\NormalTok{remotes}\SpecialCharTok{::}\FunctionTok{install\_github}\NormalTok{(}\StringTok{"tidyverse/googlesheets4"}\NormalTok{)}

\CommentTok{\# Load the library:}
\FunctionTok{library}\NormalTok{(googlesheets4)}

\CommentTok{\# Import your Google Sheets data using your (spread)sheet\textquotesingle{}s URL:}
\NormalTok{MySheetsData }\OtherTok{\textless{}{-}} \FunctionTok{read\_sheet}\NormalTok{(}\StringTok{"https://docs.google.com/spreadsheets/d/1U6Cf\_qEOhiR9AZqTqS3mbMF3zt2db48ZP5v3rkrAEJY/edit\#gid=780868077"}\NormalTok{,}
                            \AttributeTok{sheet =} \DecValTok{1}\NormalTok{,}
                            \AttributeTok{col\_names =} \ConstantTok{TRUE}\NormalTok{,}
                            \AttributeTok{na =} \StringTok{"NA"}\NormalTok{)}

\CommentTok{\# Or import your Google Sheets data using just the sheet\textquotesingle{}s ID:}
\NormalTok{MySheetsData }\OtherTok{\textless{}{-}} \FunctionTok{read\_sheet}\NormalTok{(}\StringTok{"1U6Cf\_qEOhiR9AZqTqS3mbMF3zt2db48ZP5v3rkrAEJY/edit\#gid=780868077"}\NormalTok{,}
                            \AttributeTok{sheet =} \DecValTok{1}\NormalTok{,}
                            \AttributeTok{col\_names =} \ConstantTok{TRUE}\NormalTok{,}
                            \AttributeTok{na =} \StringTok{"NA"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/GoogleSheetsTidyverseAccess.png}}

}

\caption{\label{fig-TidyverseAPI}Dialogue box to consent to the
\{tidyverse\} API Packages having access to your Google Drive to import
directly from a Google Sheet. Read this carefully before clicking on
`Continue'.}

\end{figure}%

\subsection{Importing spreadsheet files with multiple
sheets/tabs}\label{sec-MultipleSheets}

Note that, as spreadsheet software typically allow users to have several
``sheets'' or ``tabs'' within a file that each contains separate tables,
the functions \texttt{read\_excel()}, \texttt{read\_ods()}, and
\texttt{read\_sheet} include an argument called \texttt{sheet} which
allows you to specify which sheet should be imported. The default value
is \texttt{1}, which simply means that the first one is imported. If
your sheets have names, you can also use its name as the argument value,
e.g.:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MyExcelData }\OtherTok{\textless{}{-}} \FunctionTok{read\_excel}\NormalTok{(}\StringTok{"data/MyExcelSpreadsheet.xlsx"}\NormalTok{,}
                            \AttributeTok{sheet =} \StringTok{"raw data"}\NormalTok{,}
                            \AttributeTok{col\_names =} \ConstantTok{TRUE}\NormalTok{,}
                            \AttributeTok{na =} \ConstantTok{NA}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Importing data files from SPSS, SAS and Stata}\label{sec-SPPS}

If you've recently switched from working in SPSS, SAS, or Stata (or are
collaborating with someone who uses these programmes), it might be
useful to know that you can also import the data files created by
programmes directly into \texttt{R} using the \{haven\} package. Details
of all the options can be found here:
\url{https://haven.tidyverse.org/}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Install from CRAN (safest option):}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"haven"}\NormalTok{)}

\CommentTok{\# Or install the development version from Github:}
\NormalTok{remotes}\SpecialCharTok{::}\FunctionTok{install\_github}\NormalTok{(}\StringTok{"tidyverse/haven"}\NormalTok{)}

\CommentTok{\# Load the library:}
\FunctionTok{library}\NormalTok{(haven)}

\CommentTok{\# Import an SAS file}
\NormalTok{MySASData }\OtherTok{\textless{}{-}} \FunctionTok{read\_sas}\NormalTok{(}\StringTok{"MySASDataFile.sas7bdat"}\NormalTok{)}

\CommentTok{\# Import an SPSS file}
\NormalTok{MySPSSDataFile }\OtherTok{\textless{}{-}} \FunctionTok{read\_sav}\NormalTok{(}\StringTok{"MySPSSDataFile.sav"}\NormalTok{)}

\CommentTok{\# Import a Stata file}
\NormalTok{MyStataData }\OtherTok{\textless{}{-}} \FunctionTok{read\_dta}\NormalTok{(}\StringTok{"MyStataDataFile.dta"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Importing other file formats}\label{sec-OtherFileFormats}

In this textbook, we will only deal with DSV files
(Section~\ref{sec-DSV}) but, as you can imagine, there are many more
\texttt{R} packages and functions that allow you to import all kinds of
other file formats. These include \texttt{.xml}, \texttt{.json} and
\texttt{.html} files, various database formats, and other files with
complex structures (see, e.g.~\url{https://rc2e.com/inputandoutput}).

In addition, fellow linguists are constantly developing new packages to
work with file formats that are specific to our discipline. In the
spirit of Open Science (see Chapter~\ref{sec-OpenScholarship}), many are
making these packages available to the wider research community by
releasing them under open licenses. For example, Linguistics
M.A.~students Katja Wiesner and Nicolas Werner wrote an \texttt{R}
package to facilitate the import of \texttt{.eaf} files generated by the
annotation software \href{https://archive.mpi.nl/tla/elan}{ELAN}
(Lausberg \& Sloetjes 2009) into \texttt{R} as part of a seminar project
supervised by Dr.~Fahime (Fafa) Same at the University of Cologne
(\url{https://github.com/relan-package/rELAN/?tab=readme-ov-file}).

\section{\texorpdfstring{Quick-and-dirty (aka bad!) ways to import data
in
\texttt{R}}{Quick-and-dirty (aka bad!) ways to import data in R}}\label{quick-and-dirty-aka-bad-ways-to-import-data-in-r}

Feel free to skip this section if you got on just fine with the
importing method introduced above as the following two methods are
problematic for a number of reasons. However, they may come in useful in
special cases, which is why both are briefly explained below.

\subsection{\texorpdfstring{Hardcoding file paths in \texttt{R} scripts
ðŸ¥´}{Hardcoding file paths in R scripts ðŸ¥´}}\label{sec-HardcodingFilePaths}

Whilst it is certainly not recommended (see e.g. Bryan 2017), it is
nonetheless worth understanding this method of working with file paths
in \texttt{R} as you may well come across it in other people's code.

Instead of creating an RProject to determine a project's working
directory (as we did in Section~\ref{sec-RProject}), it is possible to
begin a script with a line of code that sets the \textbf{working
directory} for the script using the function \texttt{setwd()}, e.g.:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{setwd}\NormalTok{(}\StringTok{"/Users/lefoll/Documents/UzK/RstatsTextbook/Dabrowska2019"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Afterwards, data files can be imported using a \textbf{relative path}
from the working directory just like we did earlier.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L1.data }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\AttributeTok{file =} \StringTok{"data/L1\_data.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

If you have to work with an \texttt{R} script that uses this method, you
will need to amend the path designated as the working directory by
\texttt{setwd()} to the corresponding path on your own computer. This
might not sound like much of an issue, but as data scientist, \texttt{R}
expert, and statistics professor Jenny Bryan (2017) explains:

\begin{quote}
The chance of the \texttt{setwd()} command having the desired effect --
making the file paths work -- for anyone besides its author is 0\%. It's
also unlikely to work for the author one or two years or computers from
now. The project is not self-contained and portable.
\end{quote}

In the language sciences, not everyone is aware of the severity of these
issues. Hence, it is not uncommon for researchers to make their scripts
even less reproducible by not setting a working directory at all and,
instead, relying exclusively on \textbf{absolute paths}
(Section~\ref{sec-FoldersPaths}). Hence, every time they want to import
data (and, as we will see later on, export objects from \texttt{R},
too), they write out the full file path in the command like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L1.data }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\AttributeTok{file =} \StringTok{"/Users/lefoll/Documents/UzK/RstatsTextbook/Dabrowska2019/data/L1\_data.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Having to work with such a script is particularly laborious because it
means that, if you inherit such a script from a colleague, you will have
to manually change every single file path in the script to the
corresponding file paths on your own computer. And, as Bryan (2017)
points out, this will also apply if you change anything in your own
computer directory structure! I hope I've made clear that the potential
for making errors in the process is far too important to even consider
going down that route.

However, should you have to use this method at some point for whatever
reason, you can make use of Section~\ref{sec-FoldersPaths} which
explained how to copy full file paths from a file Explorer or Finder
window. Note that if there are spaces or other special characters other
than \texttt{\_} or \texttt{-} anywhere in your file path, your import
command will fail (see Section~\ref{sec-FileNaming} on naming
conventions for folders and files). The following command, for instance,
will fail and return an error (see Figure~\ref{fig-ImportErrorSpace})
because the folder ``Uni Work'' contains a space.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L1.data }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\AttributeTok{file =} \StringTok{"/Users/lefoll/Documents/Uni Work/RstatsTextbook/Dabrowska2019/data/L1\_data.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\centering{

\includegraphics[width=4.39583in,height=\textheight,keepaspectratio]{images/ImportErrorSpace.png}

}

\caption{\label{fig-ImportErrorSpace}Error message due to an error in
the file path}

\end{figure}%

The only way to fix this issue is to remove the space in the name of the
folder (in your File Finder or Navigator window) and then amend the file
path in your \texttt{R} script accordingly.

\subsection{Importing data using RStudio's GUI
ðŸ«¤}\label{sec-ImportingGUI}

You may have noticed that, if you click on a data file from the Files
pane in \emph{RStudio} (Figure~\ref{fig-RStudioImportDataset}),
\emph{RStudio} will offer to import the dataset for you. This looks like
(and genuinely is) a very convenient way to import data in an \texttt{R}
session using RStudio's GUI (graphical user interface).

\begin{figure}

\centering{

\includegraphics[width=4.20833in,height=\textheight,keepaspectratio]{images/RStudioImportDataset.png}

}

\caption{\label{fig-RStudioImportDataset}Importing a file from RStudio's
File pane}

\end{figure}%

Clicking on `Import Dataset' opens up RStudio's `Import Text Data'
dialogue box, which is similar to the one that we saw in LibreOffice
Calc (Section~\ref{sec-DSVLibreOffice}). It allows you to select the
relevant options to correctly import the file and displays a preview to
check that the options that you have selected are correct. You can also
specify the name of the \texttt{R} object to which you want to assign
the imported data. By default, the name of the data file (minus the file
extension and any special characters) is suggested.

\begin{figure}

\centering{

\includegraphics[width=7.70833in,height=\textheight,keepaspectratio]{images/RStudioReadrImportTextDataDialogue.png}

}

\caption{\label{fig-RStudioReadrImportTextDataDialogue}RStudio's `Import
Text Data' dialogue}

\end{figure}%

As soon as you click on the `Import' button, the data are imported and
opened using the \texttt{View()} function for you to check the sanity of
the data.

This importing method works a treat, so what's not to like? Well, the
first problem is that you are not in full control. You cannot select
which import function is used; \emph{RStudio} decides for you. You may
have noticed that it chooses to use the \{readr\} import functions,
rather than the base \texttt{R} ones. There are lots of good reasons to
use the \{readr\} functions (see Section~\ref{sec-readr}), but it may
not be what you wanted to do. When we do research, it is important for
us to be in control of every step of the analysis process.

Second, your data import settings are not saved in an \texttt{.R} script
as the commands were only sent to the Console: they are part of your
script. This means that if you import your data in this way, do some
analyses, and then close RStudio, you will have no way of knowing with
which settings you imported the data to obtain the results of your
analysis! This can have serious consequences for the reproducibility of
your work.

Whilst there is no way of remedying the first issue, the second can
easily be fixed. After you have successfully imported your data from
RStudio's Files pane, you can (and should!) immediately copy the import
commands from the Console into your \texttt{.R} script. In this way, the
next time you want to re-run your analysis, you can begin by running
these import commands directly from your \texttt{.R} script rather than
by via RStudio's Files pane.

If you are running into errors due to incorrect file paths, it can be
useful to try to import your data using RStudio's GUI to see where you
are going wrong by comparing your own attempts with the import commands
that \emph{RStudio} generated.

\subsection*{Check your progress ðŸŒŸ}\label{check-your-progress-5}
\addcontentsline{toc}{subsection}{Check your progress ðŸŒŸ}

You have successfully completed { out of 12 questions} in this chapter.

Are you confident that you can\ldots?

\begin{itemize}
\tightlist
\item[$\square$]
  Access, save, and examine data from published studies
  (Section~\ref{sec-AccessingData}) - (Section~\ref{sec-SavingData})
\item[$\square$]
  Set up and use projects in \emph{RStudio} (Section~\ref{sec-RProject})
\item[$\square$]
  Import data from \texttt{.csv} files into \texttt{R}
  (Section~\ref{sec-ImportingDataCSV})
\item[$\square$]
  Check whether your data are correctly imported
  (Section~\ref{sec-ImportingErrors})
\item[$\square$]
  Import tabular data in other formats
  (Section~\ref{sec-ImportingTabularData})
\item[$\square$]
  Use the \{readr\} package to import tabular files
  (Section~\ref{sec-readr})
\item[$\square$]
  Import files with single and multiple tabs from spreadsheet software
  (Section~\ref{sec-SpreadsheetSoftware}) -
  (Section~\ref{sec-MultipleSheets})
\item[$\square$]
  Find out how to import data in other file formats
  (Section~\ref{sec-SPPS} - Section~\ref{sec-OtherFileFormats})
\end{itemize}

Now it's time to start exploring research data in \texttt{R}! In
\href{@sec-VaRiablesAndFunctions}{the next chapter}, you will learn how
to work with in-built \texttt{R} functions to find out more about the
DSV data from DÄ…browska (2019) that you imported in this chapter.

\bookmarksetup{startatroot}

\chapter{\texorpdfstring{Va\texttt{R}iables and
functions}{VaRiables and functions}}\label{sec-VaRiablesAndFunctions}

\subsubsection*{\texorpdfstring{\textbf{Chapter
overview}}{Chapter overview}}\label{chapter-overview-6}
\addcontentsline{toc}{subsubsection}{\textbf{Chapter overview}}

In this chapter, you will learn how to:

\begin{itemize}
\tightlist
\item
  Use base \texttt{R} functions to inspect a dataset
\item
  Inspect and access individual variables from a dataset
\item
  Access individual data points from a dataset
\item
  Use simple base \texttt{R} functions to describe variables
\item
  Look up and change the default arguments of functions
\item
  Combine functions using two methods
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-warning-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-warning-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Prerequisites}, titlerule=0mm, leftrule=.75mm]

In this chapter and the following chapters, all analyses are based on
data from:

\begin{quote}
DÄ…browska, Ewa. 2019. Experience, Aptitude, and Individual Differences
in Linguistic Attainment: A Comparison of Native and Nonnative Speakers.
Language Learning 69(S1). 72--100.
\url{https://doi.org/10.1111/lang.12323}.
\end{quote}

You will only be able to reproduce the analyses and answer the quiz
questions from this chapter if you have created an RProject and
successfully imported the two datasets from DÄ…browska (2019) into your
local \texttt{R} environment (see Figure~\ref{fig-DataLoaded}). Detailed
instructions to do so can be found from Section~\ref{sec-RProject} to
Section~\ref{sec-ImportingDataCSV}.

Alternatively, you can download \texttt{Dabrowska2019.zip} from
\href{https://github.com/elenlefoll/RstatsTextbook/raw/69d1e31be7394f2b612825f031ebffeb75886390/Dabrowska2019.zip}{the
textbook's GitHub repository}. To launch the project correctly, first
unzip the file and then double-click on the \texttt{Dabrowska2019.Rproj}
file.

Before we get started, import both L1 and the L2 datasets to your local
\texttt{R} environment:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(here)}

\NormalTok{L1.data }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\AttributeTok{file =} \FunctionTok{here}\NormalTok{(}\StringTok{"data"}\NormalTok{, }\StringTok{"L1\_data.csv"}\NormalTok{))}
\NormalTok{L2.data }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\AttributeTok{file =} \FunctionTok{here}\NormalTok{(}\StringTok{"data"}\NormalTok{, }\StringTok{"L2\_data.csv"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Check the Environment pane in RStudio to ensure that everything has gone
to plan (see Figure~\ref{fig-DataLoaded}).

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/RStudioEnvironment.png}}

}

\caption{\label{fig-DataLoaded}Environment pane of RStudio showing that
the datasets have been correctly loaded and are ready to be used in the
current \texttt{R} session}

\end{figure}%

\end{tcolorbox}

\section{\texorpdfstring{Inspecting a dataset in
\texttt{R}}{Inspecting a dataset in R}}\label{sec-InspectingData}

In Section~\ref{sec-ImportingErrors}, we saw that we can use the
\texttt{View()} function to display tabular data in a format that
resembles that of a spreadsheet programme (see
Figure~\ref{fig-ViewData1}).

The two datasets from DÄ…browska (2019) are both long and wide so you
will need to scroll in both directions to view all the data.
\emph{RStudio} also provides a filter option and a search tool (see
Figure~\ref{fig-ViewData1}). Note that both of these tools can only be
used to visually inspect the data. You cannot alter the dataset in any
way using these tools (and that's a good thing!).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{View}\NormalTok{(L1.data)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/ViewL1data.png}}

}

\caption{\label{fig-ViewData1}The L1.data object as visualised using the
\texttt{View()} function in RStudio}

\end{figure}%

In practice, it is often useful to printing subsets of a dataset in the
Console to quickly check the sanity of the data. To do so, we can use
the function \texttt{head()} that prints the first six rows of a tabular
dataset.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(L1.data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  Participant Age Gender             Occupation OccupGroup OtherLgs
1           1  21      M                Student         PS     None
2           2  38      M Student/Support Worker         PS     None
3           3  55      M                Retired          I     None
4           4  26      F           Web designer         PS     None
5           5  55      F              Homemaker          I     None
6           6  58      F                Retired          I     None
                    Education EduYrs ReadEng1 ReadEng2 ReadEng3 ReadEng Active
1              3rd year of BA     17        1        2        2       5      8
2    NVQ IV Music Performance     13        1        2        3       6      8
3 No formal (City and Guilds)     11        3        3        4      10      8
4                 BA Fine Art     17        3        3        3       9      8
5                    O'Levels     12        3        2        3       8      8
6                    O'Levels     12        1        1        2       4      8
  ObjCl ObjRel Passive Postmod Q.has Q.is Locative SubCl SubRel GrammarR
1     8      8       8       8     8    6        8     8      8       78
2     8      8       8       8     8    7        8     8      8       79
3     8      8       8       8     7    8        8     8      8       79
4     8      8       8       8     8    8        8     8      8       80
5     8      8       8       8     8    7        8     8      8       79
6     5      1       8       8     7    6        7     8      8       66
  Grammar VocabR    Vocab CollocR Colloc Blocks ART LgAnalysis
1    95.0     48 73.33333      30 68.750     16  17         15
2    97.5     58 95.55556      35 84.375     11  31         13
3    97.5     58 95.55556      31 71.875      5  38          5
4   100.0     53 84.44444      37 90.625     20  26         15
5    97.5     55 88.88889      36 87.500     16  31         14
6    65.0     48 73.33333      21 40.625      8  15          3
\end{verbatim}

\section{Working with variables}\label{sec-Variables}

\subsection{Types of variables}\label{types-of-variables}

In statistics, we differentiate between \textbf{numeric} (or
\textbf{quantitative}) and \textbf{categorical} (or
\textbf{qualitative}) variables. Each variable type can be subdivided
into different subtypes. It is very important to understand the
differences between these types of data as we frequently have to use
different statistics and visualisations depending on the type(s) of
variable(s) that we are dealing with.

Some \textbf{numeric variables} are \textbf{continuous}: they contain
measured data that, at least theoretically, can have an infinite number
of values within a range (e.g.~time). In practice, however the number of
possible values depends on the precision of the measurement (e.g.~are we
measuring time in years, as in the age of adults, or milliseconds, as in
participants' reaction times in a linguistic experiment).
\textbf{Numeric variables} for which only a defined set of values are
possible are called \textbf{discrete} variables (e.g.~number of
occurrences of a word in a corpus). Most often, discrete numeric
variables represent counts of something.

\href{https://allisonhorst.com}{\includegraphics[width=5.30208in,height=\textheight,keepaspectratio]{images/AHorst_NumericVariables.png}}

\textbf{Categorical variables} can be \textbf{nominal} or
\textbf{ordinal}. Nominal variables contain unordered categorical values
(e.g.~participants' mother tongue or nationality), whereas ordinal
variables have categorical values that can be ordered meaningfully
(e.g.~participants' proficiency in a specific language where the values
\emph{beginner, intermediate} and \emph{advanced} or \emph{A1},
\emph{A2}, \emph{B1}, \emph{B2}, \emph{C1} and \emph{C2} have a
meaningful order). However, the difference between each category (or
level) is not necessarily equal. \textbf{Binary} variables are a special
case of nominal variable which only has two mutually exclusive outcomes
(e.g.~\emph{true} or \emph{false} in a quiz question).

\href{https://allisonhorst.com/}{\pandocbounded{\includegraphics[keepaspectratio]{images/AHorst_CategoricalVariables.png}}}

\subsection{\texorpdfstring{Inspecting variables in
\texttt{R}}{Inspecting variables in R}}\label{inspecting-variables-in-r}

In \textbf{tidy data} tabular formats (see Chapter 8), each row
corresponds to one observation and each column to a variable. Each cell,
therefore, corresponds to a single data point, which is the value of a
specific variable (column) for a specific observation (row). As we will
see in the following chapters, this data structure allows for efficient
and intuitive data manipulation, analysis, and visualisation.

The \texttt{names()} functions returns the names of all of the columns
of a data frame. Given that the datasets from DÄ…browska (2019) are
`tidy', this means that \texttt{names(L1.data)} returns a list of all
the column names in the L1 dataset.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(L1.data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "Participant" "Age"         "Gender"      "Occupation"  "OccupGroup" 
 [6] "OtherLgs"    "Education"   "EduYrs"      "ReadEng1"    "ReadEng2"   
[11] "ReadEng3"    "ReadEng"     "Active"      "ObjCl"       "ObjRel"     
[16] "Passive"     "Postmod"     "Q.has"       "Q.is"        "Locative"   
[21] "SubCl"       "SubRel"      "GrammarR"    "Grammar"     "VocabR"     
[26] "Vocab"       "CollocR"     "Colloc"      "Blocks"      "ART"        
[31] "LgAnalysis" 
\end{verbatim}

\subsection{\texorpdfstring{\texttt{R} data
types}{R data types}}\label{sec-DataTypes}

A useful way to get a quick and informative overview of a large dataset
is to use the function \texttt{str()}, which was mentioned in
Section~\ref{sec-ImportingErrors}. It returns the ``internal structure''
of any \texttt{R} object. It is particular useful for large tables with
many columns

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(L1.data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'data.frame':   90 obs. of  31 variables:
 $ Participant: chr  "1" "2" "3" "4" ...
 $ Age        : int  21 38 55 26 55 58 31 58 42 59 ...
 $ Gender     : chr  "M" "M" "M" "F" ...
 $ Occupation : chr  "Student" "Student/Support Worker" "Retired" "Web designer" ...
 $ OccupGroup : chr  "PS" "PS" "I" "PS" ...
 $ OtherLgs   : chr  "None" "None" "None" "None" ...
 $ Education  : chr  "3rd year of BA" "NVQ IV Music Performance" "No formal (City and Guilds)" "BA Fine Art" ...
 $ EduYrs     : int  17 13 11 17 12 12 13 11 11 11 ...
 $ ReadEng1   : int  1 1 3 3 3 1 3 2 1 2 ...
 $ ReadEng2   : int  2 2 3 3 2 1 2 2 1 2 ...
 $ ReadEng3   : int  2 3 4 3 3 2 3 3 1 2 ...
 $ ReadEng    : int  5 6 10 9 8 4 8 7 3 6 ...
 $ Active     : int  8 8 8 8 8 8 7 8 8 8 ...
 $ ObjCl      : int  8 8 8 8 8 5 8 4 7 5 ...
 $ ObjRel     : int  8 8 8 8 8 1 8 8 3 8 ...
 $ Passive    : int  8 8 8 8 8 8 8 8 2 8 ...
 $ Postmod    : int  8 8 8 8 8 8 7 7 6 8 ...
 $ Q.has      : int  8 8 7 8 8 7 8 1 3 0 ...
 $ Q.is       : int  6 7 8 8 7 6 7 8 7 8 ...
 $ Locative   : int  8 8 8 8 8 7 8 8 8 8 ...
 $ SubCl      : int  8 8 8 8 8 8 8 8 7 8 ...
 $ SubRel     : int  8 8 8 8 8 8 8 8 7 8 ...
 $ GrammarR   : int  78 79 79 80 79 66 77 68 58 69 ...
 $ Grammar    : num  95 97.5 97.5 100 97.5 65 92.5 70 45 72.5 ...
 $ VocabR     : int  48 58 58 53 55 48 39 48 31 42 ...
 $ Vocab      : num  73.3 95.6 95.6 84.4 88.9 ...
 $ CollocR    : int  30 35 31 37 36 21 29 33 22 29 ...
 $ Colloc     : num  68.8 84.4 71.9 90.6 87.5 ...
 $ Blocks     : int  16 11 5 20 16 8 8 10 7 9 ...
 $ ART        : int  17 31 38 26 31 15 7 10 6 6 ...
 $ LgAnalysis : int  15 13 5 15 14 3 4 5 2 6 ...
\end{verbatim}

At the top of its output, the function \texttt{str(L1.data)} first
informs us that \texttt{L1.data} is a \textbf{data frame} object,
consisting of 90 observations (i.e.~rows) and 31 variables
(i.e.~columns). Then, it returns a list of all of the variables included
in this data frame. Each line starts with a \texttt{\$} sign and
corresponds to one column. First, the name of the column
(e.g.~\texttt{Occupation}) is printed, followed by the column's
\texttt{R} data type (e.g.~\texttt{chr} for a character string vector),
and then its values for the first few rows of the table (e.g.~we can see
that the first participant in this dataset was a ``Student'' and the
second a ``Student/Support Worker'').

Compare the outputs of the \texttt{str()} and \texttt{head()} functions
in the Console with that of the \texttt{View()} function to understand
the different ways in which the same dataset can be examined in RStudio.

\subsection{\texorpdfstring{Accessing individual columns in
\texttt{R}}{Accessing individual columns in R}}\label{sec-DollarSign}

We can call up individual columns within a data frame using the
\texttt{\$} operator. This displays all of the participants' values for
this one variable. As shown below, this works for any type of data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L1.data}\SpecialCharTok{$}\NormalTok{Gender}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "M" "M" "M" "F" "F" "F" "F" "M" "M" "F" "F" "M" "M" "F" "M" "F" "M" "F" "F"
[20] "F" "F" "F" "F" "F" "F" "M" "F" "M" "F" "M" "F" "F" "F" "M" "F" "F" "M" "F"
[39] "F" "F" "F" "F" "M" "M" "F" "F" "M" "F" "F" "F" "F" "F" "F" "F" "M" "M" "M"
[58] "F" "F" "M" "M" "M" "M" "F" "M" "M" "M" "M" "M" "M" "M" "M" "F" "M" "F" "F"
[77] "M" "M" "M" "F" "F" "M" "M" "F" "F" "M" "M" "M" "F" "M"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L1.data}\SpecialCharTok{$}\NormalTok{Age}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] 21 38 55 26 55 58 31 58 42 59 32 27 60 51 32 29 41 57 60 18 41 60 21 25 26
[26] 60 57 60 52 25 23 42 59 30 21 21 60 51 62 65 19 65 29 38 37 42 20 32 29 29
[51] 27 28 29 25 33 25 25 25 52 25 53 22 65 60 61 65 65 61 30 30 32 30 39 29 55
[76] 18 32 31 20 38 44 18 17 17 17 17 17 17 17 17
\end{verbatim}

Before doing any data analysis, it is crucial to carefully visually
examine the data to spot any problems. Ask yourself:

\begin{itemize}
\tightlist
\item
  Do the values look plausible?
\item
  Are there any missing values?
\end{itemize}

Looking at the \texttt{Gender} and \texttt{Age} variables, we can see
that all the L1 participants declared being either `male' (\texttt{"M"})
or `female' (\texttt{"F"}), that the youngest were 17 years old, and
that no participant was improbably old. A single improbable value is
likely to be the result of a data entry error, e.g.~a participant or
researcher accidentally entered \texttt{188} as an age, instead of
\texttt{18}. If you spot lots of improbable or outright weird values
(e.g.~\texttt{C}, \texttt{I} and \texttt{PS} as age values!), something
is likely to have gone wrong during the data import process (see
Section~\ref{sec-ImportingErrors}).

Just like we can save individual numbers and words as \texttt{R} objects
to our \texttt{R} environment, we can also save individual variables as
individual \texttt{R} objects. As we saw in
Section~\ref{sec-WorkingRObjects}, in this case, the values of the
variable are not printed in the Console, but rather saved to our
\texttt{R} environment.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L1.Occupation }\OtherTok{\textless{}{-}}\NormalTok{ L1.data}\SpecialCharTok{$}\NormalTok{Occupation}
\end{Highlighting}
\end{Shaded}

If we want to display the content of this variable, we must print our
new \texttt{R} object by calling it up with its name,
e.g.~\texttt{L1.Occupation}. Try it out! As listing all of the L1
participant's jobs makes for a very long list, below, we only display
the first six values using the \texttt{head()} function.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(L1.Occupation)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Student"                "Student/Support Worker" "Retired"               
[4] "Web designer"           "Homemaker"              "Retired"               
\end{verbatim}

\section{\texorpdfstring{Accessing individual data points in
\texttt{R}}{Accessing individual data points in R}}\label{sec-SquareBrackets}

We can also access individual data points from a variable using the
index operator, the square brackets (\texttt{{[}{]}}). For example, we
can access the \texttt{Occupation} value for the fourth L1 participant
by specifying that we only want the fourth element of the \texttt{R}
object \texttt{L1.Occupation}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L1.Occupation[}\DecValTok{4}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Web designer"
\end{verbatim}

We can also do this from the \texttt{L1.data} data frame object
directly. To this end, we use a combination of the \texttt{\$} and the
\texttt{{[}{]}} operators.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L1.data}\SpecialCharTok{$}\NormalTok{Occupation[}\DecValTok{4}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Web designer"
\end{verbatim}

We can access a continuous range of data points using the \texttt{:}
operator.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L1.data}\SpecialCharTok{$}\NormalTok{Occupation[}\DecValTok{10}\SpecialCharTok{:}\DecValTok{15}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Housewife"             "Admin Assistant"       "Content Editor"       
[4] "School Crossing Guard" "Carer/Cleaner"         "IT Support"           
\end{verbatim}

Or, if they are not continuous, we can list the numbers of the values
that we are interesting in using the combine function (\texttt{c()}) and
commas separating each index value.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L1.data}\SpecialCharTok{$}\NormalTok{Occupation[}\FunctionTok{c}\NormalTok{(}\DecValTok{11}\NormalTok{,}\DecValTok{13}\NormalTok{,}\DecValTok{29}\NormalTok{,}\DecValTok{90}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Admin Assistant"       "School Crossing Guard" "Dental Nurse"         
[4] "Student"              
\end{verbatim}

It is also possible to access data points from a table by specifying
both the number of the row and the number of the column of the relevant
data point(s) using the following pattern:

\begin{verbatim}
[row,column]
\end{verbatim}

For example, given that we know that \texttt{Occupation} is stored in
the fourth column of \texttt{L1.data}, we can find out the occupation of
the L1 participant in the 60\textsuperscript{th} row of the dataset like
this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L1.data[}\DecValTok{60}\NormalTok{,}\DecValTok{4}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Train Driver"
\end{verbatim}

All of these approaches can be combined. For example, here we access the
values of the second, third, and fourth columns for the
11\textsuperscript{th}, 13\textsuperscript{th}, 29\textsuperscript{th},
and 90\textsuperscript{th} L1 participants.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L1.data[}\FunctionTok{c}\NormalTok{(}\DecValTok{11}\NormalTok{,}\DecValTok{13}\NormalTok{,}\DecValTok{29}\NormalTok{,}\DecValTok{90}\NormalTok{),}\DecValTok{2}\SpecialCharTok{:}\DecValTok{4}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   Age Gender            Occupation
11  32      F       Admin Assistant
13  60      M School Crossing Guard
29  52      F          Dental Nurse
90  17      M               Student
\end{verbatim}

\section{\texorpdfstring{Using built-in \texttt{R}
functions}{Using built-in R functions}}\label{sec-RFunctions}

We know from our examination of the L1 dataset from DÄ…browska (2019)
that it includes 90 English native speaker participants. To find out
their mean average age, we could add up all of their ages and divide the
sum by 90 (see Section~\ref{sec-CentralTendency} for more ways to report
the central tendency of a variable).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\DecValTok{21} \SpecialCharTok{+} \DecValTok{38} \SpecialCharTok{+} \DecValTok{55} \SpecialCharTok{+} \DecValTok{26} \SpecialCharTok{+} \DecValTok{55} \SpecialCharTok{+} \DecValTok{58} \SpecialCharTok{+} \DecValTok{31} \SpecialCharTok{+} \DecValTok{58} \SpecialCharTok{+} \DecValTok{42} \SpecialCharTok{+} \DecValTok{59} \SpecialCharTok{+} \DecValTok{32} \SpecialCharTok{+} \DecValTok{27} \SpecialCharTok{+} \DecValTok{60} \SpecialCharTok{+} \DecValTok{51} \SpecialCharTok{+} \DecValTok{32} \SpecialCharTok{+} \DecValTok{29} \SpecialCharTok{+} \DecValTok{41} \SpecialCharTok{+} \DecValTok{57} \SpecialCharTok{+} \DecValTok{60} \SpecialCharTok{+} \DecValTok{18} \SpecialCharTok{+} \DecValTok{41} \SpecialCharTok{+} \DecValTok{60} \SpecialCharTok{+} \DecValTok{21} \SpecialCharTok{+} \DecValTok{25} \SpecialCharTok{+} \DecValTok{26} \SpecialCharTok{+} \DecValTok{60} \SpecialCharTok{+} \DecValTok{57} \SpecialCharTok{+} \DecValTok{60} \SpecialCharTok{+} \DecValTok{52} \SpecialCharTok{+} \DecValTok{25} \SpecialCharTok{+} \DecValTok{23} \SpecialCharTok{+} \DecValTok{42} \SpecialCharTok{+} \DecValTok{59} \SpecialCharTok{+} \DecValTok{30} \SpecialCharTok{+} \DecValTok{21} \SpecialCharTok{+} \DecValTok{21} \SpecialCharTok{+} \DecValTok{60} \SpecialCharTok{+} \DecValTok{51} \SpecialCharTok{+} \DecValTok{62} \SpecialCharTok{+} \DecValTok{65} \SpecialCharTok{+} \DecValTok{19} \SpecialCharTok{+} \DecValTok{65} \SpecialCharTok{+} \DecValTok{29} \SpecialCharTok{+} \DecValTok{38} \SpecialCharTok{+} \DecValTok{37} \SpecialCharTok{+} \DecValTok{42} \SpecialCharTok{+} \DecValTok{20} \SpecialCharTok{+} \DecValTok{32} \SpecialCharTok{+} \DecValTok{29} \SpecialCharTok{+} \DecValTok{29} \SpecialCharTok{+} \DecValTok{27} \SpecialCharTok{+} \DecValTok{28} \SpecialCharTok{+} \DecValTok{29} \SpecialCharTok{+} \DecValTok{25} \SpecialCharTok{+} \DecValTok{33} \SpecialCharTok{+} \DecValTok{25} \SpecialCharTok{+} \DecValTok{25} \SpecialCharTok{+} \DecValTok{25} \SpecialCharTok{+} \DecValTok{52} \SpecialCharTok{+} \DecValTok{25} \SpecialCharTok{+} \DecValTok{53} \SpecialCharTok{+} \DecValTok{22} \SpecialCharTok{+} \DecValTok{65} \SpecialCharTok{+} \DecValTok{60} \SpecialCharTok{+} \DecValTok{61} \SpecialCharTok{+} \DecValTok{65} \SpecialCharTok{+} \DecValTok{65} \SpecialCharTok{+} \DecValTok{61} \SpecialCharTok{+} \DecValTok{30} \SpecialCharTok{+} \DecValTok{30} \SpecialCharTok{+} \DecValTok{32} \SpecialCharTok{+} \DecValTok{30} \SpecialCharTok{+} \DecValTok{39} \SpecialCharTok{+} \DecValTok{29} \SpecialCharTok{+} \DecValTok{55} \SpecialCharTok{+} \DecValTok{18} \SpecialCharTok{+} \DecValTok{32} \SpecialCharTok{+} \DecValTok{31} \SpecialCharTok{+} \DecValTok{20} \SpecialCharTok{+} \DecValTok{38} \SpecialCharTok{+} \DecValTok{44} \SpecialCharTok{+} \DecValTok{18} \SpecialCharTok{+} \DecValTok{17} \SpecialCharTok{+} \DecValTok{17} \SpecialCharTok{+} \DecValTok{17} \SpecialCharTok{+} \DecValTok{17} \SpecialCharTok{+} \DecValTok{17} \SpecialCharTok{+} \DecValTok{17} \SpecialCharTok{+} \DecValTok{17} \SpecialCharTok{+} \DecValTok{17}\NormalTok{) }\SpecialCharTok{/} \DecValTok{90}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 37.54444
\end{verbatim}

Of course, we would much rather not write all of this out! Especially,
as we are very likely to make errors in the process. Instead, we can use
the base \texttt{R} function \texttt{sum()} to add up all of the L1
participant's ages and divide that by 90.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sum}\NormalTok{(L1.data}\SpecialCharTok{$}\NormalTok{Age) }\SpecialCharTok{/} \DecValTok{90}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 37.54444
\end{verbatim}

This already looks much better, but it's still less than ideal: What if
we decided to exclude some participants (e.g.~because they did not
complete all of the experimental tasks)? Or decided to add data from
more participants? In both these cases, 90 will no longer be the correct
denominator to calculate their average age! That's why it is better to
work out the denominator by computing the total number of values in the
variable of interest. To this end, we can use the \texttt{length()}
function, which returns the number of values in any given vector.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{length}\NormalTok{(L1.data}\SpecialCharTok{$}\NormalTok{Age)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 90
\end{verbatim}

We can then combine the \texttt{sum()} and the \texttt{length()}
functions to calculate the participants' average age.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sum}\NormalTok{(L1.data}\SpecialCharTok{$}\NormalTok{Age) }\SpecialCharTok{/} \FunctionTok{length}\NormalTok{(L1.data}\SpecialCharTok{$}\NormalTok{Age)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 37.54444
\end{verbatim}

Base \texttt{R} includes lots of useful functions, especially to do
statistics. Hence, it will come as no surprise to find that there is a
built-in function to calculate mean average values. It is called
\texttt{mean()} and is very simple to use.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(L1.data}\SpecialCharTok{$}\NormalTok{Age)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 37.54444
\end{verbatim}

If you save the values of a variable to your \texttt{R} session
environment, you do not need to use the name of the dataset and the
\texttt{\$} sign to calculate its mean. Instead, you can directly apply
the \texttt{mean()} function to the stored \texttt{R} object.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Saving the values of the Age variable to a new R object called L1.Age:}
\NormalTok{L1.Age }\OtherTok{\textless{}{-}}\NormalTok{ L1.data}\SpecialCharTok{$}\NormalTok{Age}

\CommentTok{\# Applying the mean() function to this new R object:}
\FunctionTok{mean}\NormalTok{(L1.Age)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 37.54444
\end{verbatim}

\subsection{Function arguments}\label{function-arguments}

All of the functions that we have looked at this chapter so far work
with just a single argument: either a vector of values (e.g.~a variable
from our dataset as in \texttt{mean(L1.data\$Age)}) or an entire tabular
dataset (e.g.~\texttt{str(L1.data)}). When we looked at the
\texttt{head()} function, we saw that, per default, it displays the
first six rows but that we can change this by specifying a second
argument in the function. In \texttt{R}, arguments within a function are
always separated by a comma.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(L1.Age, }\AttributeTok{n =} \DecValTok{6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 21 38 55 26 55 58
\end{verbatim}

The names of the argument can be specified but do not have to be if they
are listed in the order specified in the documentation. You can check
the `Usage' section of a function's help file (e.g.~using
\texttt{help(head)} function or \texttt{?head}) to find out the order of
the arguments. Run the following commands and compare their output:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(}\AttributeTok{x =}\NormalTok{ L1.Age, }\AttributeTok{n =} \DecValTok{6}\NormalTok{)}
\FunctionTok{head}\NormalTok{(L1.Age, }\DecValTok{6}\NormalTok{)}
\FunctionTok{head}\NormalTok{(}\AttributeTok{n =} \DecValTok{6}\NormalTok{, }\AttributeTok{x =}\NormalTok{ L1.Age)}
\FunctionTok{head}\NormalTok{(}\DecValTok{6}\NormalTok{, L1.Age)}
\end{Highlighting}
\end{Shaded}

Whilst the first three return exactly the same output, the fourth
returns an error because the argument names are not specified and are
not in the order specified in the function's help file. To avoid making
errors and confusing your collaborators and/or future self, it's good
practice to explicitly name all the arguments except the most obvious
ones.

\section{\texorpdfstring{Combining functions in
\texttt{R}}{Combining functions in R}}\label{combining-functions-in-r}

Combining functions is where the real fun starts with programming! In
Section~\ref{sec-RFunctions}, we already combined two functions using a
mathematical operator (\texttt{/}). But what if we want to compute L1
participant's average age to two decimal places? To do this, we need to
combine the \texttt{mean()} function and the \texttt{round()} function.
We can do this in two steps.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Step 1:}
\NormalTok{L1.mean.age }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(L1.Age)}

\CommentTok{\# Step 2:}
\FunctionTok{round}\NormalTok{(L1.mean.age, }\AttributeTok{digits =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 37.54
\end{verbatim}

In step 1, we compute the mean value and save it as an \texttt{R} object
and, in step 2, we pass this object through the \texttt{round()}
function with the argument \texttt{digits\ =\ 2}. There is nothing wrong
with this method, but it often require lots of intermediary \texttt{R}
objects, which can get rather tiresome.

In the following, we will look at two further ways to combine functions
in \texttt{R}: nesting and piping.

\subsection{Nested functions}\label{sec-Nesting}

The first method involves lots of brackets (also known as
`parentheses'). This is because in nested functions, one function is
placed inside another function. The inner function is evaluated first,
and its result is passed to the next outer function. Here's an example:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(}\FunctionTok{mean}\NormalTok{(L1.Age))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 38
\end{verbatim}

In this example, the \texttt{mean()} function is nested inside the
\texttt{round()} function. The \texttt{mean()} function calculates the
mean of \texttt{L1.Age}, and the result is passed to the
\texttt{round()} function, which rounds the result to the nearest
integer.

You can also pass additional arguments to any of the functions, but you
must make sure that you place the arguments within the correct set of
brackets.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(}\FunctionTok{mean}\NormalTok{(L1.Age), }\AttributeTok{digits =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 37.54
\end{verbatim}

In this example, the argument \texttt{digits\ =\ 2} belongs to the outer
function \texttt{round()}; hence it must be placed within the outer set
of brackets.

In theory, you can nest as many functions as you like, but things can
get quite chaotic after more than a couple of functions. You need to
make sure that you can trace back which arguments and which brackets
belong to which function (see Figure~\ref{fig-NestedFunctions}).

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/NestedFunctions.png}}

}

\caption{\label{fig-NestedFunctions}A schematic representations of a)
one function with two arguments, b) two nested functions each with two
arguments, and c) three nested functions each with two arguments}

\end{figure}%

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-tip-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Time to think!}, titlerule=0mm, leftrule=.75mm]

Consider the three lines of code below. Without running them, can you
tell which of the three lines of code will output the square root of L1
participant's average age to two decimal places?

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(}\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{mean}\NormalTok{(L1.Age) }\AttributeTok{digits =} \DecValTok{2}\NormalTok{))}

\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{round}\NormalTok{(}\FunctionTok{mean}\NormalTok{(L1.Age), }\AttributeTok{digits =} \DecValTok{2}\NormalTok{))}

\FunctionTok{round}\NormalTok{(}\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{mean}\NormalTok{(L1.Age)), }\AttributeTok{digits =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The first line will return an ``unexpected symbol'' error because it is
missing a comma before the argument \texttt{digits\ =\ 2}. The second
line actually outputs \texttt{6.126989}, which has more than two decimal
places! This is because \texttt{R} interprets the functions from the
inside out: first, it calculates the mean value, then it rounds that off
to two decimal places, and only then does it compute the square root of
that rounded off value. The third line, in contrast, does the rounding
operation as the last step. Note that, in the two lines of code that do
not produce an error, the brackets around the argument
\texttt{digits\ =\ 2} are also located in different places.

It is very easy to make bracketing errors when writing code and
especially so when nesting functions (see
Figure~\ref{fig-NestedFunctions}). Watch your commas and brackets (see
also Section~\ref{sec-Errors})!

\end{tcolorbox}

\subsection{Piped Functions}\label{sec-Piping}

If you found all these brackets overwhelming: fear not! There is a
second method for combining functions in \texttt{R}, which is often more
convenient and almost always easier to decipher. It involves the pipe
operator, which in \texttt{R} is
\texttt{\textbar{}\textgreater{}}.\footnote{This is the \textbf{native R
  pipe} operator, which was introduced in May 2021 with \texttt{R}
  version 4.1.0. As a result, you will not find it in code written in
  earlier versions of \texttt{R}. Previously, piping required an
  additional \texttt{R} library, the \{magrittr\} library. The
  \{magrittr\} pipe looks like this: \texttt{\%\textgreater{}\%}. At
  first sight, they appear to work is in the same way, but there are
  some important differences. If you are familiar with the \{magrittr\}
  pipe and want to understand how it differs from the native R pipe, I
  recommend this excellent blog post by Isabella VelÃ¡squez:
  \url{https://ivelasq.rbind.io/blog/understanding-the-r-pipe/}.}

The \texttt{\textbar{}\textgreater{}} operator passes the output of one
function on to the first argument of the next function. This allows us
to chain multiple functions together in a much more intuitive way.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L1.Age }\SpecialCharTok{|\textgreater{}} 
 \FunctionTok{mean}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
 \FunctionTok{round}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 38
\end{verbatim}

In this example, the object \texttt{L1.Age} is passed on to the first
argument of the \texttt{mean()} function. This calculates the mean of
\texttt{L1.Age}. Next, this result is passed to the \texttt{round()}
function, which rounds the mean value to the nearest integer.

To pass additional arguments to any function in the pipeline, we add
them within the brackets that belong to that function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L1.Age }\SpecialCharTok{|\textgreater{}} 
 \FunctionTok{mean}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
 \FunctionTok{round}\NormalTok{(}\AttributeTok{digits =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 37.54
\end{verbatim}

Like many of the relational operators we saw in
Section~\ref{sec-RelationalOperators}, the \texttt{R} pipe is a
combination of two symbols, the computer pipe symbol \texttt{\textbar{}}
and the right angle bracket \texttt{\textgreater{}}. Don't worry if
you're not sure where these two symbols are on your keyboard as
\emph{RStudio} has a handy shortcut for you:
\texttt{Ctrl/Cmd\ +\ Shift\ +\ M}\footnote{If, in your version of
  RStudio, this shortcut produces \texttt{\%\textgreater{}\%} instead of
  \texttt{\textbar{}\textgreater{}}, you have probably not activated the
  native \texttt{R} pipe option in your \emph{RStudio} global options
  (see instructions in Section~\ref{sec-GlobalOptions}).} (see
Figure~\ref{fig-Pipes}). I strongly recommend that you write this
shortcut on a prominent post-it and learn it asap, as you will need it a
lot when you are working in \texttt{R}!

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/NativeRPipe.png}}

}

\caption{\label{fig-Pipes}Remix of RenÃ© Magritte's ``La Trahison des
images'' with the native \texttt{R} pipe and its RStudio shortcut (Le
Foll 2025. Zenodo. \url{https://doi.org/10.5281/zenodo.17440405})}

\end{figure}%

\subsection*{Check your progress ðŸŒŸ}\label{check-your-progress-6}
\addcontentsline{toc}{subsection}{Check your progress ðŸŒŸ}

You have successfully completed { out of 19 questions} in this chapter.

Are you confident that you can\ldots?

\begin{itemize}
\tightlist
\item[$\square$]
  Inspect a data set in \texttt{R} (Section~\ref{sec-InspectingData})
\item[$\square$]
  Recognise different types of variables (Section~\ref{sec-Variables})
\item[$\square$]
  Access individual columns and data points in \texttt{R}
  (Section~\ref{sec-DollarSign}) - (Section~\ref{sec-SquareBrackets})
\item[$\square$]
  Use built-in \texttt{R} functions and change function arguments
  (Section~\ref{sec-RFunctions})
\item[$\square$]
  Combine functions in \texttt{R} using both the nesting and the piping
  methods (Section~\ref{sec-Nesting}) - (Section~\ref{sec-Piping})
\end{itemize}

You are now ready to some statistics in \texttt{R}! In
\href{@sec-DescRiptiveStats}{Chapter 8}, we begin with descriptive
statistics.

\bookmarksetup{startatroot}

\chapter{\texorpdfstring{Desc\texttt{R}iptive
statistics}{DescRiptive statistics}}\label{sec-DescRiptiveStats}

\subsection*{Chapter overview}\label{chapter-overview-7}
\addcontentsline{toc}{subsection}{Chapter overview}

In this chapter, you will learn how to:

\begin{itemize}
\tightlist
\item
  Choose and interpret different measures of central tendency
\item
  Calculate the mode, mean, and median of a numeric variable in
  \texttt{R}
\item
  Interpret histograms and density plots
\item
  Recognise the characteristics of a normally distributed variable
\item
  Interpret and calculate the interquartile range in \texttt{R}
\item
  Interpret boxplots
\item
  Interpret and calculate the standard deviation in \texttt{R}
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-warning-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-warning-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Prerequisites}, titlerule=0mm, leftrule=.75mm]

In this chapter and the following chapters, all analyses are based on
data from:

\begin{quote}
DÄ…browska, Ewa. 2019. Experience, Aptitude, and Individual Differences
in Linguistic Attainment: A Comparison of Native and Nonnative Speakers.
Language Learning 69(S1). 72--100.
\url{https://doi.org/10.1111/lang.12323}.
\end{quote}

You will only be able to reproduce the analyses and answer the quiz
questions from this chapter if you have created an RProject and
successfully imported the two datasets from DÄ…browska (2019) into your
local \texttt{R} environment (see Figure~\ref{fig-DataLoaded}). Detailed
instructions to do so can be found from Section~\ref{sec-RProject} to
Section~\ref{sec-ImportingDataCSV}.

Alternatively, you can download \texttt{Dabrowska2019.zip} from
\href{https://github.com/elenlefoll/RstatsTextbook/raw/69d1e31be7394f2b612825f031ebffeb75886390/Dabrowska2019.zip}{the
textbook's GitHub repository}. To launch the project correctly, first
unzip the file and then double-click on the \texttt{Dabrowska2019.Rproj}
file.

Before we get started, make sure that both the L1 and the L2 datasets
are correctly loaded by checking the structure of the \texttt{R} objects
using the \texttt{str()} function.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(here)}

\NormalTok{L1.data }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\AttributeTok{file =} \FunctionTok{here}\NormalTok{(}\StringTok{"data"}\NormalTok{, }\StringTok{"L1\_data.csv"}\NormalTok{))}
\FunctionTok{str}\NormalTok{(L1.data)}

\NormalTok{L2.data }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\AttributeTok{file =} \FunctionTok{here}\NormalTok{(}\StringTok{"data"}\NormalTok{, }\StringTok{"L2\_data.csv"}\NormalTok{))}
\FunctionTok{str}\NormalTok{(L2.data)}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\section{Measures of central tendency}\label{sec-CentralTendency}

In Section~\ref{sec-RFunctions}, we calculated the \textbf{mean} average
age of L1 and L2 participants. Averages are a very useful way to
describe the central tendency of a numeric variable - both in science
and everyday life. For example, it is useful for me to know if a
particular bus journey lasts, on average, 12 minutes or 45 minutes. As
it's an average value, I am not expecting it to last exactly 12 or 45
minutes, but the average duration is nonetheless helpful to plan my
schedule.

In science, we use \textbf{averages} to describe the central tendency of
numeric variables that are too large for us to be able to examine every
single data point. With very small datasets, averages are unnecessary.
Imagine that a Breton\footnote{Breton is the Celtic language of Brittany
  (now in North-West France). With around 216,000 active speakers
  (\href{https://en.wikipedia.org/wiki/Breton_language}{Wikipedia},
  26/08/2024), Breton is classified as `severely endangered' in the
  UNESCO's
  \href{https://en.wikipedia.org/wiki/Atlas_of_the_World\%27s_Languages_in_Danger}{Atlas
  of the World's Languages in Danger}. It would presumably be quite a
  feat to put together a class of five Breton learners in Fiji, an
  island country far removed from Brittany in the South Pacific Ocean
  with fewer than one million inhabitants
  (\href{https://en.wikipedia.org/wiki/Fiji}{Wikipedia}, 26/08/2024)!}
language class in Fiji has five students. Their teacher hardly needs to
calculate an average of the students' vocabulary test results to get an
understanding of how her students are doing. She can simply examine all
five results!

Not only are averages of very small datasets unnecessary, they can, in
fact, be misleading. Imagine that the five Breton learners got the
following results (out of 100) on their vocabulary test:

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{89}\NormalTok{, }\DecValTok{91}\NormalTok{, }\DecValTok{86}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{82}
\end{Highlighting}
\end{Shaded}

If we calculate the average result of the class, we get:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{89}\NormalTok{, }\DecValTok{91}\NormalTok{, }\DecValTok{86}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{82}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 70.6
\end{verbatim}

This average grade does not describe very well how \emph{any} of the
students did: Four did much better than that, while one did considerably
worse! The results of quantitative studies, however, typically involve
much larger datasets so that averages \emph{can} be a very useful way to
describe central tendencies within the data. But it's important to
understand that, depending on the data, different \textbf{measures of
central tendency} make sense. Later on, we will also see that measures
of central tendency do not suffice to describe numeric variables:
\textbf{measures of variability} (Section~\ref{sec-Variability}) and
good \textbf{data visualisation} (\href{@sec-DataViz}{Chapter 10}) are
also crucial.

\subsection{Mean}\label{sec-Mean}

The measure of central tendency that we have looked at so far is the
\textbf{arithmetic mean}. When people speak of averages, they typically
mean \emph{mean} values.

In Section~\ref{sec-RFunctions}, we saw that means are calculated by
adding up all the values and dividing the sum by the total of values.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sum}\NormalTok{((}\FunctionTok{c}\NormalTok{(}\DecValTok{89}\NormalTok{, }\DecValTok{91}\NormalTok{, }\DecValTok{86}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{82}\NormalTok{))) }\SpecialCharTok{/} \DecValTok{5}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 70.6
\end{verbatim}

Means are useful because they are commonly reported and widely
understood. Their disadvantage is that they are very susceptible to
\textbf{outliers} and \textbf{skew} (which far fewer people actually
understand, see Section~\ref{sec-Distributions}). As we saw in the
example above, the fact that one `outlier' learner did very poorly in
her Breton vocabulary test led to a much lower average grade than we
would expect considering that the other four test-takers did much better
than the mean.

Means are also frequently misinterpreted as ``most likely value''. This
is rarely the case. For example, in this example, 70.6 is not even a
score that any of the five students obtained!

\subsection{Median}\label{sec-Median}

Another way to report the central tendency of a set of numeric values
like test results is to look for its ``middle value''. If we sort our
five Breton learners' test results from the lowest to the highest value,
we can see that the \textbf{middle value} is \texttt{86}. This is the
median.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sort}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{89}\NormalTok{, }\DecValTok{91}\NormalTok{, }\DecValTok{86}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{82}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1]  5 82 86 89 91
\end{verbatim}

For datasets with an even number of values (e.g.~2, 4, 6, 8), we take
the mean of the two middle values. Hence, in the following extended
dataset with six Breton learners, the median test score is \texttt{86.5}
because the two middle test results are \texttt{86} and \texttt{87} and
\texttt{(86~+~87)~/~2~=~86.5}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sort}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{89}\NormalTok{, }\DecValTok{91}\NormalTok{, }\DecValTok{86}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{82}\NormalTok{, }\DecValTok{87}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1]  5 82 86 87 89 91
\end{verbatim}

By now, you will probably not be surprised to learn that there is an
\texttt{R} function called \texttt{median()}, which allows us to easily
calculate the median value of any set of numbers.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{median}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{89}\NormalTok{, }\DecValTok{91}\NormalTok{, }\DecValTok{86}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{82}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 86
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{median}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{89}\NormalTok{, }\DecValTok{91}\NormalTok{, }\DecValTok{86}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{82}\NormalTok{, }\DecValTok{87}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 86.5
\end{verbatim}

\subsection{Mode}\label{sec-Mode}

The mean and median are measures of central tendency that only work with
numeric variables. However, data in the language sciences frequently
also include categorical data (see Section~\ref{sec-Variables}). In the
data from DÄ…browska (2019), this includes variables such as
\texttt{Gender}, \texttt{NativeLg}, \texttt{OtherLgs}, and
\texttt{Occupation}. We also need to be able to describe these variables
as part of our data analysis. For such \textbf{categorical variables},
the only available measure of central tendency is the \textbf{mode},
which corresponds to the \textbf{most frequent value} in a variable.

The \texttt{table()} function outputs how often each unique value occurs
in a variable.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(L1.data}\SpecialCharTok{$}\NormalTok{Gender)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

 F  M 
48 42 
\end{verbatim}

From this output, we can tell that the mode of the \texttt{Gender}
variable in the L1 dataset is \texttt{F}, which stands for ``female''.

When there are many different unique values (or \textbf{levels}), it
makes sense to order them according to their frequency. To do so, we can
pipe the output of the \texttt{table()} function into the
\texttt{sort()} function (piping was covered in
Section~\ref{sec-Piping}). Note that, by default, \texttt{R} sorts by
ascending order (\texttt{decreasing\ =\ FALSE}). We can change this
default to \texttt{TRUE}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(L1.data}\SpecialCharTok{$}\NormalTok{Occupation) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{sort}\NormalTok{(}\AttributeTok{decreasing =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

                  Retired                   Student                Unemployed 
                       14                        14                         4 
                Housewife            Shop Assistant                   Teacher 
                        3                         3                         3 
          Admin Assistant            Factory Worker                 Policeman 
                        2                         2                         2 
        Quantity Surveyor             Admin Officer             Administrator 
                        2                         1                         1 
              Boilermaker             Care Assisant             Carer/Cleaner 
                        1                         1                         1 
       Catering Assistant             Civil Servant                   Cleaner 
                        1                         1                         1 
                    Clerk            Content Editor    Creative Writing Tutor 
                        1                         1                         1 
 Customer Service Advisor              Dental Nurse         Finance Assistant 
                        1                         1                         1 
   Functions Co-ordinator                 Homemaker           Human Resources 
                        1                         1                         1 
               IT Support             Manual worker           Nail Technician 
                        1                         1                         1 
Office Admin Co-Ordinator         P/T Administrator         Personal Searcher 
                        1                         1                         1 
      Project Coordinator              Receptionist                    Roofer 
                        1                         1                         1 
          Sales Assistant     School Crossing Guard    School Crossing Patrol 
                        1                         1                         1 
          Senior Lecturer                    Singer         Student (college) 
                        1                         1                         1 
   Student/Support Worker     Supermarket Assistant            Support Worker 
                        1                         1                         1 
             Train Driver                 Unemploed       University lecturer 
                        1                         1                         1 
                 Waitress       Warehouse Operative              Web designer 
                        1                         1                         1 
\end{verbatim}

We can see that, among the L1 participants, there were as many
``Retired'' participants as there were ``Student''
participants.\footnote{We also see that these data needs cleaning before
  we can do any serious data analysis. There are also a few typos
  (e.g.~\emph{Unemploed}) and synonyms (\emph{School Crossing Guard} and
  \emph{School Crossing Patrol}) that we will need to standardise. This
  process is part of \textbf{data wrangling} and we will cover how to do
  this in a \textbf{reproducible} way in \texttt{R} in
  Chapter~\ref{sec-DataWrangling}.} Hence, we have two modes. In
general, modal values rarely make good summaries of variables with many
different possible values or levels. This is why the mode is not
suitable for numeric variables, unless there are only a few possible
discrete numeric values (e.g.~the values of a five or seven-point
\textbf{Likert scale}\footnote{A Likert scale is a type of rating scale
  used to measure attitudes, opinions, or feelings. It typically
  consists of a series of statements or questions with a range of
  possible responses, often on a scale from ``strongly disagree'' to
  ``strongly agree''. For example, in a study on language attitudes,
  participants might be asked to rate their agreement with the statement
  ``I think it's important to speak standard English in formal
  situations'' on a scale from ``1 (strongly disagree)'' to ``5
  (strongly agree)''. The resulting variable will therefore consist of
  numbers ranging between 1 and 5. Note also that, strictly speaking,
  Likert scales are not numeric variables, but rather ordinal variables
  (see Section~\ref{sec-Variables}). The numbers refer to different
  categories that describe an order of responses, rather than a
  quantity.}).

Cross-tabulations of more than one categorical variable (or numeric
variable with just a few unique values) can easily be generated using
the \texttt{table()} function. In the following, we cross-tabulate the
additional languages that the L1 participants speak with their gender.
This allows us to see that most male and female L1 participants did not
speak another language other than English. Hence, for both the male and
female subsets of L1 participants the mode of the variable
\texttt{OtherLgs} is ``None''.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(L1.data}\SpecialCharTok{$}\NormalTok{OtherLgs, L1.data}\SpecialCharTok{$}\NormalTok{Gender)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
         
           F  M
  French   1  1
  German   2  1
  None    44 40
  Spanish  1  0
\end{verbatim}

\section{Distributions}\label{sec-Distributions}

Data analysis typically begins with the description of individual
variables from a dataset. This is referred to as \textbf{univariate
descriptive statistics} and is all about describing the distribution of
the variables. A \textbf{distribution} is a way to summarise how the
values of a variable are dispersed. It tells us things like the
variable's most frequent values, its range of values, and how the values
are clustered or spread out. Examining the shapes and patterns of
distributions can help us understand the typical values of the variables
of our data, identify outliers, and make informed decisions about how to
analyse and visualise our data.

\subsection{Distributions of categorical variables}\label{sec-DistCat}

Tables can be an effective way to examine the distribution of
categorical variables. The \texttt{table()} function outputs the
frequency of each level of a categorical variable. By default, the
levels are ordered alphabetically.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(L1.data}\SpecialCharTok{$}\NormalTok{OtherLgs)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

 French  German    None Spanish 
      2       3      84       1 
\end{verbatim}

We saw that we can use the \texttt{sort()} function to change this
behaviour.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(L1.data}\SpecialCharTok{$}\NormalTok{OtherLgs) }\SpecialCharTok{|\textgreater{}} 
    \FunctionTok{sort}\NormalTok{(}\AttributeTok{decreasing =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

   None  German  French Spanish 
     84       3       2       1 
\end{verbatim}

The \texttt{proportions()} function allows us to describe the frequency
of each level of a categorical variable as a proportion of all data
points. This is especially useful if we want to compare the distribution
of a categorical variable across different (sub)datasets of different
sizes.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(L1.data}\SpecialCharTok{$}\NormalTok{OtherLgs) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{sort}\NormalTok{(}\AttributeTok{decreasing =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{proportions}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

      None     German     French    Spanish 
0.93333333 0.03333333 0.02222222 0.01111111 
\end{verbatim}

When computing proportions, \texttt{0} corresponds to 0\% and \texttt{1}
to 100\%. If we want to obtain percentages, we therefore need to
multiply these numbers by 100. We can therefore see that more than 90\%
of L1 participants reported not being competent in any language other
than English, their native language.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{OtherLgs.prop }\OtherTok{\textless{}{-}} 
  \FunctionTok{table}\NormalTok{(L1.data}\SpecialCharTok{$}\NormalTok{OtherLgs) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{sort}\NormalTok{(}\AttributeTok{decreasing =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{proportions}\NormalTok{()}\SpecialCharTok{*}\DecValTok{100}

\NormalTok{OtherLgs.prop}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

     None    German    French   Spanish 
93.333333  3.333333  2.222222  1.111111 
\end{verbatim}

To round the values to two decimal places, we can pipe the \texttt{R}
object that we created in the previous chunk (\texttt{OtherLgs.prop})
into the \texttt{round()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{OtherLgs.prop }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{round}\NormalTok{(}\AttributeTok{digits =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

   None  German  French Spanish 
  93.33    3.33    2.22    1.11 
\end{verbatim}

In addition to using frequency tables, we can \textbf{visualise} data
distributions graphically. \textbf{Bar plots} allow us to easily compare
the distribution of categorical variables across different datasets and
subsets of data. For example, in Figure~\ref{fig-OtherLgsGender}, we can
see that the distribution of additional languages spoken by the L1
participants is very similar in both the female and the male subset of
participants.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# This plot and all other plots in this chapter are generated using the \{ggplot2\} package from the \{tidyverse\}. You should have already installed the \{tidyverse\} in Chapter 6.}
\CommentTok{\#install.packages("tidyverse")}
\FunctionTok{library}\NormalTok{(tidyverse)}

\NormalTok{L1.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Gender =} \FunctionTok{ifelse}\NormalTok{(Gender }\SpecialCharTok{==} \StringTok{"M"}\NormalTok{, }\StringTok{"Male"}\NormalTok{, }\StringTok{"Female"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{fct\_infreq}\NormalTok{(OtherLgs), }\AttributeTok{y =}\NormalTok{..count..}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(..count..))) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ Gender) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{labels =}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{label\_percent}\NormalTok{(),}
                     \AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{),}
                     \AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FloatTok{0.5}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_discrete}\NormalTok{(}\AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.18}\NormalTok{,}\DecValTok{0}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Other languages spoken by L1 participants"}\NormalTok{, }
       \AttributeTok{x =} \StringTok{""}\NormalTok{, }
       \AttributeTok{y =} \StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{8_DescriptiveStats_files/figure-pdf/fig-OtherLgsGender-1.pdf}}

}

\caption{\label{fig-OtherLgsGender}Additional languages spoken by L1
participants in DÄ…browska (2019)}

\end{figure}%

In \href{@sec-DataViz}{Chapter 10}, you will learn how to make plots
like Figure~\ref{fig-OtherLgsGender} in \texttt{R} using the \{ggplot2\}
package.

\subsection{Distributions of numeric
variables}\label{sec-DistributionsNumeric}

In DÄ…browska (2019), on average, the L2 participants were younger than
the L1 participants.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(L1.data}\SpecialCharTok{$}\NormalTok{Age) }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(L2.data}\SpecialCharTok{$}\NormalTok{Age)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 4.828027
\end{verbatim}

The difference in mean age was more than four years. But are these two
mean values good summaries of the central tendencies of participants'
ages? To check, it is important that we examine the full distribution of
participants' ages. We begin with the distribution of L2 participants'
ages.

We first use the \texttt{table()} function to tally L2 participants'
ages.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(L2.data}\SpecialCharTok{$}\NormalTok{Age)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 37 38 39 40 41 42 46 47 48 51 
 1  1  5  3  2  3  2  2  6  3  4  5  2  3  2  2  3  2  5  1  1  1  2  1  1  1 
52 55 62 
 1  1  1 
\end{verbatim}

As the above table contains a lot of different values, it's easier to
visualise these numbers in the form of a \textbf{bar chart} (also called
\textbf{bar plot}). The mode (\texttt{28}) has been highlighted in
black.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{barplot.mode }\OtherTok{\textless{}{-}} 
  
  \CommentTok{\# Take the L2.data data frame and pipe it into the ggplot function:}
\NormalTok{  L2.data }\SpecialCharTok{|\textgreater{}}

  \CommentTok{\# Start a ggplot, mapping Age to the x{-}axis:}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Age)) }\SpecialCharTok{+} 

  \CommentTok{\# Add a bar plot layer, conditionally fill the bars; bars representing 28 years of age will have a different colour:}
  \FunctionTok{geom\_bar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{fill =}\NormalTok{ (Age }\SpecialCharTok{==} \DecValTok{28}\NormalTok{))) }\SpecialCharTok{+}    

  \CommentTok{\# Manually control the colours of the bar fill: set the bar representing Age == 28 to "\#0000EE", and remove the legend:}
  \FunctionTok{scale\_fill\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"TRUE"} \OtherTok{=} \StringTok{"black"}\NormalTok{), }\AttributeTok{guide =} \StringTok{"none"}\NormalTok{) }\SpecialCharTok{+}  
  
    \CommentTok{\# Apply ggplot2\textquotesingle{}s classic theme:}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}

  \CommentTok{\# Ensure that there are tick marks for every single whole number and do not extend the limits of y{-}scale to avoid white space on the plot:}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{name =} \StringTok{"Number of L2 participants"}\NormalTok{,}
                     \AttributeTok{breaks =}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{breaks\_width}\NormalTok{(}\DecValTok{1}\NormalTok{), }
                     \AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{)) }\SpecialCharTok{+}

  \CommentTok{\# Set the x{-}axis breaks and remove white space:}
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{breaks =}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{breaks\_width}\NormalTok{(}\DecValTok{1}\NormalTok{),}
                     \AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{)) }\SpecialCharTok{+}
  
  \CommentTok{\# Add label for mode:}
  \FunctionTok{annotate}\NormalTok{(}\StringTok{"text"}\NormalTok{, }
           \AttributeTok{x =} \DecValTok{25}\NormalTok{, }
           \AttributeTok{y =} \FloatTok{5.8}\NormalTok{, }
           \AttributeTok{label =} \StringTok{"mode"}\NormalTok{, }
           \AttributeTok{colour =} \StringTok{"black"}\NormalTok{,}
           \AttributeTok{family =} \StringTok{"mono"}\NormalTok{) }\SpecialCharTok{+}
  
  \CommentTok{\# Add curved arrow for mode:}
  \FunctionTok{annotate}\NormalTok{(}
    \AttributeTok{geom =} \StringTok{"curve"}\NormalTok{,}
    \AttributeTok{x =} \DecValTok{25}\NormalTok{,}
    \AttributeTok{y =} \FloatTok{5.65}\NormalTok{, }
    \AttributeTok{xend =} \FloatTok{27.2}\NormalTok{,}
    \AttributeTok{yend =} \DecValTok{5}\NormalTok{, }
    \AttributeTok{curvature =} \FloatTok{0.5}\NormalTok{,}
    \AttributeTok{arrow =} \FunctionTok{arrow}\NormalTok{(}\AttributeTok{length =} \FunctionTok{unit}\NormalTok{(}\FloatTok{0.2}\NormalTok{, }\StringTok{"cm"}\NormalTok{)), }
    \AttributeTok{colour =} \StringTok{"black"}\NormalTok{)}

\CommentTok{\# Print the plot}
\NormalTok{barplot.mode}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{8_DescriptiveStats_files/figure-pdf/unnamed-chunk-20-1.pdf}}

Thanks to this bar chart, it's much easier to see that the second most
frequent ages after the mode of \texttt{28} are \texttt{22}, \texttt{31}
and \texttt{39}. How do these ages compare to the median age?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{barplot.mode.median }\OtherTok{\textless{}{-}} 
\NormalTok{  barplot.mode }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{fill =} \FunctionTok{ifelse}\NormalTok{(Age }\SpecialCharTok{==} \DecValTok{31}\NormalTok{, }\StringTok{"31"}\NormalTok{, }\FunctionTok{ifelse}\NormalTok{(Age }\SpecialCharTok{==} \DecValTok{28}\NormalTok{, }\StringTok{"28"}\NormalTok{, }\StringTok{"Other"}\NormalTok{)))) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"28"} \OtherTok{=} \StringTok{"black"}\NormalTok{, }\StringTok{"31"} \OtherTok{=} \StringTok{"darkred"}\NormalTok{), }\AttributeTok{guide =} \StringTok{"none"}\NormalTok{) }\SpecialCharTok{+}
  \CommentTok{\# Add label for median:}
  \FunctionTok{annotate}\NormalTok{(}\StringTok{"text"}\NormalTok{, }
           \AttributeTok{x =} \FloatTok{30.5}\NormalTok{, }
           \AttributeTok{y =} \FloatTok{5.5}\NormalTok{, }
           \AttributeTok{label =} \StringTok{"median"}\NormalTok{, }
           \AttributeTok{colour =} \StringTok{"darkred"}\NormalTok{,}
           \AttributeTok{family =} \StringTok{"mono"}\NormalTok{) }\SpecialCharTok{+}
  
  \CommentTok{\# Add curved arrow for median:}
  \FunctionTok{annotate}\NormalTok{(}
    \AttributeTok{geom =} \StringTok{"curve"}\NormalTok{,}
    \AttributeTok{x =} \FloatTok{30.4}\NormalTok{,}
    \AttributeTok{y =} \FloatTok{5.35}\NormalTok{, }
    \AttributeTok{xend =} \FloatTok{30.4}\NormalTok{,}
    \AttributeTok{yend =} \FloatTok{4.9}\NormalTok{, }
    \AttributeTok{curvature =} \FloatTok{0.6}\NormalTok{,}
    \AttributeTok{arrow =} \FunctionTok{arrow}\NormalTok{(}\AttributeTok{length =} \FunctionTok{unit}\NormalTok{(}\FloatTok{0.2}\NormalTok{, }\StringTok{"cm"}\NormalTok{)), }
    \AttributeTok{colour =} \StringTok{"darkred"}\NormalTok{)}

\NormalTok{barplot.mode.median}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{8_DescriptiveStats_files/figure-pdf/unnamed-chunk-21-1.pdf}}

We can compare the mode (\texttt{28}) and median (\texttt{31}) to the
mean (\texttt{32.72}), which, on the following bar chart, is represented
as a blue dashed line.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{barplot.mode.median }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{xintercept =} \FunctionTok{mean}\NormalTok{(Age)), }
             \AttributeTok{color =} \StringTok{"\#0000EE"}\NormalTok{, }
             \AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{,}
             \AttributeTok{linewidth =} \FloatTok{0.8}\NormalTok{) }\SpecialCharTok{+}
  
  \CommentTok{\# Add label for mean:}
  \FunctionTok{annotate}\NormalTok{(}\StringTok{"text"}\NormalTok{, }
           \AttributeTok{x =} \DecValTok{36}\NormalTok{, }
           \AttributeTok{y =} \FloatTok{5.3}\NormalTok{, }
           \AttributeTok{label =} \StringTok{"mean"}\NormalTok{, }
           \AttributeTok{colour =} \StringTok{"\#0000EE"}\NormalTok{,}
           \AttributeTok{family =} \StringTok{"mono"}\NormalTok{) }\SpecialCharTok{+}
  
  \CommentTok{\# Add curved arrow for mean:}
  \FunctionTok{annotate}\NormalTok{(}
    \AttributeTok{geom =} \StringTok{"curve"}\NormalTok{,}
    \AttributeTok{x =} \DecValTok{36}\NormalTok{,}
    \AttributeTok{y =} \FloatTok{5.15}\NormalTok{, }
    \AttributeTok{xend =} \FloatTok{33.2}\NormalTok{,}
    \AttributeTok{yend =} \FloatTok{4.4}\NormalTok{, }
    \AttributeTok{curvature =} \SpecialCharTok{{-}}\FloatTok{0.4}\NormalTok{,}
    \AttributeTok{arrow =} \FunctionTok{arrow}\NormalTok{(}\AttributeTok{length =} \FunctionTok{unit}\NormalTok{(}\FloatTok{0.2}\NormalTok{, }\StringTok{"cm"}\NormalTok{)), }
    \AttributeTok{colour =} \StringTok{"\#0000EE"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{8_DescriptiveStats_files/figure-pdf/unnamed-chunk-22-1.pdf}}

Next, we can reduce the number of bars by adding together the number of
L2 participants aged \texttt{20}-\texttt{22}, \texttt{22}-\texttt{24},
\texttt{24}-\texttt{26}, etc. This is what we call a \textbf{histogram}.
Histograms are used to visualise distributions and their bars are called
\textbf{bins} because they ``bin together'' a number of values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{age.histo }\OtherTok{\textless{}{-}} 
\NormalTok{  L2.data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Age)) }\SpecialCharTok{+} 
    \FunctionTok{geom\_vline}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{xintercept =} \FunctionTok{mean}\NormalTok{(Age)), }
             \AttributeTok{color =} \StringTok{"\#0000EE"}\NormalTok{, }
             \AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{,}
             \AttributeTok{linewidth =} \FloatTok{0.8}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{geom\_vline}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{xintercept =} \DecValTok{28}\NormalTok{), }
             \AttributeTok{color =} \StringTok{"black"}\NormalTok{, }
             \AttributeTok{linewidth =} \FloatTok{0.8}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{geom\_vline}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{xintercept =} \DecValTok{31}\NormalTok{), }
             \AttributeTok{color =} \StringTok{"darkred"}\NormalTok{, }
             \AttributeTok{linewidth =} \FloatTok{0.8}\NormalTok{) }\SpecialCharTok{+}  
  
  \CommentTok{\# Add label for mode:}
  \FunctionTok{annotate}\NormalTok{(}\StringTok{"text"}\NormalTok{, }
           \AttributeTok{x =} \DecValTok{24}\NormalTok{, }
           \AttributeTok{y =} \FloatTok{2.9}\NormalTok{, }
           \AttributeTok{label =} \StringTok{"mode"}\NormalTok{, }
           \AttributeTok{colour =} \StringTok{"black"}\NormalTok{,}
           \AttributeTok{family =} \StringTok{"mono"}\NormalTok{) }\SpecialCharTok{+}
  
  \CommentTok{\# Add curved arrow for mode:}
  \FunctionTok{annotate}\NormalTok{(}
    \AttributeTok{geom =} \StringTok{"curve"}\NormalTok{,}
    \AttributeTok{x =} \DecValTok{24}\NormalTok{,}
    \AttributeTok{y =} \FloatTok{2.6}\NormalTok{, }
    \AttributeTok{xend =} \FloatTok{27.2}\NormalTok{,}
    \AttributeTok{yend =} \DecValTok{2}\NormalTok{, }
    \AttributeTok{curvature =} \FloatTok{0.5}\NormalTok{,}
    \AttributeTok{arrow =} \FunctionTok{arrow}\NormalTok{(}\AttributeTok{length =} \FunctionTok{unit}\NormalTok{(}\FloatTok{0.2}\NormalTok{, }\StringTok{"cm"}\NormalTok{)), }
    \AttributeTok{colour =} \StringTok{"black"}\NormalTok{) }\SpecialCharTok{+}
  
  \CommentTok{\# Add label for mean:}
  \FunctionTok{annotate}\NormalTok{(}\StringTok{"text"}\NormalTok{, }
           \AttributeTok{x =} \DecValTok{36}\NormalTok{, }
           \AttributeTok{y =} \FloatTok{1.4}\NormalTok{, }
           \AttributeTok{label =} \StringTok{"mean"}\NormalTok{, }
           \AttributeTok{colour =} \StringTok{"\#0000EE"}\NormalTok{,}
           \AttributeTok{family =} \StringTok{"mono"}\NormalTok{) }\SpecialCharTok{+}
  
  \CommentTok{\# Add curved arrow for mean:}
  \FunctionTok{annotate}\NormalTok{(}
    \AttributeTok{geom =} \StringTok{"curve"}\NormalTok{,}
    \AttributeTok{x =} \DecValTok{36}\NormalTok{,}
    \AttributeTok{y =} \FloatTok{1.1}\NormalTok{, }
    \AttributeTok{xend =} \FloatTok{33.2}\NormalTok{,}
    \AttributeTok{yend =} \FloatTok{0.4}\NormalTok{, }
    \AttributeTok{curvature =} \SpecialCharTok{{-}}\FloatTok{0.4}\NormalTok{,}
    \AttributeTok{arrow =} \FunctionTok{arrow}\NormalTok{(}\AttributeTok{length =} \FunctionTok{unit}\NormalTok{(}\FloatTok{0.2}\NormalTok{, }\StringTok{"cm"}\NormalTok{)), }
    \AttributeTok{colour =} \StringTok{"\#0000EE"}\NormalTok{) }\SpecialCharTok{+}
  
    \CommentTok{\# Add label for median:}
  \FunctionTok{annotate}\NormalTok{(}\StringTok{"text"}\NormalTok{, }
           \AttributeTok{x =} \DecValTok{25}\NormalTok{, }
           \AttributeTok{y =} \FloatTok{1.4}\NormalTok{, }
           \AttributeTok{label =} \StringTok{"median"}\NormalTok{, }
           \AttributeTok{colour =} \StringTok{"darkred"}\NormalTok{,}
           \AttributeTok{family =} \StringTok{"mono"}\NormalTok{) }\SpecialCharTok{+}
  
  \CommentTok{\# Add curved arrow for median:}
  \FunctionTok{annotate}\NormalTok{(}
    \AttributeTok{geom =} \StringTok{"curve"}\NormalTok{,}
    \AttributeTok{x =} \DecValTok{25}\NormalTok{,}
    \AttributeTok{y =} \FloatTok{1.1}\NormalTok{, }
    \AttributeTok{xend =} \FloatTok{30.7}\NormalTok{,}
    \AttributeTok{yend =} \FloatTok{0.4}\NormalTok{, }
    \AttributeTok{curvature =} \FloatTok{0.4}\NormalTok{,}
    \AttributeTok{arrow =} \FunctionTok{arrow}\NormalTok{(}\AttributeTok{length =} \FunctionTok{unit}\NormalTok{(}\FloatTok{0.2}\NormalTok{, }\StringTok{"cm"}\NormalTok{)), }
    \AttributeTok{colour =} \StringTok{"darkred"}\NormalTok{) }\SpecialCharTok{+}
  
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{name =} \StringTok{"Number of L2 participants"}\NormalTok{, }
                     \AttributeTok{breaks =}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{breaks\_width}\NormalTok{(}\DecValTok{1}\NormalTok{), }
                     \AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{breaks =}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{breaks\_width}\NormalTok{(}\DecValTok{2}\NormalTok{), }
                     \AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{))}

\NormalTok{age.histo }\SpecialCharTok{+}
    \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{position =} \StringTok{"identity"}\NormalTok{, }
                 \AttributeTok{binwidth =} \DecValTok{2}\NormalTok{,}
                 \AttributeTok{fill =} \StringTok{"black"}\NormalTok{,}
                 \AttributeTok{alpha =} \FloatTok{0.4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{8_DescriptiveStats_files/figure-pdf/unnamed-chunk-23-1.pdf}}

If we reduce the number of bins by having them cover three years instead
of two, the histogram looks like this.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{age.histo }\SpecialCharTok{+}
    \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{position =} \StringTok{"identity"}\NormalTok{, }
                 \AttributeTok{binwidth =} \DecValTok{3}\NormalTok{,}
                 \AttributeTok{fill =} \StringTok{"black"}\NormalTok{,}
                 \AttributeTok{alpha =} \FloatTok{0.4}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{breaks =}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{breaks\_width}\NormalTok{(}\DecValTok{3}\NormalTok{), }
                     \AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{8_DescriptiveStats_files/figure-pdf/unnamed-chunk-24-1.pdf}}

Alternatively, we can apply a density function to smooth over the bins
of the histogram to generate a \textbf{density plot} of L2 participants'
ages (see purple curve in Figure~\ref{fig-L2AgeDensityPlot}). Such
smoothed curves allow for a better comparison of distribution shapes
across different groups and datasets. Note that, in a density plot, the
values on the \emph{y}-axis are no longer counts, but rather
\textbf{density probabilities}. We will not use any fancy formulae to
work this out mathematically, but you should understand that the total
area under the curve (in purple) will always equal to \texttt{1}, which
corresponds to 100\% probability. In this dataset, this is because there
is a 100\% probability that an L2 participant's age is between
\texttt{20} and \texttt{62}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# There is no in{-}built function in R to calculate the mode of a numeric vector but we can define one ourselves:}
\NormalTok{get\_mode }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) \{}
\NormalTok{  ux }\OtherTok{\textless{}{-}} \FunctionTok{unique}\NormalTok{(x)}
\NormalTok{  ux[}\FunctionTok{which.max}\NormalTok{(}\FunctionTok{tabulate}\NormalTok{(}\FunctionTok{match}\NormalTok{(x, ux)))]}
\NormalTok{\}}

\NormalTok{L2.data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Age)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Age, }\AttributeTok{y =} \FunctionTok{after\_stat}\NormalTok{(density)),}
                 \AttributeTok{binwidth =} \DecValTok{3}\NormalTok{,}
                 \AttributeTok{fill =} \StringTok{"black"}\NormalTok{,}
                 \AttributeTok{alpha =} \FloatTok{0.4}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_density}\NormalTok{(}\AttributeTok{colour =} \StringTok{"purple"}\NormalTok{,}
               \AttributeTok{fill =} \StringTok{"purple"}\NormalTok{,}
               \AttributeTok{alpha =} \FloatTok{0.2}\NormalTok{,}
               \AttributeTok{linewidth =} \FloatTok{0.8}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{xintercept =} \FunctionTok{mean}\NormalTok{(Age)),}
             \AttributeTok{color =} \StringTok{"\#0000EE"}\NormalTok{,}
             \AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{,}
             \AttributeTok{linewidth =} \FloatTok{0.8}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{xintercept =} \FunctionTok{get\_mode}\NormalTok{(Age)),}
             \AttributeTok{color =} \StringTok{"black"}\NormalTok{,}
             \AttributeTok{linewidth =} \FloatTok{0.8}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{xintercept =} \FunctionTok{median}\NormalTok{(Age)),}
             \AttributeTok{color =} \StringTok{"darkred"}\NormalTok{,}
             \AttributeTok{linewidth =} \FloatTok{0.8}\NormalTok{) }\SpecialCharTok{+}  
  \CommentTok{\# Add label for mode:}
  \FunctionTok{annotate}\NormalTok{(}\StringTok{"text"}\NormalTok{,}
           \AttributeTok{x =} \DecValTok{25}\NormalTok{,}
           \AttributeTok{y =} \FloatTok{0.029}\NormalTok{,}
           \AttributeTok{label =} \StringTok{"mode"}\NormalTok{,}
           \AttributeTok{colour =} \StringTok{"black"}\NormalTok{,}
           \AttributeTok{family =} \StringTok{"mono"}\NormalTok{) }\SpecialCharTok{+}

  \CommentTok{\# Add curved arrow for mode:}
  \FunctionTok{annotate}\NormalTok{(}
    \AttributeTok{geom =} \StringTok{"curve"}\NormalTok{,}
    \AttributeTok{x =} \DecValTok{25}\NormalTok{,}
    \AttributeTok{y =} \FloatTok{0.028}\NormalTok{,}
    \AttributeTok{xend =} \FloatTok{27.5}\NormalTok{,}
    \AttributeTok{yend =} \FloatTok{0.025}\NormalTok{,}
    \AttributeTok{curvature =} \FloatTok{0.6}\NormalTok{,}
    \AttributeTok{arrow =} \FunctionTok{arrow}\NormalTok{(}\AttributeTok{length =} \FunctionTok{unit}\NormalTok{(}\FloatTok{0.2}\NormalTok{, }\StringTok{"cm"}\NormalTok{)),}
    \AttributeTok{colour =} \StringTok{"black"}\NormalTok{) }\SpecialCharTok{+}

  \CommentTok{\# Add label for mean:}
  \FunctionTok{annotate}\NormalTok{(}\StringTok{"text"}\NormalTok{,}
           \AttributeTok{x =} \DecValTok{36}\NormalTok{,}
           \AttributeTok{y =} \FloatTok{0.014}\NormalTok{,}
           \AttributeTok{label =} \StringTok{"mean"}\NormalTok{,}
           \AttributeTok{colour =} \StringTok{"\#0000EE"}\NormalTok{,}
           \AttributeTok{family =} \StringTok{"mono"}\NormalTok{) }\SpecialCharTok{+}

  \CommentTok{\# Add curved arrow for mean:}
  \FunctionTok{annotate}\NormalTok{(}
    \AttributeTok{geom =} \StringTok{"curve"}\NormalTok{,}
    \AttributeTok{x =} \DecValTok{36}\NormalTok{,}
    \AttributeTok{y =} \FloatTok{0.011}\NormalTok{,}
    \AttributeTok{xend =} \FloatTok{33.2}\NormalTok{,}
    \AttributeTok{yend =} \FloatTok{0.004}\NormalTok{,}
    \AttributeTok{curvature =} \SpecialCharTok{{-}}\FloatTok{0.4}\NormalTok{,}
    \AttributeTok{arrow =} \FunctionTok{arrow}\NormalTok{(}\AttributeTok{length =} \FunctionTok{unit}\NormalTok{(}\FloatTok{0.2}\NormalTok{, }\StringTok{"cm"}\NormalTok{)),}
    \AttributeTok{colour =} \StringTok{"\#0000EE"}\NormalTok{) }\SpecialCharTok{+}
  
  \CommentTok{\# Add label for median:}
  \FunctionTok{annotate}\NormalTok{(}\StringTok{"text"}\NormalTok{,}
           \AttributeTok{x =} \FloatTok{25.5}\NormalTok{,}
           \AttributeTok{y =} \FloatTok{0.01}\NormalTok{,}
           \AttributeTok{label =} \StringTok{"median"}\NormalTok{,}
           \AttributeTok{colour =} \StringTok{"darkred"}\NormalTok{,}
           \AttributeTok{family =} \StringTok{"mono"}\NormalTok{) }\SpecialCharTok{+}

  \CommentTok{\# Add curved arrow for median:}
  \FunctionTok{annotate}\NormalTok{(}
    \AttributeTok{geom =} \StringTok{"curve"}\NormalTok{,}
    \AttributeTok{x =} \FloatTok{25.5}\NormalTok{,}
    \AttributeTok{y =} \FloatTok{0.008}\NormalTok{,}
    \AttributeTok{xend =} \FloatTok{30.8}\NormalTok{,}
    \AttributeTok{yend =} \FloatTok{0.004}\NormalTok{,}
    \AttributeTok{curvature =} \FloatTok{0.4}\NormalTok{,}
    \AttributeTok{arrow =} \FunctionTok{arrow}\NormalTok{(}\AttributeTok{length =} \FunctionTok{unit}\NormalTok{(}\FloatTok{0.2}\NormalTok{, }\StringTok{"cm"}\NormalTok{)),}
    \AttributeTok{colour =} \StringTok{"darkred"}\NormalTok{) }\SpecialCharTok{+}  

  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{name =} \StringTok{"Density"}\NormalTok{,}
                     \AttributeTok{breaks =}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{breaks\_width}\NormalTok{(}\FloatTok{0.01}\NormalTok{),}
                     \AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{breaks =}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{breaks\_width}\NormalTok{(}\DecValTok{3}\NormalTok{),}
                     \AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{8_DescriptiveStats_files/figure-pdf/fig-L2AgeDensityPlot-1.pdf}}

}

\caption{\label{fig-L2AgeDensityPlot}Density plot showing the
distribution of L2 participants' ages}

\end{figure}%

If we wanted to work out the probability of an L2 participant being
between \texttt{42} and \texttt{62} years old, we would have to
calculate the \textbf{area under the curve} between these two points on
the \emph{x}-axis. Even without doing any maths, you can see that this
area is considerably smaller than between the ages of \texttt{22} and
\texttt{42}. This means that, in this dataset, participants are
considerably more likely to be between 22 and 42 than between 42 and 62
years old.\footnote{Of course, there is an \texttt{R} function to help
  you do the maths! The \texttt{ecdf()} function allows us to calculate
  the area under the curve between the ages of 42 and 62.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ecdf}\NormalTok{(L2.data}\SpecialCharTok{$}\NormalTok{Age)(}\DecValTok{62}\NormalTok{) }\SpecialCharTok{{-}} \FunctionTok{ecdf}\NormalTok{(L2.data}\SpecialCharTok{$}\NormalTok{Age)(}\DecValTok{42}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Verbatim}
[1] 0.119403
\end{Verbatim}

  In other words, there is a 11.94 \% probability of any L2 participant
  in this study being aged between 42 and 62 (corresponding to the light
  purple area in plot A). Compare this to the probability of a
  participant being between 22 and 42 years old.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ecdf}\NormalTok{(L2.data}\SpecialCharTok{$}\NormalTok{Age)(}\DecValTok{42}\NormalTok{) }\SpecialCharTok{{-}} \FunctionTok{ecdf}\NormalTok{(L2.data}\SpecialCharTok{$}\NormalTok{Age)(}\DecValTok{22}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Verbatim}
[1] 0.7761194
\end{Verbatim}

  This is, indeed, a much higher probability (ca. 78 \%), as depicted by
  the much larger area highlighted in plot B.

  \pandocbounded{\includegraphics[keepaspectratio]{8_DescriptiveStats_files/figure-pdf/unnamed-chunk-27-1.pdf}}}

The density plot of L2 participants' ages features a characteristic
bell-shaped curve, which indicates that the distribution resembles a
normal distribution. However, it also features a long tail towards the
older years. We are therefore dealing with a \textbf{skewed
distribution}. \textbf{Skewness} is a measure of asymmetry in a
distribution. Skewed distributions occur when one tail end of the bell
is longer than the other. Here, the asymmetry is due to the fact that
DÄ…browska (2019)'s L2 data includes quite a few participants who were
older than 40 at the time of the study, whereas there were none who were
younger than 20. As the tail is to the right of the plot, this is a
\textbf{right skewed (or positive) distribution}.

The \textbf{median} is usually better than the mean for describing the
central tendency of a skewed distribution because it is less susceptible
to the outlier(s) contained in the tail of a skewed distribution (see
Section~\ref{sec-Median}). Figure~\ref{fig-L2AgeDensityPlot} confirms
that the median is a better approximation of L2 participants' ages than
the mean.

\subsection{Normal (or Gaussian) distributions}\label{sec-Normal}

In a perfectly normally distributed variable, the mean and the median
are exactly the same. They are both found at the centre of the
distribution and the bell shape of the distribution is perfectly
symmetrical. Hence, the skewness of a normal distribution is near zero.

Perfectly normal distributions, however, are very rarely found in real
life! Here is what the \textbf{normal distribution} of 10,000
participants' age might look like in real life (using numbers randomly
generated from a perfectly normal distribution thanks to the \texttt{R}
function \texttt{rnorm()}).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# The \{truncnorm\} package contains density, probability, quantile and random number generation functions for the truncated normal distribution:}
\CommentTok{\#install.packages("truncnorm")}
\FunctionTok{library}\NormalTok{(truncnorm)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}
\NormalTok{normal.age.sd8 }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(}\FunctionTok{rtruncnorm}\NormalTok{(}\AttributeTok{mean =} \DecValTok{35}\NormalTok{, }\AttributeTok{sd =} \DecValTok{8}\NormalTok{, }\AttributeTok{n =} \DecValTok{10000}\NormalTok{, }\AttributeTok{a =} \DecValTok{10}\NormalTok{, }\AttributeTok{b =} \DecValTok{100}\NormalTok{))}
\CommentTok{\#get\_mode(normal.age.sd8) }

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ normal.age.sd8)) }\SpecialCharTok{+} 
    \FunctionTok{geom\_vline}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{xintercept =} \FunctionTok{mean}\NormalTok{(normal.age.sd8)),}
             \AttributeTok{color =} \StringTok{"\#0000EE"}\NormalTok{,}
             \AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{,}
             \AttributeTok{linewidth =} \FloatTok{0.8}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{geom\_vline}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{xintercept =} \FunctionTok{median}\NormalTok{(normal.age.sd8)),}
             \AttributeTok{color =} \StringTok{"darkred"}\NormalTok{,}
             \CommentTok{\#linetype = "dotted",}
             \AttributeTok{linewidth =} \FloatTok{0.6}\NormalTok{) }\SpecialCharTok{+}
    \CommentTok{\# geom\_vline(aes(xintercept = get\_mode(normal.age.sd8)),}
    \CommentTok{\#          color = "black",}
    \CommentTok{\#          linewidth = 0.8) +  }
  
  \CommentTok{\# Add label for mean:}
  \FunctionTok{annotate}\NormalTok{(}\StringTok{"text"}\NormalTok{,}
           \AttributeTok{x =} \FloatTok{30.6}\NormalTok{,}
           \AttributeTok{y =} \FloatTok{0.014}\NormalTok{,}
           \AttributeTok{label =} \StringTok{"mean"}\NormalTok{,}
           \AttributeTok{colour =} \StringTok{"\#0000EE"}\NormalTok{,}
           \AttributeTok{family =} \StringTok{"mono"}\NormalTok{) }\SpecialCharTok{+}

  \CommentTok{\# Add curved arrow for mean:}
  \FunctionTok{annotate}\NormalTok{(}
    \AttributeTok{geom =} \StringTok{"curve"}\NormalTok{,}
    \AttributeTok{x =} \FloatTok{30.5}\NormalTok{,}
    \AttributeTok{y =} \FloatTok{0.013}\NormalTok{,}
    \AttributeTok{xend =} \FloatTok{34.7}\NormalTok{,}
    \AttributeTok{yend =} \FloatTok{0.008}\NormalTok{,}
    \AttributeTok{curvature =} \FloatTok{0.5}\NormalTok{,}
    \AttributeTok{arrow =} \FunctionTok{arrow}\NormalTok{(}\AttributeTok{length =} \FunctionTok{unit}\NormalTok{(}\FloatTok{0.2}\NormalTok{, }\StringTok{"cm"}\NormalTok{)),}
    \AttributeTok{colour =} \StringTok{"\#0000EE"}\NormalTok{) }\SpecialCharTok{+}  
  
  \CommentTok{\# Add label for median:}
  \FunctionTok{annotate}\NormalTok{(}\StringTok{"text"}\NormalTok{,}
           \AttributeTok{x =} \DecValTok{38}\NormalTok{,}
           \AttributeTok{y =} \FloatTok{0.007}\NormalTok{,}
           \AttributeTok{label =} \StringTok{"median"}\NormalTok{,}
           \AttributeTok{colour =} \StringTok{"darkred"}\NormalTok{,}
           \AttributeTok{family =} \StringTok{"mono"}\NormalTok{) }\SpecialCharTok{+}

  \CommentTok{\# Add curved arrow for median:}
  \FunctionTok{annotate}\NormalTok{(}
    \AttributeTok{geom =} \StringTok{"curve"}\NormalTok{,}
    \AttributeTok{x =} \DecValTok{38}\NormalTok{,}
    \AttributeTok{y =} \FloatTok{0.006}\NormalTok{,}
    \AttributeTok{xend =} \FloatTok{35.2}\NormalTok{,}
    \AttributeTok{yend =} \FloatTok{0.004}\NormalTok{,}
    \AttributeTok{curvature =} \SpecialCharTok{{-}}\FloatTok{0.5}\NormalTok{,}
    \AttributeTok{arrow =} \FunctionTok{arrow}\NormalTok{(}\AttributeTok{length =} \FunctionTok{unit}\NormalTok{(}\FloatTok{0.2}\NormalTok{, }\StringTok{"cm"}\NormalTok{)),}
    \AttributeTok{colour =} \StringTok{"darkred"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{name =} \StringTok{"Density"}\NormalTok{,}
                     \AttributeTok{breaks =}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{breaks\_width}\NormalTok{(}\FloatTok{0.01}\NormalTok{),}
                     \AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{name =} \StringTok{"Age"}\NormalTok{,}
                     \AttributeTok{breaks =}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{breaks\_width}\NormalTok{(}\DecValTok{2}\NormalTok{),}
                     \AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ normal.age.sd8, }\AttributeTok{y =} \FunctionTok{after\_stat}\NormalTok{(density)),}
                 \AttributeTok{binwidth =} \DecValTok{2}\NormalTok{,}
                 \AttributeTok{fill =} \StringTok{"black"}\NormalTok{,}
                 \AttributeTok{alpha =} \FloatTok{0.4}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_density}\NormalTok{(}\AttributeTok{colour =} \StringTok{"purple"}\NormalTok{,}
               \AttributeTok{linewidth =} \FloatTok{0.8}\NormalTok{,}
               \AttributeTok{fill =} \StringTok{"purple"}\NormalTok{,}
               \AttributeTok{alpha =} \FloatTok{0.3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{8_DescriptiveStats_files/figure-pdf/fig-NormalDistribution-1.pdf}}

}

\caption{\label{fig-NormalDistribution}A normal distribution of the age
of a fictitious group of participants}

\end{figure}%

In Figure~\ref{fig-NormalDistribution}, the mean (34.9235) and the
median (35) age of our 10,000 fictitious learners are very close to each
other. So close that, on the plot, the lines overlap. The skew is near
zero, hence the density curve forms near-symmetrical bell shape. This
means that the purple area under the curve to the left of the central
tendency is pretty much the same as the area to the right of the line.
In other words, there are as many people whose age is below the central
tendency (50\%) as there are people whose age is above the central
tendency (50\%). These are the characteristics of a normal distribution.

\begin{figure}[H]

{\centering \includegraphics[width=4.21875in,height=\textheight,keepaspectratio]{images/AHorst_Non-normal.png}

}

\caption{Understanding and being able to recognise the characteristics
of a normal distribution is important as many statistical tests assume
that the variables entered in these tests are (approximately) normally
distributed (see Chapter~\ref{sec-Inferential}).}

\end{figure}%

\subsection{Non-normal (or non-parametric)
distributions}\label{sec-NonNormal}

We saw that the distribution of L2 participants' ages in DÄ…browska
(2019) was close to a normal distribution but with a right skew towards
older years. This meant that the mean age was higher than median age.

Figure~\ref{fig-L1AgeDensityPlot} shows the distribution of L1
participants' ages both as a histogram (in grey) and a density plot (in
purple). Both of these visualisations make quite clear that this
distribution of ages is not at all normal! It is \textbf{non-normal} or
\textbf{non-parametric}. This is because it does \emph{not} consist of
one (more or less) symmetrical bell shape. Instead, we can see several
small bell shapes.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L1.data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Age)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Age, }\AttributeTok{y =} \FunctionTok{after\_stat}\NormalTok{(density)),}
                 \AttributeTok{binwidth =} \DecValTok{2}\NormalTok{,}
                 \AttributeTok{fill =} \StringTok{"black"}\NormalTok{,}
                 \AttributeTok{alpha =} \FloatTok{0.4}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_density}\NormalTok{(}\AttributeTok{colour =} \StringTok{"purple"}\NormalTok{,}
               \AttributeTok{fill =} \StringTok{"purple"}\NormalTok{,}
               \AttributeTok{alpha =} \FloatTok{0.2}\NormalTok{,}
               \AttributeTok{linewidth =} \FloatTok{0.8}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{xintercept =} \FunctionTok{mean}\NormalTok{(Age)),}
             \AttributeTok{color =} \StringTok{"\#0000EE"}\NormalTok{,}
             \AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{,}
             \AttributeTok{linewidth =} \FloatTok{0.8}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{xintercept =} \FunctionTok{get\_mode}\NormalTok{(Age)),}
             \AttributeTok{color =} \StringTok{"black"}\NormalTok{,}
             \AttributeTok{linewidth =} \FloatTok{0.8}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{xintercept =} \FunctionTok{median}\NormalTok{(Age)),}
             \AttributeTok{color =} \StringTok{"darkred"}\NormalTok{,}
             \AttributeTok{linewidth =} \FloatTok{0.8}\NormalTok{) }\SpecialCharTok{+} 
  
  \CommentTok{\# Add label for mode:}
  \FunctionTok{annotate}\NormalTok{(}\StringTok{"text"}\NormalTok{,}
           \AttributeTok{x =} \FloatTok{20.5}\NormalTok{,}
           \AttributeTok{y =} \FloatTok{0.064}\NormalTok{,}
           \AttributeTok{label =} \StringTok{"mode"}\NormalTok{,}
           \AttributeTok{colour =} \StringTok{"black"}\NormalTok{,}
           \AttributeTok{family =} \StringTok{"mono"}\NormalTok{) }\SpecialCharTok{+}

  \CommentTok{\# Add curved arrow for mode:}
  \FunctionTok{annotate}\NormalTok{(}
    \AttributeTok{geom =} \StringTok{"curve"}\NormalTok{,}
    \AttributeTok{x =} \FloatTok{20.5}\NormalTok{,}
    \AttributeTok{y =} \FloatTok{0.062}\NormalTok{,}
    \AttributeTok{xend =} \DecValTok{18}\NormalTok{,}
    \AttributeTok{yend =} \FloatTok{0.058}\NormalTok{,}
    \AttributeTok{curvature =} \SpecialCharTok{{-}}\FloatTok{0.4}\NormalTok{,}
    \AttributeTok{arrow =} \FunctionTok{arrow}\NormalTok{(}\AttributeTok{length =} \FunctionTok{unit}\NormalTok{(}\FloatTok{0.2}\NormalTok{, }\StringTok{"cm"}\NormalTok{)),}
    \AttributeTok{colour =} \StringTok{"black"}\NormalTok{) }\SpecialCharTok{+}

  \CommentTok{\# Add label for mean:}
  \FunctionTok{annotate}\NormalTok{(}\StringTok{"text"}\NormalTok{,}
           \AttributeTok{x =} \DecValTok{40}\NormalTok{,}
           \AttributeTok{y =} \FloatTok{0.03}\NormalTok{,}
           \AttributeTok{label =} \StringTok{"mean"}\NormalTok{,}
           \AttributeTok{colour =} \StringTok{"\#0000EE"}\NormalTok{,}
           \AttributeTok{family =} \StringTok{"mono"}\NormalTok{) }\SpecialCharTok{+}

  \CommentTok{\# Add curved arrow for mean:}
  \FunctionTok{annotate}\NormalTok{(}
    \AttributeTok{geom =} \StringTok{"curve"}\NormalTok{,}
    \AttributeTok{x =} \DecValTok{40}\NormalTok{,}
    \AttributeTok{y =} \FloatTok{0.028}\NormalTok{,}
    \AttributeTok{xend =} \FloatTok{37.8}\NormalTok{,}
    \AttributeTok{yend =} \FloatTok{0.025}\NormalTok{,}
    \AttributeTok{curvature =} \SpecialCharTok{{-}}\FloatTok{0.4}\NormalTok{,}
    \AttributeTok{arrow =} \FunctionTok{arrow}\NormalTok{(}\AttributeTok{length =} \FunctionTok{unit}\NormalTok{(}\FloatTok{0.2}\NormalTok{, }\StringTok{"cm"}\NormalTok{)),}
    \AttributeTok{colour =} \StringTok{"\#0000EE"}\NormalTok{) }\SpecialCharTok{+}
  
  \CommentTok{\# Add label for median:}
  \FunctionTok{annotate}\NormalTok{(}\StringTok{"text"}\NormalTok{,}
           \AttributeTok{x =} \DecValTok{28}\NormalTok{,}
           \AttributeTok{y =} \FloatTok{0.01}\NormalTok{,}
           \AttributeTok{label =} \StringTok{"median"}\NormalTok{,}
           \AttributeTok{colour =} \StringTok{"darkred"}\NormalTok{,}
           \AttributeTok{family =} \StringTok{"mono"}\NormalTok{) }\SpecialCharTok{+}

  \CommentTok{\# Add curved arrow for median:}
  \FunctionTok{annotate}\NormalTok{(}
    \AttributeTok{geom =} \StringTok{"curve"}\NormalTok{,}
    \AttributeTok{x =} \DecValTok{28}\NormalTok{,}
    \AttributeTok{y =} \FloatTok{0.008}\NormalTok{,}
    \AttributeTok{xend =} \FloatTok{31.7}\NormalTok{,}
    \AttributeTok{yend =} \FloatTok{0.004}\NormalTok{,}
    \AttributeTok{curvature =} \FloatTok{0.4}\NormalTok{,}
    \AttributeTok{arrow =} \FunctionTok{arrow}\NormalTok{(}\AttributeTok{length =} \FunctionTok{unit}\NormalTok{(}\FloatTok{0.2}\NormalTok{, }\StringTok{"cm"}\NormalTok{)),}
    \AttributeTok{colour =} \StringTok{"darkred"}\NormalTok{) }\SpecialCharTok{+}  

  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{name =} \StringTok{"Density"}\NormalTok{,}
                     \AttributeTok{breaks =}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{breaks\_width}\NormalTok{(}\FloatTok{0.01}\NormalTok{),}
                     \AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{breaks =}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{breaks\_width}\NormalTok{(}\DecValTok{2}\NormalTok{),}
                     \AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{8_DescriptiveStats_files/figure-pdf/fig-L1AgeDensityPlot-1.pdf}}

}

\caption{\label{fig-L1AgeDensityPlot}Density plot showing the
distribution of L1 participants' ages}

\end{figure}%

The histogram also shows that the most frequent age (the \textbf{mode})
corresponds to the lowest age in the dataset (\texttt{17}) and that both
the \textbf{median} and \textbf{mean} are far removed from most
participants' ages. This distribution of L1 participants' ages points to
the limits of measures of central tendency. They are useful to describe
approximately normally distributed variables, but are far less
informative when it comes to other types of distributions.

\section{Measures of variability}\label{sec-Variability}

Measures of central tendency should never be reported alone. As we saw
with L1 participants' age in Section~\ref{sec-NonNormal}, measures of
central tendency are not always very informative and can even be
misleading. But, even when they are informative, they only tell us part
of the story: the average value of a dataset, but not the
\textbf{spread} or \textbf{variability} of the data. For example, a mean
age of 25 might suggest a group of young adults, but without a measure
of variability, we can't tell if the group is relatively homogeneous
(e.g.~all students in their twenties) or heterogeneous (e.g.~with some
participants in their teens and others in their thirties or older).
Therefore, it is essential to report measures of central tendency in
conjunction with measures of variability, such as the range,
interquartile range, or standard deviation, to provide a more complete
picture of the data.

Consider the three distributions of ages presented in
Figure~\ref{fig-3NormalDistributions}. As you can tell from their
shapes, these are three normal distributions. They each have exactly the
same mean and median of 35; yet they evidently correspond to very
different groups of people!

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# The \{truncnorm\} package contains density, probability, quantile and random number generation functions for the truncated normal distribution. You will need to install it before you can load it.}

\CommentTok{\#install.packages("truncnorm")}
\FunctionTok{library}\NormalTok{(truncnorm)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}
\NormalTok{normal.age.A }\OtherTok{\textless{}{-}} \FunctionTok{rtruncnorm}\NormalTok{(}\AttributeTok{mean =} \DecValTok{35}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{, }\AttributeTok{n =} \DecValTok{10000}\NormalTok{, }\AttributeTok{a =} \DecValTok{10}\NormalTok{, }\AttributeTok{b =} \DecValTok{64}\NormalTok{)}
\NormalTok{normal.age.B }\OtherTok{\textless{}{-}} \FunctionTok{rtruncnorm}\NormalTok{(}\AttributeTok{mean =} \DecValTok{35}\NormalTok{, }\AttributeTok{sd =} \DecValTok{5}\NormalTok{, }\AttributeTok{n =} \DecValTok{10000}\NormalTok{, }\AttributeTok{a =} \DecValTok{10}\NormalTok{, }\AttributeTok{b =} \DecValTok{64}\NormalTok{)}
\NormalTok{normal.age.C }\OtherTok{\textless{}{-}} \FunctionTok{rtruncnorm}\NormalTok{(}\AttributeTok{mean =} \DecValTok{35}\NormalTok{, }\AttributeTok{sd =} \DecValTok{10}\NormalTok{, }\AttributeTok{n =} \DecValTok{10000}\NormalTok{, }\AttributeTok{a =} \DecValTok{10}\NormalTok{, }\AttributeTok{b =} \DecValTok{64}\NormalTok{)}

\NormalTok{normal.density }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(ages) \{}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ ages)) }\SpecialCharTok{+} 
      \FunctionTok{geom\_vline}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{xintercept =} \FunctionTok{mean}\NormalTok{(ages)),}
               \AttributeTok{color =} \StringTok{"\#0000EE"}\NormalTok{,}
               \AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{,}
               \AttributeTok{linewidth =} \FloatTok{0.8}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{name =} \StringTok{"Density"}\NormalTok{,}
                       \CommentTok{\#breaks = scales::breaks\_width(0.01),}
                       \AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{)) }\SpecialCharTok{+}
    \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{name =} \StringTok{"Age"}\NormalTok{,}
                       \AttributeTok{breaks =}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{breaks\_width}\NormalTok{(}\DecValTok{2}\NormalTok{),}
                       \AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{,}\DecValTok{66}\NormalTok{),}
                       \AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{)) }\SpecialCharTok{+}
    \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ ages, }\AttributeTok{y =} \FunctionTok{after\_stat}\NormalTok{(density)),}
                   \AttributeTok{binwidth =} \DecValTok{1}\NormalTok{,}
                   \AttributeTok{fill =} \StringTok{"black"}\NormalTok{,}
                   \AttributeTok{alpha =} \FloatTok{0.4}\NormalTok{) }\SpecialCharTok{+} 
    \FunctionTok{geom\_density}\NormalTok{(}\AttributeTok{colour =} \StringTok{"purple"}\NormalTok{,}
                 \AttributeTok{linewidth =} \FloatTok{0.8}\NormalTok{,}
                 \AttributeTok{fill =} \StringTok{"purple"}\NormalTok{,}
                 \AttributeTok{alpha =} \FloatTok{0.3}\NormalTok{)}
\NormalTok{\}}

\CommentTok{\# The three plots are printed as one figure using the \{patchwork\} library. You will need to install this library before you can load it and use it.}

\CommentTok{\#install.packages("patchwork")}
\FunctionTok{library}\NormalTok{(patchwork)}

\FunctionTok{normal.density}\NormalTok{(normal.age.A) }\SpecialCharTok{/} \FunctionTok{normal.density}\NormalTok{(normal.age.B) }\SpecialCharTok{/} \FunctionTok{normal.density}\NormalTok{(normal.age.C) }\SpecialCharTok{+}
    \CommentTok{\# Add captions A, B, C}
    \FunctionTok{plot\_annotation}\NormalTok{(}\AttributeTok{tag\_levels =} \StringTok{\textquotesingle{}A\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{8_DescriptiveStats_files/figure-pdf/fig-3NormalDistributions-1.pdf}}

}

\caption{\label{fig-3NormalDistributions}Three normal distributions of
ages with a mean/median of 35 years.}

\end{figure}%

Whereas Group A only includes adults aged 32 to 39, the Group C includes
children as young as 10 as well as adults well into their 50s and early
60s - even though they both have the same mean/median on 35. This is why
both measures of central tendency \emph{and} variability are important
when describing numeric variables! Measures of variability help us to
understand how far each data point is from the central tendency. Hence,
for Group A in Figure~\ref{fig-3NormalDistributions}, we can say that
all data points are pretty close to the mean/median of 35. In Group B,
participants' ages are, on average, more `spread out' to the left and
right of the central tendency. And this is even more notable in Group C.

\subsection{Range}\label{sec-Range}

The most basic measure of variability is one that you will already be
familiar with: range. It is easily calculated by subtracting the highest
value of a variable from its lowest value. For example, in DÄ…browska
(2019), the range of results obtained by the L1 participants in the
English grammar comprehension test is:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{max}\NormalTok{(L1.data}\SpecialCharTok{$}\NormalTok{GrammarR) }\SpecialCharTok{{-}} \FunctionTok{min}\NormalTok{(L1.data}\SpecialCharTok{$}\NormalTok{GrammarR)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 22
\end{verbatim}

By contrast, the range of results in this same test among the L2
participants is:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{max}\NormalTok{(L2.data}\SpecialCharTok{$}\NormalTok{GrammarR) }\SpecialCharTok{{-}} \FunctionTok{min}\NormalTok{(L2.data}\SpecialCharTok{$}\NormalTok{GrammarR)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 40
\end{verbatim}

In practice, the range is usually reported by explicitly mentioning a
variable's lowest and highest values as this is usually much more
informative than the range itself. Here is how DÄ…browska (2019) reports
the age of the participants in the published article:

\begin{quote}
The L1 participants were all born and raised in the United Kingdom and
were selected to ensure a range of ages, occupations, and educational
backgrounds. The age range was from 17 to 65 years {[}\ldots{]}. The
nonnative participants ranged in age from 20 to 62 years {[}\ldots{]}
(DÄ…browska 2019: 6).
\end{quote}

\subsection{Interquartile range}\label{sec-IQR}

We saw that the median is a measure of central tendency that represents
the middle value. This means that 50\% of the data falls below the
median and 50\% falls above the median. Going back to the test results
of our six learners of Breton in Fiji, this means that half of the class
scored below \texttt{86.5} and the other half above \texttt{86.5}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{median}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{82}\NormalTok{, }\DecValTok{86}\NormalTok{, }\DecValTok{87}\NormalTok{, }\DecValTok{89}\NormalTok{, }\DecValTok{91}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 86.5
\end{verbatim}

We can further subdivide the distribution into chunks of 25\% of the
data, or \textbf{quartiles} (see Figure~\ref{fig-MakingIQR}).

\begin{itemize}
\item
  The \textbf{first quartile (Q\textsubscript{1})} is the value below
  which 25\% of the data falls. In other words, the first quartile
  corresponds to a value that lies above one-quarter of the values in
  the data set.
\item
  The \textbf{second quartile (Q\textsubscript{2})} is the
  \textbf{median} and, as we know, half of the data (25\% + 25\% = 50\%)
  are below this value, the other half are above.
\item
  The \textbf{third quartile (Q\textsubscript{3})} is the value below
  which 75\% of the data falls. In other words, it is also the value
  above which the upper 25\% of the data are.
\item
  The \textbf{interquartile range (IQR)} is the range between the second
  and the third quartile: it therefore covers the middle 50\% of the
  data. This is illustrated below with a growing number of imaginary
  Breton learners.
\end{itemize}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/GifCh8a.png}}

}

\caption{\label{fig-MakingIQR}Animation showing the interquartile range
of five different sets of values (Le Foll 2024. Zenodo.
\url{https://doi.org/10.5281/zenodo.17440319})}

\end{figure}%

The easiest way to examine a variable's IQR in \texttt{R} is to use the
handy \texttt{summary()} function which, when applied to a numeric
variable, returns a number of useful descriptive statistics including
its first and third quartiles.\footnote{Quartiles can also be computed
  using the \texttt{quantile()} function, which takes two arguments: the
  variable and a value between 0 and 1 corresponding to our
  \textbf{quantile} of interest. We are interested in the \textbf{first}
  and \textbf{third quartiles}, therefore in the values below which lie
  one quarter (\texttt{0.25}) and three-quarters (\texttt{0.75}) of all
  the data.

  To compute the first quantile (Q\textsubscript{1}), we therefore
  enter:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{quantile}\NormalTok{(L1.data}\SpecialCharTok{$}\NormalTok{GrammarR, }\FloatTok{0.25}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Verbatim}
  25% 
71.25 
\end{Verbatim}

  For the third quantile (Q\textsubscript{3}), we need:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{quantile}\NormalTok{(L1.data}\SpecialCharTok{$}\NormalTok{GrammarR, }\FloatTok{0.75}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Verbatim}
75% 
 79 
\end{Verbatim}
}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(L1.data}\SpecialCharTok{$}\NormalTok{GrammarR)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  58.00   71.25   76.00   74.42   79.00   80.00 
\end{verbatim}

From the output of the \texttt{summary()} function, we can easily
calculate the IQR, which we know is equal to the range between the first
and the third quantile.

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{79} \SpecialCharTok{{-}} \FloatTok{71.25}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 7.75
\end{verbatim}

Alternatively, we can compute the IQR directly using the \texttt{IQR()}
function.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{IQR}\NormalTok{(L1.data}\SpecialCharTok{$}\NormalTok{GrammarR)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 7.75
\end{verbatim}

The reason that the \texttt{summary()} function is probably more useful
than \texttt{IQR()} is that, like the full range, the interquantile
range is not usually reported as the difference between
Q\textsubscript{3} and Q\textsubscript{1}. This is because it is more
informative to consider the first quartile (Q\textsubscript{1}), the
median (Q\textsubscript{2}), and the third quartile (Q\textsubscript{3})
together to grasp both the central tendency of a set of numbers and the
amount of variability there is around this central tendency.

In practice, quartiles are rarely reported as numbers. Instead, they are
usually visualised as \textbf{boxplots}. Boxplots present a visual
summary of a numeric variable's central tendency and variability around
this central tendency. On a boxplot, the \textbf{box} represents the
\textbf{IQR}. Its \textbf{dividing line} is the \textbf{median}. The
\textbf{whiskers} and any \textbf{outlier points} represent the rest of
the distribution (see Figure~\ref{fig-MakingBoxplot}). In other words,
the lower whisker roughly covers the lower 25\% of the data and the
upper whisker the top 25\% of the data. Boxplots are most often
displayed vertically and are used to visually compare the main
characteristics of distributions of numeric values across different
groups (e.g.~grammar comprehension across different language proficiency
groups).

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/GifCh8b.png}}

}

\caption{\label{fig-MakingBoxplot}Screenshot of animation showing the
making of a boxplot (Le Foll 2024. Zenodo.
\url{https://doi.org/10.5281/zenodo.17440384})}

\end{figure}%

Remember that, in a perfectly normal distribution, the mean and median
are equal. When a variable follows a normal distribution, its box is
divided into two equal halves and the whiskers are of equal length (see
\textbf{?@fig-NormalBoxplots}). This symmetry comes from the fact that
the values are equally distributed to the left and right of the
median/mean. For the same reason, the bells of the normal distributions
in Figure~\ref{fig-3NormalDistributions} were all (almost) symmetrical,
although they had very different heights and widths.

\subsection{Standard deviation}\label{sec-SD}

In the language sciences and in many other disciplines, standard
deviation is the most common reported measure of variability. Whereas
the interquartile range (IQR) is a measure of variability around the
median, \textbf{standard deviation (SD)} measures variability around the
mean. In other words, if you report a mean value as a measure of central
tendency, you should report the standard deviation along side it.
However, if you report the median, than it makes more sense to report
the IQR in the form of a boxplot (see Section~\ref{sec-IQR}).

In a nutshell, the standard deviation tells us how far away, on average,
each data point is from the mean.

~Considering the test scores of our five Breton learners, we already
know that the standard deviation is likely to be large because the mean
(70.6) is quite far away from all five data points.

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{5}\NormalTok{, }\DecValTok{82}\NormalTok{, }\DecValTok{86}\NormalTok{, }\DecValTok{89}\NormalTok{, }\DecValTok{91}
\end{Highlighting}
\end{Shaded}

To calculate how far exactly, we first measure how far each point is
from the mean, e.g.~for the first data point we calculate
\texttt{5\ -\ 70.6}, for the second \texttt{82\ -\ 70.6}, etc.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Breton.scores }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{82}\NormalTok{, }\DecValTok{86}\NormalTok{, }\DecValTok{89}\NormalTok{, }\DecValTok{91}\NormalTok{)}

\NormalTok{Breton.scores }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(Breton.scores)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] -65.6  11.4  15.4  18.4  20.4
\end{verbatim}

As you can see, some of the differences between the data points and the
mean value are negative, whilst others are positive. For standard
deviation, we are not interested in whether data points are above or
below the mean, but rather in how far removed they are from the mean. To
remove any negative sign, we therefore square all these distances. The
squaring operation (\texttt{\^{}2}) also has the effect making large
differences even larger.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(Breton.scores }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(Breton.scores))}\SpecialCharTok{\^{}}\DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 4303.36  129.96  237.16  338.56  416.16
\end{verbatim}

Remember that standard deviation is a measure of how different, on
average, a set of numbers are from one another, with respect to the
mean. We have just calculated the sum of the squared differences from
the mean and we now need to calculate the average of these squared
differences. To calculate the mean squared difference, we sum the
differences and divide them by the number of data points.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sum}\NormalTok{((Breton.scores }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(Breton.scores))}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{/} \DecValTok{5}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1085.04
\end{verbatim}

This is the \textbf{variance}. The problem with the variance is that it
is not in the original scale of our variable, but rather in squared
units, i.e.~here, in squared test scores, which is rather difficult to
interpret! This is why we more commonly report the \textbf{standard
deviation}, which is the square root of the variance. The square root
function in \texttt{R} is \texttt{sqrt()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{sum}\NormalTok{((Breton.scores }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(Breton.scores))}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{/} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 32.93995
\end{verbatim}

From the above result, we can deduce that, on average, learners' test
scores are 32 points away from the group mean of 70.6 points.

Of course, there is a base \texttt{R} function to calculate the standard
deviation. It is called \texttt{sd()}. However, if we use the
\texttt{sd()} function to calculate the standard deviation of our five
Breton learners' test scores, we get a slightly different result.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sd}\NormalTok{(Breton.scores)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 36.82798
\end{verbatim}

This is because, in practice, we almost always divide the sum of squares
not by the total number of data points (\emph{N} ), but by the total
number minus one (\emph{N}-1). This is the difference between the
\textbf{population standard deviation} and the \textbf{sample standard
deviation}. The population standard deviation is used when we have
access to the entire population (e.g.~all L2 English users worldwide!),
which is rare in real-world scenarios. In most cases, we work with
samples (e.g.~as in DÄ…browska 2019, a sample of 67 L2 English users).
Dividing by \emph{N}-1 gives us a more accurate estimate of the
population's standard deviation based on our sample. It helps to reduce
the bias in our estimate, making it a more reliable measure of
variability around the mean.

In \texttt{R}, the \texttt{sd()} function calculates the sample standard
deviation.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{sum}\NormalTok{((Breton.scores }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(Breton.scores))}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{/} \DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 36.82798
\end{verbatim}

With a normal distribution, the standard deviation informs us about the
width of the bell around the central tendency. In
Figure~\ref{fig-3NormalDistributions} we saw that three normal
distributions, all with a median/mean of 35 could have very different
bell shapes. This is because they have very different standard
deviations around that central tendency. Let us compare the distribution
shapes of these three distributions in detail.

Distribution A (Figure~\ref{fig-DensityA}) is a normal distribution with
a mean of 35 years (xÌ… = 35) and a standard deviation of one year (sd =
1).

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{8_DescriptiveStats_files/figure-pdf/fig-DensityA-1.pdf}}

}

\caption{\label{fig-DensityA}Density plot of Distribution A}

\end{figure}%

Distribution B (Figure~\ref{fig-DensityB}) is a normal distribution with
a mean of 35 years (xÌ… = 35) and a standard deviation of 5 years (sd =
5).

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{8_DescriptiveStats_files/figure-pdf/fig-DensityB-1.pdf}}

}

\caption{\label{fig-DensityB}Density plot of Distribution B}

\end{figure}%

Distribution C (Figure~\ref{fig-DensityC}) is a normal distribution with
a mean of 35 years (xÌ… = 35) and a standard deviation of 10 years (sd =
10).

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{8_DescriptiveStats_files/figure-pdf/fig-DensityC-1.pdf}}

}

\caption{\label{fig-DensityC}Density plot of Distribution C}

\end{figure}%

The standard deviation provides a single metric of the variability
around the mean. This means that knowing the mean and standard deviation
of a numeric variable is not enough to tell whether a distribution is
(approximately) normal or skewed. Like the range and the IQR, a large
standard deviation value indicates greater variability within a
variable, but tells us nothing more. For instance, comparing the
following two SDs tells us that there is more variability around the
mean in L2 participants' grammar comprehension test scores than in that
of the L1 participants, but nothing more about the distribution of the
test scores in either group.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sd}\NormalTok{(L1.data}\SpecialCharTok{$}\NormalTok{GrammarR) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{round}\NormalTok{(}\AttributeTok{digits =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 5.01
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sd}\NormalTok{(L2.data}\SpecialCharTok{$}\NormalTok{GrammarR) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{round}\NormalTok{(}\AttributeTok{digits =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 13.48
\end{verbatim}

In this respect, boxplots are more informative (compare the above SDs
with \textbf{?@fig-GrammarRBoxplots}). To evaluate the full shape of a
numeric variable's distribution, however, there is no alternative to
plotting it as a histogram or density plot.

In sum, remember that, when describing variables, it is important to
report both an appropriate measure of central tendency \emph{and} an
appropriate measure of variability. In addition, it is good practice to
visualise the full distribution of a variable's values in the form of a
table, histogram, or density plot (see Chapter 11 on data
visualisation). This is because any combination of a single measure of
central tendency and a single measure of variability can correspond to
an array of different distribution shapes.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Further reading}, titlerule=0mm, leftrule=.75mm]

As a follow-up, I highly recommend reading this short and highly
accessible article by Fahd Alhazmi (2020), who provides a wonderful
visual guide to understanding standard deviation:
\url{https://medium.com/data-science/a-visual-interpretation-of-the-standard-deviation-30f4676c291c}.

\end{tcolorbox}

\subsection*{Check your progress ðŸŒŸ}\label{check-your-progress-7}
\addcontentsline{toc}{subsection}{Check your progress ðŸŒŸ}

You have successfully completed { out of 21 questions} in this chapter.

Are you confident that you can\ldots?

\begin{itemize}
\tightlist
\item[$\square$]
  Use and interpret different measures of central tendency
  (Section~\ref{sec-CentralTendency})
\item[$\square$]
  Calculate the mode, mean, median of a numeric variable in \texttt{R}
  (Section~\ref{sec-Mean} - Section~\ref{sec-Median})
\item[$\square$]
  Interpret histograms and density plots
  (Section~\ref{sec-Distributions})
\item[$\square$]
  Recognise the characteristics of a normal distribution
  (Section~\ref{sec-Normal})
\item[$\square$]
  Interpret and calculate the interquartile range in \texttt{R}
  (Section~\ref{sec-IQR})
\item[$\square$]
  Interpret boxplots (Section~\ref{sec-IQR})
\item[$\square$]
  Interpret and calculate the standard deviation (Section~\ref{sec-SD})
\end{itemize}

In \href{@sec-DataViz}{Chapter 10}, we will cover the basics of
\textbf{data visualisation} and learn how to create a range of
informative and elegant plots (including histograms and density plots)
using the popular \texttt{R} package \texttt{ggplot2}. But, first, we
need to learn about \textbf{data wrangling}
(\href{https://elenlefoll.github.io/RstatsTextbook/9_DataWrangling.html}{Chapter
9}) to prepare our data for data visualisation and multivariable
analyses. Are you ready? ðŸ¤“

\bookmarksetup{startatroot}

\chapter{\texorpdfstring{Data
w\texttt{R}angling}{Data wRangling}}\label{sec-DataWrangling}

\subsection*{Chapter overview}\label{chapter-overview-8}
\addcontentsline{toc}{subsection}{Chapter overview}

In this chapter, you will learn how to:

\begin{itemize}
\tightlist
\item
  Recognise whether a dataset is in a tidy data format
\item
  Check the sanity of an imported dataset
\item
  Pre-process data in a reproducible way using tidyverse functions
\item
  Convert character vectors representing categorical data to factors
\item
  Add and replace columns in a table
\item
  Transform several columns of a table at once
\item
  Use \{stringr\} functions to manipulate text values
\item
  Reshape and combine tables
\item
  Save and export \texttt{R} objects in different formats
\end{itemize}

\section{Welcome to the tidyverse! ðŸª}\label{sec-tidyverse}

This chapter explains how to examine, clean, and manipulate data mostly
using functions from the
\href{https://www.tidyverse.org}{\{tidyverse\}}: a collection of useful
\texttt{R} packages increasingly used for all kinds of data analysis
projects. Tidyverse functions are designed to work with \textbf{tidy
data} (see Figure~\ref{fig-tidydata}) and, as a result, they are often
easier to combine.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/AHorst_tidydata.jpg}}

}

\caption{\label{fig-tidydata}Tidy data illustration from the
\href{https://www.openscapes.org/}{Openscapes} blog
\href{https://www.openscapes.org/blog/2020/10/12/tidy-data/}{Tidy Data
for reproducibility, efficiency, and collaboration} by Horst \& Lowndes
(2020).}

\end{figure}%

Learning to manipulate data and conduct data analysis in \texttt{R}
``the tidyverse-way'' can help make your workflows more efficient.

\begin{quote}
If you ensure that your data are tidy, you'll spend less time fighting
with the tools and more time working on your analysis. (Wickham, Vaughan
\& Girlich)
\end{quote}

\section{\texorpdfstring{Base \texttt{R} vs.~tidyverse
functions}{Base R vs.~tidyverse functions}}\label{sec-Conflicts}

Novice \texttt{R} users may find it confusing that many operations can
be performed using either a base \texttt{R} function or a tidyverse one.
For example, in Chapter~\ref{sec-ImportingData}, we saw that both the
base \texttt{R} function \texttt{read.csv()} and the tidyverse function
\texttt{read\_csv()} can be used to import CSV files. The functions have
slightly different arguments and default values, which can be annoying,
even though they are fundamentally designed to perform the same task.
But don't fret over this too much: it's fine for you to use whichever
function you find most convenient and intuitive and it's also absolutely
fine to combine base \texttt{R} and tidyverse functions!

You will no doubt have noticed that the functions \texttt{read.csv()}
and \texttt{read\_csv()} have very similar but not exactly identical
names. This is helpful to differentiate between the two functions.
Unfortunately, some function names are found in several packages, which
can lead to confusion and errors! For example, you may have noticed that
when you load the tidyverse library the first time in a project, a
message similar to Figure~\ref{fig-tidyverseConflicts} is printed in the
Console.

\begin{figure}

\centering{

\includegraphics[width=5.375in,height=\textheight,keepaspectratio]{images/TidyverseConflicts.png}

}

\caption{\label{fig-tidyverseConflicts}Screenshot of the \texttt{R}
Console after having loaded the \{tidyverse\} library}

\end{figure}%

First, the error message reproduced in
Figure~\ref{fig-tidyverseConflicts} confirms that loading the
\{tidyverse\} package has led to the successful loading of a total of
nine packages and that these are now ready to use. Crucially, the
message also warns us about \textbf{conflicts} between some
\{tidyverse\} packages and base \texttt{R} packages. These conflicts are
due to the fact that two functions from the \{dplyr\} package have
exactly the same name as functions from the base \texttt{R} \{stats\}
package. The warning informs us that, by default, the \{dplyr\}
functions will be applied.

To force \texttt{R} to use a function from a specific package, we can
use the \texttt{package::function()} syntax. Hence, to force \texttt{R}
to use the base \texttt{R} \{stats\} \texttt{filter()} function rather
than the tidyverse one, we would use \texttt{stats::filter()}. On the
contrary, if we want to be absolutely certain that the tidyverse one is
used, we can use \texttt{dplyr::filter()}.

\begin{figure}

\centering{

\includegraphics[width=3.125in,height=\textheight,keepaspectratio]{images/AHorst_tidyverse.png}

}

\caption{\label{fig-tidyverse}A galaxy of tidyverse-related hex stickers
(artwork by
\href{https://allisonhorst.com/allison-horst}{@allison\_horst}).}

\end{figure}%

In this chapter, we will explore functions from
\{\href{https://dplyr.tidyverse.org/}{dplyr}\},
\{\href{https://stringr.tidyverse.org/}{stringr}\}, and
\{\href{https://tidyr.tidyverse.org/}{tidyr}\}. The popular
\{\href{https://ggplot2.tidyverse.org/}{ggplot2}\} tidyverse library for
data visualisation following the ``Grammar of Graphics'' approach will
be introduced in \href{@sec-DataViz}{Chapter 10}. Make sure that you
have loaded the tidyverse packages before proceeding with the rest of
this chapter.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

\section{Checking data sanity}\label{sec-Sanity}

Before beginning any data analysis, it is important to always check the
sanity of our data. In the following, we will use tables and descriptive
statistics to do this. In \href{@sec-DataViz}{Chapter 10}, we will learn
how to use data visualisation to check for outliers and other issues
that may affect our analyses.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-warning-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-warning-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Prerequisites}, titlerule=0mm, leftrule=.75mm]

In this chapter and the following chapters, all examples, tasks, and
quiz questions are based on data from:

\begin{quote}
DÄ…browska, Ewa. 2019. Experience, Aptitude, and Individual Differences
in Linguistic Attainment: A Comparison of Native and Nonnative Speakers.
Language Learning 69(S1). 72--100.
\url{https://doi.org/10.1111/lang.12323}.
\end{quote}

You will only be able to reproduce the analyses and answer the quiz
questions from this chapter if you have created an RProject and
successfully imported the two datasets from DÄ…browska (2019) into your
local \texttt{R} environment (see Figure~\ref{fig-DataLoaded}). Detailed
instructions to do so can be found from Section~\ref{sec-RProject} to
Section~\ref{sec-ImportingDataCSV}.

Alternatively, you can download \texttt{Dabrowska2019.zip} from
\href{https://github.com/elenlefoll/RstatsTextbook/raw/69d1e31be7394f2b612825f031ebffeb75886390/Dabrowska2019.zip}{the
textbook's GitHub repository}. To launch the project correctly, first
unzip the file and then double-click on the \texttt{Dabrowska2019.Rproj}
file.

\end{tcolorbox}

Before we get started, make sure that both the L1 and the L2 datasets
are correctly loaded by checking the structure of the \texttt{R} objects
using the \texttt{str()} function.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(here)}

\NormalTok{L1.data }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\AttributeTok{file =} \FunctionTok{here}\NormalTok{(}\StringTok{"data"}\NormalTok{, }\StringTok{"L1\_data.csv"}\NormalTok{))}
\FunctionTok{str}\NormalTok{(L1.data)}

\NormalTok{L2.data }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\AttributeTok{file =} \FunctionTok{here}\NormalTok{(}\StringTok{"data"}\NormalTok{, }\StringTok{"L2\_data.csv"}\NormalTok{))}
\FunctionTok{str}\NormalTok{(L2.data)}
\end{Highlighting}
\end{Shaded}

\subsection{Numeric variables}\label{sec-CheckNumeric}

In Section~\ref{sec-IQR}, we used the \texttt{summary()} function to
obtain some useful descriptive statistics on a single numeric variable,
namely the range, mean, median, and interquartile range (IQR).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(L1.data}\SpecialCharTok{$}\NormalTok{GrammarR)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  58.00   71.25   76.00   74.42   79.00   80.00 
\end{verbatim}

To check the sanity of a dataset, we can use this same function on an
entire data table (provided that the data are in the tidy format, see
Section~\ref{sec-tidyverse}). Thus, the command
\texttt{summary(L1.data)}\footnote{Note that, throughout this chapter,
  long code output is shortened to save space. When you run this command
  on your own computer, however, you will see that the output is much
  longer than what is reprinted in this chapter. You will likely need to
  scroll up in your Console window to view it all.} outputs summary
statistics on all the variables of the L1 dataset - in other words, on
all the columns of the data frame \texttt{L1.data}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(L1.data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 Participant             Age           Gender           Occupation       
 Length:90          Min.   :17.00   Length:90          Length:90         
 Class :character   1st Qu.:25.00   Class :character   Class :character  
 Mode  :character   Median :32.00   Mode  :character   Mode  :character  
                    Mean   :37.54                                        
                    3rd Qu.:55.00                                        
                    Max.   :65.00                                        
  OccupGroup          OtherLgs          Education             EduYrs     
 Length:90          Length:90          Length:90          Min.   :10.00  
 Class :character   Class :character   Class :character   1st Qu.:12.00  
 Mode  :character   Mode  :character   Mode  :character   Median :13.00  
                                                          Mean   :13.71  
                                                          3rd Qu.:14.00  
                                                          Max.   :21.00  
    ReadEng1        ReadEng2        ReadEng3        ReadEng      
 Min.   :0.000   Min.   :0.000   Min.   :0.000   Min.   : 0.000  
 1st Qu.:1.000   1st Qu.:1.000   1st Qu.:2.000   1st Qu.: 5.000  
 Median :2.000   Median :2.000   Median :2.000   Median : 7.000  
 Mean   :2.522   Mean   :2.433   Mean   :2.233   Mean   : 7.189  
 3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.: 9.750  
 Max.   :5.000   Max.   :5.000   Max.   :4.000   Max.   :14.000  
\end{verbatim}

For the numeric variables in the dataset, the \texttt{summary()}
function provides us with many useful descriptive statistics to check
the sanity of the data. For example, we can check whether the minimum
values include improbably low values (e.g.~a five-year-old participant
in a written language exam) or outright impossible ones (e.g.~a minus
18-year old participant!). Equally, if we know that the maximum number
of points that could be obtained in the English grammar test is 100, a
maximum value of more than 100 would be highly suspicious and warrant
further investigation.

As far as we can see from the output of \texttt{summary(L1.data)} above,
the numeric variables in DÄ…browska (2019)'s L1 dataset do not appear to
feature any obvious problematic values.

\subsection{Categorical variables as factors}\label{sec-Factors}

Having examined the numeric variables, we now turn to the non-numeric,
categorical ones (see Section~\ref{sec-Variables}). For these variables,
the descriptive statistics returned by \texttt{summary(L1.data)} are not
as insightful. They only tell us that they each include 90 values, which
corresponds to the 90 participants in the L1 dataset. As we can see from
the output of the \texttt{str()} function, these categorical variables
are stored in \texttt{R} as character string vectors (abbreviated in the
\texttt{str()} output to ``chr'').

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(L1.data}\SpecialCharTok{$}\NormalTok{Gender)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 chr [1:90] "M" "M" "M" "F" "F" "F" "F" "M" "M" "F" "F" "M" "M" "F" "M" "F" ...
\end{verbatim}

Character string vectors are a useful \texttt{R} object type for text
but, in \texttt{R}, categorical variables are best stored as factors.
Factors are a more efficient way to store character values because each
unique character value is stored only once. The data itself are stored
as a vector of integers. Let's look at an example.

First, we convert the categorical variable \texttt{Gender} from
\texttt{L1.data} that is currently stored as a character string vector
to a factor vector called \texttt{L1.Gender.fct}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L1.Gender.fct }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(L1.data}\SpecialCharTok{$}\NormalTok{Gender)}
\end{Highlighting}
\end{Shaded}

When we now inspect its structure using \texttt{str()}, we can see that
\texttt{L1.Gender.fct} is a factor with two levels ``F'' and ``M''. The
values themselves, however, are no longer listed as ``M'' ``M'' ``M''
``F'' ``F''\ldots, but rather as integers: 2 2 2 1 1 1\ldots.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(L1.Gender.fct)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 Factor w/ 2 levels "F","M": 2 2 2 1 1 1 1 2 2 1 ...
\end{verbatim}

By default, the levels of a factor are ordered alphabetically, hence in
\texttt{L1.Gender.fct}, 1 corresponds to ``F'' and 2 to ``M''.

The summary output of factor vectors are far more insightful than of
character variables (and look rather like the output of the
\texttt{table()} function that we used in Section~\ref{sec-Mode}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(L1.Gender.fct)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 F  M 
48 42 
\end{verbatim}

The tidyverse package \href{https://forcats.tidyverse.org/}{\{forcats\}}
has a lot of very useful functions to manipulate factors. They all start
with \texttt{fct\_}.

\section{Pre-processing data}\label{sec-Preprocessing}

\subsection{\texorpdfstring{Using \texttt{mutate()} to add and replace
columns}{Using mutate() to add and replace columns}}\label{sec-mutate}

In the previous section, we stored the factor representing L1
participants' gender as a separate \texttt{R} object called
\texttt{L1.Gender.fct}. If, instead, we want to add this factor as an
additional column to our dataset, we can use the \texttt{mutate()}
function from \href{https://dplyr.tidyverse.org/}{\{dplyr\}}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L1.data }\OtherTok{\textless{}{-}}\NormalTok{ L1.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Gender.fct =} \FunctionTok{factor}\NormalTok{(L1.data}\SpecialCharTok{$}\NormalTok{Gender))}
\end{Highlighting}
\end{Shaded}

The \texttt{mutate()} function allows us to add new columns to a
dataset. By default, it also keeps all the existing ones (to control
which columns are retained, check the help file and read about the
``.keep ='' argument).

\begin{figure}

\centering{

\includegraphics[width=4.84375in,height=\textheight,keepaspectratio]{images/AHorst_mutate.png}

}

\caption{\label{fig-mutate}Artwork explaining the
\texttt{dplyr::mutate()} function by
\href{https://allisonhorst.com/allison-horst}{@allison\_horst}.}

\end{figure}%

We can use the \texttt{colnames()} function to check that the new column
has been correctly appended to the table. Alternatively, you can use the
\texttt{View()} function to display the table in full in a new
\emph{RStudio} tab. In both cases, you should see that the new column is
now the last column in the table (column number 32).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{colnames}\NormalTok{(L1.data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "Participant" "Age"         "Gender"      "Occupation"  "OccupGroup" 
 [6] "OtherLgs"    "Education"   "EduYrs"      "ReadEng1"    "ReadEng2"   
[11] "ReadEng3"    "ReadEng"     "Active"      "ObjCl"       "ObjRel"     
[16] "Passive"     "Postmod"     "Q.has"       "Q.is"        "Locative"   
[21] "SubCl"       "SubRel"      "GrammarR"    "Grammar"     "VocabR"     
[26] "Vocab"       "CollocR"     "Colloc"      "Blocks"      "ART"        
[31] "LgAnalysis"  "Gender.fct" 
\end{verbatim}

Watch out: if you add a new column to a table using an existing column
name, \texttt{mutate()} will overwrite the entire content of the
existing column with the new values! In the following code chunk, we are
therefore overwriting the character vector \texttt{Gender} with a factor
vector also called \texttt{Gender}. We should only do this if we are
certain that we won't need to compare the original values with the new
ones!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L1.data }\OtherTok{\textless{}{-}}\NormalTok{ L1.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Gender =} \FunctionTok{factor}\NormalTok{(L1.data}\SpecialCharTok{$}\NormalTok{Gender))}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{Using \texttt{across()} to transform
multiple
columns}{Using across() to transform multiple columns}}\label{sec-across}

In addition to \texttt{Gender}, there are quite a few more character
vectors in \texttt{L1.data} that represent categorical variables and
that would therefore be better stored as factors. We could use
\texttt{mutate()} and \texttt{factor()} to convert them one by one like
we did for \texttt{Gender} above, but that would require several lines
of code in which we could easily make a silly error or two. Instead, we
can use a series of neat tidyverse functions to convert all character
vectors to factor vectors in one go.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L1.data.fct }\OtherTok{\textless{}{-}}\NormalTok{ L1.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{where}\NormalTok{(is.character), factor))}
\end{Highlighting}
\end{Shaded}

Above, we use \texttt{mutate()} to convert \texttt{across()} the entire
dataset all columns \texttt{where()} there are character vectors to
\texttt{factor()} vectors (using the \texttt{is.character()} function to
determine which columns contain character vectors).

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/AHorst_across.png}}

}

\caption{\label{fig-across}Artwork explaining the \texttt{across()}
function by
\href{https://allisonhorst.com/allison-horst}{@allison\_horst}.}

\end{figure}%

We can check that the correct variables have been converted by comparing
the output of \texttt{summary(L1.data)} (partially printed in
Section~\ref{sec-CheckNumeric}) with the output of
\texttt{summary(L1.data.fct)} (partially printed below).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(L1.data.fct)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  Participant      Age        Gender          Occupation OccupGroup
 1      : 1   Min.   :17.00   F:48   Retired       :14   C  :22    
 100    : 1   1st Qu.:25.00   M:42   Student       :14   I  :23    
 101    : 1   Median :32.00          Unemployed    : 4   M  :20    
 104    : 1   Mean   :37.54          Housewife     : 3   PS :24    
 106    : 1   3rd Qu.:55.00          Shop Assistant: 3   PS : 1    
 107    : 1   Max.   :65.00          Teacher       : 3             
 (Other):84                          (Other)       :49             
    OtherLgs                                  Education      EduYrs     
 French : 2   student                              : 8   Min.   :10.00  
 German : 3   A level                              : 5   1st Qu.:12.00  
 None   :84   BA                                   : 5   Median :13.00  
 Spanish: 1   GCSEs                                : 5   Mean   :13.71  
              NVQ                                  : 4   3rd Qu.:14.00  
              Northern Counties School Leaving Exam: 3   Max.   :21.00  
              (Other)                              :60                  
    ReadEng1        ReadEng2        ReadEng3        ReadEng      
 Min.   :0.000   Min.   :0.000   Min.   :0.000   Min.   : 0.000  
 1st Qu.:1.000   1st Qu.:1.000   1st Qu.:2.000   1st Qu.: 5.000  
 Median :2.000   Median :2.000   Median :2.000   Median : 7.000  
 Mean   :2.522   Mean   :2.433   Mean   :2.233   Mean   : 7.189  
 3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.: 9.750  
 Max.   :5.000   Max.   :5.000   Max.   :4.000   Max.   :14.000  
                                                                 
\end{verbatim}

\section{Data cleaning ðŸ§¼}\label{data-cleaning}

By closely examining the data, we noticed that the values of the
categorical variables were not always entered in a consistent way, which
may lead to incorrect analyses. For example, in the L2 dataset, most
female participants' gender is recorded as \texttt{F} except for six
participants, where it is \texttt{f}. As \texttt{R} is a case-sensitive
language, these two factor levels are treated as two different levels of
the \texttt{Gender} variable. This means that any future analyses on the
effect of \texttt{Gender} on language learning will compare participants
across these three groups.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(L2.data.fct}\SpecialCharTok{$}\NormalTok{Gender)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 f  F  M 
 6 40 21 
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-important-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-important-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Important}, titlerule=0mm, leftrule=.75mm]

To ensure that our analyses are \textbf{reproducible} (see
Section~\ref{sec-Reproducibility}) from the beginning to the end, it is
crucial that we document \emph{all} of our corrections in a script. This
ensures that if we need to go back on any data pre-processing decision
that we made or if we need to make any additional corrections, we can do
so without having to re-do our entire analyses. In addition, it means
that our corrections and other data pre-processing steps are transparent
and can be inspected and challenged by our peers.

\end{tcolorbox}

\subsection{Using \{stringr\} functions}\label{sec-stringR}

To convert all of the lower-case ``f'' in the \texttt{Gender} variable
to upper-case ``F'', we can combine the \texttt{mutate()} with the
\texttt{str\_to\_upper()} function. This ensures that all values in the
new \texttt{Gender.corrected} column are in capital letters.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L2.data.cleaned }\OtherTok{\textless{}{-}}\NormalTok{ L2.data.fct }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Gender.corrected =} \FunctionTok{str\_to\_upper}\NormalTok{(Gender))}
\end{Highlighting}
\end{Shaded}

We should check that our correction has gone to plan by comparing the
original \texttt{Gender} variable with the new
\texttt{Gender.corrected}. To this end, we display them side by side
using the \texttt{select()} function from \{dplyr\}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L2.data.cleaned }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(Gender, Gender.corrected)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  Gender Gender.corrected
1      F                F
2      f                F
3      F                F
4      F                F
5      M                M
6      F                F
\end{verbatim}

Like \texttt{mutate()} and \texttt{select()}, \texttt{str\_to\_upper()}
also comes from a tidyverse package\footnote{The equivalent base
  \texttt{R} function is \texttt{toupper()}.}. All functions that begin
with \texttt{str\_} come from the \{stringr\} package, which features
lots of useful functions to manipulate character string vectors. These
include:

\begin{itemize}
\tightlist
\item
  \texttt{str\_to\_upper()} converts to string upper case.
\item
  \texttt{str\_to\_lower()} converts to string lower case.
\item
  \texttt{str\_to\_title()} converts to string title case (i.e.~only the
  first letter of each word is capitalised).
\item
  \texttt{str\_to\_sentence()} converts string to sentence case
  (i.e.~only the first letter of each sentence is capitalised).
\end{itemize}

For more useful functions to manipulate character strings, check out the
\{stringr\} cheatsheet:
\url{https://github.com/rstudio/cheatsheets/blob/main/strings.pdf}.

Note that in the code chunk above, we did not save the output to a new
\texttt{R} object. We merely printed the output in the Console. Once we
have checked that our data wrangling operation went well, we can
overwrite the original \texttt{Gender} variable with the cleaned version
by using the original variable name as the name of the new column.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L2.data.cleaned }\OtherTok{\textless{}{-}}\NormalTok{ L2.data.fct }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Gender =} \FunctionTok{str\_to\_upper}\NormalTok{(Gender))}
\end{Highlighting}
\end{Shaded}

Using \texttt{summary()} or \texttt{class()}, we can see that
manipulating the \texttt{Gender} variable with a function from
\{stringr\} has resulted in the factor variable being converted back to
a character variable.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(L2.data.cleaned}\SpecialCharTok{$}\NormalTok{Gender)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   Length     Class      Mode 
       67 character character 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(L2.data.cleaned}\SpecialCharTok{$}\NormalTok{Gender)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "character"
\end{verbatim}

We therefore need to add a line of code to reconvert it to a factor. We
can do this within a single \texttt{mutate()} command.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L2.data.cleaned }\OtherTok{\textless{}{-}}\NormalTok{ L2.data.fct }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Gender =} \FunctionTok{str\_to\_upper}\NormalTok{(Gender),}
         \AttributeTok{Gender =} \FunctionTok{factor}\NormalTok{(Gender))}

\FunctionTok{class}\NormalTok{(L2.data.cleaned}\SpecialCharTok{$}\NormalTok{Gender)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "factor"
\end{verbatim}

Now the \texttt{summary()} function provides a tally of male and female
participants that corresponds to the values reported in DÄ…browska (2019:
5).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(L2.data.cleaned}\SpecialCharTok{$}\NormalTok{Gender)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 F  M 
46 21 
\end{verbatim}

So far, we have looked at rather simple data cleaning cases. Let's now
turn to a slightly more complex one: In the L2 dataset, the variable
\texttt{NativeLg} contains character string values that correspond to
the L2 participants' native language. Using the base \texttt{R} function
\texttt{unique()}, we can see that there are a total of 22 unique values
in this variable. However using \texttt{sort()} to order these 22 values
alphabetically, we can easily see that there are, in fact, fewer unique
native languages in this dataset due to different spellings and the
inconsistent use of upper-case letters.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L2.data}\SpecialCharTok{$}\NormalTok{NativeLg }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{unique}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{sort}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "Cantonese"           "Cantonese/Hokkein"   "chinese"            
 [4] "Chinese"             "french"              "German"             
 [7] "greek"               "Italian"             "Lithuanian"         
[10] "Lithunanina"         "Lituanian"           "Mandarin"           
[13] "Mandarin Chinese"    "Mandarin/ Cantonese" "mandarin/malaysian" 
[16] "Mandarine Chinese"   "polish"              "Polish"             
[19] "Polish/Russian"      "russian"             "Russian"            
[22] "Spanish"            
\end{verbatim}

If we convert all \texttt{NativeLg} values to title case, we can reduce
the number of unique languages to 19.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L2.data}\SpecialCharTok{$}\NormalTok{NativeLg }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{str\_to\_title}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{unique}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{sort}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "Cantonese"           "Cantonese/Hokkein"   "Chinese"            
 [4] "French"              "German"              "Greek"              
 [7] "Italian"             "Lithuanian"          "Lithunanina"        
[10] "Lituanian"           "Mandarin"            "Mandarin Chinese"   
[13] "Mandarin/ Cantonese" "Mandarin/Malaysian"  "Mandarine Chinese"  
[16] "Polish"              "Polish/Russian"      "Russian"            
[19] "Spanish"            
\end{verbatim}

Second, to facilitate further analyses, we may decide to only retain the
first word/language from each entry as this will further reduce the
number of different levels in this categorical variable. To abbreviate
``Mandarin Chinese'' to ``Mandarin'', we can use the \texttt{word()}
function from the \{stringr\} package.

Below is an extract of the help page for the \texttt{word()} function
(accessed with the command \texttt{?word}). Can you work out how to
extract the first word of a character string?

\begin{quote}
\begin{longtable}[]{@{}lr@{}}
\toprule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
word \{stringr\} & R Documentation \\
\end{longtable}

\section*{Extract words from a
sentence}\label{extract-words-from-a-sentence}
\addcontentsline{toc}{section}{Extract words from a sentence}

\markright{Extract words from a sentence}

\subsection*{Description}\label{description}
\addcontentsline{toc}{subsection}{Description}

Extract words from a sentence

\subsection*{Usage}\label{usage}
\addcontentsline{toc}{subsection}{Usage}

\begin{verbatim}
word(string, start = 1L, end = start, sep = fixed(" "))
\end{verbatim}

\subsection*{Arguments}\label{arguments}
\addcontentsline{toc}{subsection}{Arguments}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.1223}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.8777}}@{}}
\toprule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{string} & Input vector. Either a character vector, or something
coercible to one. \\
\texttt{start}, \texttt{end} & Pair of integer vectors giving range of
words (inclusive) to extract. If negative, counts backwards from the
last word.

The default value select the first word. \\
\texttt{sep} & Separator between words. Defaults to single space. \\
\end{longtable}
\end{quote}

The help file tells us that ``The default value select the first word''.
In our case, this means that we can simply use the \texttt{word()}
function with no specified argument as this will automatically retain
only the first word of every entry.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L2.data}\SpecialCharTok{$}\NormalTok{NativeLg }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{str\_to\_title}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{word}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{unique}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{sort}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "Cantonese"          "Cantonese/Hokkein"  "Chinese"           
 [4] "French"             "German"             "Greek"             
 [7] "Italian"            "Lithuanian"         "Lithunanina"       
[10] "Lituanian"          "Mandarin"           "Mandarin/"         
[13] "Mandarin/Malaysian" "Mandarine"          "Polish"            
[16] "Polish/Russian"     "Russian"            "Spanish"           
\end{verbatim}

Alternatively, we can choose to specify the ``start'' argument as a
reminder of what we did and to better document our code. The output is
exactly the same.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L2.data}\SpecialCharTok{$}\NormalTok{NativeLg }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{str\_to\_title}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{word}\NormalTok{(}\AttributeTok{start =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{unique}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{sort}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "Cantonese"          "Cantonese/Hokkein"  "Chinese"           
 [4] "French"             "German"             "Greek"             
 [7] "Italian"            "Lithuanian"         "Lithunanina"       
[10] "Lituanian"          "Mandarin"           "Mandarin/"         
[13] "Mandarin/Malaysian" "Mandarine"          "Polish"            
[16] "Polish/Russian"     "Russian"            "Spanish"           
\end{verbatim}

As you can tell from the output above, the \texttt{word()} function uses
white space to identify word boundaries. In this dataset, however, some
of the participants' native languages are separated by forward slashes
(\texttt{/}) rather than or in addition to spaces. The ``Usage'' section
of the help file for the \texttt{word()} function (see \texttt{?word}
and above) also confirms that the default word separator symbol is a
space and shows us the syntax for changing the default separator. Below
we change it to a forward slash.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L2.data}\SpecialCharTok{$}\NormalTok{NativeLg }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{str\_to\_title}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{word}\NormalTok{(}\AttributeTok{start =} \DecValTok{1}\NormalTok{, }\AttributeTok{sep =} \FunctionTok{fixed}\NormalTok{(}\StringTok{"/"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{unique}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{sort}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "Cantonese"         "Chinese"           "French"           
 [4] "German"            "Greek"             "Italian"          
 [7] "Lithuanian"        "Lithunanina"       "Lituanian"        
[10] "Mandarin"          "Mandarin Chinese"  "Mandarine Chinese"
[13] "Polish"            "Russian"           "Spanish"          
\end{verbatim}

Now we can combine these two word extraction methods using the pipe
operator (\texttt{\textbar{}\textgreater{}}) so that
``Cantonese/Hokkein'' is abbreviated to ``Cantonese'' and ``Mandarin/
Cantonese'' to ``Mandarin''.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L2.data}\SpecialCharTok{$}\NormalTok{NativeLg }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{str\_to\_title}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{word}\NormalTok{(}\AttributeTok{start =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# Extracts the first word before the first space}
  \FunctionTok{word}\NormalTok{(}\AttributeTok{start =} \DecValTok{1}\NormalTok{, }\AttributeTok{sep =} \FunctionTok{fixed}\NormalTok{(}\StringTok{"/"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# Extracts the first word before the first forward slash}
  \FunctionTok{unique}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{sort}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "Cantonese"   "Chinese"     "French"      "German"      "Greek"      
 [6] "Italian"     "Lithuanian"  "Lithunanina" "Lituanian"   "Mandarin"   
[11] "Mandarine"   "Polish"      "Russian"     "Spanish"    
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Going further: Using regular expressions (regex) ðŸ¤“}, titlerule=0mm, leftrule=.75mm]

Many functions of the \{stringr\} package involve \textbf{regular
expressions} (short: \textbf{regex}). The second page of the
\href{https://github.com/rstudio/cheatsheets/blob/main/strings.pdf}{\{stringr\}
cheatsheet} provides a nice overview of how regular expressions can be
used to manipulate character strings in \texttt{R}.

Using the \texttt{str\_extract()} function together with the regex
\texttt{\textbackslash{}\textbackslash{}w+}, it is possible to extract
the first word of each \texttt{NativeLg} value with just one line of
code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L2.data}\SpecialCharTok{$}\NormalTok{NativeLg }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{str\_to\_title}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{str\_extract}\NormalTok{(}\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{w+"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{unique}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{sort}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "Cantonese"   "Chinese"     "French"      "German"      "Greek"      
 [6] "Italian"     "Lithuanian"  "Lithunanina" "Lituanian"   "Mandarin"   
[11] "Mandarine"   "Polish"      "Russian"     "Spanish"    
\end{verbatim}

Regular expressions provide incredibly powerful and versatile ways to
work with text in all kinds of programming languages. When conducting
\textbf{corpus linguistics} research, they also allow us to conduct
complex corpus queries.

Each programming language/software has a slightly different flavour of
regex but the basic principles are the same across all
languages/software and are well worth learning. To get started, I highly
recommend this beautifully designed interactive regex tutorial for
beginners: \url{https://regexlearn.com/learn/regex101}. Have fun! ðŸ¤“

\end{tcolorbox}

\subsection{\texorpdfstring{Using
\texttt{case\_when()}}{Using case\_when()}}\label{sec-casewhen}

We have now reduced the number of levels in the \texttt{NativeLg}
variable to just 14 unique languages. But we still have some typos to
correct, e.g.~``Lithunanina'' and ``Lituanian''.

We can correct these on a case-by-case basis using
\texttt{case\_when()}. This is a very useful tidyverse function from the
\{dplyr\} package that is easy to use once you have gotten used to its
syntax. Figure~\ref{fig-case_when} illustrates the syntax with a toy
example dataset about the dangerousness of dragons (\texttt{df}). In
this annotated line of code in Figure~\ref{fig-case_when},
\texttt{mutate()} is used to add a new column called \texttt{danger}
whose values depend on the type of dragon that we are dealing with. The
first argument of \texttt{case\_when()} determines that, when the dragon
\texttt{type} is equal to ``kraken'', then the \texttt{danger} value is
set to ``extreme'', otherwise the danger value is set to ``high''. You
can see the outcome in the appended \texttt{danger} column.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/AHorst_case_when.png}}

}

\caption{\label{fig-case_when}Artwork explaining the
\texttt{case\_when()} function by
\href{https://allisonhorst.com/allison-horst}{@allison\_horst}).}

\end{figure}%

Applying \texttt{case\_when()} to fix the typos in the \texttt{NativeLg}
variable in \texttt{L2.data}, we determine that:

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  if the shortened \texttt{NativeLg} value is ``Mandarine'', we replace
  it with ``Mandarin'', and
\item
  if the shortened \texttt{NativeLg} value corresponds to either
  ``Lithunanina'' or ``Lituanian'', we replace it with ``Lithuanian''.
\end{enumerate}

Using \texttt{mutate()}, we save this cleaned-up version of the
\texttt{NativeLg} variable as a new column in our \texttt{L2.data}
table, which we call \texttt{NativeLg.cleaned}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L2.data }\OtherTok{\textless{}{-}}\NormalTok{ L2.data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{NativeLg.cleaned =} \FunctionTok{str\_to\_title}\NormalTok{(NativeLg) }\SpecialCharTok{|\textgreater{}} 
      \FunctionTok{word}\NormalTok{(}\AttributeTok{start =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
      \FunctionTok{word}\NormalTok{(}\AttributeTok{start =} \DecValTok{1}\NormalTok{, }\AttributeTok{sep =} \FunctionTok{fixed}\NormalTok{(}\StringTok{"/"}\NormalTok{)),}
    \AttributeTok{NativeLg.cleaned =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{      NativeLg.cleaned }\SpecialCharTok{==} \StringTok{"Mandarine"} \SpecialCharTok{\textasciitilde{}} \StringTok{"Mandarin"}\NormalTok{,}
\NormalTok{      NativeLg.cleaned }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"Lithunanina"}\NormalTok{, }\StringTok{"Lituanian"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"Lithuanian"}\NormalTok{,}
      \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}}\NormalTok{ NativeLg.cleaned)}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

Whenever we do any data wrangling, it is crucial that we take the time
to carefully check that we have not made any mistakes in the process. To
this end, we display the original \texttt{NativeLg} and the new
\texttt{NativeLg.cleaned} variables side by side using the
\texttt{select()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L2.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(NativeLg, NativeLg.cleaned)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    NativeLg NativeLg.cleaned
1 Lithuanian       Lithuanian
2     polish           Polish
3     Polish           Polish
4    Italian          Italian
5  Lituanian       Lithuanian
6     Polish           Polish
\end{verbatim}

As you can see, only the first six rows of the table are printed above.
Run the code yourself to check all the other rows.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Using base \texttt{R} functions instead}, titlerule=0mm, leftrule=.75mm]

This chapter focuses on \{tidyverse\} functions, however all of the
above data wrangling and cleaning operations can equally be achieved
using base \texttt{R} functions. For example, the \texttt{mutate()} code
chunk above could be replaced by the following lines of base \texttt{R}
code.

\phantomsection\label{annotated-cell-472}%
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L2.data}\SpecialCharTok{$}\NormalTok{NativeLg.cleaned.base }\OtherTok{\textless{}{-}} \FunctionTok{gsub}\NormalTok{(}\StringTok{"([a{-}zA{-}Z]+).*"}\NormalTok{, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{1"}\NormalTok{, L2.data}\SpecialCharTok{$}\NormalTok{NativeLg) }\hspace*{\fill}\NormalTok{\circled{1}}
\NormalTok{L2.data}\SpecialCharTok{$}\NormalTok{NativeLg.cleaned.base }\OtherTok{\textless{}{-}}\NormalTok{ tools}\SpecialCharTok{::}\FunctionTok{toTitleCase}\NormalTok{(L2.data}\SpecialCharTok{$}\NormalTok{NativeLg.cleaned.base) }\hspace*{\fill}\NormalTok{\circled{2}}
\NormalTok{L2.data}\SpecialCharTok{$}\NormalTok{NativeLg.cleaned.base[L2.data}\SpecialCharTok{$}\NormalTok{NativeLg.cleaned.base }\SpecialCharTok{==} \StringTok{"Mandarine"}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"Mandarin"} \hspace*{\fill}\NormalTok{\circled{3}}
\NormalTok{L2.data}\SpecialCharTok{$}\NormalTok{NativeLg.cleaned.base[L2.data}\SpecialCharTok{$}\NormalTok{NativeLg.cleaned.base }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"Lithunanina"}\NormalTok{, }\StringTok{"Lituanian"}\NormalTok{)] }\OtherTok{\textless{}{-}} \StringTok{"Lithuanian"} \hspace*{\fill}\NormalTok{\circled{4}}
\end{Highlighting}
\end{Shaded}

\begin{description}
\tightlist
\item[\circled{1}]
With the first line, we extract the first string of letters before any
space or slash in \texttt{NativeLg} and save this to a new variable
called \texttt{NativeLg.cleaned.base}.
\item[\circled{2}]
This line converts all the values of the new variable to title case
using a base \texttt{R} function from the \{tools\} package. The
\{tools\} package comes with \texttt{R} so you don't need to install it
separately but, if you haven't loaded it earlier in your \texttt{R}
session, you need to call the function with the prefix \texttt{tools::}
so that \texttt{R} knows where to find the \texttt{toTitleCase()}
function.
\item[\circled{3}]
The third line corrects a typo with a direct replacement.
\item[\circled{4}]
This line replaces two typos with a single correction.
\end{description}

If we now compare the variable created with the tidyverse code
(\texttt{NativeLg.cleaned}) vs.~the one created using base R functions
only (\texttt{NativeLg.cleaned.base}), we can see that they are exactly
the same.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L2.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(NativeLg.cleaned, NativeLg.cleaned.base) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  NativeLg.cleaned NativeLg.cleaned.base
1       Lithuanian            Lithuanian
2           Polish                Polish
3           Polish                Polish
4          Italian               Italian
5       Lithuanian            Lithuanian
6           Polish                Polish
\end{verbatim}

An undeniable advantage of sticking to base \texttt{R} functions is that
your code is more portable as it does not require the installation of
any additional packages, keeping dependencies on external packages to
the minimum. However, base \texttt{R} lacks the consistency of the
tidyverse framework, which can make certain data transformation tasks
considerably more tricky and code less readable (and therefore less
transparent) to yourself and others.

As we don't need two versions of the cleaned \texttt{NativLg} variable,
we will now remove the \texttt{NativeLg.cleaned.base} column from
\texttt{L2.data}. To do so, we use the \texttt{select()} function
combined with the \texttt{-} operator to ``unselect'' the column we no
longer need.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L2.data }\OtherTok{\textless{}{-}}\NormalTok{ L2.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{ NativeLg.cleaned.base)}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\section{Combining datasets}\label{sec-CombiningDatasets}

So far, we have analysed the L1 and L2 datasets individually. In the
following chapters, however, we will conduct comparative analyses,
comparing the performance of the L1 and L2 participants in the various
language-related tests conduced as part of DÄ…browska (2019). To this
end, we need to create a combined table that includes the data of all
participants from DÄ…browska (2019).

Remember that both tables, \texttt{L1.data} and \texttt{L2.data}, are in
a tidy data format. This means that:

\begin{itemize}
\tightlist
\item
  each row represents an observation (i.e.~here, a participant),
\item
  each cell represents a measurement, and
\item
  each variable forms a column.
\end{itemize}

To combine the two datasets, therefore, we need to combine the rows of
the two tables. However, we cannot simply add the rows of the
\texttt{L2.data} table to the bottom of \texttt{L1.data} table because,
as shown below, the two tables do not have the same number of columns
and the shared columns are not in the same position! We therefore need
to ensure that, when the two datasets are combined, the shared columns
are aligned.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{colnames}\NormalTok{(L1.data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "Participant" "Age"         "Gender"      "Occupation"  "OccupGroup" 
 [6] "OtherLgs"    "Education"   "EduYrs"      "ReadEng1"    "ReadEng2"   
[11] "ReadEng3"    "ReadEng"     "Active"      "ObjCl"       "ObjRel"     
[16] "Passive"     "Postmod"     "Q.has"       "Q.is"        "Locative"   
[21] "SubCl"       "SubRel"      "GrammarR"    "Grammar"     "VocabR"     
[26] "Vocab"       "CollocR"     "Colloc"      "Blocks"      "ART"        
[31] "LgAnalysis"  "Gender.fct" 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{colnames}\NormalTok{(L2.data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "Participant"      "Gender"           "Occupation"       "OccupGroup"      
 [5] "NativeLg"         "NativeLgFamily"   "OtherLgs"         "EdNative"        
 [9] "EdUK"             "Age"              "EduYrsNat"        "EduYrsEng"       
[13] "EduTotal"         "FirstExp"         "Arrival"          "LoR"             
[17] "EngWork"          "EngPrivate"       "ReadEng1"         "ReadOth1"        
[21] "ReadEng2"         "ReadOth2"         "ReadEng3"         "ReadOth3"        
[25] "ReadEng"          "ReadOth"          "Active"           "ObjCl"           
[29] "ObjRel"           "Passive"          "Postmod"          "Q.has"           
[33] "Q.is"             "Locative"         "SubCl"            "SubRel"          
[37] "GrammarR"         "Grammar"          "VocabR"           "Vocab"           
[41] "CollocR"          "Colloc"           "Blocks"           "ART"             
[45] "LgAnalysis"       "UseEngC"          "NativeLg.cleaned"
\end{verbatim}

Note, also, that participants' total number of years in education is
stored in the \texttt{EduYrs} column in \texttt{L1.data}, whereas the
corresponding column in \texttt{L2.data} is called \texttt{EduTotal}.
Hence, we first use the \{dplyr\} function \texttt{rename()} to rename
\texttt{EduYrs} in \texttt{L1.data} as \texttt{EduTotal} before we merge
the two tables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L1.data }\OtherTok{\textless{}{-}}\NormalTok{ L1.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{EduTotal =}\NormalTok{ EduYrs)}
\end{Highlighting}
\end{Shaded}

The \{dplyr\} package boasts an array of useful functions to combine
tables (see Figure~\ref{fig-dplyrcheatsheet}). For our purposes,
\texttt{bind\_rows()} appears to be the perfect function.\footnote{We
  could also use the \texttt{full\_join()} function since we want to
  retain all rows and all columns from both datasets.}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/dplyr_cheatsheet_extract.png}}

}

\caption{\label{fig-dplyrcheatsheet}Extract of the
\href{https://rstudio.github.io/cheatsheets/data-transformation.pdf}{data
transformation with \{dplyr\} cheatsheet} (CC BY SA Posit Software,
PBC)}

\end{figure}%

However, when we try to combine \texttt{L1.data} and \texttt{L2.data}
using \texttt{bind\_rows()}, we get an error message\ldots{} ðŸ˜¢ Does
this error remind you of Q10 from Section~\ref{sec-DataTypes} by any
chance?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{combined.data }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(L1.data, L2.data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Error in `bind_rows()`:
! Can't combine `..1$Participant` <character> and `..2$Participant` <integer>.
\end{verbatim}

What this error message tells us is that the \texttt{bind\_rows()}
function cannot combine the two \texttt{Participant} columns because in
\texttt{L1.data} it is a string character vector, whereas in
\texttt{L2.data} it is an integer vector. However, to avoid data loss,
\texttt{bind\_rows()} can only match columns of the same data type!

We must therefore first convert the \texttt{Participant} variable in
\texttt{L2.data} to a character vector.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L2.data }\OtherTok{\textless{}{-}}\NormalTok{ L2.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Participant =} \FunctionTok{as.character}\NormalTok{(Participant))}
\end{Highlighting}
\end{Shaded}

Now, we can combine the two data frames using \texttt{bind\_rows()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{combined.data }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(L1.data, L2.data)}
\end{Highlighting}
\end{Shaded}

The problem is that now that we have merged our two datasets into one,
it's not obvious which rows correspond to L1 participants and which to
L2 participants! There are various ways to solve this, but here's a
simple \textbf{three-step solution} that relies exclusively on functions
that you are already familiar with.

\textbf{Step 1:} We add a new column to \texttt{L1.data} called
\texttt{Group} and fill this column with the value ``L1'' for all rows.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L1.data }\OtherTok{\textless{}{-}}\NormalTok{ L1.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Group =} \StringTok{"L1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbf{Step 2:} We add a new column to \texttt{L2.data} also called
\texttt{Group} and fill this column with the value ``L2'' for all rows.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L2.data }\OtherTok{\textless{}{-}}\NormalTok{ L2.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Group =} \StringTok{"L2"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbf{Step 3:} We use \texttt{bind\_rows()} as above to combine the
two datasets that now both include the extra \texttt{Group}
column.\footnote{Alternatively, you may have gathered from the
  cheatsheet (Figure~\ref{fig-dplyrcheatsheet}) that the
  \texttt{bind\_rows()} function has an optional ``.id'' argument that
  can be used to create an additional column to disambiguate between the
  two combined datasets. In this case, we do not need to add a
  \texttt{Group} column to both datasets prior to combining them.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{combined.data }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(}\AttributeTok{L1 =}\NormalTok{ L1.data, }
                           \AttributeTok{L2 =}\NormalTok{ L2.data, }
                           \AttributeTok{.id =} \StringTok{"Group"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{combined.data }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(L1.data, L2.data)}
\end{Highlighting}
\end{Shaded}

\textbf{Verification step:} The \texttt{combined.data} table now
includes the column \texttt{Group}, which we can use to easily identify
the observations that belong to L1 and L2 participants. As expected, our
combined dataset includes 90 participants from the L1 group and 67 from
the L2 group:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{combined.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{count}\NormalTok{(Group)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  Group  n
1    L1 90
2    L2 67
\end{verbatim}

Our combined dataset contains all the columns that appear in either
\texttt{L1.data} or \texttt{L2.data}. Check that this is the case by
examining the structure of the new dataset with
\texttt{str(combined.data)}.

You will have noticed that, in some columns, there are lots of
\texttt{NA} (``Not Available'') values. These represent \textbf{missing
data}. \texttt{R} has inserted these \texttt{NA} values in the columns
that only appear in one of the two datasets. For example, the L1 dataset
does not include an \texttt{Arrival} variable (indicating the age when
participants first arrived in an English-speaking country), presumably
because they were all \emph{born} in an English-speaking country! We
only have this information for the L2 participants and this explains the
90 \texttt{NA} values in the \texttt{Arrival} column of the combined
dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{combined.data}\SpecialCharTok{$}\NormalTok{Arrival}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
 [26] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
 [51] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
 [76] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA 17 18 19 19 19 19 20 22 22 23
[101] 24 25 26 26 28 28 30 44 45 49 17 23 23 24 19 22 23 26 16 18 24 24 25 29 30
[126] 32 33 22 25 30 44 27 18 20 21 33 38 47 16 19 20 24 25 28 33 20 22 25 28 26
[151] 26 16 24 32 20 16 20
\end{verbatim}

We can also check this by cross-tabulating the \texttt{Group} and the
\texttt{Arrival} variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{combined.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{count}\NormalTok{(Group, Arrival)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  Group Arrival  n
1    L1      NA 90
2    L2      16  4
3    L2      17  2
4    L2      18  3
5    L2      19  6
6    L2      20  6
\end{verbatim}

Run \texttt{View(combined.data)} to inspect the combined dataset and
check in which other columns there are \texttt{NA} values.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{What if my data are not yet in tidy format? ðŸ¤¨}, titlerule=0mm, leftrule=.75mm]

Combining the two datasets from DÄ…browska (2019) was relatively easy
because the data was already in tidy format. But, fear not: if you need
to first convert your data to tidy format, the
\href{https://tidyr.tidyverse.org/}{\{tidyr\}} package has got you
covered! ðŸ˜Ž

The \texttt{pivot\_longer()} and \texttt{pivot\_wider()} functions allow
you to easily convert tables from ``long'' to ``wide'' format and vice
versa (see Figure~\ref{fig-tidyrcheatsheet}).

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/tidyr_cheatsheet_extract.png}}

}

\caption{\label{fig-tidyrcheatsheet}Extract of the
\href{https://github.com/rstudio/cheatsheets/blob/main/tidyr.pdf}{data
tidying with tidyr cheatsheet} (CC BY SA Posit Software, PBC)}

\end{figure}%

Remember to carefully check the output of any data manipulation that you
do \emph{before} moving on to doing any analyses! To this end, the
\texttt{View()} function is particularly helpful.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-important-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-important-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Using AI tools for coding âš ï¸}, titlerule=0mm, leftrule=.75mm]

Note that older textbooks/tutorials, and especially AI tools such as
ChatGPT that have been trained on older web data, will frequently
suggest \textbf{superseded} (i.e.~outdated) functions for data
manipulation such as \texttt{spread()}, \texttt{gather()},
\texttt{select\_all()}, and \texttt{mutate\_if()}. If you use superseded
functions, your code will still work, but \texttt{R} will print a
warning in the Console and usually suggest a modern alternative.

AI tools may also suggest using functions that are \textbf{deprecated}.
As with superseded functions, you will get a warning message with a
recommended alternative. In this case, however, you must follow the
advice of the warning, as writing new code with deprecated functions is
really asking for trouble! Deprecated functions are scheduled for
removal, which means that your code will eventually no longer run on
up-to-date \texttt{R} versions.

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/images/lifecycle.pdf}}

}

\caption{\label{fig-lifecycle}The four main stages of the lifecycle of
\texttt{R} packages, functions, function arguments:
\textbf{experimental} developments can become \textbf{stable} and stable
can eventually become \textbf{deprecated} or \textbf{superseded} (image
by Henry and Wickham 2023 for Posit Software, PBC,
\url{https://lifecycle.r-lib.org/articles/stages.html}).}

\end{figure}%

In sum, to ensure the future compatibility of your code, do not ignore
warnings about deprecated functions and, in general, \emph{never ever}
blindly trust the output of AI tools!

\end{tcolorbox}

\section{A pre-processing pipeline}\label{sec-filter}

So far in this chapter, we have learnt how to pre-process data for
future statistical analyses and data visualisation. In the process, we
have learnt about lots of different functions, mostly from the tidyverse
environment (see Section~\ref{sec-tidyverse} ðŸª). Now it's time to put
everything together and save our pre-processed combined dataset for
future use.

But, first, let's recap all of the data wrangling operations that we
performed in this chapter and combine them into one code chunk. Before
running this code, we first reload the original data from DÄ…browska
(2019) to overwrite any changes that were made during this chapter. This
will ensure that we all have exactly the same version of the dataset for
the following chapters. Detailed instructions to download and load the
original data can be found in Chapter~\ref{sec-ImportingData}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(here)}

\NormalTok{L1.data }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\AttributeTok{file =} \FunctionTok{here}\NormalTok{(}\StringTok{"data"}\NormalTok{, }\StringTok{"L1\_data.csv"}\NormalTok{))}
\NormalTok{L2.data }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\AttributeTok{file =} \FunctionTok{here}\NormalTok{(}\StringTok{"data"}\NormalTok{, }\StringTok{"L2\_data.csv"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Then, run the following lines of code to create a new \texttt{R} object
called \texttt{combined.data} that contains the wrangled data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L2.data }\OtherTok{\textless{}{-}}\NormalTok{ L2.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Participant =} \FunctionTok{as.character}\NormalTok{(Participant)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Group =} \StringTok{"L2"}\NormalTok{)  }

\NormalTok{L1.data }\OtherTok{\textless{}{-}}\NormalTok{ L1.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Group =} \StringTok{"L1"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{EduTotal =}\NormalTok{ EduYrs)}

\NormalTok{combined.data }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(L1.data, L2.data) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{where}\NormalTok{(is.character), str\_to\_title)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{where}\NormalTok{(is.character), str\_trim)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{OccupGroup =} \FunctionTok{str\_to\_upper}\NormalTok{(OccupGroup)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{NativeLg =} \FunctionTok{word}\NormalTok{(NativeLg, }\AttributeTok{start =} \DecValTok{1}\NormalTok{),}
    \AttributeTok{NativeLg =} \FunctionTok{word}\NormalTok{(NativeLg, }\AttributeTok{start =} \DecValTok{1}\NormalTok{, }\AttributeTok{sep =} \FunctionTok{fixed}\NormalTok{(}\StringTok{"/"}\NormalTok{)),}
    \AttributeTok{NativeLg =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{      NativeLg }\SpecialCharTok{==} \StringTok{"Mandarine"} \SpecialCharTok{\textasciitilde{}} \StringTok{"Mandarin"}\NormalTok{,}
\NormalTok{      NativeLg }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"Lithunanina"}\NormalTok{, }\StringTok{"Lithunanina"}\NormalTok{, }\StringTok{"Lituanian"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"Lithuanian"}\NormalTok{,}
      \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}}\NormalTok{ NativeLg)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{NativeLgFamily =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{    NativeLg }\SpecialCharTok{==} \StringTok{"Lithuanian"} \SpecialCharTok{\textasciitilde{}} \StringTok{"Baltic"}\NormalTok{,}
\NormalTok{    NativeLg }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"Cantonese"}\NormalTok{, }\StringTok{"Mandarin"}\NormalTok{, }\StringTok{"Chinese"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"Chinese"}\NormalTok{,}
\NormalTok{    NativeLg }\SpecialCharTok{==} \StringTok{"German"} \SpecialCharTok{\textasciitilde{}} \StringTok{"Germanic"}\NormalTok{,}
\NormalTok{    NativeLg }\SpecialCharTok{==} \StringTok{"Greek"} \SpecialCharTok{\textasciitilde{}} \StringTok{"Hellenic"}\NormalTok{,}
\NormalTok{    NativeLg }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"French"}\NormalTok{, }\StringTok{"Italian"}\NormalTok{, }\StringTok{"Spanish"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"Romance"}\NormalTok{,}
\NormalTok{    NativeLg }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"Polish"}\NormalTok{, }\StringTok{"Russian"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"Slavic"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{where}\NormalTok{(is.character), factor))}
\end{Highlighting}
\end{Shaded}

Don't forgot to check the result by examining the output of
\texttt{View(combined.data)} and \texttt{str(combined.data)}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(combined.data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  Participant       Age        Gender             Occupation OccupGroup
 1      :  1   Min.   :17.00   F:94   Student          :27   C :32     
 100    :  1   1st Qu.:25.00   M:63   Retired          :15   I :26     
 101    :  1   Median :31.00          Product Operative: 5   M :41     
 104    :  1   Mean   :35.48          Teacher          : 5   PS:58     
 106    :  1   3rd Qu.:42.00          Cleaner          : 4             
 107    :  1   Max.   :65.00          Unemployed       : 4             
 (Other):151                          (Other)          :97             
              OtherLgs    Education     EduTotal        ReadEng1    
 None             :98   Student: 8   Min.   : 8.50   Min.   :0.000  
 No               :11   A Level: 5   1st Qu.:13.00   1st Qu.:1.000  
 English          : 8   Ba     : 5   Median :14.00   Median :3.000  
 German           : 6   Gcses  : 5   Mean   :14.62   Mean   :2.599  
 English At School: 3   Nvq    : 4   3rd Qu.:17.00   3rd Qu.:4.000  
 English, German  : 2   (Other):63   Max.   :24.00   Max.   :5.000  
 (Other)          :29   NA's   :67                                  
    ReadEng2        ReadEng3        ReadEng      
 Min.   :0.000   Min.   :0.000   Min.   : 0.000  
 1st Qu.:1.000   1st Qu.:1.000   1st Qu.: 5.000  
 Median :2.000   Median :2.000   Median : 7.000  
 Mean   :2.465   Mean   :2.019   Mean   : 7.083  
 3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:10.000  
 Max.   :5.000   Max.   :4.000   Max.   :14.000  
                                                 
\end{verbatim}

\section{\texorpdfstring{Saving and exporting \texttt{R}
objects}{Saving and exporting R objects}}\label{sec-ExportingRObjects}

As a final step, we want to save the \texttt{R} object
\texttt{combined.data} to a local file on our computer so that, when we
continue our analyses in a new \texttt{R} session, we can immediately
start working with the wrangled dataset. We can either save the wrangled
dataset as an \texttt{R} object (\texttt{.rds}) or export it as a DSV
file (e.g.~\texttt{.csv}, see Section~\ref{sec-DSV}). The pros and cons
of the two solutions are summarised in Table~\ref{tbl-saving}.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.4153}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.5847}}@{}}
\caption{Pros and cons of saving DSV and \texttt{R} data
files.}\label{tbl-saving}\tabularnewline
\toprule\noalign{}
\multicolumn{2}{@{}>{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{1.0000} + 2\tabcolsep}@{}}{%
\begin{minipage}[b]{\linewidth}\raggedright
DSV files (e.g.~\texttt{.csv}, \texttt{.tsv}, \texttt{.tab}) \textbar{}
\texttt{R} data files (\texttt{.rds}) \textbar{}
\end{minipage}} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\multicolumn{2}{@{}>{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{1.0000} + 2\tabcolsep}@{}}{%
\begin{minipage}[b]{\linewidth}\raggedright
DSV files (e.g.~\texttt{.csv}, \texttt{.tsv}, \texttt{.tab}) \textbar{}
\texttt{R} data files (\texttt{.rds}) \textbar{}
\end{minipage}} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
âœ… Highly portable (i.e., can be opened in all standard spreadsheet
software and text editors). & âŒ Specific to \texttt{R} and cannot be
opened in standard spreadsheet software or text editors. \\
âŒ Inefficient for very large datasets. & âœ… Efficient memory usage for
more compact data storage and faster loading times in \texttt{R}. \\
âœ… Universal, language-independent format and therefore suitable for
long-term archiving. & âŒ No guarantee that older \texttt{.rds} files
will be compatible with newer versions of \texttt{R} and therefore
unsuitable for long-term archiving. \\
âŒ Loss of metadata. & âœ… Preserve \texttt{R} data structures (e.g.,
factor variables remain stored as factors). \\
\end{longtable}

We will save both a \texttt{.csv} and an \texttt{.rds} version of the
wrangled data but in the following chapters, we will use the
\texttt{.rds} file.

We will save both files to a subfolder of our project ``data'' folder
called ``processed''. If we try to save the file to this subfolder
before it has been created at this location we get an error message.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{saveRDS}\NormalTok{(combined.data, }\AttributeTok{file =} \FunctionTok{here}\NormalTok{(}\StringTok{"data"}\NormalTok{, }\StringTok{"processed"}\NormalTok{, }\StringTok{"combined\_L1\_L2\_data.rds"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Error in gzfile(file, mode) : cannot open the connection
\end{verbatim}

We first need to create the ``processed'' subfolder before we can save
to this location! There are two ways of doing this:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Either in the Files pane of \emph{RStudio} or in a File
  Navigator/Explorer window, navigate to the ``data'' folder and, from
  there, click on the ``Create a new folder'' icon to create a new
  subfolder called ``processed''.
\item
  Alternatively, we can use the \texttt{dir.create()} function to create
  the subfolder from \texttt{R} itself. If the folder already exists at
  this location, we will get a warning.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dir.create}\NormalTok{(}\FunctionTok{file.path}\NormalTok{(}\FunctionTok{here}\NormalTok{(}\StringTok{"data"}\NormalTok{, }\StringTok{"processed"}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

Now that the subfolder exists, we can save \texttt{combined.data} as an
\texttt{.rds} file. We will work with this file in the following
chapters.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{saveRDS}\NormalTok{(combined.data, }\AttributeTok{file =} \FunctionTok{here}\NormalTok{(}\StringTok{"data"}\NormalTok{, }\StringTok{"processed"}\NormalTok{, }\StringTok{"combined\_L1\_L2\_data.rds"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

If you want to share your wrangled dataset with a colleague who does not
(yet? ðŸ˜‰) use \texttt{R}, you can use the tidyverse function
\texttt{write\_csv()}.\footnote{As usual, there is a base \texttt{R}
  alternative: \texttt{write.csv()} will work just as well but, for
  larger datasets, it is considerably slower than \texttt{write\_csv()}.
  For finer differences, check out the functions' respective help files.}
Your colleague will be able to open this file in any standard
spreadsheet programme or text editor (but do warn them about the dangers
of opening \texttt{.csv} file in spreadsheets, see
Section~\ref{sec-ExcelWarning}!).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write\_csv}\NormalTok{(combined.data, }\AttributeTok{file =} \FunctionTok{here}\NormalTok{(}\StringTok{"data"}\NormalTok{, }\StringTok{"processed"}\NormalTok{, }\StringTok{"combined\_L1\_L2\_data.csv"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\section*{Check your progress ðŸŒŸ}\label{check-your-progress-8}
\addcontentsline{toc}{section}{Check your progress ðŸŒŸ}

\markright{Check your progress ðŸŒŸ}

You have successfully completed { out of 15 questions} in this chapter.

That was a lot of data wrangling, but we are now ready to proceed with
some comparative analyses of L1 and L2 English speakers' language
skills!

Are you confident that you can\ldots?

\begin{itemize}
\tightlist
\item[$\square$]
  Define tidy data (Section~\ref{sec-tidyverse})
\item[$\square$]
  Check the sanity of a dataset (Section~\ref{sec-Sanity})
\item[$\square$]
  Convert character vectors representing categorical data to factors
  (Section~\ref{sec-Factors})
\item[$\square$]
  Add and replace columns in a table (Section~\ref{sec-mutate})
\item[$\square$]
  Transform several columns at once (Section~\ref{sec-across})
\item[$\square$]
  Use \{stringr\} functions to manipulate text values
  (Section~\ref{sec-stringR})
\item[$\square$]
  Interpret \texttt{R} package cheatsheets
\item[$\square$]
  Gain insights from the help file of \texttt{R} functions
\item[$\square$]
  Use tidyverse functions to pre-process data in an readable and
  reproducible way (Section~\ref{sec-Preprocessing}) and
  (Section~\ref{sec-filter})
\item[$\square$]
  Save \texttt{R} objects as \texttt{.rds} and \texttt{.csv} files on
  your computer (Section~\ref{sec-ExportingRObjects})
\end{itemize}

In Chapter~\ref{sec-DataViz} we continue to explore the tidyverse as we
learn how to use the popular tidyverse package \{ggplot2\} to visualise
the pre-processed data from DÄ…browska (2019). Are you ready to get
creative? ðŸŽ¨

\bookmarksetup{startatroot}

\chapter{\texorpdfstring{The Gramma\texttt{R} of
Graphics}{The GrammaR of Graphics}}\label{sec-DataViz}

One of the advantages of working in \texttt{R} is that it allows us to
create highly customised graphs for effective data visualisation. The
\textbf{Grammar of Graphics} (Wilkinson 2005) is a theoretical framework
that defines a structured approach to building and understanding
statistical graphs. We will focus on using the tidyverse package
\href{https://ggplot2.tidyverse.org/}{\{ggplot2\}} (Wickham 2016) to
create effective data visualisations. The \{ggplot2\} is an
implementation of the Grammar of Graphics (GG) syntax in \texttt{R}.

This chapter is divided into two parts: the first explains the
\textbf{syntax} of the Grammar of Graphics (Wilkinson 2005) and how the
\{ggplot2\} package works, while the second part focuses on the
\textbf{semantics} of statistical graphics and provides an introduction
to the many different types of data visualisations that can be created
using the package.

\subsection*{Chapter overview}\label{chapter-overview-9}
\addcontentsline{toc}{subsection}{Chapter overview}

In this chapter, you will learn how to:

\begin{itemize}
\tightlist
\item
  Create and interpret bar plots to visualise categorical variables.
\item
  Create and interpret histograms, density plots, and violin plots to
  visualise continuous numeric variables.
\item
  Create and interpret boxplots to visualise the distribution of
  continuous numeric variables across different subsets of the data.
\item
  Create and interpret scatter plots to visualise correlations between
  pairs of numeric variables.
\item
  Create and interpret facetted plots to explore the relationship
  between three or more variables at once.
\item
  Create interactive plots for data exploration.
\end{itemize}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/AHorst_ggplot2.png}}

}

\caption{\label{fig-DataArtists}Building a data masterpiece with
\{ggplot2\} (artwork by
\href{https://allisonhorst.com/}{@allison\_horst})}

\end{figure}%

\subsection*{Set-up and data import}\label{set-up-and-data-import}
\addcontentsline{toc}{subsection}{Set-up and data import}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-warning-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-warning-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Prerequisites}, titlerule=0mm, leftrule=.75mm]

This chapter assumes that you are familiar with the concepts of
descriptive statistics explained in Chapter~\ref{sec-DescRiptiveStats},
and the use of the data wrangling functions introduced in
Chapter~\ref{sec-DataWrangling}.

All examples and quiz questions are based on data from:

\begin{quote}
DÄ…browska, Ewa. 2019. Experience, Aptitude, and Individual Differences
in Linguistic Attainment: A Comparison of Native and Nonnative Speakers.
Language Learning 69(S1). 72-100.
\url{https://doi.org/10.1111/lang.12323}.
\end{quote}

Our starting point for this chapter is the wrangled combined dataset
that we created and saved in Chapter~\ref{sec-DataWrangling}. Follow the
instructions in Section~\ref{sec-filter} to create this \texttt{R}
object.

Alternatively, you can download \texttt{Dabrowska2019.zip} from
\href{https://github.com/elenlefoll/RstatsTextbook/raw/69d1e31be7394f2b612825f031ebffeb75886390/Dabrowska2019.zip}{the
textbook's GitHub repository}. To launch the project correctly, first
unzip the file and then double-click on the \texttt{Dabrowska2019.Rproj}
file.

\end{tcolorbox}

Before we begin, we must load the \texttt{combined\_L1\_L2\_data.rds}
file that we created and saved in Chapter~\ref{sec-DataWrangling}. This
file contains the data of all the L1 and L2 participants of DÄ…browska
(2019). The categorical variables are stored as factors and obvious data
entry inconsistencies and typos have been corrected (see
Chapter~\ref{sec-DataWrangling}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(here)}
\FunctionTok{library}\NormalTok{(tidyverse)}

\NormalTok{Dabrowska.data }\OtherTok{\textless{}{-}} \FunctionTok{readRDS}\NormalTok{(}\AttributeTok{file =} \FunctionTok{here}\NormalTok{(}\StringTok{"data"}\NormalTok{, }\StringTok{"processed"}\NormalTok{, }\StringTok{"combined\_L1\_L2\_data.rds"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Check that your data are correctly imported by examining the output of
\texttt{View(Dabrowska.data)} and \texttt{str(Dabrowska.data)}. Once you
are satisfied that that's the case, we ready to get creative! ðŸŽ¨

\section{The syntax of graphics}\label{sec-Syntax}

The syntax of the Grammar of Graphics (Wilkinson 2005) is made up of
\textbf{layers} (Figure~\ref{fig-layers}), which allow us to create
highly effective and efficient data visualisations, while giving us lots
of flexibility and control.

\begin{figure}

\centering{

\includegraphics[width=5.82292in,height=\textheight,keepaspectratio]{images/gglayers.png}

}

\caption{\label{fig-layers}The syntax of the Grammar of Graphics as
visualised in the
\href{https://r.qcbs.ca/workshop03/book-en/grammar-of-graphics-gg-basics.html}{QCBS
R Workshop Series}
(\href{https://creativecommons.org/licenses/by-nc-sa/4.0/}{CC-BY-NC-SA})}

\end{figure}%

The \textbf{data} layer and the \textbf{aesthetics} layer are compulsory
as you cannot build a graph that does not map \emph{some} data onto
\emph{some} visual aspect (= aesthetic) of a graph. The remaining layers
are optional, but some are very important. In the following, we will
explain how the \textbf{geometries}, \textbf{facet}, \textbf{scales},
\textbf{coordinates}, and \textbf{theme} layers are used to build and
customise graphs using the \{ggplot2\} library in \texttt{R}.

\subsection{Aesthetics}\label{sec-aes}

As explained in the documentation, the \texttt{ggplot()}
function\footnote{Note that, while the library is called \{ggplot2\},
  its main function is called \texttt{ggplot()}.} has two compulsory
arguments. First, we must select the \textbf{data} that we want to
visualise. Second, we must specify which variable(s) from the data
should be mapped onto which visual property or \textbf{aesthetics}
(short: \texttt{aes}) of the plot.

\begin{quote}
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
ggplot \{ggplot2\} & R Documentation \\
\end{longtable}

\subsection*{Description}\label{description-1}
\addcontentsline{toc}{subsection}{Description}

\texttt{ggplot()} initializes a ggplot object. It can be used to declare
the input data frame for a graphic and to specify the set of plot
aesthetics intended to be common throughout all subsequent layers unless
specifically overridden.

\subsection*{Usage}\label{usage-1}
\addcontentsline{toc}{subsection}{Usage}

\begin{verbatim}
ggplot(data = NULL, mapping = aes(), ...)
\end{verbatim}
\end{quote}

For example, to create a bar plot visualising the distribution of
participants' occupational groups in the combined dataset from DÄ…browska
(2019) (\texttt{Dabrowska.data}), we need to map the \texttt{OccupGroup}
variable from \texttt{Dabrowska.data} onto our plot's \emph{x-}axis
(\texttt{x}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ Dabrowska.data,}
       \AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ OccupGroup))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/fig-EmptyBarplot-1.pdf}}

}

\caption{\label{fig-EmptyBarplot}Attempt to plot the distribution of
participants' occupational groups in DÄ…browska (2019)}

\end{figure}%

As you can see from Figure~\ref{fig-EmptyBarplot}, however, running this
code returns an empty plot: All we get is a grid background and a nicely
labelled \emph{x-}axis, but no data\ldots{} Why might that be? ~ðŸ¤”

\subsection{Geometries}\label{sec-geoms}

The reason we are not seeing any data is that we have not yet specified
with which kind of \textbf{geometry} (short: \textbf{geom}) we would
like to plot the data. The \{ggplot2\} library features more than 30
different geom functions! They all begin with the prefix
\texttt{geom\_}. To create a bar plot showing participants' occupational
groups, we need to add a \texttt{geom\_bar()} \textbf{layer} to our
empty \texttt{ggplot} object (see Figure~\ref{fig-barplot1}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ Dabrowska.data,}
       \AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ OccupGroup)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/fig-barplot1-1.pdf}}

}

\caption{\label{fig-barplot1}Distribution of participants' occupational
groups (C = clerical position, I = inactive (i.e.~unemployed, retired,
or homemakers), M = manual jobs, PS = professional-level job or studying
for a degree)}

\end{figure}%

Note that we use the \texttt{+} operator to \emph{add} layers to
\texttt{ggplot} objects. If we try to use the pipe operator
(\texttt{\textbar{}\textgreater{}}) \emph{within} the \texttt{ggplot()}
function, we will get an error message.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ Dabrowska.data,}
       \AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ OccupGroup)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{geom\_bar}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Error in `geom_bar()`: 
! `mapping` must be created by `aes()`. 
â„¹ Did you use `%>%` or `|>` instead of `+`? 
Run `rlang::last_trace()` to see where the error occurred.
\end{verbatim}

\subsection{Statistics and labels}\label{sec-StatsLabs}

We now have a simple bar plot that represents the distribution of
participants' occupational groups in DÄ…browska (2019). By default, the
axis labels are simply the names of the variables that are mapped onto
the plot's aesthetics. That's why, in Figure~\ref{fig-barplot1}, our
\emph{x}-axis is labelled ``OccupGroup''.

What about the \emph{y}-axis? We did not specify a \emph{y}-aesthetic
within the mapping argument of our \texttt{ggplot()} object, yet the
\emph{y}-axis is labelled ``count''. This is because
\texttt{geom\_bar()} automatically computes a ``count''
\textbf{statistic} that gets mapped to the \emph{y}-aesthetic.

If we want to change these axis labels, we can do so by adding a
\texttt{labs()} \textbf{layer} to our plot (see
Figure~\ref{fig-BarOccupGroup}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ Dabrowska.data,}
       \AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ OccupGroup)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Occupational group"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of participants"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/fig-BarOccupGroup-1.pdf}}

}

\caption{\label{fig-BarOccupGroup}Distribution of participants'
occupational groups (C = clerical position, I = inactive
(i.e.~unemployed, retired, or homemakers), M = manual jobs, PS =
professional-level job or studying for a degree)}

\end{figure}%

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{What is alt-text and why is it important?}, titlerule=0mm, leftrule=.75mm]

Alternative text, or \textbf{alt-text}, is a concise description of an
image used to make its informational content accessible to people with
visual impairments. Using a screen reader programme, blind and
low-vision readers can have the alt-text associated with an image read
out to them.

A good alt-text aims to convey the main message and insights of the
graph, allowing someone who cannot see it to understand the information
being presented (WAI) (2022). In the context of online publications,
alt-text is also useful in regions with low bandwidth as images may take
a very long time to load. By including alt-text, we can therefore make
our work more accessible and inclusive, enabling more people to engage
with and understand our data and analyses.

Section~\ref{sec-QuartoFigures} explains how to add alt-text to plots
and figures in Quarto documents. Whilst many so-called ``AI'' tools will
now automatically generate alt-text for us, it is best to write alt-text
ourselves. This is because auto-generated alt-text often does not focus
on the visual information that we want to convey, misses out on
important aspects, and/or overwhelms the user with redundant
information.

Blind and low-vision readers may also want to check out the
\href{https://ajrgodfrey.github.io/BrailleR/articles/BrailleR.html}{\{BrailleR\}}
package (Godfrey et al. 2025), which converts plots generated in
\texttt{R} into a textual form that can be interpreted by blind and
low-vision \texttt{R} users who cannot access the graphs without
printing the image to a tactile embosser, or who need the extra text to
support any tactile images that they do create.

\end{tcolorbox}

\subsection{Data}\label{sec-ggplotData}

Instead of using the \textbf{data} argument of the \texttt{ggplot()}
function as we did above, we can \textbf{pipe} the data into the
function's first argument (see Section~\ref{sec-Piping}). Compare these
two methods and their outputs.

\textbf{Using the data argument of \texttt{ggplot()}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ Dabrowska.data,}
       \AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ OccupGroup))      }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Occupational group"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of participants"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/unnamed-chunk-6-1.pdf}}

\textbf{Piping the data into \texttt{ggplot()}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ OccupGroup)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Occupational group"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of participants"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/unnamed-chunk-7-1.pdf}}

The outputs are exactly the same! Piping the dataset into the
\texttt{ggplot()} function, however, allows us to easily wrangle the
data that we want to visualise `on the fly', without transforming the
data object itself. For example, we can use the tidyverse
\texttt{filter()} function (see Section~\ref{sec-filter}) to examine the
distribution of occupational groups among L2 participants only (see
Figure~\ref{fig-BarOccupGroupL2}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(Group }\SpecialCharTok{==} \StringTok{"L2"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ OccupGroup)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Occupational group"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of participants"}\NormalTok{,}
       \AttributeTok{title =} \StringTok{"Occupational groups of L2 participants"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/fig-BarOccupGroupL2-1.pdf}}

}

\caption{\label{fig-BarOccupGroupL2}}

\end{figure}%

We can also combine several \texttt{filter()} conditions using the
\texttt{\&} (AND) and \texttt{\textbar{}} (OR) operators. For example,
we may want to visualise the distribution of the occupational groups of
participants who are L2 speakers of English \emph{and} whose first
language is Polish (see Figure~\ref{fig-BarOccupGroupL2Polish}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(Group }\SpecialCharTok{==} \StringTok{"L2"} \SpecialCharTok{\&}\NormalTok{ NativeLg }\SpecialCharTok{==} \StringTok{"Polish"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ OccupGroup)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Occupational group"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of participants"}\NormalTok{,}
       \AttributeTok{title =} \StringTok{"Occupational groups of Polish L2 participants"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/fig-BarOccupGroupL2Polish-1.pdf}}

}

\caption{\label{fig-BarOccupGroupL2Polish}}

\end{figure}%

\subsection{Facets}\label{sec-facets}

If we want to compare two subsets of the data, we can add a
\textbf{facet layer} to subdivide the plot into several plots each
representing a subset of the data. In the following, we use the
\texttt{facet\_wrap()} function to subdivide our bar plot by the
\texttt{Group} variable (\texttt{\textasciitilde{}\ Group}). This allows
us to easily compare the distribution of occupations across L1 and the
L2 participants (see Figure~\ref{fig-BarOccupGroupFacet}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ OccupGroup)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ Group) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Occupational group"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of participants"}\NormalTok{,}
       \AttributeTok{title =} \StringTok{"Occupational groups of participants"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/fig-BarOccupGroupFacet-1.pdf}}

}

\caption{\label{fig-BarOccupGroupFacet}}

\end{figure}%

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-warning-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-warning-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Error message: ``seq.default(from, to, by) : invalid''}, titlerule=0mm, leftrule=.75mm]

If you get the error message

\begin{verbatim}
Error in seq.default(from, to, by) : invalid '(to - from)/by' 
\end{verbatim}

when trying to run a chunk of code that generates a plot, this is most
likely due to your Plots pane in \emph{RStudio} not being large enough
to accommodate the plot. If you increase its size and rerun the chunk,
your plot should appear in the Plots pane as expected.

If you have a small screen, you can also click on the ``ðŸ”Ž Zoom'' button
at the top of the Plots pane to view your plot in a separate
\emph{RStudio} window, which you can resize according to your needs.

\end{tcolorbox}

To compare the distributions of occupations of the male and female L2
participants, we can combine a \texttt{filter()} operation to select
only the L2 participants, with a \texttt{facet\_wrap()} layer (see
Figure~\ref{fig-BarOccupGroupL2Facet}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(Group }\SpecialCharTok{==} \StringTok{"L2"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ OccupGroup)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ Gender) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Occupational group"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Participants"}\NormalTok{,}
       \AttributeTok{title =} \StringTok{"Occupational groups of L2 participants"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/fig-BarOccupGroupL2Facet-1.pdf}}

}

\caption{\label{fig-BarOccupGroupL2Facet}}

\end{figure}%

To explore potential gender differences in occupational groups across
both L1 and L2 groups, we can combine the two variables within the
\texttt{facet\_wrap()} function (see Figure~\ref{fig-TwoFacetsBarplot}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ OccupGroup)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ Group }\SpecialCharTok{+}\NormalTok{ Gender) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Occupational group"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Participants"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/fig-TwoFacetsBarplot-1.pdf}}

}

\caption{\label{fig-TwoFacetsBarplot}}

\end{figure}%

\subsection{Scales}\label{sec-scales}

\textbf{Scale} layers allow us to map data values to the visual values
of an aesthetic. For example, to make our facetted plot in
Figure~\ref{fig-TwoFacetsBarplot} easier to read, we could add some
colour using a \textbf{fill} aesthetic to fill each bar with a colour
that corresponds to the participants' gender. To do so, we map each
unique value of the variable \texttt{Gender} (``F'' and ``M'') onto a
colour that is then used to fill the corresponding bars of our bar plot.
Adding the \textbf{fill} aesthetic automatically generates a legend (see
Figure~\ref{fig-ColourBarplot}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ OccupGroup, }
                       \AttributeTok{fill =}\NormalTok{ Gender)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ Group }\SpecialCharTok{+}\NormalTok{ Gender) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Occupational group"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Participants"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/fig-ColourBarplot-1.pdf}}

}

\caption{\label{fig-ColourBarplot}}

\end{figure}%

As we did not specify any \textbf{fill} colours for
Figure~\ref{fig-ColourBarplot}, \{ggplot2\} used default colours taken
from the \{scales\} package of the tidyverse environment. This is
because, in the Grammar of Graphics, colour palettes are governed by
\textbf{scales}. To specify a different set of colours, we therefore
need to specify a \textbf{scale layer}.

One way to do this is to use \texttt{scale\_fill\_manual()} to manually
pick our own colours, either using
\href{https://r-charts.com/colors/}{\texttt{R} colour codes} (such as
\texttt{purple}) or
\href{https://www.w3schools.com/colors/colors_picker.asp}{hexadecimal
colour codes} (such as \texttt{\#34027d}). Note that both types of
colour codes must be enclosed in quotation marks.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ OccupGroup, }
                       \AttributeTok{fill =}\NormalTok{ Gender)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ Group }\SpecialCharTok{+}\NormalTok{ Gender) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Occupational group"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Participants"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"purple"}\NormalTok{, }\StringTok{"\#34027d"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/fig-PurpleBarplot-1.pdf}}

}

\caption{\label{fig-PurpleBarplot}A facetted bar plot with hand-picked
colours}

\end{figure}%

Although it makes the plot easier to interpret, the colour aesthetic
(here \texttt{fill}) is not strictly necessary to understand the data
represented in Figure~\ref{fig-PurpleBarplot}. After all, the two
\texttt{gender} subgroups are already distinguished by the
\texttt{facet\_wrap()} layer. That's not necessarily a bad thing, but
you must consider whether such redundant elements facilitate the
interpretation of the data visualised or not.

In some cases, colour is used as the \emph{only} way of identifying
subgroups in the data, for example in a \textbf{stacked bar plot} (see
Figure~\ref{fig-ColourStackedBarplot}). In such cases, it is important
to consider how the plot will be perceived by different people (see note
on colour blindness below).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ OccupGroup, }
                       \AttributeTok{fill =}\NormalTok{ Gender)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ Group) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Occupational group"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Participants"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"purple"}\NormalTok{, }\StringTok{"\#34027d"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/fig-ColourStackedBarplot-1.pdf}}

}

\caption{\label{fig-ColourStackedBarplot}A stacked bar plot with
hand-picked colours}

\end{figure}%

\textbf{Scale} layers can be used to control the axes of your plots.
Play around with the ``expand'' and ``limits'' arguments of the
\texttt{scale\_y\_continuous()} function to understand how they work.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ OccupGroup)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Occupational group"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Participants"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{),}
                     \AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{60}\NormalTok{))  }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/fig-PurpleBarplot0-1.pdf}}

}

\caption{\label{fig-PurpleBarplot0}A bar plot with a y-axis that starts
at zero.}

\end{figure}%

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{A note on colours and colour blindness ðŸŒˆ}, titlerule=0mm, leftrule=.75mm]

Colour blindness is a condition that results in a decreased ability to
see colours and perceive differences in colour. There are different
types of colour blindness but, in general, it is best to avoid red-green
contrasts. To ensure that your data visualisations are accessible to as
many people as possible, you may want to use the
\href{https://cran.r-project.org/web/packages/colorBlindness/vignettes/colorBlindness.html}{\{colorBlindness\}}
package (Ou 2021) to simulate the appearance of a set of colours for
people with different forms of colour blindness.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#install.packages("colorBlindness")}
\FunctionTok{library}\NormalTok{(colorBlindness)}

\NormalTok{colorBlindness}\SpecialCharTok{::}\FunctionTok{displayAllColors}\NormalTok{(scales}\SpecialCharTok{::}\FunctionTok{hue\_pal}\NormalTok{()(}\DecValTok{6}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/unnamed-chunk-8-1.pdf}}

Using the \{colorBlindness\} package, we can immediately see that the
default \{scales\} discrete palette that \{ggplot2\} used in
Figure~\ref{fig-ColourBarplot} is not accessible to colour blind people
(deuteranope and protanope), nor is it distinguishable when printed in
grey-scale (desaturate). In contrast, our hand-picked colours from
Figure~\ref{fig-PurpleBarplot} fare much better.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{colorBlindness}\SpecialCharTok{::}\FunctionTok{displayAllColors}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"pink"}\NormalTok{, }\StringTok{"\#34027d"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/unnamed-chunk-9-1.pdf}}

But you need not manually pick colours, as many people have developed
and shared \texttt{R} packages that feature attractive, ready-to-use
colour-blind friendly palettes. The
\href{https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html}{\{viridis\}}
package (Garnier et al. 2023), for example, includes eight such palettes
(``magma'', ``inferno'', ``plasma'', ``cividis'', ``rocket'', ``turbo'')
that also reproduce well in grey-scale. And, as it is included in the
\{ggplot2\} installation, you don't even need to install the \{viridis\}
package separately!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{colorBlindness}\SpecialCharTok{::}\FunctionTok{displayAllColors}\NormalTok{(viridis}\SpecialCharTok{::}\FunctionTok{viridis}\NormalTok{(}\DecValTok{6}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/unnamed-chunk-10-1.pdf}}

Choosing an appropriate palette is not the only way to make your
visualisations accessible to colour-blind readers. Another way is to
provide redundant mappings to other aesthetics such as size, line type,
shape, or pattern. Finally, it is important to remember that colour
blindness is by no means the only type of visual impairment you should
consider when creating visualisations. Worldwide, far more people are
affected by \textbf{blindness} and \textbf{low vision}.
Section~\ref{sec-QuartoFigures} explains how to add \textbf{alternative
texts} (alt-text) to plots and images. Many people with visual
impairments rely on screen readers that use these alternative texts to
provide audio descriptions of images and plots. These alternative texts
can also improve the experience of users facing internet connection
issues resulting in images that do not load properly or quickly enough.

\end{tcolorbox}

Some academic publishers still require grey-scale plots, in which case
you will want to use the \textbf{scale} layer
\texttt{scale\_fill\_grey()}. Alternatively, the colour palettes of the
\{viridis\} package (see information box on colour blindness) render
well in grey, too. The \{viridis\} function for a discrete colour scale
(as needed for a categorical variable such as \texttt{Gender}) can be
called up using the \texttt{scale\_fill\_viridis\_d()} function. With
the ``option'' argument, you can switch between eight different viridis
palettes (``magma'', ``inferno'', ``plasma'', ``cividis'', ``rocket'',
``turbo'').

\subsubsection{scale\_fill\_grey()}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ OccupGroup, }\AttributeTok{fill =}\NormalTok{ Gender)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ Group) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Occupational group"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Participants"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{),}
                     \AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{35}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_grey}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/fig-GreyBarplot-1.pdf}}

}

\caption{\label{fig-GreyBarplot}}

\end{figure}%

\subsubsection{scale\_fill\_viridis\_d}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ OccupGroup, }\AttributeTok{fill =}\NormalTok{ Gender)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ Group) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Occupational group"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Participants"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{),}
                     \AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{35}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_viridis\_d}\NormalTok{(}\AttributeTok{option =} \StringTok{"viridis"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/fig-ViridisBarplot-1.pdf}}

}

\caption{\label{fig-ViridisBarplot}}

\end{figure}%

\subsubsection{scale\_fill\_viridis\_d(option = ``turbo'')}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ OccupGroup, }\AttributeTok{fill =}\NormalTok{ Gender)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ Group) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Occupational group"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Participants"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{),}
                     \AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{35}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_viridis\_d}\NormalTok{(}\AttributeTok{option =} \StringTok{"turbo"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/fig-ViridisTurboBarplot-1.pdf}}

}

\caption{\label{fig-ViridisTurboBarplot}}

\end{figure}%

If you like colours, check out the \{paletteer\} package (Hvitfeldt
2021), which provides a neat interface to access a very large collection
of \texttt{R} colour packages, some of which are very fun! The advantage
is that you only need to install one package
(\texttt{install.packages("paletteer")}) to have a huge range of
palettes at your disposal. Below is a small selection of some personal
favourites.

\subsubsection{beyonce::X11}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ OccupGroup, }\AttributeTok{fill =}\NormalTok{ OccupGroup)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Occupational group"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Participants"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{),}
                     \AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{60}\NormalTok{)) }\SpecialCharTok{+}
\NormalTok{  paletteer}\SpecialCharTok{::}\FunctionTok{scale\_fill\_paletteer\_d}\NormalTok{(}\StringTok{"beyonce::X11"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/fig-BeyonceBarplot-1.pdf}}

}

\caption{\label{fig-BeyonceBarplot}}

\end{figure}%

\subsubsection{BridgetRiley}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ OccupGroup, }\AttributeTok{fill =}\NormalTok{ OccupGroup)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Occupational group"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Participants"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{),}
                     \AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{60}\NormalTok{)) }\SpecialCharTok{+} 
\NormalTok{  paletteer}\SpecialCharTok{::}\FunctionTok{scale\_fill\_paletteer\_d}\NormalTok{(}\StringTok{"lisa::BridgetRiley"}\NormalTok{, }\AttributeTok{direction =} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/fig-BridgetRileyBarplot-1.pdf}}

}

\caption{\label{fig-BridgetRileyBarplot}}

\end{figure}%

\subsubsection{janelle}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ OccupGroup, }\AttributeTok{fill =}\NormalTok{ OccupGroup)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Occupational group"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Participants"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{),}
                     \AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{60}\NormalTok{)) }\SpecialCharTok{+}
\NormalTok{  paletteer}\SpecialCharTok{::}\FunctionTok{scale\_fill\_paletteer\_d}\NormalTok{(}\StringTok{"rockthemes::janelle"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/fig-JanelleBarplot-1.pdf}}

}

\caption{\label{fig-JanelleBarplot}}

\end{figure}%

\subsubsection{FridaKahlo}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ OccupGroup, }\AttributeTok{fill =}\NormalTok{ OccupGroup)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Occupational group"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Participants"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{),}
                     \AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{60}\NormalTok{)) }\SpecialCharTok{+}
\NormalTok{  paletteer}\SpecialCharTok{::}\FunctionTok{scale\_fill\_paletteer\_d}\NormalTok{(}\StringTok{"lisa::FridaKahlo"}\NormalTok{, }\AttributeTok{direction =} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/fig-FridaKahloBarplot-1.pdf}}

}

\caption{\label{fig-FridaKahloBarplot}}

\end{figure}%

\subsubsection{kiss}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ OccupGroup, }\AttributeTok{fill =}\NormalTok{ OccupGroup)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Occupational group"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Participants"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{),}
                     \AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{60}\NormalTok{)) }\SpecialCharTok{+}
\NormalTok{  paletteer}\SpecialCharTok{::}\FunctionTok{scale\_fill\_paletteer\_d}\NormalTok{(}\StringTok{"ltc::kiss"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/fig-KissBarplot-1.pdf}}

}

\caption{\label{fig-KissBarplot}}

\end{figure}%

\subsubsection{speakNow}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ OccupGroup, }\AttributeTok{fill =}\NormalTok{ OccupGroup)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Occupational group"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Participants"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{),}
                     \AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{60}\NormalTok{)) }\SpecialCharTok{+}
\NormalTok{  paletteer}\SpecialCharTok{::}\FunctionTok{scale\_fill\_paletteer\_d}\NormalTok{(}\StringTok{"tayloRswift::speakNow"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/fig-SpeakNowBarplot-1.pdf}}

}

\caption{\label{fig-SpeakNowBarplot}}

\end{figure}%

\subsection{Themes}\label{sec-themes}

The \{ggplot2\} framework also allows for the addition of an optional
\texttt{theme()} \textbf{layer} to further customise the look of plots.
The default \{ggplot2\} theme is \texttt{theme\_grey()}. Here are some
of the pre-built themes that come with the \{ggplot2\} library for you
to compare. As with colour palettes, you can install additional
libraries that will give you access to literally hundreds of ready-made
themes for you to explore.

\subsubsection{theme\_bw()}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ Dabrowska.data,}
       \AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ OccupGroup)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Occupational group"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of participants"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/unnamed-chunk-11-1.pdf}}

\subsubsection{theme\_dark()}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ Dabrowska.data,}
       \AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ OccupGroup)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Occupational group"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of participants"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_dark}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/unnamed-chunk-12-1.pdf}}

\subsubsection{theme\_light()}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ Dabrowska.data,}
       \AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ OccupGroup)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Occupational group"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of participants"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_light}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/unnamed-chunk-13-1.pdf}}

\subsubsection{theme\_minimal()}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ Dabrowska.data,}
       \AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ OccupGroup)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Occupational group"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of participants"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/unnamed-chunk-14-1.pdf}}

\subsubsection{theme\_void()}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ Dabrowska.data,}
       \AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ OccupGroup)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Occupational group"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of participants"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_void}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/unnamed-chunk-15-1.pdf}}

Pretty much all aspects of plot themes can be customised. To demonstrate
this, Figure~\ref{fig-SillyTheme} displays a bar plot with some highly
customised aesthetics. I will let you judge how meaningful these custom
choices are and whether they genuinely help the reader to interpret the
data\ldots{} ðŸ¤¨

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ Dabrowska.data,}
       \AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ OccupGroup, }\AttributeTok{fill =}\NormalTok{ Gender)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Occupational group"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of participants"}\NormalTok{,}
        \AttributeTok{title =} \StringTok{"An example of an extravagantly customised ggplot..."}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{theme}\NormalTok{(}
      \AttributeTok{panel.background =} \FunctionTok{element\_rect}\NormalTok{(}\AttributeTok{fill =} \StringTok{"\#FFC080"}\NormalTok{, }\AttributeTok{color =} \ConstantTok{NA}\NormalTok{),}
      \AttributeTok{panel.grid.major =} \FunctionTok{element\_line}\NormalTok{(}\AttributeTok{color =} \StringTok{"gold"}\NormalTok{, }\AttributeTok{linewidth =} \FloatTok{1.5}\NormalTok{),}
      \AttributeTok{panel.grid.minor =} \FunctionTok{element\_line}\NormalTok{(}\AttributeTok{color =} \StringTok{"grey20"}\NormalTok{, }\AttributeTok{linewidth =} \FloatTok{0.5}\NormalTok{),}
      \AttributeTok{axis.title.x =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{face =} \StringTok{"bold"}\NormalTok{, }\AttributeTok{size =} \DecValTok{12}\NormalTok{, }\AttributeTok{color =} \StringTok{"brown"}\NormalTok{, }\AttributeTok{angle =} \DecValTok{10}\NormalTok{),}
      \AttributeTok{axis.title.y =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{25}\NormalTok{, }\AttributeTok{color =} \StringTok{"green"}\NormalTok{, }\AttributeTok{family =} \StringTok{"Courier New"}\NormalTok{),}
      \AttributeTok{axis.text.x =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{face =} \StringTok{"italic"}\NormalTok{, }\AttributeTok{size =} \DecValTok{12}\NormalTok{, }\AttributeTok{color =} \StringTok{"cyan"}\NormalTok{),}
      \AttributeTok{axis.text.y =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{14}\NormalTok{, }\AttributeTok{color =} \StringTok{"grey"}\NormalTok{),}
      \AttributeTok{plot.title =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{face =} \StringTok{"bold"}\NormalTok{, }\AttributeTok{size =} \DecValTok{10}\NormalTok{, }\AttributeTok{color =} \StringTok{"purple"}\NormalTok{, }\AttributeTok{family =} \StringTok{"Comic Sans MS"}\NormalTok{)}
\NormalTok{      )}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/SillyTheme.png}}

}

\caption{\label{fig-SillyTheme}}

\end{figure}%

\subsection{Coordinates}\label{sec-coordinates}

By default, the coordinate system that is used in \texttt{ggplot}
objects is the \textbf{Cartesian coordinate system}, which has a
horizontal axis (\emph{x}) and a vertical axis (\emph{y}) that are
perpendicular to each other. To change this default Cartesian coordinate
system, we need to add a \textbf{coordinate layer}.

For example, if we want to display the full names of the four
occupational groups used in DÄ…browska (2019), we can change the labels
of the categories using \texttt{mutate()} and \texttt{fct\_recode()}
\emph{before} pipping the data into \texttt{ggplot()} (see
Section~\ref{sec-ggplotData}) and then flip the \emph{x} and \emph{y}
axes using the coordinate layer \texttt{coord\_flip()}. As shown in
Figure~\ref{fig-FlippedBarplot}, this makes long labels much easier to
read.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{OccupGroup =} \FunctionTok{fct\_recode}\NormalTok{(OccupGroup,}
                                 \StringTok{\textasciigrave{}}\AttributeTok{Professionally inactive}\StringTok{\textasciigrave{}} \OtherTok{=} \StringTok{"I"}\NormalTok{,}
                                 \StringTok{\textasciigrave{}}\AttributeTok{Clerical profession}\StringTok{\textasciigrave{}} \OtherTok{=} \StringTok{"C"}\NormalTok{,}
                                 \StringTok{\textasciigrave{}}\AttributeTok{Manual profession}\StringTok{\textasciigrave{}} \OtherTok{=} \StringTok{"M"}\NormalTok{,}
                                 \StringTok{\textasciigrave{}}\AttributeTok{Professional{-}level job/}\SpecialCharTok{\textbackslash{}n}\AttributeTok{student}\StringTok{\textasciigrave{}} \OtherTok{=} \StringTok{"PS"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Gender =} \FunctionTok{fct\_rev}\NormalTok{(Gender)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ OccupGroup, }\AttributeTok{fill =}\NormalTok{ Gender)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \ConstantTok{NULL}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Participants"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_viridis\_d}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{coord\_flip}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.text =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{12}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/fig-FlippedBarplot-1.pdf}}

}

\caption{\label{fig-FlippedBarplot}}

\end{figure}%

The vast majority of statistical graphs use the Cartesian coordinate
system. Pie charts and other circular plots, however, use the
\textbf{polar coordinate system} (\texttt{coord\_polar}), whereby
quantities are mapped onto angles rather than distances. In general,
humans are much better at judging lengths than angles or areas
(Cleveland \& McGill 1987), which is why circular graphs such as pie
charts are typically \emph{not} recommended forms of good data
visualisations (see, e.g. Few). That said, they \emph{can} be produced
using the \{ggplot2\} library by adding the \textbf{coordinate} layer
\texttt{coord\_polar("y")} and modifying a few parameters.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{OccupGroup =} \FunctionTok{fct\_recode}\NormalTok{(OccupGroup,}
                                 \StringTok{\textasciigrave{}}\AttributeTok{Professionally inactive}\StringTok{\textasciigrave{}} \OtherTok{=} \StringTok{"I"}\NormalTok{,}
                                 \StringTok{\textasciigrave{}}\AttributeTok{Clerical profession}\StringTok{\textasciigrave{}} \OtherTok{=} \StringTok{"C"}\NormalTok{,}
                                 \StringTok{\textasciigrave{}}\AttributeTok{Manual profession}\StringTok{\textasciigrave{}} \OtherTok{=} \StringTok{"M"}\NormalTok{,}
                                 \StringTok{\textasciigrave{}}\AttributeTok{Professional{-}level job/}\SpecialCharTok{\textbackslash{}n}\AttributeTok{student}\StringTok{\textasciigrave{}} \OtherTok{=} \StringTok{"PS"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \StringTok{""}\NormalTok{, }\AttributeTok{fill =}\NormalTok{ OccupGroup)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{width =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{fill =} \StringTok{"Occupational group"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_viridis\_d}\NormalTok{(}\AttributeTok{direction =} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{coord\_polar}\NormalTok{(}\StringTok{"y"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_void}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/fig-OccupGroupPieChart-1.pdf}}

}

\caption{\label{fig-OccupGroupPieChart}}

\end{figure}%

\section{The semantics of graphics}\label{sec-Semantics}

So far, we have seen how the syntax of the Grammar of Graphics can be
used to build statistical graphs layer by layer. We now turn to the
\textbf{semantics} of graphics. As linguists are well placed to know,
semantics is the study of \textbf{meaning}. In the Grammar of Graphics,
the semantics of graphics is defined as ``the meanings of the
representative symbols and arrangements we use to display information''
(Wilkinson 2005: 20). In what follows, we will see how thinking about
the semantics of graphics can help us to think about how the different
components of a graph interact to convey insightful visual information
from raw data. This will help us to make informed choices when choosing
the geometries, scales, facets, and themes of our data visualisations.

But, first, let's think about \textbf{why} we visualise data. Data
visualisation is about more than just communicating the results of our
analyses to \textbf{others} at the publication stage. In fact, good data
visualisation can help \textbf{us} make informed decisions throughout
the research process from the data wrangling stage to the evaluation of
complex statistical models. Here are some reasons for visualising data.
Can you think of others? ðŸ¤”

\textbf{For yourself}

\begin{itemize}
\tightlist
\item
  To explore your data
\item
  To detect data processing errors and outliers
\item
  To check assumptions of statistical tests or models (see
  Section~\ref{sec-Assumptions} and Section~\ref{sec-AssumptionsLR})
\item
  To examine variation across different subsets of the data
\item
  To better interpret the results of statistical tests (see
  Chapter~\ref{sec-Inferential}) and models (see Chapter~\ref{sec-SLR}
  and \ref{sec-MLR})
\end{itemize}

\textbf{For others}

\begin{itemize}
\tightlist
\item
  To communicate the results of your analyses more effectively
\item
  To communicate about your data (in more detail)
\item
  To communicate complex information more efficiently
\item
  To attract the reader's attention
\item
  To allow the reader to reach their own conclusions
\end{itemize}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/AHorst_ggplot2_explore.png}}

}

\caption{\label{fig-ExplorerMonster}Using the \{ggplot2\} package for
data exploration (artwork by
\href{https://allisonhorst.com/}{@allison\_horst})}

\end{figure}%

Depending on the type of data that we want to visualise and why, we can
choose different types of plots. A great resource to choose a graphic
that is suitable for your data is
\href{https://r-graph-gallery.com/}{the \texttt{R} Graph Gallery}.

In the following, we will first look at how we can plot
\textbf{categorical variables} and \textbf{discrete numeric variables},
before we move on to visualising \textbf{continuous numeric variables}
and combinations of different types of variables (see
Section~\ref{sec-Variables}).

\subsection{Bar plots}\label{bar-plots}

As we saw in Section~\ref{sec-Syntax}, bar plots (also called
\textbf{bar charts}) are a great way to visualise \textbf{categorical
variables}. We also saw that, when using horizontal writing systems, it
is often easier to interpret a bar plot if its coordinates are flipped
so that longer labels can be read more readily.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(Group }\SpecialCharTok{==} \StringTok{"L2"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{NativeLg =} \FunctionTok{fct\_rev}\NormalTok{(}\FunctionTok{fct\_infreq}\NormalTok{(NativeLg))) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ NativeLg, }
           \AttributeTok{fill =}\NormalTok{ NativeLgFamily)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{coord\_flip}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_viridis\_d}\NormalTok{(}\AttributeTok{option =} \StringTok{"F"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{40}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \ConstantTok{NULL}\NormalTok{, }
       \AttributeTok{y =} \ConstantTok{NULL}\NormalTok{, }
       \AttributeTok{fill =} \StringTok{"Language family"}\NormalTok{,}
       \AttributeTok{title =} \StringTok{"Native languages of L2 participants"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{(}\AttributeTok{base\_size =} \DecValTok{16}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/fig-NativeLanguagesBarplot-1.pdf}}

}

\caption{\label{fig-NativeLanguagesBarplot}}

\end{figure}%

To create Figure~\ref{fig-NativeLanguagesBarplot}, we first reordered
the factor levels of the \texttt{NativeLg} variable using two functions
from the \{forcats\} package (see Section~\ref{sec-Factors}):
\texttt{fct\_infreq()} is first used to order the factors according to
their frequency (by default, they are sorted alphabetically), and then
\texttt{fct\_rev()} is used to reverse that order. The latter step is
needed because the \texttt{coord\_flip()} functions reverses everything.
You can check the order of a factor's level using the function
\texttt{levels()}. Note that, if two levels have the same number of
occurrences, they are ordered alphabetically (as seen in
Figure~\ref{fig-NativeLanguagesBarplot}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{levels}\NormalTok{(Dabrowska.data}\SpecialCharTok{$}\NormalTok{NativeLg)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "Cantonese"  "Chinese"    "French"     "German"     "Greek"     
 [6] "Italian"    "Lithuanian" "Mandarin"   "Polish"     "Russian"   
[11] "Spanish"   
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{levels}\NormalTok{(}\FunctionTok{fct\_infreq}\NormalTok{(Dabrowska.data}\SpecialCharTok{$}\NormalTok{NativeLg))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "Polish"     "Mandarin"   "Lithuanian" "Cantonese"  "Chinese"   
 [6] "Spanish"    "French"     "Russian"    "German"     "Greek"     
[11] "Italian"   
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{levels}\NormalTok{(}\FunctionTok{fct\_rev}\NormalTok{(}\FunctionTok{fct\_infreq}\NormalTok{(Dabrowska.data}\SpecialCharTok{$}\NormalTok{NativeLg)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "Italian"    "Greek"      "German"     "Russian"    "French"    
 [6] "Spanish"    "Chinese"    "Cantonese"  "Lithuanian" "Mandarin"  
[11] "Polish"    
\end{verbatim}

\subsection{Histograms}\label{sec-histograms}

In Section~\ref{sec-DistributionsNumeric}, we visually examined the
distribution of participants' ages in a \textbf{bar plot}. This was
possible because the age variable in DÄ…browska (2019) was recorded as a
discrete numeric variable (i.e.~either as 18 or 19, but not 18.4 years
of age).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ Dabrowska.data,}
       \AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(Age)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/fig-AgeBarPlot-1.pdf}}

}

\caption{\label{fig-AgeBarPlot}}

\end{figure}%

Bar plots are best suited for categorical data and should only be used
to visualise discrete numeric variables that have a fairly limited
number of possible values. As we can see from the output of the
\texttt{unique()} function, this is not the case for the \texttt{Age}
variable in \texttt{Dabrowska.data}, as it includes 40 different age
values, ranging from 17 to 65.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{unique}\NormalTok{(Dabrowska.data}\SpecialCharTok{$}\NormalTok{Age) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{sort}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 37 38 39 40 41 42
[26] 44 46 47 48 51 52 53 55 57 58 59 60 61 62 65
\end{verbatim}

The distribution of participants' ages is therefore better visualised as
a histogram or density plot. To visualise participants' age as
\textbf{histogram} (@fig-AgeHistogram) rather than as a bar plot
(@fig-AgeBarPlot), change the plot geometry (see
Section~\ref{sec-geoms}) from \texttt{geom\_bar()} to
\texttt{geom\_histogram()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ Dabrowska.data,}
       \AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Age)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Age (in years)"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of participants"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
`stat_bin()` using `bins = 30`. Pick better value `binwidth`.
\end{verbatim}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/fig-AgeHistogram-1.pdf}}

}

\caption{\label{fig-AgeHistogram}}

\end{figure}%

When generating this histogram, a message appears in the \texttt{R}
console that informs us that, by default, the \texttt{geom\_histogram()}
function subdivided the \texttt{Age} values into 30 \textbf{bins}. This
means that the age range from 17 to 65 has been subdivided into 30
groups of equal size. Given that there is a range of 48 in the
\texttt{Age} values in this dataset, this is not a great way to
subdivide the values of this variable.

As indicated in the message, to change this behaviour, we can adjust the
value of the ``binwidth'' argument. This argument determines how many
years go in each bin. So if we choose to have two years in each
subdivision of the \texttt{Age} variable, we will end up with 24 bins.
Logically, if we decide to group four years in each subdivision of the
\texttt{Age} variable, we will end up with just 12 bins.

Compare the three histograms below. In your opinion, which binwidth
provides the most effective way to visualise the distribution of
participants' ages? ðŸ¤”

\subsubsection{\texorpdfstring{\texttt{default\ bin\ width}}{default bin width}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ Dabrowska.data,}
       \AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Age)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Age (in years)"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of participants"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/unnamed-chunk-21-1.pdf}}

\subsubsection{\texorpdfstring{\texttt{binwidth\ =\ 2}}{binwidth = 2}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ Dabrowska.data,}
       \AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Age)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Age (in years)"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of participants"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/unnamed-chunk-22-1.pdf}}

\subsubsection{\texorpdfstring{\texttt{binwidth\ =\ 4}}{binwidth = 4}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ Dabrowska.data,}
       \AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Age)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \DecValTok{4}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Age (in years)"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of participants"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/unnamed-chunk-23-1.pdf}}

The distribution of two continuous variables can also be compared by
superimposing two histograms as in @fig-OverlappingHisto. This requires
the addition of a \texttt{fill} aesthetics (see
Section~\ref{sec-scales}) and the argument
\texttt{position\ =\ "identity"}. For both distributions to be visible,
it is also necessary to add some transparency to the fill colour of the
bars. This is achieved using the \texttt{alpha} argument of the
\texttt{geom\_histogram()} function. An alpha value of \texttt{0}
corresponds to full transparency (e.g.~no colour), whilst an alpha value
of \texttt{1} corresponds to complete opacity.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ Dabrowska.data,}
       \AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Age,}
                     \AttributeTok{fill =}\NormalTok{ Group)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \DecValTok{5}\NormalTok{,}
                 \AttributeTok{position =} \StringTok{"identity"}\NormalTok{,}
                 \AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_viridis\_d}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Age (in years)"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of participants"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/fig-OverlappingHisto-1.pdf}}

}

\caption{\label{fig-OverlappingHisto}}

\end{figure}%

\subsection{Density plots}\label{sec-Density}

An alternative to displaying the data in discrete bins is to apply a
density function to smooth over the bins of the histogram. This is what
we call a density plot. Figure~\ref{fig-GrammarDensity} is a density
plot of participants' grammar test scores.

Create density plots in \texttt{R} using \{ggplot2\} is very simple.
Because, yes, you've guessed it: there's a \texttt{geom\_} function for
density plots and it's called\ldots{} \texttt{geom\_density()}! ðŸ˜ƒ

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ Dabrowska.data,}
       \AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Grammar)) }\SpecialCharTok{+}
  \FunctionTok{geom\_density}\NormalTok{(}\AttributeTok{fill =} \StringTok{"purple"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Scores on English grammar test"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/fig-GrammarDensity-1.pdf}}

}

\caption{\label{fig-GrammarDensity}Distribution of English vocabulary
test results}

\end{figure}%

Density plots are particularly useful to examine distribution shapes
visually. Looking at Figure~\ref{fig-GrammarDensity}, we can immediately
see that the values of the \texttt{Grammar} variable are not normally
distributed (see Section~\ref{sec-DistributionsNumeric}).

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Going further: Setting properties within geoms}, titlerule=0mm, leftrule=.75mm]

You can also change the \textbf{attributes} of any geometry layer by
specifying them as \textbf{arguments} within their \texttt{geom\_}
function.

The help file of each \texttt{geom\_} function provides a list of the
\textbf{aesthetics} arguments that each function has (see below for
relevant extract). If we do not specify any of the optional aesthetics
of the \texttt{geom\_} functions, sensible default values will be used.
For instance, the line colour of density plots will be black, unless
otherwise specified with the argument ``colour''.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{?geom\_density}
\end{Highlighting}
\end{Shaded}

\begin{quote}
{[}\ldots{]} \textbf{Aesthetics}

\texttt{geom\_density()} understands the following aesthetics (required
aesthetics are in bold):

\begin{itemize}
\tightlist
\item
  \href{https://ggplot2.tidyverse.org/reference/aes_position.html}{\textbf{\texttt{x}}}
\item
  \href{https://ggplot2.tidyverse.org/reference/aes_position.html}{\textbf{\texttt{y}}}
\item
  \href{https://ggplot2.tidyverse.org/reference/aes_colour_fill_alpha.html}{\texttt{alpha}}
  â†’ \texttt{NA}
\item
  \href{https://ggplot2.tidyverse.org/reference/aes_colour_fill_alpha.html}{\texttt{colour}}
  â†’ via
  \href{https://ggplot2.tidyverse.org/reference/theme.html}{\texttt{theme()}}
\item
  \href{https://ggplot2.tidyverse.org/reference/aes_colour_fill_alpha.html}{\texttt{fill}}
  â†’ via
  \href{https://ggplot2.tidyverse.org/reference/theme.html}{\texttt{theme()}}
\item
  \href{https://ggplot2.tidyverse.org/reference/aes_group_order.html}{\texttt{group}}
  â†’ inferred
\item
  \href{https://ggplot2.tidyverse.org/reference/aes_linetype_size_shape.html}{\texttt{linetype}}
  â†’ via
  \href{https://ggplot2.tidyverse.org/reference/theme.html}{\texttt{theme()}}
\item
  \href{https://ggplot2.tidyverse.org/reference/aes_linetype_size_shape.html}{\texttt{linewidth}}
  â†’ via
  \href{https://ggplot2.tidyverse.org/reference/theme.html}{\texttt{theme()}}
\item
  \texttt{weight} â†’ \texttt{1}
\end{itemize}

Learn more about setting these aesthetics in
\texttt{vignette("ggplot2-specs")}. {[}\ldots{]}
\end{quote}

Figure~\ref{fig-SillyDensityPlot} is an example of a density plot with
some highly customised aesthetics. It goes without saying that, just
because you \emph{can} customise many aspects of a \texttt{geom\_}
layer, it doesn't necessarily mean that it's a good idea to do so! ðŸ™ƒ

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(Group }\SpecialCharTok{==} \StringTok{"L2"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Blocks)) }\SpecialCharTok{+}
    \FunctionTok{geom\_density}\NormalTok{(}\AttributeTok{colour =} \StringTok{"purple"}\NormalTok{, }\CommentTok{\# Sets the colour of the outline of the density plot}
                \AttributeTok{linewidth =} \DecValTok{5}\NormalTok{, }\CommentTok{\# Sets the width of the outline}
                \AttributeTok{linetype =} \StringTok{"dotdash"}\NormalTok{, }\CommentTok{\# Sets the line type of the outline}
                \AttributeTok{fill =} \StringTok{"pink"}\NormalTok{, }\CommentTok{\# Sets the colour of the area under the density plot}
                \AttributeTok{alpha =} \FloatTok{0.6} \CommentTok{\# Sets the transparency level of the fill colour (with 0 being fully transparent and 1 being completely opaque)}
\NormalTok{    ) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Blocks test results"}\NormalTok{,}
         \AttributeTok{title =} \StringTok{"L2 participants\textquotesingle{} non{-}verbal IQ scores"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/fig-SillyDensityPlot-1.pdf}}

}

\caption{\label{fig-SillyDensityPlot}}

\end{figure}%

It can, however, be very useful to help identify different elements
within a complex plot as in Figure~\ref{fig-UsefulDensityPlot}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# We first create an R object that contains the mean Block scores for both L1 and L2 participants.}
\NormalTok{mean.blocks }\OtherTok{\textless{}{-}}\NormalTok{ Dabrowska.data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(Group) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{mean =} \FunctionTok{mean}\NormalTok{(Blocks))}

\CommentTok{\# Next we plot density plots and also add a geom\_vline layer to plot the mean values as vertical lines. We use the linetype argument to make the lines dotted.}
\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Blocks, }
                       \AttributeTok{fill =}\NormalTok{ Group,}
                       \AttributeTok{colour =}\NormalTok{ Group)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_density}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.6}\NormalTok{, }
               \AttributeTok{position =} \StringTok{"identity"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{data =}\NormalTok{ mean.blocks, }
             \FunctionTok{aes}\NormalTok{(}\AttributeTok{xintercept =}\NormalTok{ mean, }
                 \AttributeTok{colour =}\NormalTok{ Group), }
             \AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{,}
             \AttributeTok{linewidth =} \FloatTok{0.8}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_colour\_viridis\_d}\NormalTok{(}\AttributeTok{guide =} \ConstantTok{NULL}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_viridis\_d}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Non{-}verbal IQ test (Blocks) test scores"}\NormalTok{,}
       \AttributeTok{fill =} \ConstantTok{NULL}\NormalTok{,}
       \AttributeTok{title =} \StringTok{"On average, non{-}native English speakers have higher IQ scores}\SpecialCharTok{\textbackslash{}n}\StringTok{than native speakers."}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"The dotted lines represent the means of each group."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/fig-UsefulDensityPlot-1.pdf}}

}

\caption{\label{fig-UsefulDensityPlot}}

\end{figure}%

To create Figure~\ref{fig-UsefulDensityPlot}, we first calculated the
mean Blocks scores for both L1 and L2 participants and stored these
values in a new object called \texttt{mean.blocks} (see code link
above). These values are then called within the \texttt{geom\_vline()}
function, which draws vertical lines. In other words, in addition to
setting \texttt{aes()} mappings within the main \texttt{ggplot()}
function at the start of our plot code, we can also add
\textbf{additional data mappings} within a specific \texttt{geom\_}
function. With all these options, it's no exaggeration to say that, if
you really set your mind to, pretty much anything is possible with
\{ggplot2\}! ðŸ™ƒ

\end{tcolorbox}

\subsection{Boxplots}\label{boxplots}

In Section~\ref{sec-IQR}, we saw that \textbf{boxplots} are a great way
to visualise both the central tendency (median) of a numeric variable
and the spread around this central tendency (IQR). There is an in-built
function to create boxplots in \{ggplot2\}. No prizes will be awarded
for guessing that the necessary \texttt{geom\_} function is
called\ldots{} \texttt{geom\_density()}! ðŸ˜†

Whilst it's possible to plot just a single boxplot, that rarely makes
sense. In fact, the \texttt{x}-axis in Figure~\ref{fig-GrammarBoxPlot}
is entirely nonsensical! The distribution of \texttt{Grammar} scores
across the entire dataset is much better visualised as a histogram or
density plot (see Section~\ref{sec-Density}) than as a single boxplot.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ Grammar)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{y =} \StringTok{"Grammar scores"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/fig-GrammarBoxPlot-1.pdf}}

}

\caption{\label{fig-GrammarBoxPlot}}

\end{figure}%

If, however, we want to compare the \texttt{Grammar} scores of two or
more different groups of participants, a boxplot makes a lot more sense
(see Figure~\ref{fig-GrammarBoxPlotGroup}). To achieve this, we add a
second argument within the \texttt{aes()} function, which maps the
values of the \texttt{Group} variable (which are either ``L1'' or
``L2'') to the plot's \texttt{x}-axis.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ Grammar, }
                       \AttributeTok{x =}\NormalTok{ Group)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{y =} \StringTok{"Grammar scores"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/fig-GrammarBoxPlotGroup-1.pdf}}

}

\caption{\label{fig-GrammarBoxPlotGroup}}

\end{figure}%

The meaning conveyed by Figure~\ref{fig-GrammarBoxPlotGroup} is clear:
there is hardly any difference between the average (median) grammar
comprehension test scores of L1 and L2 participants in DÄ…browska
(2019)'s dataset. Indeed, we can see that the thicker, middle lines
within each boxplot are almost at the same level. However, the two
boxplots have very different shapes and overall lengths: the scores of
the 50\% of L2 participants who scored below the median are much more
spread out than those of the L1 participants who obtained below-average
scores. This makes intuitive sense: native English speakers living in
the UK who volunteer for such a study are likely to all have a fairly
high to very high understanding of English grammar. By contrast, the L2
speakers are much more varied: some are highly proficient in English,
while others are not. This range of proficiency could due to all sorts
of reasons.

What are some of the possible reasons that you can think of? ðŸ¤” Make a
note of them as we will explore these hypotheses further in
Section~\ref{sec-Scatterplots}.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Going further: Dot plots and violin plots}, titlerule=0mm, leftrule=.75mm]

The \{ggplot2\} library offers many more \texttt{geom\_} functions for
you to explore. Here are two more types of graphs that are currently
only rarely used in the language sciences, but which can be very
effective ways to visualise the distribution of a numeric variable
across different levels of a categorical variable.

\paragraph*{Dot plots}\label{dot-plots}
\addcontentsline{toc}{paragraph}{Dot plots}

In a dot plot, each data point (corresponding, here, to a single
participant) is represented by a single dot. The size of each dot
corresponds to the chosen \textbf{bin width}. This makes dot plots a
combination of a boxplot (see Section~\ref{sec-IQR}) and a histogram
(see Section~\ref{sec-histograms}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ Dabrowska.data,}
    \AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ Colloc, }
                     \AttributeTok{x =}\NormalTok{ Group)) }\SpecialCharTok{+}
    \FunctionTok{geom\_dotplot}\NormalTok{(}\AttributeTok{binaxis =} \StringTok{"y"}\NormalTok{, }
              \AttributeTok{stackdir =} \StringTok{"center"}\NormalTok{,}
              \AttributeTok{binwidth =} \DecValTok{3}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{y =} \StringTok{"Scores on English collocation test"}\NormalTok{,}
         \AttributeTok{x =} \ConstantTok{NULL}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/unnamed-chunk-39-1.pdf}}

\paragraph*{Violin plots}\label{violin-plots}
\addcontentsline{toc}{paragraph}{Violin plots}

The help file of the \texttt{geom\_violin()} function describes violin
plots as follows:

\begin{quote}
A violin plot is a compact display of a continuous distribution. It is a
blend of
\href{http://127.0.0.1:19143/help/library/ggplot2/help/geom_boxplot}{\texttt{geom\_boxplot()}}
and
\href{http://127.0.0.1:19143/help/library/ggplot2/help/geom_density}{\texttt{geom\_density()}}:
a violin plot is a mirrored density plot displayed in the same way as a
boxplot.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ Dabrowska.data,}
       \AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ Colloc, }
                       \AttributeTok{x =}\NormalTok{ Group)) }\SpecialCharTok{+}
    \FunctionTok{geom\_violin}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{y =} \StringTok{"Scores on English collocation test"}\NormalTok{,}
         \AttributeTok{x =} \ConstantTok{NULL}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/unnamed-chunk-41-1.pdf}}

By themselves, \textbf{violin plots} are rather abstract representations
of variable distributions. However, in combination with
\textbf{boxplots}, they can be an effective way to visualise and compare
data distributions. Note that, here, the order of the layers is
important because, if we first draw the boxplots and then the violin
plots, the violin plots will mask the boxplots completely.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ Colloc, }
                       \AttributeTok{x =}\NormalTok{ Group)) }\SpecialCharTok{+}
    \FunctionTok{geom\_violin}\NormalTok{(}\AttributeTok{width =} \DecValTok{1}\NormalTok{, }
                \AttributeTok{colour =} \StringTok{"grey"}\NormalTok{, }
                \AttributeTok{fill =} \StringTok{"grey"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{width =} \FloatTok{0.08}\NormalTok{, }
                 \AttributeTok{alpha =} \FloatTok{0.2}\NormalTok{, }
                 \AttributeTok{outliers =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{y =} \StringTok{"Scores on English collocation test"}\NormalTok{,}
         \AttributeTok{x =} \ConstantTok{NULL}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/unnamed-chunk-43-1.pdf}}

\end{tcolorbox}

\subsection{Scatter plots}\label{sec-Scatterplots}

Scatter plots are ideal to examine the relationship between two numeric
variables. They are best suited to continuous numeric variables.

In the following, we will build a scatter plot to explore the following
hypothesis:

\begin{itemize}
\tightlist
\item
  In the data from DÄ…browska (2019), English grammar comprehension
  scores are more strongly associated with the level of formal education
  among L2 speakers than among L1 speakers.
\end{itemize}

To explore this hypothesis, we map the total number of years that
participants spent in formal education (\texttt{EduTotal}) onto the
\emph{x}-axis and their \texttt{Grammar} scores onto the \emph{y}-axis.
In addition, we use the \texttt{facet\_wrap()} function to split the
data into two panels: one for the L1 participants and the other for the
L2 group.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ EduTotal, }
                     \AttributeTok{y =}\NormalTok{ Grammar)) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ Group) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Number of years in formal education"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Grammar comprehension test scores"}\NormalTok{,}
       \AttributeTok{title =} \StringTok{"Exploring hypothesis 1"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/unnamed-chunk-44-1.pdf}}

At first glance, it would seem that our data do not support our initial
hypothesis: among the L2 participants, there is no obvious trend
suggesting that those who scored lowest on the grammar test were the
ones who spent fewer years in formal education.-

In Figure~\ref{fig-GrammarScatterPlot}, we add a regression line (in
blue) per panel to our facetted scatter plot using the
\texttt{geom\_smooth(method\ =\ "lm")} function. This allows us to
visualise the \textbf{correlation} between participants' grammar scores
and the number of years they spent in formal education.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ EduTotal, }
                       \AttributeTok{y =}\NormalTok{ Grammar)) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ Group) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{,}
              \AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Number of years in formal education"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Grammar comprehension test scores"}\NormalTok{,}
       \AttributeTok{title =} \StringTok{"Exploring hypothesis 1"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/fig-GrammarScatterPlot-1.pdf}}

}

\caption{\label{fig-GrammarScatterPlot}}

\end{figure}%

Regression lines in scatter plots are interpreted as follows:

\begin{itemize}
\tightlist
\item
  If the regression line goes up, there is a \textbf{positive
  correlation} between the two numeric variables. â†—ï¸
\item
  If the line goes down, there is a \textbf{negative correlation}. â†˜ï¸
\item
  The steeper the line, the stronger the correlation. ðŸ’ª
\item
  If the line is flat (or nearly flat), there is no (linear) correlation
  between the two variables. âž¡ï¸
\item
  Be aware that even very strong correlations do not necessarily imply
  (direct) causation. âŒ
\end{itemize}

The regression lines added by the \texttt{geom\_smooth(method\ =\ "lm")}
function are \textbf{lines of best fit}: the better the fit, the closer
the points are to the line. If few points are on or close to the line,
it means that the regression line is not a good approximation of the
relationship between the two variables. This is clearly the case in
Figure~\ref{fig-GrammarScatterPlot} - especially in the L2 panel (more
on this in Section~\ref{sec-Correlations}). Our data visualisation
therefore do not support our hypothesis that, in the DÄ…browska (2019)
data, grammar scores are more strongly associated with the level of
formal education among the L2 speakers than among the L1 speakers. If
anything, our data show the opposite pattern! Our line of fit is both
closer to the data points and steeper in the L1 panel than in the L2
panel.

So far, all of our data visualisations have only displayed the
characteristics of the collected data. In other words, they display
\textbf{descriptive statistics} (see Chapter~\ref{sec-DescRiptiveStats})
that do not allow us to make inferences about other participants who
were not tested as part of DÄ…browska (2019)'s study. Tests of
statistical significance, including of correlations, are introduced in
Chapter~\ref{sec-Inferential}.

\subsection{Word clouds}\label{word-clouds}

The \{ggplot2\} library does not include an in-built function to create
word clouds. However, members of the \texttt{R} community are
continuously creating and sharing new functions and packages to improve
and extend data visualisation options in \texttt{R}. One such community
member, Erwan Le Pennec, created the
\href{https://lepennec.github.io/ggwordcloud/}{\{ggwordcloud\}} package,
which adds the \texttt{geom\_text\_wordcloud()} function to the
\{ggplot2\} environment.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#install.packages("ggwordcloud")}
\FunctionTok{library}\NormalTok{(ggwordcloud)}
\end{Highlighting}
\end{Shaded}

We will use the \texttt{geom\_text\_wordcloud()} function to create a
word cloud representing L2 participants' native languages. To this end,
we first need to create a table that tallies how often each native
language was mentioned. We also include a column for the language family
(see {\textbf{Task 9.3}} in Section~\ref{sec-casewhen}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{NativeLg\_freq }\OtherTok{\textless{}{-}}\NormalTok{ Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(Group }\SpecialCharTok{==} \StringTok{"L2"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{count}\NormalTok{(NativeLg, NativeLgFamily)}

\NormalTok{NativeLg\_freq}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     NativeLg NativeLgFamily  n
1   Cantonese        Chinese  4
2     Chinese        Chinese  3
3      French        Romance  2
4      German       Germanic  1
5       Greek       Hellenic  1
6     Italian        Romance  1
7  Lithuanian         Baltic  5
8    Mandarin        Chinese  8
9      Polish         Slavic 37
10    Russian         Slavic  2
11    Spanish        Romance  3
\end{verbatim}

Next, we enter this new dataset, \texttt{NativeLg\_freq}, in a
\texttt{ggplot()} function and map each native language to a text
\texttt{label} \textbf{aesthetics}, the number of participants to have
this native language to the \texttt{size} of the words, and each native
language family to a \texttt{colour}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ NativeLg\_freq,}
       \FunctionTok{aes}\NormalTok{(}\AttributeTok{label =}\NormalTok{ NativeLg,}
           \AttributeTok{size =}\NormalTok{ n,}
           \AttributeTok{colour =}\NormalTok{ NativeLgFamily)) }\SpecialCharTok{+}
  \FunctionTok{geom\_text\_wordcloud}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_size\_area}\NormalTok{(}\AttributeTok{max\_size =} \DecValTok{30}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/unnamed-chunk-52-1.pdf}}

Word clouds are widely used in the media and corporate world, but they
are much rarer in academic research. Although word clouds can help to
visualise the most prominent words in a set of words, they mostly serve
decorative purposes. Data visualisation experts warn against them for
two main reasons:

\begin{itemize}
\tightlist
\item
  Longer words appear larger simply due to their length.
\item
  The human eye is not good at discerning small differences in font
  sizes.
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Going further: Using \texttt{geom\_text()}}, titlerule=0mm, leftrule=.75mm]

We have seen that, on a scatter plot, each point represents an
observation, in our case, a participant. For instance, in
Figure~\ref{fig-ColourScatterPlot}, each point represents an L2
participant and displays their age and grammar comprehension test score.
In addition, we can colour the points according to their native language
family.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(Group }\SpecialCharTok{==} \StringTok{"L2"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Age, }
           \AttributeTok{y =}\NormalTok{ Grammar,}
           \AttributeTok{colour =}\NormalTok{ NativeLgFamily)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Age"}\NormalTok{, }
       \AttributeTok{y =} \StringTok{"Grammar comprehension test score"}\NormalTok{, }
       \AttributeTok{color =} \StringTok{"Language family"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{(}\AttributeTok{base\_size =} \DecValTok{14}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/fig-ColourScatterPlot-1.pdf}}

}

\caption{\label{fig-ColourScatterPlot}}

\end{figure}%

Instead of plotting points on the scatter plot, we can plot text. Thus,
instead of using \texttt{geom\_point()}, we can use
\texttt{geom\_text()}. For example, we can display the native language
of each participant by mapping \texttt{NativeLg} to the \texttt{label}
aesthetics.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(Group }\SpecialCharTok{==} \StringTok{"L2"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Age, }
           \AttributeTok{y =}\NormalTok{ Grammar,}
           \AttributeTok{colour =}\NormalTok{ NativeLgFamily,}
           \AttributeTok{label =}\NormalTok{ NativeLg)) }\SpecialCharTok{+}
  \FunctionTok{geom\_text}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\DecValTok{15}\NormalTok{,}\DecValTok{65}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Age"}\NormalTok{, }
       \AttributeTok{y =} \StringTok{"Grammar comprehension test score"}\NormalTok{, }
       \AttributeTok{color =} \StringTok{"Language family"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{(}\AttributeTok{base\_size =} \DecValTok{14}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/fig-TextScatterPlot-1.pdf}}

}

\caption{\label{fig-TextScatterPlot}}

\end{figure}%

Clearly, there are too many L2 participants for
Figure~\ref{fig-TextScatterPlot} to be legible. For now, we will
therefore focus on female L2 participants only. Moreover, we can see
that there is dense cluster of L2 participants who are young and scored
very high on the grammar comprehension test. In this exploratory
analysis, we will therefore focus on female participants who obtained
fewer than 90 points on the test.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(Group }\SpecialCharTok{==} \StringTok{"L2"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(Grammar }\SpecialCharTok{\textless{}} \DecValTok{90} \SpecialCharTok{\&}\NormalTok{ Gender }\SpecialCharTok{==} \StringTok{"F"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
\FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Age, }
           \AttributeTok{y =}\NormalTok{ Grammar,}
           \AttributeTok{colour =}\NormalTok{ NativeLgFamily,}
           \AttributeTok{label =}\NormalTok{ NativeLg)) }\SpecialCharTok{+}
  \FunctionTok{geom\_text}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\DecValTok{15}\NormalTok{,}\DecValTok{55}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Age"}\NormalTok{, }
       \AttributeTok{y =} \StringTok{"Grammar comprehension test score"}\NormalTok{, }
       \AttributeTok{color =} \StringTok{"Language family"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"(Subgroup analysis of female L2 participants with grammar scores below 90.)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{(}\AttributeTok{base\_size =} \DecValTok{14}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/fig-TextScatterPlot2-1.pdf}}

}

\caption{\label{fig-TextScatterPlot2}}

\end{figure}%

Note that we had to expand the limits of the \emph{x}-axis in order for
labels of the youngest and oldest participants to be displayed in full
on Figure~\ref{fig-TextScatterPlot2}. Still, there are a few labels that
overlap nonetheless. If this bothers you, you'll be pleased to hear that
there is a handy \{ggplot2\} extension called
\{\href{https://cran.r-project.org/web/packages/ggrepel/vignettes/ggrepel.html}{ggrepel}\},
which was designed to avoid text labels overlapping.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#install.packages("ggrepel")}
\FunctionTok{library}\NormalTok{(ggrepel)}

\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(Group }\SpecialCharTok{==} \StringTok{"L2"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(Grammar }\SpecialCharTok{\textless{}} \DecValTok{90} \SpecialCharTok{\&}\NormalTok{ Gender }\SpecialCharTok{==} \StringTok{"F"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
\FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Age, }
           \AttributeTok{y =}\NormalTok{ Grammar,}
           \AttributeTok{colour =}\NormalTok{ NativeLgFamily,}
           \AttributeTok{label =}\NormalTok{ NativeLg)) }\SpecialCharTok{+}
  \FunctionTok{geom\_text\_repel}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\DecValTok{15}\NormalTok{,}\DecValTok{55}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Age"}\NormalTok{, }
       \AttributeTok{y =} \StringTok{"Grammar comprehension test score"}\NormalTok{, }
       \AttributeTok{color =} \StringTok{"Language family"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"(Subgroup analysis of female L2 participants with grammar scores below 90.)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{(}\AttributeTok{base\_size =} \DecValTok{14}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{10_Dataviz_files/figure-pdf/fig-TextScatterPlot3-1.pdf}}

}

\caption{\label{fig-TextScatterPlot3}}

\end{figure}%

\end{tcolorbox}

\subsection{Complex plots}\label{sec-CombiningGeoms}

Part of the magic of the Grammar of Graphics is that we can easily
\textbf{combine} \textbf{different geometries} and \textbf{aesthetics}
to create highly informative graphs. If you just read about the
\texttt{geom\_text()} function (see Figure~\ref{fig-TextScatterPlot3}),
then you have already seen how this can work.

Figure~\ref{fig-GrammarPointBoxplot} is another example. In this plot,
we combine \texttt{geom\_boxplot()} with \texttt{geom\_point()} to see
both the defining characteristics of the distributions of grammar test
scores among L1 and L2 participants, as well as the exact score of each
individual participant.

By default, the \texttt{geom\_boxplot()} function plots dots for any
outliers. Since we are now also plotting \emph{all} the data points
using the \texttt{geom\_point()} function, we need to switch off this
option, otherwise the outliers will be plotted twice.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ Grammar, }
                       \AttributeTok{x =}\NormalTok{ Group)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{outliers =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.6}\NormalTok{,}
             \AttributeTok{size =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \ConstantTok{NULL}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Grammar comprehension test score"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics[width=0.5\linewidth,height=\textheight,keepaspectratio]{10_Dataviz_files/figure-pdf/fig-GrammarPointBoxplot-1.pdf}

}

\caption{\label{fig-GrammarPointBoxplot}}

\end{figure}%

When there are too many points overlapping, even adding some
transparency to the points with the \texttt{alpha} option may not
suffice to tell the points apart. An alternative is to add a little bit
of randomness to the position of the dots using the
\texttt{geom\_jitter()} function to ensure that there are fewer
overlaps.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ Grammar, }
                       \AttributeTok{x =}\NormalTok{ Group)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{outliers =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_jitter}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.6}\NormalTok{,}
              \AttributeTok{size =} \DecValTok{2}\NormalTok{,}
              \AttributeTok{width =} \FloatTok{0.1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \ConstantTok{NULL}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Grammar comprehension test score"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics[width=0.5\linewidth,height=\textheight,keepaspectratio]{10_Dataviz_files/figure-pdf/fig-JitteredGrammar-1.pdf}

}

\caption{\label{fig-JitteredGrammar}}

\end{figure}%

We can also map the colour of the jittered points to another categorical
variable from our dataset, e.g.~\texttt{Gender}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ Grammar, }
                       \AttributeTok{x =}\NormalTok{ Group, }
                       \AttributeTok{colour =}\NormalTok{ Gender)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{outliers =} \ConstantTok{FALSE}\NormalTok{,}
               \AttributeTok{colour =} \StringTok{"black"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_jitter}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.6}\NormalTok{,}
              \AttributeTok{size =} \DecValTok{2}\NormalTok{,}
              \AttributeTok{width =} \FloatTok{0.1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \ConstantTok{NULL}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Grammar comprehension test score"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics[width=0.6\linewidth,height=\textheight,keepaspectratio]{10_Dataviz_files/figure-pdf/fig-JitteredGrammarGender-1.pdf}

}

\caption{\label{fig-JitteredGrammarGender}}

\end{figure}%

Figure~\ref{fig-JitteredGrammarGender} allows us to see that the lowest
grammar comprehension test scores among L1 participants come from male
participants, whereas the lowest scores in the L2 group come from female
participants. Can you think of why this might be? ðŸ¤” Make a note of your
hypothesis as you will have a chance to explore it further in
Section~\ref{sec-InteractivePlots}.

We can, in theory, visualise even more of our data by adding a
\textbf{shape} \texttt{aesthetics} to the \texttt{geom\_jitter()} (see
Figure~\ref{fig-JitteredGrammarGender}). However, increased plot
complexity does not necessarily result in more meaningful or effective
statistical graphics. How effective is
Figure~\ref{fig-JitteredGrammarComplex} in your opinion?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ Grammar, }
                       \AttributeTok{x =}\NormalTok{ Group, }
                       \AttributeTok{colour =}\NormalTok{ OccupGroup)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{colour =} \StringTok{"black"}\NormalTok{,}
               \AttributeTok{outliers =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_jitter}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{shape =}\NormalTok{ Gender),}
              \AttributeTok{size =} \DecValTok{3}\NormalTok{,}
              \AttributeTok{alpha =} \FloatTok{0.7}\NormalTok{, }
              \AttributeTok{width =} \FloatTok{0.1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \ConstantTok{NULL}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Grammar comprehension test score"}\NormalTok{,}
       \AttributeTok{colour =} \StringTok{"Occupational}\SpecialCharTok{\textbackslash{}n}\StringTok{group"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{10_Dataviz_files/figure-pdf/fig-JitteredGrammarComplex-1.pdf}

}

\caption{\label{fig-JitteredGrammarComplex}}

\end{figure}%

As the saying goes, less is often more. In other words, simpler plots
are often more effective than complex ones. With \{ggplot2\} and its
many extensions, the possibilities are almost endless, but that doesn't
mean that simple bar charts, histograms, and scatter plots should be
abandoned! When designing effective data visualisation to communicate
your research, think carefully about your \textbf{audience} and the
\textbf{message} that you want to convey.

\subsection{Interactive plots}\label{sec-InteractivePlots}

In Section~\ref{sec-CombiningGeoms}, we saw that we can combine
\textbf{geometries} and \textbf{aesthetics}, but that there are some
limits as to what is meaningful and effective. In particular, it is
difficult to visualise categorical variables with many different levels
on a static graph.

The good news is that now that you know how to create a plot using
\{ggplot2\}, you are 10 seconds away from making it interactive!
Interactive plots are particularly useful for \textbf{data sanity
checks} and \textbf{data exploration}.

The \href{https://plotly-r.com}{\{plotly\}} package provides access to a
popular javascript visualisation toolkit within \texttt{R}. You will
first need to install the package and load it.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#install.packages("plotly")}
\FunctionTok{library}\NormalTok{(plotly)}
\end{Highlighting}
\end{Shaded}

Then, all you need to do is create and save a \texttt{ggplot} object,
and call that object with the \texttt{ggplotly()} function. In RStudio,
your interactive plot will be displayed in the Viewer pane. Hover your
mouse over the data points of Figure~\ref{fig-InteractivePlot} to
explore the data interactively. You will see that, in addition to the
variables mapped onto the plot itself, the variable mapped onto the
\texttt{label} aesthetics (\texttt{OccupGroup}) also appears when you
hover the mouse over any single data point.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{myplot }\OtherTok{\textless{}{-}}\NormalTok{ Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ Grammar, }
                       \AttributeTok{x =}\NormalTok{ Group, }
                       \AttributeTok{colour =}\NormalTok{ Gender,}
                       \AttributeTok{label =}\NormalTok{ Occupation)) }\SpecialCharTok{+}
  \FunctionTok{geom\_jitter}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.8}\NormalTok{, }
              \AttributeTok{width =} \FloatTok{0.1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{alpha =} \DecValTok{0}\NormalTok{, }
               \AttributeTok{colour =} \StringTok{"black"}\NormalTok{,}
               \AttributeTok{outliers =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \ConstantTok{NULL}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Grammar comprehension test score"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}

\FunctionTok{ggplotly}\NormalTok{(myplot)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/IntPlotCh10a.png}}

}

\caption{\label{fig-InteractivePlot}}

\end{figure}%

If you want to display more than one additional variable on hover, you
can do using the \texttt{text} mapping. Here you can customise which
variables are displayed and how. As the interactive plot is coded in
HTML, you need to use the ``'' tag to add a new line.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{myplot2 }\OtherTok{\textless{}{-}}\NormalTok{ Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ Grammar, }
                       \AttributeTok{x =}\NormalTok{ Group, }
                       \AttributeTok{colour =}\NormalTok{ Gender,}
                       \AttributeTok{text =} \FunctionTok{paste}\NormalTok{(}\StringTok{"Age:"}\NormalTok{, Age, }\StringTok{"\textless{}/br\textgreater{}Years in formal education:"}\NormalTok{, EduTotal, }\StringTok{"\textless{}/br\textgreater{}Job:"}\NormalTok{, Occupation))) }\SpecialCharTok{+}
  \FunctionTok{geom\_jitter}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.8}\NormalTok{, }
              \AttributeTok{width =} \FloatTok{0.1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{alpha =} \DecValTok{0}\NormalTok{, }
               \AttributeTok{colour =} \StringTok{"black"}\NormalTok{,}
               \AttributeTok{outliers =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \ConstantTok{NULL}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Grammar comprehension test score"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}

\FunctionTok{ggplotly}\NormalTok{(myplot2)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{images/IntPlotCh10b.png}}

\section{Exporting plots ðŸš€}\label{sec-ExploringPlots}

You may want to share the plots that you have created in \texttt{R} with
others or insert them in a research paper or presentation. \{ggplot2\}
provides a convenient way to export plots in various formats using the
\texttt{ggsave()} function. You can specify the file name, format, and
other options such as the width, height, and resolution of the image.

If you run the \texttt{ggsave()} function without specifying which plot
should be saved, it will save the last plot that was displayed. The
image file type is determined by the extension of the file name that you
provide, e.g.~a file name ending in \texttt{.png} will export your graph
to a PNG file. You can specify its dimensions in centimetres
(\texttt{cm}), millimetres (\texttt{mm}), inches (\texttt{in}), and
pixel (\texttt{px}). Working out appropriate dimensions can be tricky
and typically requires a bit of experimenting until you get it right.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Blocks,}
                       \AttributeTok{fill =}\NormalTok{ Group)) }\SpecialCharTok{+}
  \FunctionTok{geom\_density}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.7}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_viridis\_d}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{),}
                     \AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{30}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Non{-}verbal IQ (Blocks test)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}

\FunctionTok{ggsave}\NormalTok{(}\StringTok{"BlocksDensityPlot.png"}\NormalTok{,}
       \AttributeTok{width =} \DecValTok{10}\NormalTok{,}
       \AttributeTok{height =} \DecValTok{8}\NormalTok{,}
       \AttributeTok{units =} \StringTok{"cm"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

By default, plots will be exported to the project's working directory.
You can save it elsewhere by specifying a different file path (see
Section~\ref{sec-FoldersPaths}). For example, this function will save
the last plot displayed as an SVG file in the project's subfolder
``figures''. Note that, if the subfolder to which you want to save your
graph does not yet exist, \texttt{R} will return an error. So make sure
that the subfolder exists at the path that you specified before
attempting to save anything in it!

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggsave}\NormalTok{(}\StringTok{"figures/BlocksDensityPlot.svg"}\NormalTok{,}
       \AttributeTok{width =} \DecValTok{1500}\NormalTok{,}
       \AttributeTok{height =} \DecValTok{1000}\NormalTok{,}
       \AttributeTok{units =} \StringTok{"px"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection*{Check your progress ðŸŒŸ}\label{check-your-progress-9}
\addcontentsline{toc}{subsection}{Check your progress ðŸŒŸ}

Well done! You have successfully completed this chapter on data
visualisation using the Grammar of Graphics approach. You have answered
{ out of 21 questions} correctly.

Are you confident that you can\ldots?

\begin{itemize}
\tightlist
\item[$\square$]
  Use the syntax of the Grammar of Graphics to build and customise your
  graphs layer-by-layer (using data, aesthetics, geometries, facetting,
  scales, coordinates, and theme layers) (Section~\ref{sec-Syntax})
\item[$\square$]
  Create and interpret bar plots, histograms, density plots, boxplots,
  scatter plots, and word clouds (Section~\ref{sec-Semantics})
\item[$\square$]
  Create and interpret complex plots and interactive plots for data
  exploration (Section~\ref{sec-CombiningGeoms}) and
  (Section~\ref{sec-InteractivePlots})
\item[$\square$]
  Export and share your plots in various formats
  (Section~\ref{sec-ExploringPlots})
\end{itemize}

If so, you are ready to move out on inferential statistics in
Chapter~\ref{sec-Inferential}!

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Charting your way to success ðŸ—ºï¸}, titlerule=0mm, leftrule=.75mm]

If you have completed this chapter, you can now produce some of the most
widely used plots in the language sciences: Congratulations!

Bar plots, boxplots, scatter plots, density plots and histograms may
well be all that you need for now. But you should know that the
possibilities with \texttt{R}, the \{ggplot2\} library, and its many
extensions are pretty much limitless: The
\href{https://r-graph-gallery.com/}{\texttt{R} Graph Gallery} lists some
50 different types of charts with over 400 examples with code!

Another great resource is \href{https://r-graphics.org}{The R Graphics
Cookbook} by Winston Chang. To go beyond the basics and to find out more
information about the theoretical underpinnings of the \{ggplot2\}
package, I recommend \href{https://ggplot2-book.org}{ggplot2: Elegant
Graphics for Data Analysis}.

There are also great \texttt{R} packages to produce more specialised
types of graphs frequently used in linguistics such as
\href{https://joeystanley.com/blog/making-vowel-plots-in-r-part-1/}{vowel
charts} and \href{https://doi.org/10.1017/jlg.2024.11}{dialectal maps}
(see list of
\href{https://elenlefoll.github.io/RstatsTextbook/A_FurtherResources.html}{next-step
resources}).

\end{tcolorbox}

\bookmarksetup{startatroot}

\chapter{\texorpdfstring{Infe\texttt{R}ential
statistics}{InfeRential statistics}}\label{sec-Inferential}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-warning-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-warning-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Warning}, titlerule=0mm, leftrule=.75mm]

As with the rest of this textbook (see
\href{https://elenlefoll.github.io/RstatsTextbook/}{Preface}), this
chapter is very much \textbf{work in progress}. All feedback is very
welcome.

\end{tcolorbox}

\subsection*{Chapter overview}\label{chapter-overview-10}
\addcontentsline{toc}{subsection}{Chapter overview}

This chapter provides an introduction to:

\begin{itemize}
\tightlist
\item
  sampling procedures
\item
  null hypothesis significance testing (NHST)
\item
  \emph{t}-tests
\item
  correlations and correlation tests
\item
  \emph{p-}values
\item
  effect sizes
\item
  confidence intervals
\item
  assumptions of statistical significance tests
\end{itemize}

Describing and visualising sample data belongs to the realm of
descriptive statistics. Attempting to generalise trends from our
observed data to a larger population takes us to inferential statistics.
Whereas descriptive statistics is about summarising and describing a
specific dataset (our sample), with inferential statistics, we draw on
our sample data to make educated guesses about a larger population that
we have not directly studied. This helps us to determine whether the
patterns that we observed thanks to descriptive statistics and data
visualisations are likely to reflect broader trends that apply to the
larger population or, instead, can more likely be attributed to random
variation in the sample data.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-warning-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-warning-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Prerequisites}, titlerule=0mm, leftrule=.75mm]

As with previous chapters, all the examples, tasks, and quiz questions
from this chapter are based on data from:

\begin{quote}
DÄ…browska, Ewa. 2019. Experience, Aptitude, and Individual Differences
in Linguistic Attainment: A Comparison of Native and Nonnative Speakers.
Language Learning 69(S1). 72--100.
\url{https://doi.org/10.1111/lang.12323}.
\end{quote}

Our starting point for this chapter is the wrangled combined dataset
that we created and saved in Chapter~\ref{sec-DataWrangling}. Follow the
instructions in Section~\ref{sec-filter} to create this \texttt{R}
object.

Alternatively, you can download \texttt{Dabrowska2019.zip} from
\href{https://github.com/elenlefoll/RstatsTextbook/raw/69d1e31be7394f2b612825f031ebffeb75886390/Dabrowska2019.zip}{the
textbook's GitHub repository}. To launch the project correctly, first
unzip the file and then double-click on the \texttt{Dabrowska2019.Rproj}
file.

To begin, load the \texttt{combined\_L1\_L2\_data.rds} file that we
created in Chapter~\ref{sec-DataWrangling}. This file contains the full
data of all the L1 and L2 participants of DÄ…browska (2019). The
categorical variables are stored as factors and obvious data entry
inconsistencies and typos have been corrected (see
Chapter~\ref{sec-DataWrangling}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(here)}

\NormalTok{Dabrowska.data }\OtherTok{\textless{}{-}} \FunctionTok{readRDS}\NormalTok{(}\AttributeTok{file =} \FunctionTok{here}\NormalTok{(}\StringTok{"data"}\NormalTok{, }\StringTok{"processed"}\NormalTok{, }\StringTok{"combined\_L1\_L2\_data.rds"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Before you get started, check that you have correctly imported the data
by examining the output of \texttt{View(Dabrowska.data)} and
\texttt{str(Dabrowska.data)}. In addition, run the following lines of
code to load the \{tidyverse\} and create ``clean'' versions of both the
L1 and L2 datasets as separate \texttt{R} objects.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}

\NormalTok{L1.data }\OtherTok{\textless{}{-}}\NormalTok{ Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(Group }\SpecialCharTok{==} \StringTok{"L1"}\NormalTok{)}

\NormalTok{L2.data }\OtherTok{\textless{}{-}}\NormalTok{ Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(Group }\SpecialCharTok{==} \StringTok{"L2"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Once you are satisfied that your data are sound, read on to learn about
frequentist statistical inference and significance testing!

\end{tcolorbox}

\section{From the sample to the population}\label{sec-Sampling}

So far, we have described the data collected by DÄ…browska (2019). In
Chapter~\ref{sec-DescRiptiveStats}, we described these data using
\textbf{descriptive statistics} thanks to measures of
\hyperref[sec-CentralTendency]{central tendencies}
(e.g.~\hyperref[sec-means]{means}) and
\hyperref[sec-Variability]{measures of variability} around the central
tendency (e.g.~\hyperref[sec-SD]{standard deviations}). In
Chapter~\ref{sec-DataViz}, we described the data \textbf{visually} using
different kinds of statistical plots. These descriptive analyses enabled
us to spot some interesting patterns (and there are many more for you to
explore!). For instance, we noticed that:

\begin{itemize}
\item
  On average, L2 participants scored lower than the L1 participants on
  the English grammar, vocabulary and collocation comprehension tests.
  This was to be expected, but our visualisations also revealed that
  many L2 participants scored at least as well and sometimes even better
  than average L1 participants.
\item
  On average, L2 participants obtained higher non-verbal IQ scores (as
  measured by the Blocks test) than L1 participants but, here, too,
  there was a lot of overlap between the two distributions.
\item
  For both L1 and L2 participants, there was a positive correlation
  between the number of years they were in formal education and their
  English grammar comprehension test scores: the longer they were in
  education, the better they performed on the test.
\end{itemize}

In this chapter, we ask whether these observations are \textbf{likely to
be generalisable} beyond DÄ…browska (2019)'s \textbf{sample} of 90
English native speakers and 67 non-native English speakers to a broader
\textbf{population}. In the context of this study, we will define the
full population as all adult English native and non-native speakers
living in the UK.

In the language and education sciences we rarely have access to the
entire population for which we would ideally like to generalise our
findings. For example, we can hardly go and test \emph{all} English
speakers living in the UK. Instead, we have to make due with a
\textbf{sample} of L1 and L2 English speakers from the UK. Since our
studies attempt to \emph{infer} information about entire populations
based only sample data, the quality of our samples is crucial: no
sophisticated statistical procedure can produce any meaningful
inferential statistics from a biased or otherwise flawed sample!

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Sampling methods}, titlerule=0mm, leftrule=.75mm]

There are different ways to draw samples from a population. The most
common methods are summarised below.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.0534}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2180}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.3095}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1494}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2652}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
What it means
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Typical use in linguistics
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Main advantages
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Main limitations
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Random sampling} & Every member of the target population has an
equal chance of being selected. & Rarely feasible for human populations,
but can sometimes be approximated with census lists. Feasible in other
contexts, e.g.~when the target population are all words featured in a
specific dictionary. & Minimises systematic bias. The results can be
generalised using inferential statistics methods. & Requires a complete
sampling frame (e.g.~a list of all English speakers living in the UK)
and a perfect world in which everyone sampled consents to participating
in study. \\
\begin{minipage}[t]{\linewidth}\raggedright
\textbf{Stratified sampling\\
}(a subtype of random sampling)\strut
\end{minipage} & The population is divided into strata (e.g.~by
dialectal region, age group, education level) and a random sample is
drawn from each stratum. & Used when researchers need balanced data from
distinct subâ€‘populations (e.g.~speakers from several dialectal areas)
and their intersections (e.g.~a good balance of genders across each
dialectal area). & Guarantees coverage of all relevant subâ€‘populations;
reduces sampling error within the strata. & As above, requires reliable,
exhaustive lists for each stratum and consent for all selected; more
complex to organise. \\
\begin{minipage}[t]{\linewidth}\raggedright
\textbf{Cluster sampling\\
}(a subtype of random sampling)\strut
\end{minipage} & Whole clusters (e.g.~villages, schools, neighbourhoods)
are randomly selected, then all members of the chosen clusters are
studied. & Data collection in schools and fieldwork in remote regions
where a full speaker list is impractical. & Efficient when clusters are
naturally defined; reduces costs and organisational burden. & Increases
sampling error if clusters are internally homogeneous; may miss
variation outside selected clusters. \\
\begin{minipage}[t]{\linewidth}\raggedright
\textbf{Representative\\
(quota) sampling}\strut
\end{minipage} & The sample is designed so as to match the population on
key characteristics (e.g.~age, gender, region, education). & Researchers
recruit speakers until the sample mirrors known demographics from census
data or other sources. & Often considered to provide a ``good enough''
picture when true random sampling is impossible. & Often difficult to
implement because we rarely known enough about the characteristics of
the full population; vulnerable to self-selection bias. \\
\textbf{Convenience sampling} & Participants are chosen because they are
reachable and willing to participate. & Most common in experimental
linguistics, online, and classroomâ€‘based surveys. & Quicker, cheaper,
and simpler. & Overâ€‘represents certain groups; suffers from
self-selection bias. \\
\end{longtable}

In practice, it is quite common for combination of these methods to be
used. For example, when researchers run linguistics experiments via
commercial platforms such as \emph{Qualtrics} or \emph{Amazon}
\emph{MechanicalTurk,} they can select their participants based on some
demographic data to obtain a more \textbf{representative}
\textbf{sample}. But the sample nonetheless remains a
\textbf{convenience} \textbf{sample} as only people who sign up to earn
money on these platforms can, by definition, be recruited on these
platforms. As you can imagine, these click-workers are hardly
representative of the full population, even if they are carefully
sampled for age, gender, or socio-economic status.

\end{tcolorbox}

Provided we have a sufficiently \textbf{representative sample},
inferential statistics can help us to answer questions such as: Across
all adult English speakers in the UK\ldots{}

\begin{itemize}
\item
  do L1 speakers, on average, achieve higher scores than L2 speakers on
  English grammar and vocabulary comprehension tests?
\item
  do L2 English speakers, on average, perform better on the nonâ€‘verbal
  IQ (Blocks) test than L1 speakers?
\item
  is there a positive linear relationship between the number of years
  speakers were in formal education and their performance in an English
  grammar comprehension test?
\item
  does the strength of this educationâ€‘grammar comprehension relationship
  differ between L1 and L2 speakers, or across different age cohorts?
\end{itemize}

Though by no means the only framework available to us, the most common
approach to answering such questions is \textbf{null hypothesis
significance testing (NHST)} within the framework of
\textbf{frequentist} statistical philosophy. What's \emph{philosophy}
got to with statistics, you may ask? It turns out that, contrary to
popular belief, statistics is anything but an exact science. By
definition, statistical inference involves making \emph{inferences}
about the unknown. Therefore, there are different ways to approach these
questions.

A promising alternative framework that is gaining traction in many
disciplines --- including in the language sciences --- is
\textbf{Bayesian statistics} (see Appendix of
\href{https://elenlefoll.github.io/RstatsTextbook/A_FurtherResources.html}{next-step
resources} for recommended readings). In this textbook, however, we will
focus on the basic principles of statistical inference within the
\textbf{frequentist framework} --- not because it is easier than
Bayesian statistics, but rather because it remains the most widely used
framework to date. Hence, even if you decide not to use frequentist
statistics for your own research, you will certainly need to understand
its principles in order to correctly interpret the results of published
studies.

\subsection{Null hypothesis significance testing (NHST)}\label{sec-NHST}

Let's begin by considering the following, intriguing research question:

\begin{itemize}
\tightlist
\item
  Across all adult English speakers in the UK, do L2 English speakers,
  on average, perform better on the nonâ€‘verbal IQ (Blocks) test than L1
  speakers?
\end{itemize}

In the sample collected by DÄ…browska (2019), we can see that L2
speakers, on average, performed better than L1 speakers on the Blocks
test. For the two groups, the mean \texttt{Blocks} test scores were:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{group\_by}\NormalTok{(Group) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{mean =} \FunctionTok{mean}\NormalTok{(Blocks),}
            \AttributeTok{SD =} \FunctionTok{sd}\NormalTok{(Blocks))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 3
  Group  mean    SD
  <fct> <dbl> <dbl>
1 L1     13.8  5.56
2 L2     17.5  4.70
\end{verbatim}

However, we are also aware that there is a lot of variation around these
average values. The standard deviations (SD) around these mean values
inform us that there is more variability in the L1 group than in the L2
group. Figure~\ref{fig-IQTestPlot} visualises this variability (see
Section~\ref{sec-IQR} on how to interpret boxplots). On this boxplot,
diamonds represent the mean values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{group\_means }\OtherTok{\textless{}{-}}\NormalTok{ Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{group\_by}\NormalTok{(Group) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{mean\_blocks =} \FunctionTok{mean}\NormalTok{(Blocks))}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ Dabrowska.data, }
       \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Group, }
           \AttributeTok{y =}\NormalTok{ Blocks)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{width =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{stat\_summary}\NormalTok{(}
    \AttributeTok{fun =}\NormalTok{ mean,                     }\CommentTok{\# what to plot}
    \AttributeTok{geom =} \StringTok{"point"}\NormalTok{,                 }\CommentTok{\# as a point}
    \AttributeTok{shape =} \DecValTok{18}\NormalTok{,                     }\CommentTok{\# in a diamond shape}
    \AttributeTok{size =} \DecValTok{4}\NormalTok{,                       }\CommentTok{\# a little larger than the default}
    \AttributeTok{colour =} \StringTok{"purple"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}
    \AttributeTok{data =}\NormalTok{ group\_means,                   }
    \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{as.numeric}\NormalTok{(Group), }
        \AttributeTok{y =}\NormalTok{ mean\_blocks),}
    \AttributeTok{colour =} \StringTok{"purple"}\NormalTok{,}
    \AttributeTok{linewidth =} \DecValTok{1}\NormalTok{,}
    \AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_text}\NormalTok{(}\AttributeTok{data =}\NormalTok{ group\_means,}
    \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{as.numeric}\NormalTok{(Group), }
        \AttributeTok{y =}\NormalTok{ mean\_blocks,}
        \AttributeTok{label =} \FunctionTok{sprintf}\NormalTok{(}\StringTok{"\%.2f"}\NormalTok{, mean\_blocks)), }\CommentTok{\# print the means to two decimal points}
    \AttributeTok{vjust =} \SpecialCharTok{{-}}\FloatTok{1.4}\NormalTok{,}
    \AttributeTok{colour =} \StringTok{"purple"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \ConstantTok{NULL}\NormalTok{,}
    \AttributeTok{y =} \StringTok{"Non{-}verbal IQ (Blocks) test"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{11_Inferential_files/figure-pdf/fig-IQTestPlot-1.pdf}}

}

\caption{\label{fig-IQTestPlot}Comparison of non-verbal IQ (blocks) test
scores between groups L1 and L2}

\end{figure}%

Following the NHST framework, we can quantify how \emph{likely} it is
that this observed difference in means --- which is visualised as the
dotted line in Figure~\ref{fig-IQTestPlot} --- could have occurred due
to chance, i.e.~due to naturally occurring variation, only.

We start with the assumption that the patterns observed in our sample
occurred by chance alone. This initial assumption is known as the
\textbf{null hypothesis (H\textsubscript{0})}. It suggests that any
patterns observed in our data arose by mere coincidence rather than due
to any real effect or underlying relationship. To answer our research
question, we formulate the following null hypothesis:

\begin{itemize}
\tightlist
\item
  \textbf{H\textsubscript{0}}: On average, adult English L1 and L2
  speakers in the UK perform \emph{equally well} on the non-verbal IQ
  (Blocks) test.
\end{itemize}

We can also formulate an \textbf{alternative hypothesis}. This is the
hypothesis that we will adopt if we have enough evidence to reject the
null hypothesis:

\begin{itemize}
\tightlist
\item
  \textbf{H\textsubscript{1}}: On average, adult English L2 speakers in
  the UK perform \emph{differently} on the non-verbal IQ (Blocks) test
  than L1 speakers.
\end{itemize}

We can conduct a \textbf{statistical significance test} to test the
likelihood of the null hypothesis given our observed data. Such tests
allow us to estimate the probability of observing the patterns that we
have noticed (or more extreme ones) in a sample size similar to ours,
assuming that there is \emph{no} real pattern or relationship in the
full population. In other words, these tests help us evaluate whether
our findings are likely to be a random occurrence within our sample or
indicative of an actual trend that is likely to be found in the entire
population.

It is important to understand that these tests do not \emph{prove}
anything. They rely on probabilities and, as such, can only inform us as
to how \emph{likely} our observations (or more extreme ones) are,
assuming that the null hypothesis is true. Thus, statistical
significance tests can only provide information about whether we can
reasonably \textbf{reject} or \textbf{fail to reject a null hypothesis}
based on our sample data.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-warning-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-warning-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Common misconception}, titlerule=0mm, leftrule=.75mm]

Contrary to what is sometimes claimed, significance tests do \emph{not}
provide any information about the likelihood of either the null or the
alternative hypothesis being true or false. In fact, what they provide
can be conceptualised as the opposite: they help us to estimate the
likelihood of our data given the null hypothesis.

\begin{quote}
Always remember that the null hypothesis is an assumption --- its truth
cannot be known. (Winter 2020: 171)
\end{quote}

By definition, inferential statistics is about attempting to
\emph{infer} unknown information about a population from a - often very
small - sample of that population. As such, we cannot use statistics to
prove or disprove a hypothesis that makes a claim about a population to
which we do not have full access. Statistics may be a powerful science,
but it's not magic! ðŸ§™â€â™€ï¸

\end{tcolorbox}

\section{\texorpdfstring{Using \emph{t}-tests to compare two
groups}{Using t-tests to compare two groups}}\label{sec-ttest}

The null hypothesis that we formulated concerns the average non-verbal
IQ test scores of two groups of individuals (English native speakers and
non-native English speakers):

\begin{itemize}
\tightlist
\item
  \textbf{H\textsubscript{0}}: On \ul{average}, adult English L1 and L2
  speakers in the UK perform \emph{equally well} on the non-verbal IQ
  (Blocks) test.
\end{itemize}

Provided that certain assumptions are met (see
Section~\ref{sec-Assumptions}), we can test null hypotheses involving
the comparison of two \textbf{mean values} using a
\textbf{\emph{t}-test}. The \emph{t}-test takes three things into
account:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The magnitude of the difference between the two mean values (i.e.~how
  big is the difference?)
\item
  The amount of variability around the two means (i.e.~how much
  variation is there around the means?)
\item
  The sample size (i.e.~how many data points --- in this case,
  participants --- are there?)
\end{enumerate}

Our descriptive analysis showed that, on average, the L2 participants
scored 3.62 points higher than the L1 participants in the sample. But we
know that mean values alone are not sufficient to describe data and, as
we saw in Figure~\ref{fig-IQTestPlot}, there was a lot of variability
around these mean values. Moreover, we know that our sample is
relatively small: it only has 90 L1 speakers and 67 L2 speakers (though
many experimental linguistics study have fewer data points). The less
data we have, the more likely we are to observe extreme values and this
aspect is also taken into account by the significance tests such as the
\emph{t}-test.\footnote{If you are not immediately convinced by this
  statement, imagine that a friend flips a coin three times and gets
  heads all three times. On the basis of these observed data, he claims
  that the coin is biased towards heads. How likely are you to believe
  his claim that the coin is unfair? What about if he flips the coin 10
  times and it lands on head all 10 times? This seems far more unlikely.
  In fact, the probability of getting three heads in a row with a fair
  coin is 0.125\%, which means that it will happen about 1 in 8 times,
  whereas the probability of getting 10 heads in a row is just 0.001\%,
  which is a one-in-a-thousand occurrence!}

The \texttt{R} function \texttt{t.test()} takes a \textbf{formula} as
its first argument and the \textbf{data} at its second argument. In
\texttt{R}, formulas rely on the tilde symbol
(\texttt{\textasciitilde{}}) to indicate that the variable to the left
of the tilde is \emph{dependent} on the variables to the right of the
tilde. By specifying the formula as
\texttt{Blocks\ \textasciitilde{}\ Group}, we are therefore testing
whether the mean results of the \texttt{Blocks} test are
\emph{dependent} on whether the participants are L1 or L2 speakers of
English (\texttt{Group}). In other words, we apply the \emph{t}-test to
test our null hypothesis, which can be reformulated as:

\begin{itemize}
\tightlist
\item
  \textbf{H\textsubscript{0}}: On average, the results of the non-verbal
  IQ (Blocks) test are \emph{not} dependent on whether the test-takers
  are L1 or L2 speakers of English.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ Blocks }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Group, }
       \AttributeTok{data =}\NormalTok{ Dabrowska.data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Welch Two Sample t-test

data:  Blocks by Group
t = -4.4084, df = 152.46, p-value = 1.956e-05
alternative hypothesis: true difference in means between group L1 and group L2 is not equal to 0
95 percent confidence interval:
 -5.239791 -1.996693
sample estimates:
mean in group L1 mean in group L2 
        13.84444         17.46269 
\end{verbatim}

The output of the \texttt{t.test()} function is a bit overwhelming at
first, so let's focus on the most relevant aspects:

\begin{itemize}
\item
  The first line of the output informs us that we ran a
  \texttt{Welch\ Two\ Sample\ t-test}. We ran a \textbf{two-sample test}
  because we are comparing the means of two \emph{independent} groups:
  L1 vs.~L2 speakers. And it is a \textbf{Welch \emph{t}-test} as
  opposed to a \textbf{Student's \emph{t}-test} because we are not
  assuming that the standard deviation of the two groups' test scores in
  the full population is equal.\footnote{Which, based on the results of
    our descriptive statistics, is a very reasonable assumption to make
    about the full population. If, however, you wanted to conduct a
    Student's \emph{t}-test that treats the variance of both groups as
    equal, then you would need to change the default value of the
    \texttt{var.equal} argument of the \texttt{t.test()} function to
    \texttt{TRUE} (for details see \texttt{?t.test}).}
\item
  Next, the \textbf{\emph{t}-statistic} is reported as
  \texttt{t\ =\ -4.4084}. In this case, it is a negative value which
  means that the mean score of the first group (here L1) is \emph{lower}
  than that of the second group (here L2). Note that, by default, the
  groups are ordered alphabetically. The larger the absolute value of
  the \emph{t}-statistic, the greater the difference between the group
  means. At the same time, however, the more variability there is in the
  data, the lower the absolute value of the \emph{t}-statistic.
\item
  \texttt{df\ =\ 152.46} corresponds to the \textbf{degrees of freedom}.
  These are automatically calculated by the \texttt{t.test()} function
  based on the number of data points in our sample and the number of
  constraints in our test.
\item
  The \textbf{\emph{p}-value} is reported as \texttt{1.956e-05}. This is
  scientific notation for: 1.956 multiplied by 10 to the power of minus
  5 (\texttt{1.956~*~10\^{}-5}), which equals
  \texttt{0.00001956}.\footnote{In scientific notation, ``E'' stands for
    ``exponent'', which refers to the number of times a number needs to
    be multiplied by 10 or, if it is followed by a minus sign,
    multiplied by minus 10. This notation is used as a shorthand way of
    writing very large or very small numbers. One way to convert values
    from scientific notation to standard notation in \texttt{R} is to
    use the \texttt{format()} function like this:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{format}\NormalTok{(}\FloatTok{1.956e{-}05}\NormalTok{, }\AttributeTok{scientific =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Verbatim}
[1] "0.00001956"
\end{Verbatim}
  } This means that our test estimates that there is a very, very small
  probability --- namely 0.001956\% --- of observing a difference in
  mean scores on the Blocks test as large as we observed (3.62 points)
  or an even larger one \ul{under the null hypothesis}, i.e.~under the
  assumption that there is \emph{no} real-world difference between L1
  and L2 speakers' performance on this test.
\item
  The output also includes a \textbf{95\% confidence interval (CI)} of
  the difference between the means. It ranges from \texttt{-5.239791} to
  \texttt{-1.996693}. In our sample, we observed a mean difference
  between L1 participants' and L2 participants' Blocks test scores of
  -3.62 points. If we were to repeat this experiment a 100 times with a
  100 different samples of L1 and L2 speakers, we can be confident that,
  in 95 out of 100 repetitions, the confidence interval that we compute
  would include the true average difference across the entire
  population. In other words, the average difference between L1 and L2
  speakers could be quite a bit larger than in Dabrowska's sample or
  quite a bit lower, but is very unlikely to be zero (which would
  correspond to the null hypothesis of no difference). Given the same
  observed difference, the larger our sample, the smaller our confidence
  interval.
\item
  At the very bottom of the output, we can read the \textbf{sample
  estimates} for the L1 and the L2 groups. These are the mean Blocks
  test scores that we had already calculated using descriptive
  statistics (see Section~\ref{sec-NHST}). They simply serve as a
  reminder that we are testing the statistical significance of the
  difference between these two means under the null hypothesis of no
  difference.
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{How to report}, titlerule=0mm, leftrule=.75mm]

To summarise these results, we can write that we conducted a Welch
two-sample \emph{t}-test to compare the mean Blocks score of L1 and L2
English speakers. On average, L2 speakers performed significantly better
(\emph{M}~=~17.46, \emph{SD}~=~4.70) than L1 speakers (\emph{M}~=~13.84,
\emph{SD}~=~5.56), \emph{t}\textsubscript{(152.46)}~=~-4.4084,
\emph{p}~\textless~0.001.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-warning-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-warning-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Common misconception}, titlerule=0mm, leftrule=.75mm]

Unfortunately, \textbf{confidence intervals (CI)} are a bit of a
misnomer, which frequently leads to misunderstandings. Contrary to what
so-called ``AI'' tools (see Chapter~\ref{sec-AI}) and even some
statistics textbooks may claim, confidence intervals do \emph{not} tell
us that we can be 95\% confident that the true difference across the
entire population lies within the 95\% confidence interval. Bodo Winter
(2020: 165) clarifies this common misconception as follows:

\begin{quote}
{[}T{]}he actual population parameter of interest {[}i.e.~in the case of
a \emph{t}-test, the difference in mean values{]} may or may not be
inside the confidence interval -- you will actually never know for sure.
However, if you imagine an infinite series of experiments and compute a
confidence interval each time, 95\% of the time this interval would
contain the true population parameter.
\end{quote}

In other words, a 95\% confidence interval does not tell us how
confident we can be about any specific value, but rather that, in the
long-run, if the study were to be repeated many times, 95\% of the time,
the 95\% confidence interval would contain the true value.

\end{tcolorbox}

\section{\texorpdfstring{Statistical significance and
\emph{p}-values}{Statistical significance and p-values}}\label{sec-PValues}

When used correctly, the \emph{p}â€‘value is a very useful metric that can
help us to determine whether an observed statistic, such as a difference
in means in a \emph{t}-test, is likely to be due to chance variation in
the sample rather than indicative of a true effect in the population. A
common way\footnote{Again, it is important to note that this is by no
  means the \emph{only} way to interpret \emph{p}-values. As with many
  things in statistics, how you interpret \emph{p}-values depends on the
  statistical philosophy that you subscribe to. The approach described
  in this textbook corresponds to the statistical hypothesis testing
  approach developed by Neyman \& Pearson
  (\href{https://lakens.github.io/statistical_inferences/references.html\#ref-neyman_problem_1933}{1933}).
  ``In a Neyman-Pearson framework, the goal of statistical tests is to
  guide the behavior of researchers with respect to these two
  hypotheses. Based on the results of a statistical test, and without
  ever knowing whether the hypothesis is true or not, researchers choose
  to tentatively act as if the null hypothesis or the alternative
  hypothesis is true.'' (Lakens 2022: Section 1.1)} to use
\emph{p}â€‘values is to define --- prior to conducting the analysis --- a
\textbf{significance level} threshold (also called \textbf{alpha} or
\textbf{Î±-level}), which corresponds to the risk that we are willing to
accept of mistakenly concluding that there is an effect when, in fact,
there is none (this is called a false positive result). In the language
and social sciences, the significance level is typically set to 0.05.
This means that, if the null hypothesis is true, we accept a 5\% risk of
obtaining a \emph{p}-value that suggests that we should reject the null
hypothesis when, in fact, we shouldn't.\footnote{It is worth noting
  that, just because 0.05 is (currently) the most widely used threshold,
  doesn't mean that you have to use 0.05, too. If you are not
  comfortable accepting a 5\% risk (which, statistically speaking, will
  happen one in 20 times, after all), you can define a lower threshold,
  e.g.~0.01, corresponding to a 1\% risk. However, depending on how
  large your observed effect is and how much data you have, you may find
  that, with a lower significanceâ€‘level, you fail to reject the null
  hypothesis even when there is a true effect in the population so it's
  a difficult balance to strike. To find out more, I recommend reading
  about \textbf{statistical power}, e.g.~in Lakens (2022: Chapter 2).}
The significance level should be chosen \emph{before} looking at the
data and be clearly mentioned in the methods section of every study that
uses statistical significance testing.

In the NHST framework, when the calculated \emph{p}â€‘value is
\emph{smaller} than our chosen significance level (Î±), we reject the
null hypothesis in favour of the alternative hypothesis and say that the
result is \textbf{statistically significant}. When the \emph{p}â€‘value is
larger than Î±, we fail to reject the null hypothesis and say that the
result is not statistically significant. However, the latter does not
\emph{prove} that the null hypothesis is true. It only tells us that the
data that we have do not provide enough evidence against the null
hypothesis. Note that, following this school of statistics, the actual
\emph{p}-value is irrelevant: it is either \emph{below} or \emph{above}
the Î±-level threshold. We do not compare \emph{p}-values and it does not
make sense to claim that one result is more or less statistically
significant than the other.

\begin{quote}
Once \emph{p} \textless{} Î±, a result is claimed to be `statistically
significant', which is just the same as saying that the data are
sufficiently incompatible with the null hypothesis. If the researcher
obtained a significant result for a \emph{t}-test, the researcher may
act as if there actually was a group difference in the population.
(Winter 2020: 168)
\end{quote}

Contrary to what some researchers seem to believe, in and of themselves,
\emph{p}-values are not the holy grail! They can only meaningfully be
interpreted together with other important contextual information such as
the \textbf{context} in which the data were collected, the magnitude of
the observed effect (the \textbf{effect size}), and the
\textbf{variability} around the estimated effect (e.g.~as shown in data
visualisation) (see Figure~\ref{fig-pvalue}).

\begin{figure}

\centering{

\includegraphics[width=4.72917in,height=\textheight,keepaspectratio]{images/AHorst_p-value.jpg}

}

\caption{\label{fig-pvalue}Don't let your \emph{p}-values sing solo!
(artwork by \href{https://twitter.com/allison_horst}{Allison Horst}
(\href{https://creativecommons.org/licenses/by/4.0/}{CC BY 4.0})}

\end{figure}%

The problem with \emph{p}-values is that they are a composite metric
that is dependent on three aspects:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The \textbf{size of the observed effect} (the larger the effect, the
  smaller the \emph{p}-value)
\item
  The \textbf{variability within the data} (the less variability, the
  smaller the \emph{p}-value)
\item
  The \textbf{sample size} (the larger the sample size, the smaller the
  \emph{p}-value)
\end{enumerate}

Note that the size (or magnitude) of the observed effect is only
\emph{one} of three factors that influence the \emph{p}-value! It is
therefore incorrect to claim that an effect (e.g.~a difference in means)
is particularly large based on a particularly small \emph{p}-value. It
is equally incorrect to claim that a \emph{p}-value that falls below the
chosen significance level points to a (statistically) relevant result.
To evaluate the \emph{relevance} of a result, we need contextual
information that goes far beyond the results of a single statistical
test. In statistics, ``significance'' and ``significant'' are terms that
have nothing to do with either the relevance or importance of results.

\section{Effect sizes and confidence intervals}\label{sec-EffectSize}

In Section~\ref{sec-ttest}, we saw that the larger the absolute value of
the \emph{t}-statistic, the greater the difference between the group
means. At the same time, the more variability there is in the data, the
lower the absolute value of the \emph{t}-statistic. This makes the
\textbf{\emph{t-}statistic} a measure of effect size. However, it is an
\textbf{unstandardised measure}, which means that t-statistic values
cannot be compared across different studies.

By contrast, \textbf{Cohen's \emph{d}} is a \textbf{standardised effect
size measure}. As such, it can be used to compare the magnitude of the
difference in mean values across different variables, samples, and
studies. Cohen's \emph{d} (the \emph{d} stands for difference) can be
calculated by dividing the difference between two means (the raw
strength of an effect) by the standard deviation of both groups together
(the overall variability of the data). But fear not: we don't need to do
the maths ourselves as the formula is implemented in several \texttt{R}
packages. In the following, we will use \texttt{cohens\_d()} from the
\{\href{https://easystats.github.io/effectsize/}{effectsize}\} package
(Ben-Shachar, LÃ¼decke \& Makowski 2020) which, like the
\texttt{t.test()} function, also takes a formula as its first argument.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"effectsize"}\NormalTok{)}
\FunctionTok{library}\NormalTok{(effectsize)}
\end{Highlighting}
\end{Shaded}

Recall the difference that we observed between L1 and L2 English
speakers' non-verbal IQ (Blocks) test results in
Figure~\ref{fig-IQTestPlot}. With the \texttt{cohens\_d()} function, we
can now answer the question: How large is this effect?

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cohens\_d}\NormalTok{(Blocks }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Group, }
         \AttributeTok{data =}\NormalTok{ Dabrowska.data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Cohen's d |         95% CI
--------------------------
-0.69     | [-1.02, -0.37]

- Estimated using pooled SD.
\end{verbatim}

The output shows that Cohen's \emph{d} is \texttt{-0.69}. As with the
\emph{t}-statistic, the minus sign tells us that the L1 group performed
worse than the L2 participants. The absolute value, here \texttt{0.69},
corresponds to the strength of the effect. According to Cohen's (1988)
own rule of thumb, the absolute values can be interpreted as follows:

\begin{itemize}
\tightlist
\item
  Cohen's \emph{d}~=~0.2 - 0.5 â†’ Small effect size
\item
  Cohen's \emph{d}~=~0.5 - 0.8 â†’ Medium effect size
\item
  Cohen's \emph{d}~\textgreater~0.8 â†’ Large effect size
\end{itemize}

However, Cohen (1988: 25) himself cautioned that:

\begin{quote}
The terms ``small,'' ``medium,'' and ``large'' are relative, not only to
each other, but to the area of behavioral science or even more
particularly to the specific content and research method being employed
in any given investigation {[}\ldots{]}.
\end{quote}

Hence, it is important that linguists and education researchers base
their interpretation of standardised effect sizes on prior research
relevant to their field of research (see, e.g. Plonsky \& Oswald (2014)
for L2 research).

The output of the \texttt{cohens\_d()} function above also includes a
95\% confidence interval (CI) around Cohen's~\emph{d}. It turns out that
there is a direct relationship between the confidence interval around an
effect size and the statistical significance of a null hypothesis
significance test: if an effect is statistically significant in a
two-sided\footnote{All of the statistical tests performed in this
  chapter are two-sided. For a discussion of one-sided vs.~two-sided
  tests, see Lakens (2022): Section 5.10).} independent \emph{t}-test
with a significance (Î±) level of 0.05, the 95\% confidence interval (CI)
for the mean difference between the two groups will \emph{not} include
zero. The \emph{t}-test that we conducted on the results of the Blocks
test across the L1 and L2 groups produced a \emph{p}-value of 0.00001956
which is less than 0.05 and was therefore statistically significant at
the Î±-level of 0.05. But we didn't really need to check the
\emph{p}-value because we can see that the effect is statistically
significant at the Î±-level of 0.05 by looking at the 95\% CI around
Cohen's \emph{d}: the lower bound is \texttt{-1.02} and the upper bound
\texttt{-0.37}. In other words, the CI does not straddle zero.

Now, let's consider a new research question and a new null hypothesis:

\begin{itemize}
\tightlist
\item
  \textbf{H\textsubscript{0}}: On average, the results of the non-verbal
  IQ (Blocks) test are not dependent on the gender of the test-takers.
\end{itemize}

Recall that, in this dataset, \texttt{Gender} is a binary variable. The
descriptive statistics suggest that male participants perform slightly
better than female participants on the non-verbal IQ test, but that
there is quite a bit of variability in the data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{group\_by}\NormalTok{(Gender) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{mean =} \FunctionTok{mean}\NormalTok{(Blocks),}
            \AttributeTok{SD =} \FunctionTok{sd}\NormalTok{(Blocks))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 3
  Gender  mean    SD
  <fct>  <dbl> <dbl>
1 F       15.2  5.30
2 M       15.7  5.81
\end{verbatim}

We now compute a standardised effect size for this gender gap: Cohen's
\emph{d}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cohens\_d}\NormalTok{(Blocks }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Gender, }
       \AttributeTok{data =}\NormalTok{ Dabrowska.data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Cohen's d |        95% CI
-------------------------
-0.10     | [-0.42, 0.22]

- Estimated using pooled SD.
\end{verbatim}

Here, the \texttt{cohens\_d()} function compares the scores of the
female participants with those of the male participants because `female'
comes first alphabetically. Hence, the negative Cohen's \emph{d} value
means that, on average, males perform better than females. However, we
can see that the effect size (\texttt{-0.10}) is very small.

Now turning to the 95\% confidence interval (CI) also output by the
function, we can see that, while the lower confidence bound corresponds
to a negative effect size, the upper bound is positive, which means that
the confidence interval contains the possibility of an effect size of
zero, corresponding to no effect at all. Hence, we must conclude that
this difference in scores between female and male participants is not
statistically significant at an Î±-level of 0.05. We can confirm this by
performing a \emph{t}-test. It returns a \emph{p}-value that is greater
than 0.05:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ Blocks }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Gender, }
       \AttributeTok{data =}\NormalTok{ Dabrowska.data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Welch Two Sample t-test

data:  Blocks by Gender
t = -0.59558, df = 124.58, p-value = 0.5525
alternative hypothesis: true difference in means between group F and group M is not equal to 0
95 percent confidence interval:
 -2.352092  1.263946
sample estimates:
mean in group F mean in group M 
       15.17021        15.71429 
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{How to report}, titlerule=0mm, leftrule=.75mm]

To summarise these results, we can write that, while male participants
performed marginally better (\emph{M}~=~15.71, \emph{SD}~=~5.81) than
female participants (\emph{M}~=~15.17, \emph{SD}~=~5.30), this
difference is very small (Cohen's \emph{d} = -0.10; 95\% CI {[}-042,
0.22{]}) and is not statistically significant at an Î±-level of 0.05:
\emph{t}\textsubscript{(124.58)}~=~0.5956, \emph{p}~=~0.5525.

\end{tcolorbox}

By default, the \texttt{cohens\_d()} function computes a 95\% confidence
interval, but, if we had chosen a lower Î±-level of, say, 0.01, we can
change this default:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cohens\_d}\NormalTok{(Blocks }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Gender, }
       \AttributeTok{data =}\NormalTok{ Dabrowska.data,}
       \AttributeTok{ci =} \FloatTok{0.99}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Cohen's d |        99% CI
-------------------------
-0.10     | [-0.52, 0.32]

- Estimated using pooled SD.
\end{verbatim}

As you can see, this increases the size of the interval and hence makes
it harder to obtain a statistically significant result. This is because
lowering the Î±-level means that we are less willing to risk reporting a
false positive result, i.e.~reporting a difference based on our data
where no real difference exists.

\section{Correlation tests}\label{sec-Correlations}

So far, we have looked at just one type of statistical significance
test, the \emph{t}-test, which we used to compare two mean values. This
kind of \emph{t}-test is used to test the association between a numeric
variable (e.g.~test scores ranging from 0 to 100) and a binary
categorical variable (e.g.~L1 vs.~L2 status).

In this section, we return to correlations --- a concept that we
encountered in Section~\ref{sec-Scatterplots} when we generated and
interpreted scatter plots. Recall that correlations capture the strength
of the association between two numeric variables (e.g.~age and grammar
test scores).

At the beginning of the chapter, we summarised the following
observations based on our descriptive analyses of the DÄ…browska (2019)
data:

\begin{itemize}
\tightlist
\item
  For both L1 and L2 participants, there is a positive correlation
  between the number of years they were in formal education and their
  English grammar comprehension test scores: the longer they were in
  formal education, the better they did on the test (see
  Figure~\ref{fig-YearEducationPlot}).
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ EduTotal, }
                       \AttributeTok{y =}\NormalTok{ Grammar)) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ Group) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{,}
              \AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Number of years in formal education"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"English grammar comprehension test scores"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{11_Inferential_files/figure-pdf/fig-YearEducationPlot-1.pdf}}

}

\caption{\label{fig-YearEducationPlot}Relationship between years in
formal education and English grammar comprehension test scores for
groups L1 and L2}

\end{figure}%

How strong are the correlations visualised by the blue regression lines
on Figure~\ref{fig-YearEducationPlot}? And how likely is it that we
might observe such correlations or stronger ones by chance alone? The
first question is about the size of the effect, whilst the second is
about its statistical significance.

We can answer both questions using the \texttt{cor.test()} function.
This function also takes a formula as its first argument: the two
numeric variables whose correlation we want to estimate come after the
tilde (\texttt{\textasciitilde{}}) and the two variables are combined
using the plus (\texttt{+}) operator:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor.test}\NormalTok{(}\AttributeTok{formula =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ EduTotal }\SpecialCharTok{+}\NormalTok{ Grammar,}
         \AttributeTok{data =}\NormalTok{ L1.data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Pearson's product-moment correlation

data:  EduTotal and Grammar
t = 3.5821, df = 88, p-value = 0.0005581
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 0.1615747 0.5250334
sample estimates:
      cor 
0.3567294 
\end{verbatim}

As with the \texttt{t.test()} (see Section~\ref{sec-ttest}), the output
of the \texttt{cor.test()} function includes a \emph{t}-statistic
(\texttt{t}), the number of degrees of freedom (\texttt{df}; which here
corresponds to the number of data points minus two), and a
\emph{p}-value. Immediately, we can see that the \emph{p}-value is
\textless~0.05, which means that, for the L1 population, we can reject
the null hypothesis that there is no correlation between the total
number of years that participants spent in education and their English
grammar comprehension test scores. But how \emph{strong} is the
correlation? To find out, we turn to the sample estimate,
\textbf{Pearson's \emph{r}}: \texttt{cor\ =\ 0.36}. Like Cohen's
\emph{d}, Pearson's \emph{r} is also a standardised effect size. It can
range between \texttt{-1} and \texttt{+1}.

\begin{itemize}
\item
  A correlation coefficient of \textbf{1} means that there is a
  \textbf{perfect positive linear relationship} between the two
  variables.

  \begin{itemize}
  \item
    \ul{Example}: The relationship between the number of questions that
    a person correctly answered in a test and the percentage of
    questions that they got right.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# First, let\textquotesingle{}s imagine that 100 learners took a test with 10 questions. We simulate the number of questions that they answered correctly using the \textasciigrave{}sample()\textasciigrave{} function that generates random numbers, specifying that learners can get between zero and 10 questions right:}
\NormalTok{correct.answers }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\DecValTok{0}\SpecialCharTok{:}\DecValTok{10}\NormalTok{, }\DecValTok{100}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}

\CommentTok{\# Next, we convert the number of correctly answered questions into the percentage of questions that they answered correctly:}
\NormalTok{accuracy }\OtherTok{\textless{}{-}}\NormalTok{ correct.answers }\SpecialCharTok{/} \DecValTok{10} \SpecialCharTok{*} \DecValTok{100}

\CommentTok{\# Finally, we compute the correlation coefficient between the number of  and the percentage of correctly answered questions:}
\FunctionTok{cor}\NormalTok{(correct.answers, accuracy)}

\CommentTok{\# The correlation coefficient (Pearson\textquotesingle{}s r) equals 1 because the two variables are perfectly correlated with each other. If we know one variable, there is a simple mathematical formula that allows us to obtain the exact value of the other!}
\end{Highlighting}
\end{Shaded}
  \end{itemize}
\item
  A correlation coefficient of \textbf{0} means that there is \textbf{no
  linear relationship} between the two variables. Note that, in the real
  world, we will never find correlations of exactly zero, but rather
  very close to zero.

  \begin{itemize}
  \item
    Example: The relationship between two completely randomly generated
    strings of numbers.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# First, we set a seed to ensure that the outcome of our randomly generated number series are always exactly the same:}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}

\CommentTok{\# Next, we generate two series of a thousand randomly generated numbers ranging between 0 and 100:}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\DecValTok{0}\SpecialCharTok{:}\DecValTok{100}\NormalTok{, }\DecValTok{1000}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\DecValTok{0}\SpecialCharTok{:}\DecValTok{100}\NormalTok{, }\DecValTok{1000}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}

\CommentTok{\# We can now compute the correlation coefficient. As expected, we find that it is very close to zero but not exactly zero:}
\FunctionTok{cor}\NormalTok{(x, y)}

\CommentTok{\# However, if we run a correlation test on our two randomly generated variables x and y, we find that a) the p{-}value is \textgreater{}Â 0.05 and b) our 95\% confidence interval includes 0. }
\FunctionTok{cor.test}\NormalTok{(x, y)}

\CommentTok{\# We can therefore conclude there is not enough evidence in our sample data to reject the null hypothesis of no correlation in the full population. This makes sense because we are testing the correlation of two independently, randomly generated strings of numbers that shouldn\textquotesingle{}t have anything to do with each other!}
\end{Highlighting}
\end{Shaded}
  \end{itemize}
\item
  A correlation coefficient of \textbf{-1} means that there is a
  \textbf{perfect negative linear relationship} between the two
  variables.

  \begin{itemize}
  \item
    \ul{Example}: The relationship between the number of errors a person
    makes in a test and the percentage of questions that they got right.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# First, let\textquotesingle{}s imagine that 100 learners took a test with 20 questions. We simulate the number of errors that they each made:}
\NormalTok{errors }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\DecValTok{0}\SpecialCharTok{:}\DecValTok{20}\NormalTok{, }\DecValTok{100}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}

\CommentTok{\# Next, we convert the number of errors into the percentage of questions that they correctly answered:}
\NormalTok{accuracy }\OtherTok{\textless{}{-}}\NormalTok{ ((}\DecValTok{20} \SpecialCharTok{{-}}\NormalTok{ errors)}\SpecialCharTok{/}\DecValTok{20}\NormalTok{) }\SpecialCharTok{*} \DecValTok{100}

\CommentTok{\# Finally, we compute the correlation coefficient between the number of errors and the percentage of correctly answered questions:}
\FunctionTok{cor}\NormalTok{(errors, accuracy)}
\end{Highlighting}
\end{Shaded}
  \end{itemize}
\end{itemize}

Going back to the output of the \texttt{cor.test()} function above, we
have a \textbf{correlation coefficient} of \texttt{0.36}, which is
positive, meaning that we are looking at a positive correlation. We
already knew this from the direction of the regression line in the L1
panel of Figure~\ref{fig-YearEducationPlot}. The question is now: is
\texttt{0.36} a \emph{weak}, \emph{medium} or \emph{strong} positive
correlation? Again, it depends\ldots{} For some research questions in
the language sciences, \texttt{0.36} may be considered a medium-sized
correlation but, for others, it may be considered a small
correlation\ldots{} As always, numbers alone do not suffice to draw
conclusions: we need contextual information about research domain (see
Plonsky \& Oswald 2014).

The output of the \texttt{cor.test()} function also returned a 95\%
confidence interval around the correlation coefficient:
\texttt{{[}0.16,\ 0.53{]}}. It does not straddle zero which is why our
\emph{p}-value was \textless~0.05. That said, the lower bound of the
interval corresponds to a very small correlation, suggesting that the
correlation in the full L1 population may be considerably smaller than
what we observed in our data (or quite a bit larger as demonstrated by
the upper bound). In other words, there is quite a bit of uncertainty
around the strength of this correlation coefficient because there is a
lot variability in the data and we do not have a particularly large
sample size.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{How to report}, titlerule=0mm, leftrule=.75mm]

To summarise these results, we can write that, in this dataset, there is
a positive and statistically significant correlation between the number
of years that L1 participants reported spending in formal education and
their receptive English vocabulary test scores, \emph{r}~=~0.36, 95\% CI
{[}0.16, 0.53{]}, df~=~88, \emph{p}~=~0.0005581.

\end{tcolorbox}

How about L2 speakers? From the L2 panel of
Figure~\ref{fig-YearEducationPlot}, we can see that the \texttt{Grammar}
scores of L2 participants are, on average, much further away from the
regression line than in the L1 panel, suggesting that it summarises the
data far less well. Indeed, when we run the correlation test on the L2
data, we not only find that the correlation coefficient is much smaller
(\texttt{0.13}), the 95\% confidence interval around this coefficient
{[}\texttt{-0.11,\ 0.36}{]} includes zero:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor.test}\NormalTok{(}\AttributeTok{formula =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ EduTotal }\SpecialCharTok{+}\NormalTok{ Grammar,}
         \AttributeTok{data =}\NormalTok{ L2.data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Pearson's product-moment correlation

data:  EduTotal and Grammar
t = 1.0936, df = 65, p-value = 0.2782
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 -0.1093254  0.3629045
sample estimates:
      cor 
0.1344131 
\end{verbatim}

Along with a \emph{p}-value of \textgreater~0.05, this suggests that we
do not have enough evidence to reject the null hypothesis of no
correlation between years in formal education and English grammar
comprehension in the L2 population.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{How to report}, titlerule=0mm, leftrule=.75mm]

To summarise these results, we can write that, in this dataset, there is
a small positive, but non-significant correlation between the number of
years that L2 participants reported spending in formal education and
their receptive English vocabulary test scores, \emph{r}~=~0.13, 95\% CI
{[}-0.11, 0.36{]}, df~=~65, \emph{p}~=~0.2782.

\end{tcolorbox}

Confidence intervals around correlation coefficients can be difficult to
interpret as numbers. The good news is that they can easily be
visualised using the \{ggplot2\} library. In
Section~\ref{sec-Scatterplots}, we used the argument
\texttt{se\ =\ FALSE} inside the \texttt{geom\_smooth()} function. If,
instead, we set it to \texttt{TRUE}, 95\% confidence intervals will be
displayed as grey bands around the regression lines. To change the
Î±-level, you will need to change the default value of the \texttt{level}
argument.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ EduTotal, }
                       \AttributeTok{y =}\NormalTok{ Grammar)) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ Group) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{,}
              \AttributeTok{se =} \ConstantTok{TRUE}\NormalTok{,}
              \AttributeTok{level =} \FloatTok{0.95}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Number of years in formal education"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"English grammar comprehension test scores"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{11_Inferential_files/figure-pdf/fig-YearEducationPLotGrayBand-1.pdf}}

}

\caption{\label{fig-YearEducationPLotGrayBand}Relationship between years
in formal education and English grammar comprehension test scores for
groups L1 and L2 with 95\% confidence bands}

\end{figure}%

The interpretation of the grey bands is as follows: if it's possible to
draw a horizontal (flat) line that stays within the 95 \% confidence
band, it is very likely that there is \emph{no} statistically
significant correlation between the two numeric variables displayed on
the plot at the Î±-level of 0.05.

As illustrated in
Figure~\ref{fig-YearEducationPLotGrayBandNullEffectLines}, it is
impossible to draw such a line in the L1 panel, which is why we reject
the null hypothesis of no correlation between years spent in formal
education and grammar comprehension test scores for L1 speakers. We
conclude that this correlation is significantly different from zero. By
contrast, it is perfectly possible to draw such a horizontal line in the
L2 panel, which is why we must conclude that, at the Î±-level of 0.05, we
do not have enough evidence to reject the null hypothesis of no
correlation in the L2 population. The observed correlation is not
statistically significantly different from zero.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{11_Inferential_files/figure-pdf/fig-YearEducationPLotGrayBandNullEffectLines-1.pdf}}

}

\caption{\label{fig-YearEducationPLotGrayBandNullEffectLines}The dotted
lines are compatible with the null hypothesis of no correlation.}

\end{figure}%

\section{Assumptions of statistical tests}\label{sec-Assumptions}

A crucial aspect that we have glossed over so far in this chapter is
that the results of statistical tests, such as \emph{t}-tests and
correlation tests, can only be considered accurate if the test's
underlying assumptions are met.

\subsection{Randomisation}\label{sec-Randomisation}

The only assumption that we have mentioned so far is that of
\textbf{random sampling} (Section~\ref{sec-Sampling}). In practice, this
assumption is rarely met in the language sciences. For instance, the L1
and L2 participants recruited for DÄ…browska (2019) were not randomly
drawn from the entire adult UK population. DÄ…browska (2019: 5-6) reports
that the participants ``were recruited through personal contacts, church
and social clubs, and advertisements in local newspapers''. Such
transparency over the sampling procedure is crucial to interpreting the
results of a study.

\begin{quote}
When the assumption of random sampling is not met, inferences to the
population become difficult. In this case, researchers should describe
the sample and population in sufficient detail to justify that the
sample was at least representative of the intended population
(\href{https://doi.org/10.1037/0003-066X.54.8.594}{Wilkinson and APA
Task Force on Statistical Inference, 1999}). If such a justification is
not made, readers are left to their own interpretation as to the
generalizability of the results. (Nimon 2012: 1)
\end{quote}

\subsection{Independence}\label{sec-Independence}

Another crucial assumption for both tests covered in this chapter is
that the data points be independent of each other. We assumed that this
is the case with the DÄ…browska (2019) data because it only has one
observation per participant (although interdependencies may occur at
other levels, e.g.~when several participants come from the same school,
work place, or neighbourhood). If, however, we had multiple observations
per participant because they completed the tests twice, say once in the
morning and once in the evening, and then entered both the morning and
the evening data into a single statistical test, our data would violate
the assumption of independence and the results of the test would be
inaccurate. When our data violate the assumption of independence, we
cannot use statistical tests like \emph{t}-tests. Instead, we must turn
to statistical methods that allow us to model these interdependencies in
the data. In the language sciences, this is most commonly achieved using
\textbf{mixed-effects models} (see e.g. Winter 2020: Ch. 14-15).

\subsection{Normality}\label{sec-Normality}

For many inferential statistical tests commonly reported in the language
sciences, it is also assumed that the \textbf{population data} are
normally distributed (see Section~\ref{sec-Normal}). This assumption is
often quite reasonable, because many real-world quantities are normally
distributed. This is why we typically say that this assumption can be
relaxed if we have more than 30 observations per group.

However, there are some things in the world that are inherently
non-normally distributed. For instance, word frequencies in a text or a
collection of texts (i.e.~a corpus) are never normally distributed: a
handful of words occur extremely frequently (in written English
typically: \emph{the}, \emph{of}, \emph{a}, \emph{in}, \emph{to}, etc.),
some words are fairly frequent, but the vast majority are very
infrequent. Reaction times is another example of a kind of variable that
hardly ever meets the criterion of normality. One way to deal with
highly skewed distributions like word frequencies and reaction times is
to apply \textbf{transformations} to these variables before attempting
to do any inferential statistics (see Winter 2020: Chapter 5).

Of course, we cannot check if the population data meet the assumption of
normality because we do not have the entire population data (and if we
did, we wouldn't \emph{need} inferential statistics!) so the best we can
do is check if our \emph{data} are normally distributed. This is best
achieved visually. For instance, before conducting a \emph{t}-test
comparing the mean difference in \texttt{Grammar} scores in the L1 and
L2 groups using the \texttt{t.test()} function, we should first
visualise the two distributions of \texttt{Grammar} scores to check that
they are approximately normally distributed.
Figure~\ref{fig-GrammarDensityPlot} shows that this is clearly
\emph{not} the case!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Grammar,}
                       \AttributeTok{fill =}\NormalTok{ Group)) }\SpecialCharTok{+}
  \FunctionTok{geom\_density}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.7}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_viridis\_d}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"English grammar comprehension test scores"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{11_Inferential_files/figure-pdf/fig-GrammarDensityPlot-1.pdf}}

}

\caption{\label{fig-GrammarDensityPlot}Density plot comparing English
grammar comprehension test scores between groups L1 and L2}

\end{figure}%

We are dealing here with \textbf{non-normal} or \textbf{non-parametric
data}, hence we need a non-parametric version of the \emph{t}-test: the
\textbf{Wilcoxon test} (also known as the Mann-Whitney test or the
\emph{U}-test):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{wilcox.test}\NormalTok{(Grammar }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Group, }
            \AttributeTok{data =}\NormalTok{ Dabrowska.data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Wilcoxon rank sum test with continuity correction

data:  Grammar by Group
W = 3640, p-value = 0.026
alternative hypothesis: true location shift is not equal to 0
\end{verbatim}

The output of the \texttt{wilcox.test()} function tells us that there is
a 2.6\% chance of obtaining our observed difference in \textbf{median}
\texttt{Grammar} scores in an L1 and L2 sample of this size or an even
larger difference under the assumption of the null hypothesis (of no
difference across the two groups). Hence, if we had previously defined
our Î±-level as 0.05, we reject the null hypothesis. If, however, we had
chosen a stricter Î±-level of 0.01, we fail to reject the null
hypothesis.

The Wilcoxon test does not make any assumptions about the distributions
of the population data, which means that it can be used for all kinds of
continuous variables including word frequencies. However, there is a
drawback: it is usually less powerful than the standard \emph{t}-test.
Using it, we are more likely to report a false negative (i.e.~to fail to
report a real difference in two means), which is why a transformation
may be wiser (for more about transformations, see Winter 2020: Chapter
5).

\subsection{Linearity and outliers}\label{sec-Linearity}

All statistical significance tests are sensitive to outliers. Hence, it
is important to identify any outliers (e.g.~by plotting your data as
part of preliminary data exploration) and to carefully consider the
extent to which they may influence the results of your analysis.

Another important assumption of Pearson's correlation coefficient
(\emph{r}) is that the continuous variables are linearly related. Again,
this is best perceived visually: if you plot the two variables against
one another in a scatter plot (see Section~\ref{sec-Scatterplots}), the
points should fall roughly along a single, straight line. When the true
association is non-linear (e.g.~curvilinear), Pearson's \emph{r} will
underestimate the strength of that association as it only captures the
linear component of an association (Tabachnick \& Fidell 2014: 117).

In 1973, the statistician Frank Anscombe put together a small dataset
with four pairs of variables (\emph{x} and \emph{y}) with the aim of
illustrating the necessity to visualise data and not rely solely on
statistics. The dataset is included in base \texttt{R} so we can access
it by calling its name without downloading or installing anything:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{anscombe}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   x1 x2 x3 x4    y1   y2    y3    y4
1  10 10 10  8  8.04 9.14  7.46  6.58
2   8  8  8  8  6.95 8.14  6.77  5.76
3  13 13 13  8  7.58 8.74 12.74  7.71
4   9  9  9  8  8.81 8.77  7.11  8.84
5  11 11 11  8  8.33 9.26  7.81  8.47
6  14 14 14  8  9.96 8.10  8.84  7.04
7   6  6  6  8  7.24 6.13  6.08  5.25
8   4  4  4 19  4.26 3.10  5.39 12.50
9  12 12 12  8 10.84 9.13  8.15  5.56
10  7  7  7  8  4.82 7.26  6.42  7.91
11  5  5  5  8  5.68 4.74  5.73  6.89
\end{verbatim}

Known as the Anscombe Quartet, the dataset's four pairs of variables
have exactly the same correlation coefficient (when rounded to two
decimal places):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{(anscombe}\SpecialCharTok{$}\NormalTok{x1, anscombe}\SpecialCharTok{$}\NormalTok{y1) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{round}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.82
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{(anscombe}\SpecialCharTok{$}\NormalTok{x2, anscombe}\SpecialCharTok{$}\NormalTok{y2) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{round}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.82
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{(anscombe}\SpecialCharTok{$}\NormalTok{x3, anscombe}\SpecialCharTok{$}\NormalTok{y3) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{round}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.82
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{(anscombe}\SpecialCharTok{$}\NormalTok{x4, anscombe}\SpecialCharTok{$}\NormalTok{y4) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{round}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.82
\end{verbatim}

However, when we visualise the data in scatter plots, it turns out that
the relationships between each pair are completely different!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Pair1 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ anscombe, }
       \AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x1, }\AttributeTok{y =}\NormalTok{ y1)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{colour =} \StringTok{"blue"}\NormalTok{,}
             \AttributeTok{size =} \DecValTok{2}\NormalTok{,}
             \AttributeTok{alpha =} \FloatTok{0.8}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title=}\StringTok{"Pair 1"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{, }
              \AttributeTok{colour =} \StringTok{"blue"}\NormalTok{,}
              \AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}

\NormalTok{Pair2 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ anscombe, }
       \AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x2, }\AttributeTok{y =}\NormalTok{ y2)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{colour =} \StringTok{"orange"}\NormalTok{,}
             \AttributeTok{size =} \DecValTok{2}\NormalTok{,}
             \AttributeTok{alpha =} \FloatTok{0.8}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title=}\StringTok{"Pair 2"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{,}
              \AttributeTok{colour =} \StringTok{"orange"}\NormalTok{,}
              \AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}

\NormalTok{Pair3 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ anscombe, }
       \AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x3, }\AttributeTok{y =}\NormalTok{ y3)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{colour =} \StringTok{"darkgreen"}\NormalTok{,}
             \AttributeTok{size =} \DecValTok{2}\NormalTok{,}
             \AttributeTok{alpha =} \FloatTok{0.8}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title=}\StringTok{"Pair 3"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{,}
              \AttributeTok{colour =} \StringTok{"darkgreen"}\NormalTok{,}
              \AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}

\NormalTok{Pair4 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ anscombe, }
       \AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x4, }\AttributeTok{y =}\NormalTok{ y4)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{colour =} \StringTok{"darkred"}\NormalTok{,}
             \AttributeTok{size =} \DecValTok{2}\NormalTok{,}
             \AttributeTok{alpha =} \FloatTok{0.8}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title=}\StringTok{"Pair 4"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{, }
              \AttributeTok{colour =} \StringTok{"darkred"}\NormalTok{,}
              \AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}

\CommentTok{\#install.packages("patchwork")}
\FunctionTok{library}\NormalTok{(patchwork)}
\NormalTok{Pair1 }\SpecialCharTok{+}\NormalTok{ Pair2 }\SpecialCharTok{+}\NormalTok{ Pair3 }\SpecialCharTok{+}\NormalTok{ Pair4}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{11_Inferential_files/figure-pdf/fig-4PairPlot-1.pdf}}

}

\caption{\label{fig-4PairPlot}Four plots showing the relationship
between pairs of variables}

\end{figure}%

From Figure~\ref{fig-4PairPlot}, we can immediately see that the
relationship between the two variables in Pair 2 is \textbf{non-linear},
which makes the yellow linear regression line nonsensical. The data
visualisations for Pairs 3 and 4 illustrate how a single
\textbf{outlier} can either create an illusion of a correlation that
does not exist (Pair 4) or overestimate one that does exist but is
probably considerably weaker than Pearson's r would lead us to believe
(Pair 3).

The assumption of linearity is relevant to many widely used statistical
methods -- notably linear regression models (see Chapter~\ref{sec-SLR}
and Chapter~\ref{sec-MLR}) -- which rely on Pearson's correlation
coefficients. In practice, researchers usually assume linearity unless
there is a strong theoretical reason to expect a nonâ€‘linear pattern
(Cohen\,et\,al.\,2003). However, this assumption should always be
checked. The most straightforward way to do this is to visualise the
data.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Non-parametric correlations}, titlerule=0mm, leftrule=.75mm]

For an in-depth explanation of non-parametric correlation coefficients
(Spearman's Ï and Kendall's Ï„) and statistical significance tests with
examples from the language sciences, I recommend reading Levshina (2015:
Chapter~6).

\end{tcolorbox}

\subsection{Homogeneity of variance and
homoscedasticity}\label{sec-Homoscedasticity}

When conducting a Student's \emph{t}-test, the variances of the samples
should be constant, or homogeneous. This is referred to as the
assumption of homogeneity of variance. It means that the variances of
the groups entered in a test should be roughly the same.

Again, this assumption is best examined visually. For example, we can
generate a boxplot to check that the variance of the two groups that we
want to compare with a \emph{t}-test are roughly equal (see
Figure~\ref{fig-GrammarBoxplot}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(Dabrowska.data,}
       \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Group, }\AttributeTok{y =}\NormalTok{ Grammar)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{11_Inferential_files/figure-pdf/fig-GrammarBoxplot-1.pdf}}

}

\caption{\label{fig-GrammarBoxplot}Comparison of grammar scores between
L1 and L2 groups}

\end{figure}%

As we already saw in Section~\ref{sec-Normality}, the \texttt{Grammar}
scores of the L2 group are not normally distributed; hence, we can also
see from Figure~\ref{fig-GrammarBoxplot} that there is much more
variance in the L2 data than there is in the L1. In other words, we have
heterogeneity of variance. One way to deal with this is to conduct a
non-parametric test instead: Wilcox's test does not assume homogeneity
of variance. However, if your data meet the criterion of normality and
only fails to meet that homogeneity of variance, you can still conduct a
parametric \emph{t}-test because the standard \texttt{t.test()} function
in \texttt{R} includes Welch's adjustment to correct for unequal
variances (Field, Miles \& Field 2012: 373).

A related assumption is made in correlation tests and linear regression
and is called the assumption of \textbf{homoscedasticity} (see also
Section~\ref{sec-Residuals}). To check the assumption of
homoscedasticity for a correlation, we can visualise the variance (or
variability) around the correlation (the linear regression line) with
the help of a scatter plot. In Figure~\ref{fig-VocabEduPlot}, we can see
that this variance is much larger for higher \texttt{Vocab} scores than
for low to medium scores.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(Group }\SpecialCharTok{==} \StringTok{"L1"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Vocab, }\AttributeTok{y =}\NormalTok{ EduTotal)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{,}
              \AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{11_Inferential_files/figure-pdf/fig-VocabEduPlot-1.pdf}}

}

\caption{\label{fig-VocabEduPlot}The relationship between vocabulary and
total education years}

\end{figure}%

In other words, these data do not meet the assumption of
homoscedasticity. In this case, we should therefore use a non-parametric
correlation statistic, such as Spearman's Ï (`rho') and Kendall's Ï„
(`tau'). As we are looking at a relatively small dataset here (only L1
speakers) and some speakers performed equally well on the \texttt{Vocab}
test (in other words, we have some ties in the ranks), Kendall's Ï„ is
recommended:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor.test}\NormalTok{(}\AttributeTok{formula =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ Vocab }\SpecialCharTok{+}\NormalTok{ EduTotal,}
         \AttributeTok{data =}\NormalTok{ L1.data,}
         \AttributeTok{method =} \StringTok{"kendall"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Kendall's rank correlation tau

data:  Vocab and EduTotal
z = 3.5406, p-value = 0.0003992
alternative hypothesis: true tau is not equal to 0
sample estimates:
      tau 
0.2761618 
\end{verbatim}

It returns a correlation coefficient Ï„ of \texttt{0.28} and a
\emph{p}-value of \textless~0.05, which allows us to reject the null
hypothesis of no association between \texttt{Vocab} and
\texttt{EduTotal} scores among L1 English speakers.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-caution-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-caution-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{{Optional (fun) task} ðŸ¦–}, titlerule=0mm, leftrule=.75mm]

Inspired by the Anscombe Quartet, Matejka \& Fitzmaurice (2017) created
a set of 12 pairs of variables that have the same descriptive statistics
as the data that produce a scatter plot representing a tyrannosaurus
(see Figure~\ref{fig-DinoPLot}).

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{11_Inferential_files/figure-pdf/fig-DinoPLot-1.pdf}}

}

\caption{\label{fig-DinoPLot}Scatter plot depicting a tyrannosaurus
(originally created by
\href{https://web.archive.org/web/20240620205540/http://www.thefunctionalart.com/2016/08/download-datasaurus-never-trust-summary.html}{Alberto
Cairo})}

\end{figure}%

{\textbf{1.}} Install and load the \texttt{datasauRus} package to access
the \texttt{R} data object \texttt{datasaurus\_dozen}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"datasauRus"}\NormalTok{)}
\FunctionTok{library}\NormalTok{(datasauRus)}

\NormalTok{datasaurus\_dozen}
\end{Highlighting}
\end{Shaded}

{\textbf{2.}} Run the following code to compare the descriptive
statistics of all three datasets within the \texttt{datasaurus\_dozen}.
The fourth set, \texttt{dino}, is the one visualised in
Figure~\ref{fig-DinoPLot}. Note that there is a very small, negative
correlation between all pairs of variables of \texttt{-0.0656}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{datasaurus\_dozen }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{group\_by}\NormalTok{(dataset) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{summarise}\NormalTok{(}
    \AttributeTok{mean\_x =} \FunctionTok{mean}\NormalTok{(x), }
    \AttributeTok{mean\_y =} \FunctionTok{mean}\NormalTok{(y), }
    \AttributeTok{std\_dev\_x =} \FunctionTok{sd}\NormalTok{(x), }
    \AttributeTok{std\_dev\_y =} \FunctionTok{sd}\NormalTok{(y), }
    \AttributeTok{corr\_x\_y =} \FunctionTok{cor}\NormalTok{(x, y))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 13 x 6
   dataset    mean_x mean_y std_dev_x std_dev_y corr_x_y
   <chr>       <dbl>  <dbl>     <dbl>     <dbl>    <dbl>
 1 away         54.3   47.8      16.8      26.9  -0.0641
 2 bullseye     54.3   47.8      16.8      26.9  -0.0686
 3 circle       54.3   47.8      16.8      26.9  -0.0683
 4 dino         54.3   47.8      16.8      26.9  -0.0645
 5 dots         54.3   47.8      16.8      26.9  -0.0603
 6 h_lines      54.3   47.8      16.8      26.9  -0.0617
 7 high_lines   54.3   47.8      16.8      26.9  -0.0685
 8 slant_down   54.3   47.8      16.8      26.9  -0.0690
 9 slant_up     54.3   47.8      16.8      26.9  -0.0686
10 star         54.3   47.8      16.8      26.9  -0.0630
11 v_lines      54.3   47.8      16.8      26.9  -0.0694
12 wide_lines   54.3   47.8      16.8      26.9  -0.0666
13 x_shape      54.3   47.8      16.8      26.9  -0.0656
\end{verbatim}

{\textbf{3.}} Run the following code to visualise the relationships
between the other 12 pairs of variables.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(datasaurus\_dozen, }
       \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ y, }\AttributeTok{colour =}\NormalTok{ dataset)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ dataset, }\AttributeTok{ncol =} \DecValTok{3}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \ConstantTok{NULL}\NormalTok{,}
       \AttributeTok{y =} \ConstantTok{NULL}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

{\textbf{4.}} Add a \texttt{geom\_} layer (see Section~\ref{sec-geoms})
to the following \{ggplot2\} code to add a blue linear regression line
in all 13 panels.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(datasaurus\_dozen, }
       \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ y, }\AttributeTok{colour =}\NormalTok{ dataset)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ dataset, }\AttributeTok{ncol =} \DecValTok{3}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{,}
              \AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{,}
              \AttributeTok{colour =} \StringTok{"blue"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \ConstantTok{NULL}\NormalTok{,}
       \AttributeTok{y =} \ConstantTok{NULL}\NormalTok{)  }
\end{Highlighting}
\end{Shaded}

Clearly, if we had only used correlation statistics to describe the
relationships between these 13 pairs of variables, we would have missed
some literally dinosaur-sized patterns! ðŸ¤¯

\end{tcolorbox}

!{[}Remember that the best way to check test assumptions is to visualise
your data! (artwork by \href{https://twitter.com/allison_horst}{Allison
Horst} (\href{https://creativecommons.org/licenses/by/4.0/}{CC BY
4.0})(images/AHorst\_NotNormal.png)\{\#fig-NotNormal fig-alt=``Cartoon
of a normal distribution looking skeptically at an excited looking
bimodal / negatively skewed distribution. The first says to the
second,''you're not normal.''\,'' width=``448''\}

\section{Multiple testing problem}\label{sec-pHacking}

In Section~\ref{sec-PValues} we saw that conducting a statistical test
with an Î±-level of 0.05 means that we accept a 5\% risk of falsely
rejecting the null hypothesis when it is actually true. Falsely
rejecting the null hypothesis leads to a \textbf{false positive} result,
also referred to as making a \textbf{Type I error}. It is crucial to
understand that if we perform multiple tests on the same data, we
dramatically increase the risk of reporting such false positive results.
This is known as the multiple testing or multiple comparisons problem.

Imagine that we want to test 20 independent null hypotheses on a single
dataset using Î±~=~0.05 as our significance threshold. Even if all null
hypotheses are actually true, the probability of obtaining at least one
significant result, i.e.~at least one \emph{p}-value \textless~ 0.05, by
chance is equal to 64\%, as shown below:

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}} \FloatTok{0.05}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{20}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.6415141
\end{verbatim}

Thus, having conducted 20 independent tests, we have a 64\% chance of
finding at least one statistically significant result purely by chance.
As the number of tests increases, this probability approaches certainty
(see also Baayen 2008: 106-107). With 100 tests, we reach 99\%!

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}} \FloatTok{0.05}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{100}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.9940795
\end{verbatim}

To avoid reporting false positive results, it is therefore recommended
that we correct our Î±-level to account for the number of tests performed
on the data. The simplest (but also most conservative) approach to do so
is called the \textbf{Bonferroni correction}. It consists in adjusting
our chosen Î±-level by dividing it by the number of tests that we are
conducting on the data. Hence, if we want to use 0.05 as our
significance level and conduct 20 independent tests on the same data, we
divide 0.05 by 20:

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{0.05} \SpecialCharTok{/} \DecValTok{20}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.0025
\end{verbatim}

This means that we would now only report the outcome of statistical test
as ``statistically significant'' if its \emph{p}-value
is~\textless~0.0025. Other correction methods for multiple testing
exist. \textbf{Holm's} method, for example, is a popular, slightly less
conservative alternative to the Bonferroni correction. The base
\texttt{R} function \texttt{p.adjust()} can be used to automatically
adjust \emph{p}-values using different methods including Bonferroni's
and Holm's.

The following line of code creates an \texttt{R} object that contains
seven \emph{p}-values. These are the \emph{p}-values that we obtained
from the seven independent statistical tests that we performed on the
DÄ…browska (2019) data as part of this chapter:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p\_values }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{1.956e{-}05}\NormalTok{, }\FloatTok{0.000007032}\NormalTok{, }\FloatTok{0.5525}\NormalTok{, }\FloatTok{0.0005581}\NormalTok{, }\FloatTok{0.2782}\NormalTok{, }\FloatTok{0.026}\NormalTok{, }\FloatTok{0.0003992}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

At our chosen Î±-level of 0.05, five of these \emph{p}-values were
statistically significant, leading us to reject the corresponding five
null hypotheses. However, if we apply Holm's correction using the
\texttt{p.adjust()} function, we find that we can only reject four of
these null hypotheses.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p\_values.adjusted }\OtherTok{\textless{}{-}} \FunctionTok{p.adjust}\NormalTok{(p\_values, }\AttributeTok{method =} \StringTok{"holm"}\NormalTok{)}

\NormalTok{p\_values.adjusted}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1.1736e-04 4.9224e-05 5.5640e-01 2.2324e-03 5.5640e-01 7.8000e-02 1.9960e-03
\end{verbatim}

It can be difficult to interpret numbers displayed in scientific
notation, so here they are in standard notation:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{format}\NormalTok{(p\_values.adjusted, }\AttributeTok{scientific =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "0.000117360" "0.000049224" "0.556400000" "0.002232400" "0.556400000"
[6] "0.078000000" "0.001996000"
\end{verbatim}

To find out which of these corrected \emph{p}-values are below 0.05, we
can use the \texttt{\textless{}} operator like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p\_values.adjusted }\SpecialCharTok{\textless{}} \FloatTok{0.05}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1]  TRUE  TRUE FALSE  TRUE FALSE FALSE  TRUE
\end{verbatim}

It is crucial to understand that every \emph{p}-value represents a
probabilistic finding, not a definitive statement about reality. A
\emph{p}-value of 0.03 does \emph{not} mean that there is a 97\% chance
that the alternative hypothesis is true. Rather it indicates that, if
the null hypothesis were true and we were to run this experiment many
times, we would observe data at least as extreme as in our sample only
3\% of the time. Given the probabilistic nature of statistical inference
and the multiple testing problem, the \textbf{replication} of findings
across independent studies is essential for building scientific
knowledge (see Section~\ref{sec-Reproducibility}). No single study,
regardless of its sample size or \emph{p}-value, can ever provide us
with definitive evidence.

Because of the multiple testing problem, only reporting statistically
significant results and failing to report the number of tests conducted
to find them is considered a \textbf{Questionable Research Practice
(QRP)}. It is one way to
\href{https://forrt.org/glossary/english/p_hacking/}{\emph{p}-hack}
results, along with not correcting for multiple comparisons. A great
resource for definitions and links to further literature on good
research practices is the \href{https://forrt.org/glossary}{FORRT
Glossary} --- a community-sourced glossary of Open Scholarship terms
(Parsons et al. 2022). Below is the FORRT Glossary's entry for QRPs.

\begin{quote}
\subsubsection*{Questionable Research Practices or Questionable
Reporting Practices
(QRPs)}\label{questionable-research-practices-or-questionable-reporting-practices-qrps}
\addcontentsline{toc}{subsubsection}{Questionable Research Practices or
Questionable Reporting Practices (QRPs)}

\textbf{Also available in:}
\href{https://forrt.org/glossary/arabic/questionable_research_practices_or_questionable_reporting_practices/}{Arabic}
\textbar{}
\href{https://forrt.org/glossary/german/questionable_research_practices_or_questionable_reporting_practices/}{German}
\textbar{}

\textbf{Definition:} A range of activities that intentionally or
unintentionally distort data in favour of a researcher's own hypotheses
- or omissions in reporting such practices - including; selective
inclusion of data, hypothesising after the results are known (HARKing),
and \emph{p}-hacking. Popularized by John et al.~(2012).

\textbf{Related terms:} Creative use of outliers, Fabrication,
\href{https://forrt.org/glossary/english/publication_bias/}{File-drawer},
\href{https://forrt.org/glossary/english/garden_of_forking_paths/}{Garden
of forking paths},
\href{https://forrt.org/glossary/english/harking/}{HARKing},
Nonpublication of data,
\href{https://forrt.org/glossary/english/p_hacking/}{\emph{p}-hacking},
\emph{p}-value fishing, Partial publication of data, Post-hoc
storytelling,
\href{https://forrt.org/glossary/english/preregistration/}{Preregistration},
\href{https://forrt.org/glossary/english/questionable_measurement_practices/}{Questionable
Measurement Practices (QMP)},
\href{https://forrt.org/glossary/english/researcher_degrees_of_freedom/}{Researcher
degrees of freedom},
\href{https://forrt.org/glossary/english/reverse_p_hacking/}{Reverse
\emph{p}-hacking},
\href{https://forrt.org/glossary/english/salami_slicing/}{Salami
slicing}

\textbf{Reference:} Banks et al.~(2016); Fiedler and Schwartz (2016);
Hardwicke et al.~(2014); John et al.~(2012); Neuroskeptic (2012);
Sijtsma (2016); Simonsohn et al.~(2011)

\textbf{Drafted and Reviewed by:} Mahmoud Elsherif, Tamara Kalandadze,
William Ngiam, Sam Parsons, Mariella Paul, Eike Mark Rinke, Timo
Roettger, FlÃ¡vio Azevedo
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{In search of the truth\ldots{}}, titlerule=0mm, leftrule=.75mm]

Inferential statistics, even if used correctly, when all test
assumptions are met and \emph{p}-values corrected for multiple
comparisons, tell us nothing about the validity or reliability of our
results. Fancy statistics cannot fix inaccurate measurements or
inconsistent annotations! In her study, Ewa DÄ…browska largely relied on
tests that had been tested for \textbf{validity} and
\textbf{reliability} in previous studies, which is why we trust that
these test instruments mostly \emph{do} measure what they claim to
measure (which makes them valid) and that the results that they produce
are consistent across participants (which makes them reliable). However,
if this is not the case, we need to be extremely careful about the
claims that we are making based on such data!

Recall how we observed that, for both L1 and L2 participants, there was
a positive correlation between the number years they were in formal
education and their English grammar comprehension test scores: the
longer they were in formal education, the higher their test scores. Even
if we show that, post-adjustment for multiple testing, this correlation
is statistically significant at a conventional level of significance,
this result could well be an artefact of our measuring instrument:
perhaps the participants who spent less time in formal education simply
have less practice completing academic-style tests. Maybe, as a result,
they did not fully understand the instructions or were unable to
complete the test within the given time. Not being good at completing
this test, however, may not be reflective of how well they can actually
understand complex grammatical structures in English because
understanding language typically doesn't happen in artificial test
contexts!

\end{tcolorbox}

\section{What's next?}\label{whats-next}

This chapter has given you the keys to understanding the basics of
inferential statistics following the frequentist null hypothesis
significance testing (NHST) framework. We looked at how we can use
\emph{t}-tests and correlation tests to infer information about a
population based on a random sample from the population. We introduced
\emph{p}-values, standardised effect sizes, and confidence intervals.
These are complex concepts that take time to understand. Frequentist
inferential statistics can be very powerful and useful, but it isn't
intuitive. Most people will need to read this chapter and other
resources (see recommended readings) several times to really get to
grips with these concepts.

In the following two chapters, we will move from single statistical
tests to multiple linear regression models. In Chapter~\ref{sec-SLR}, we
will first see how \emph{t}-tests and correlation tests can be
understood and computed as simple linear regression models before
exploring the potential of multiple linear regression models in
Chapter~\ref{sec-MLR}. The latter allow us to quantify and test the
effects of several variables in a single model. Multiple linear
regression naturally handles multiple predictors simultaneously, thus
providing more nuanced insights into the contributions of each variable
and their potential interactions.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-tip-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Recommended readings ðŸ“š}, titlerule=0mm, leftrule=.75mm]

\begin{itemize}
\item
  Ã‡etinkaya-Rundel, Mine \& Johanna Hardin. 2021. \textbf{Foundations of
  inference}. In \emph{Introduction to Modern Statistics
  2\textsuperscript{e}}. Open Access online book:
  \url{https://openintro-ims.netlify.app/foundations-of-inference}
\item
  JanÃ©, Matthew B., Qinyu Xiao, Siu Kit Yeung, Flavio Azevedo, Mattan S.
  Ben-Shachar, Aaron R Caldwell, Denis Cousineau, et al.~2024.
  \textbf{Guide to effect sizes and confidence intervals}. An Open
  Educational Resource: \url{https://doi.org/10.17605/OSF.IO/D8C4G}.
\item
  Lakens, DaniÃ«l. 2022. \textbf{Improving Your Statistical Inferences}.
  \url{https://doi.org/10.5281/ZENODO.6409077}. An Open Educational
  Resource: \url{https://lakens.github.io/statistical_inferences/}
\end{itemize}

\end{tcolorbox}

\subsection*{Check your progress ðŸŒŸ}\label{check-your-progress-10}
\addcontentsline{toc}{subsection}{Check your progress ðŸŒŸ}

Well done! You have successfully completed this chapter introducing the
complex topic of inferential statistics. You have answered { out of 17
questions} correctly.

Are you confident that you can\ldots?

\begin{itemize}
\tightlist
\item[$\square$]
  Differentiate between different methods of sampling
  (Section~\ref{sec-Sampling})
\item[$\square$]
  Formulate null hypotheses and alternative hypotheses
  (Section~\ref{sec-NHST})
\item[$\square$]
  Test null hypotheses using \emph{t}-tests in \texttt{R}
  (Section~\ref{sec-ttest})
\item[$\square$]
  Explain what statistical significance and \emph{p}-values mean
  (Section~\ref{sec-PValues})
\item[$\square$]
  Calculate Cohen's \emph{d} in \texttt{R}
  (Section~\ref{sec-EffectSize})
\item[$\square$]
  Calculate, test, and interpret correlations in \texttt{R}
  (Section~\ref{sec-Correlations})
\item
  Check that the main underlying assumptions of the most common
  statistical tests are met (Section~\ref{sec-Assumptions})
\item[$\square$]
  Correct \emph{p}-values for multiple comparisons and explain why this
  is important (Section~\ref{sec-pHacking})
\end{itemize}

\bookmarksetup{startatroot}

\chapter{\texorpdfstring{Int\texttt{R}oduction to statistical
modelling}{IntRoduction to statistical modelling}}\label{sec-SLR}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-warning-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-warning-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Warning}, titlerule=0mm, leftrule=.75mm]

As with the rest of this textbook (see
\href{https://elenlefoll.github.io/RstatsTextbook/}{Preface}), this
chapter is very much \textbf{work in progress}. All feedback is very
welcome.

\end{tcolorbox}

\subsection*{From tests to models}\label{from-tests-to-models}
\addcontentsline{toc}{subsection}{From tests to models}

This chapter will take you from statistical tests to statistical
modelling. By the end of this chapter, you will be able to:

\begin{itemize}
\tightlist
\item
  Use the \texttt{lm()} function to fit linear regression models with:

  \begin{itemize}
  \tightlist
  \item
    a single numeric predictor variable
  \item
    a single (binary) categorical predictor variable
  \end{itemize}
\item
  Understand the relationship between:

  \begin{itemize}
  \tightlist
  \item
    correlation tests and linear regression models with a single numeric
    predictor
  \item
    \emph{t}-tests and linear regression models with a single binary
    categorical predictor
  \end{itemize}
\item
  Interpret the summary output of simple linear regression models
\item
  Explain what the intercept of a simple linear regression model
  corresponds to
\item
  Visualise the predictions of simple linear regression models
\item
  Explain why ``association does not imply causation''
\item
  Check the most important assumptions of linear regression models.
\end{itemize}

\section{Correlations as regression over a numeric
variable}\label{sec-Correlationsregression}

This section explains how the principle of correlation, which we covered
in Section~\ref{sec-Correlations}, is integral to linear regression
modelling. We begin with a `toy' example to familiarise ourselves with
the concept of statistical modelling. It is important to take the time
to genuinely understand how simple linear regression works before moving
on to more complex, real-world research questions.

\subsection{A perfect prediction}\label{a-perfect-prediction}

In the following, we will fit a simple linear regression model to
predict the percentage of correct answers that a student obtained in a
multiple-choice test based on the number of questions that they
correctly answered in this same test. Clearly, this is a purely
hypothetical example because, if we know the number of questions that a
student correctly answered and how many questions there were in the
test, we can easily calculate the percentage of questions that the
student answered correctly.

If a test has 10 questions and a student answered 8 of them correctly,
we can calculate the percentage of questions that they successfully
answered like this:

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{8} \SpecialCharTok{/} \DecValTok{10} \SpecialCharTok{*} \DecValTok{100}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 80
\end{verbatim}

And we can simplify this operation to a single multiplication:

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{8} \SpecialCharTok{*} \DecValTok{10}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 80
\end{verbatim}

We will fit our first simple linear regression model on a simulated
dataset. It consists of 100 test results expressed:

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  as the number of correctly answered questions (\texttt{N.correct}),
  and
\item
  as a percentage (\texttt{Accuracy}).
\end{enumerate}

The dataset contains 100 rows corresponding to the results of 100
test-takers. Table~\ref{tbl-RTTable} displays the first six rows of this
simulated dataset (\texttt{test.results}).

\begin{table}

\caption{\label{tbl-RTTable}}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# First, we set a seed to ensure that the outcome of our randomly generated number series are always exactly the same:}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}

\CommentTok{\# Second, we simulate the number of questions that 100 learners answered correctly using the \textasciigrave{}sample()\textasciigrave{} function that generates random numbers, specifying that learners got between 1 and 10 questions right:}
\NormalTok{N.correct }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{, }\DecValTok{100}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}

 \CommentTok{\# Third, we convert the number of correctly answered questions into the percentage of questions that these learners answered correctly:}
\NormalTok{Accuracy }\OtherTok{\textless{}{-}}\NormalTok{ N.correct }\SpecialCharTok{/} \DecValTok{10} \SpecialCharTok{*} \DecValTok{100}

\CommentTok{\# Finally, we put these two variables together in a dataframe:}
\NormalTok{test.results }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(N.correct, Accuracy)}
\end{Highlighting}
\end{Shaded}

}

\end{table}%

\begin{table}
\fontsize{12.0pt}{14.0pt}\selectfont
\begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}rr}
\toprule
N.correct & Accuracy \\ 
\midrule\addlinespace[2.5pt]
1 & 10 \\ 
5 & 50 \\ 
1 & 10 \\ 
9 & 90 \\ 
10 & 100 \\ 
4 & 40 \\ 
\bottomrule
\end{tabular*}
\end{table}

We can now plot the two variables of our simulated dataset as a scatter
plot to visualise the correlation between the number of questions
learners answered correctly (\texttt{N.correct}) and the percentage of
questions they answered accurately (\texttt{Accuracy}). As expected,
these two variables are perfectly correlated: every data point sits
exactly on the regression line.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ test.results,}
       \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ N.correct, }\AttributeTok{y =}\NormalTok{ Accuracy)) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{,}
              \AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{,}
             \AttributeTok{size =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Number of correct answers"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"\% of questions correctly answered"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{10.5}\NormalTok{), }\AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{), }\AttributeTok{breaks =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{6}\NormalTok{,}\DecValTok{8}\NormalTok{,}\DecValTok{10}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{105}\NormalTok{), }\AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{), }\AttributeTok{breaks =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{60}\NormalTok{, }\DecValTok{80}\NormalTok{, }\DecValTok{100}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{12_SimpleLinearRegression_files/figure-pdf/fig-RTScatterPLot-1.pdf}}

}

\caption{\label{fig-RTScatterPLot}The correlation between learners' test
results expressed as the number of correct answers and the percentage of
questions answered correctly}

\end{figure}%

This is confirmed by a correlation test (see below), which shows:

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  a correlation coefficient (Pearson's \emph{r}) of exactly \texttt{1},
\item
  an extremely small \emph{p}-value (the smallest that \texttt{R} can
  display: \texttt{\textless{}\ 2.2e-16}), and
\item
  the narrowest 95\% confidence interval possible \texttt{{[}1,\ 1{]}}.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor.test}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ N.correct }\SpecialCharTok{+}\NormalTok{ Accuracy,}
         \AttributeTok{data =}\NormalTok{ test.results)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Pearson's product-moment correlation

data:  N.correct and Accuracy
t = 156587349, df = 98, p-value < 2.2e-16
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 1 1
sample estimates:
cor 
  1 
\end{verbatim}

Now, we use the base \texttt{R} function \texttt{lm()} to fit a simple
linear model to predict the percentage of questions that learners
correctly answered based on the number of correct answers that they
gave. Like the significance test functions that we saw in
Chapter~\ref{sec-Inferential}, \texttt{lm()} takes a \textbf{formula} as
its first argument. We want to predict the proportion of questions that
learners correctly answered as a percentage (\texttt{Accuracy}) so this
variable comes \emph{before} the tilde (\texttt{\textasciitilde{}}). In
statistical modelling, the variable that we want to predict is referred
to as the \textbf{outcome variable}\footnote{This variable is sometimes
  referred to as the \textbf{dependent variable} (see
  Section~\ref{sec-IntSummary}).}. We want to use the number of correct
answers that the learners gave to make our prediction, so we place the
variable \texttt{N.correct} \emph{after} the tilde. Variables used to
predict the outcome are called \textbf{predictors}\footnote{These
  variables are also sometimes referred to as \textbf{independent
  variables} (see Section~\ref{sec-IntSummary}).}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test.model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ Accuracy }\SpecialCharTok{\textasciitilde{}}\NormalTok{ N.correct,}
                 \AttributeTok{data =}\NormalTok{ test.results)}
\end{Highlighting}
\end{Shaded}

We have saved our model to our local environment as an \texttt{R} object
called \texttt{test.model}. We can now use the \texttt{summary()}
function to examine the model:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(test.model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = Accuracy ~ N.correct, data = test.results)

Residuals:
       Min         1Q     Median         3Q        Max 
-3.648e-14 -1.351e-15 -4.300e-16  7.280e-16  7.057e-14 

Coefficients:
             Estimate Std. Error   t value Pr(>|t|)    
(Intercept) 9.681e-15  1.781e-15 5.436e+00    4e-07 ***
N.correct   1.000e+01  2.890e-16 3.461e+16   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 8.319e-15 on 98 degrees of freedom
Multiple R-squared:      1, Adjusted R-squared:      1 
F-statistic: 1.198e+33 on 1 and 98 DF,  p-value: < 2.2e-16
\end{verbatim}

In addition to this model summary, the code above outputs a warning
about an ``essentially perfect fit'' in the Console. We will come to
this warning at the end of this section, but for now let's start by
looking at the \textbf{coefficients} of our model summary. Every model
has an \textbf{intercept} coefficient estimate (\texttt{(Intercept)}),
which is the value that the model predicts for the outcome variable when
the predictor variables are zero. In other words, it is where the
regression line would cross the \emph{y}-axis if the line were extended
to go all the way to zero as in Figure~\ref{fig-RTScatterPLotIntercept}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ test.results,}
       \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ N.correct, }\AttributeTok{y =}\NormalTok{ Accuracy)) }\SpecialCharTok{+}
  \FunctionTok{geom\_abline}\NormalTok{(}\AttributeTok{slope =} \DecValTok{10}\NormalTok{, }
              \AttributeTok{intercept =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{),}
              \AttributeTok{linetype =} \StringTok{"dotted"}\NormalTok{,}
              \AttributeTok{colour =} \StringTok{"blue"}\NormalTok{) }\SpecialCharTok{+}       
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{,}
              \AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{,}
             \AttributeTok{size =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Number of correct answers"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"\% of questions correctly answered"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{10.5}\NormalTok{), }\AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{), }\AttributeTok{breaks =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{6}\NormalTok{,}\DecValTok{8}\NormalTok{,}\DecValTok{10}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{105}\NormalTok{), }\AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{), }\AttributeTok{breaks =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{60}\NormalTok{, }\DecValTok{80}\NormalTok{, }\DecValTok{100}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{12_SimpleLinearRegression_files/figure-pdf/fig-RTScatterPLotIntercept-1.pdf}}

}

\caption{\label{fig-RTScatterPLotIntercept}The correlation between
learners' test results expressed as the number of correct answers and
the percentage of questions answered correctly}

\end{figure}%

In our case, we only have one predictor, so the intercept coefficient
that our model estimates (\texttt{5.151e-15}) corresponds to the
predicted percentage of questions answered correctly (outcome) when the
number of correct answers (predictor) is equal to zero. Instinctively,
we know that this value should be zero because 0 points in a test~=~0\%
correct in the test. And, indeed, our model predicts an intercept
extremely close to zero (\texttt{5.151e-15}). Using the
\texttt{format()} function, we can convert this coefficient estimate
from \textbf{scientific notation} to standard notation:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{format}\NormalTok{(}\FloatTok{5.151e{-}15}\NormalTok{, }\AttributeTok{scientific =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "0.000000000000005151"
\end{verbatim}

The second, \texttt{N.correct} coefficient estimate in our model summary
is displayed as \texttt{1.000e+01} which is equal to:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{format}\NormalTok{(}\FloatTok{1.000e+01}\NormalTok{, }\AttributeTok{scientific =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "10"
\end{verbatim}

This coefficient estimate means that, for every correct answer, our
prediction for the percentage of questions answered correctly increases
by 10\%. For example, if a student answered 9 questions correctly, our
model predicts that the percentage of correctly answered question is
equal to the intercept coefficient of (nearly) zero plus 9 multiplied by
10, which is equal to 90\%:

\begin{Shaded}
\begin{Highlighting}[]
\SpecialCharTok{{-}}\FloatTok{5.151e{-}15} \SpecialCharTok{+} \DecValTok{9} \SpecialCharTok{*} \DecValTok{10}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 90
\end{verbatim}

In our model summary, the coefficient estimate for \texttt{N.correct} is
associated with a \emph{p}-value of \texttt{\textless{}2e-16}. It
corresponds to the \emph{p}-value of our \texttt{cor.test()} on the same
data, and is actually the smallest value that \texttt{R} will display.
In other words, it is extremely small. This \emph{p}-value indicates
that we can safely reject the null hypothesis that this coefficient is
zero under the assumptions of a linear regression model. In other words,
we can reject the null hypothesis that the number of correct answers is
\emph{not} a useful predictor to predict the percentage of correctly
answered questions under the assumption of a linear regression model
(more on these assumptions in Section~\ref{sec-Assumptions}).

The penultimate line of the model summary is also important. It features
two \textbf{R-squared (R\textsuperscript{2}) values}. Like the absolute
values of correlation coefficients, these can range between 0 and 1.
R\textsuperscript{2}~=~0 means that the model accounts for 0\% of the
variance in the outcome variable. R\textsuperscript{2}~=~1 means that
the model accounts for 100\% of the variance, i.e.~that it can perfectly
predict the values of the outcome variable from the predictor
variable(s). As our simulated \texttt{test.results} dataset is based on
a perfect correlation, our model is able to perfectly predict the
outcome variable, hence both our R\textsuperscript{2} values equal
\texttt{1}. In this chapter and Chapter~\ref{sec-MLR}, however, we will
focus on the \textbf{adjusted R-squared value} because it accounts for
the fact that the more predictors we include in our model, the easier it
is to predict the outcome variable.

Finally, you may have also noticed that the \texttt{lm()} function
returned a warning message when we fitted this practice model:

\begin{verbatim}
Warning message:
In summary.lm(test.model) :
  essentially perfect fit: summary may be unreliable
\end{verbatim}

With this message, the authors of the \texttt{lm()} function are warning
us that our model can make almost perfect predictions. Given that in
real-life research this is extremely unlikely, an ``essentially
perfect'' prediction is usually a sign that we may have made an error of
some kind. Here, however, we can safely ignore this warning because we
know that the number of correct answers can, indeed, be converted to
percentages of correctly answered questions with perfect accuracy (hence
our R\textsuperscript{2} of 1 or 100\%).

\subsection{A real-life prediction}\label{a-real-life-prediction}

In the remaining sections of the chapter, we fit simple regression
models to real data from:

\begin{quote}
DÄ…browska, Ewa. 2019. Experience, Aptitude, and Individual Differences
in Linguistic Attainment: A Comparison of Native and Nonnative Speakers.
Language Learning 69(S1). 72--100.
\url{https://doi.org/10.1111/lang.12323}.
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-warning-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-warning-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Prerequisites}, titlerule=0mm, leftrule=.75mm]

Our starting point for this chapter is the wrangled combined dataset
that we created and saved in Chapter~\ref{sec-DataWrangling}. Follow the
instructions in Section~\ref{sec-filter} to create this \texttt{R}
object.

Alternatively, you can download \texttt{Dabrowska2019.zip} from
\href{https://github.com/elenlefoll/RstatsTextbook/raw/69d1e31be7394f2b612825f031ebffeb75886390/Dabrowska2019.zip}{the
textbook's GitHub repository}. To launch the project correctly, first
unzip the file and then double-click on the \texttt{Dabrowska2019.Rproj}
file.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(here)}

\NormalTok{Dabrowska.data }\OtherTok{\textless{}{-}} \FunctionTok{readRDS}\NormalTok{(}\AttributeTok{file =} \FunctionTok{here}\NormalTok{(}\StringTok{"data"}\NormalTok{, }\StringTok{"processed"}\NormalTok{, }\StringTok{"combined\_L1\_L2\_data.rds"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Before you get started, check that you have correctly imported the data
by examining the output of \texttt{View(Dabrowska.data)} and
\texttt{str(Dabrowska.data)}. In addition, run the following lines of
code to load the \{tidyverse\} packages and create ``clean'' versions of
the L1 and L2 datasets as separate \texttt{R} objects:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}

\NormalTok{L1.data }\OtherTok{\textless{}{-}}\NormalTok{ Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(Group }\SpecialCharTok{==} \StringTok{"L1"}\NormalTok{)}

\NormalTok{L2.data }\OtherTok{\textless{}{-}}\NormalTok{ Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(Group }\SpecialCharTok{==} \StringTok{"L2"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Once you are satisfied that the data are correctly loaded, you are ready
to start modelling! ðŸš€

\end{tcolorbox}

Recall that, in Section~\ref{sec-Correlations}, we saw that there is a
\textbf{positive correlation} between the total number of years that
English L1 speakers spent in formal education (\texttt{EduTotal}) and
their English grammar comprehension test scores (\texttt{Grammar}).
Figure~\ref{fig-EducationYearsVocab} suggests that this positive
correlation also holds for the association between the number of years
participants spent in formal education and their receptive vocabulary
test scores (\texttt{Vocab}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L1.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ EduTotal, }
                       \AttributeTok{y =}\NormalTok{ Vocab)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{,}
              \AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{12_SimpleLinearRegression_files/figure-pdf/fig-EducationYearsVocab-1.pdf}}

}

\caption{\label{fig-EducationYearsVocab}Relationship between years spent
in formal education and vocabulary test scores among L1 speakers}

\end{figure}%

On average, the longer L1 speakers were in formal education, the better
they performed on the vocabulary test. The correlation is larger than
for \texttt{Grammar} scores, and it is statistically significant at an
Î±-level of 0.05 (\emph{r} = 0.43, 95\% CI {[}0.24, 0.58{]}):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor.test}\NormalTok{(}\AttributeTok{formula =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ Vocab }\SpecialCharTok{+}\NormalTok{ EduTotal,}
         \AttributeTok{data =}\NormalTok{ L1.data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Pearson's product-moment correlation

data:  Vocab and EduTotal
t = 4.4281, df = 88, p-value = 2.721e-05
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 0.2410910 0.5824698
sample estimates:
      cor 
0.4268695 
\end{verbatim}

The straight (i.e.~linear) regression line going through our scatter
plot in Figure~\ref{fig-EducationYearsVocab}, does \emph{not} go through
all the data points like it did in Figure~\ref{fig-RTScatterPLot}. This
is because, if we know how long an L1 participant was in formal
education, we cannot \emph{perfectly} predict how well they will perform
on the \texttt{Vocab} test, even though we do know that, on average,
having spent longer in formal education correlates with \texttt{Vocab}
test scores. Indeed, Figure~\ref{fig-EducationYearsVocab} clearly shows
that some of the individuals who attended formal education for the least
amount of time scored very low, whilst others scored very high in the
\texttt{Vocab} test. This is not surprising, as we can expect that many
other factors will play a role in L1 vocabulary knowledge.

We now fit a simple linear regression model using the \texttt{lm()}
function (which stands for \textbf{linear model}) to the L1 data from
DÄ…browska (2019) with the aim of predicting \texttt{Vocab} scores (our
outcome variable) based on the number of years that they spent in formal
education (\texttt{EduTotal}; our predictor variable):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model1 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ Vocab }\SpecialCharTok{\textasciitilde{}}\NormalTok{ EduTotal,}
             \AttributeTok{data =}\NormalTok{ L1.data)}
\end{Highlighting}
\end{Shaded}

We examine the model using the \texttt{summary()} function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(model1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = Vocab ~ EduTotal, data = L1.data)

Residuals:
    Min      1Q  Median      3Q     Max 
-57.725 -10.716   0.526  12.417  36.033 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  20.5159    11.1519   1.840   0.0692 .  
EduTotal      3.5460     0.8008   4.428 2.72e-05 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 18.51 on 88 degrees of freedom
Multiple R-squared:  0.1822,    Adjusted R-squared:  0.1729 
F-statistic: 19.61 on 1 and 88 DF,  p-value: 2.721e-05
\end{verbatim}

This time, we'll begin our interpretation of the model summary from the
top:

\begin{itemize}
\item
  The first line is a reminder of the model \textbf{formula} and of the
  \textbf{data} on which we fitted our linear model (\texttt{lm}).
\item
  The second paragraph provides information about the
  \textbf{residuals}. They represent the difference between
  participants' actual \texttt{Vocab} scores (the observed values) and
  the model's predictions (the predicted values). Hence, they correspond
  to the variance in \texttt{Vocab} scores that is left unaccounted for
  by the model. The closer to zero the residuals are, the better the
  model fits the data. However, no real-life model is perfect so we
  expect there to be some ``left-over'' variance.

  \begin{itemize}
  \tightlist
  \item
    \texttt{Min}, \texttt{1Q}, \texttt{Median}, \texttt{3Q}, and
    \texttt{Max} are the minimum, first quartile, median, third
    quartile, and maximum residuals, respectively. These are the
    descriptive statistics of a distribution that are typically
    displayed in a boxplot (see Section~\ref{sec-IQR}). These statistics
    give us a sense of the spread of the residuals. Whilst these values
    can be informative, it's best to plot residuals to get a sense of
    their distribution (see Section~\ref{sec-Residuals}). Ideally, the
    residuals should be normally distributed and centred around zero
    (see Section~\ref{sec-AssumptionsLR}).
  \end{itemize}
\item
  The \textbf{intercept coefficient estimate} of \texttt{20.516} is the
  model's prediction for the vocabulary score of participants who spent
  zero years (!) in formal education (\texttt{EduTotal} = 0). It is the
  model's baseline or reference \texttt{Vocab} score.
\item
  The \textbf{EduTotal coefficient estimate} of \texttt{3.546} means
  that, for each year of formal education, our model predicts that
  \texttt{Vocab} test scores will increase by an additional 3.546 units
  on top of the baseline score of \texttt{20.516}, provided that all
  other variables remain the same. Hence, if an individual spent 14
  years in formal education, their predicted \texttt{Vocab} score is:

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{20.5159} \SpecialCharTok{+} \DecValTok{14}\SpecialCharTok{*}\FloatTok{3.5460}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 70.1599
\end{verbatim}
\item
  The \textbf{\emph{p-}value} associated with the \texttt{EduTotal}
  coefficient is very small (\texttt{2.72e-05}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{format}\NormalTok{(}\FloatTok{2.72e{-}05}\NormalTok{, }\AttributeTok{scientific =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "0.0000272"
\end{verbatim}

  It suggests that the predictor \texttt{EduTotal} makes a statistically
  significant contribution to our model predicting \texttt{Vocab} scores
  among L1 speakers.
\item
  The \textbf{adjusted R-squared (RÂ²)} is \texttt{0.1729}, which means
  that our model accounts for ca. 17\% of the total variance in
  \texttt{Vocab} scores. That may not seem a lot, but it is worth
  recalling that our model only includes one predictor variable. We can
  reasonably assume that other predictors will help us to predict L1
  speakers' vocabulary knowledge more accurately (e.g.~their age, how
  much they read, perhaps their profession, etc.).
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{It's all linear regression!}, titlerule=0mm, leftrule=.75mm]

Did you notice that the \textbf{\emph{t}-value} and the
\textbf{\emph{p}-value} associated with the \texttt{EduTotal}
coefficient in our linear regression model are \emph{exactly the same}
as those that we obtained earlier using the \texttt{cor.test()}
function? This is because linear regression with a single numeric
predictor is -- under the hood -- exactly the same as a Pearson's
correlation test!

Moreover, we said that \textbf{RÂ²} stands for the \textbf{squared
coefficient of correlation}, which is why, if we take the square root of
our unadjusted RÂ² coefficient, we get 0.43, which corresponds to the
\textbf{correlation coefficient} (Pearson's \emph{r}) that we obtained
from the \texttt{cor.test()} function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sqrt}\NormalTok{(}\FloatTok{0.1822}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.4268489
\end{verbatim}

Having gone through Chapter~\ref{sec-Inferential}, you may now ask
yourself: why do we need correlation tests if simple linear regression
does the same thing? ðŸ§ Honestly, we don't.
Chapter~\ref{sec-Inferential} introduced correlation tests and
\emph{t}-tests because they are widely used in the language sciences, so
it is important that you understand them, but the truth is: you can
achieve much more by taking a modelling rather than a testing approach
to analysing data. Read on to find out more!

\end{tcolorbox}

\subsection{Predicted values and
residuals}\label{predicted-values-and-residuals}

Figure~\ref{fig-EduPredictedVocab} visualises the \texttt{Vocab} scores
that our first model (\texttt{model1}) predicts as a function of the
number of years that L1 speakers have spent in formal education. We can
access our model's predictions by applying the \texttt{predict()}
function to our model object (\texttt{model1}). By definition,
regression models predict perfect linear associations. As shown in
Figure~\ref{fig-EduPredictedVocab}, here, our model predicts a perfect,
positive linear association between our predictor variable
(\texttt{EduTotal}) and our outcome variable (\texttt{Vocab}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(L1.data, }
       \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ EduTotal, }
           \AttributeTok{y =} \FunctionTok{predict}\NormalTok{(model1))) }\SpecialCharTok{+} 
  \FunctionTok{geom\_abline}\NormalTok{(}\AttributeTok{slope =} \FloatTok{3.55}\NormalTok{, }
              \AttributeTok{intercept =} \FloatTok{20.51}\NormalTok{,}
              \AttributeTok{linetype =} \StringTok{"dotted"}\NormalTok{,}
              \AttributeTok{colour =} \StringTok{"blue"}\NormalTok{) }\SpecialCharTok{+}   
  \FunctionTok{geom\_count}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_size\_area}\NormalTok{(}\AttributeTok{guide =} \ConstantTok{NULL}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{y=}\StringTok{\textquotesingle{}Predicted Vocab scores\textquotesingle{}}\NormalTok{, }
       \AttributeTok{x=}\StringTok{\textquotesingle{}Years in formal education\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{12_SimpleLinearRegression_files/figure-pdf/fig-EduPredictedVocab-1.pdf}}

}

\caption{\label{fig-EduPredictedVocab}Relationship between years spent
in formal education and predicted vocabulary test scores}

\end{figure}%

You may have noticed that there are fewer dots on
Figure~\ref{fig-EduPredictedVocab} than data points in the dataset that
we used to fit the model (N~=~90). This is because, if more than one L1
participant reported having been in formal education for the same
duration of time, the model predicts the same \texttt{Vocab} score for
all these people and this means that the dots overlap on the plot of
predicted values. To ensure that we keep in mind that a single dot may
represent more than one participant, in
Figure~\ref{fig-EduPredictedVocab}, the \texttt{geom\_count()} function
is used to map the size of each dot onto the number of participants that
it represents.

This is all very well, but as we learnt from
Figure~\ref{fig-EducationYearsVocab}, in reality, our predictor variable
\texttt{EduTotal} does not correlate perfectly with \texttt{Vocab}
scores --- far from it! Let us now compare the \textbf{predicted values}
of our model with the \textbf{observed values} from the data. How well
does our model fit the data? To help us answer this question,
Figure~\ref{fig-ActualPredictedVocab} visualises the relationship
between L1 participants' actual \texttt{Vocab} scores (\emph{x}-axis)
and the scores that the model predicted for these participants
(\emph{y}-axis).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred\_vals   }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(model1) }\CommentTok{\# Vector of predictions}
\NormalTok{xmin\_pred   }\OtherTok{\textless{}{-}} \FunctionTok{min}\NormalTok{(pred\_vals)  }\CommentTok{\# Lowest predicted score}

\NormalTok{highlight\_df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(xmin\_pred, pred\_vals[}\DecValTok{68}\NormalTok{]),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(L1.data}\SpecialCharTok{$}\NormalTok{Vocab[}\FunctionTok{which.min}\NormalTok{(pred\_vals)],}
      \DecValTok{91}\NormalTok{))}

\FunctionTok{ggplot}\NormalTok{(L1.data, }
       \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ pred\_vals, }
           \AttributeTok{y =}\NormalTok{ Vocab)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \CommentTok{\# 45Â° reference line (the "perfectâ€‘prediction" line):}
  \FunctionTok{geom\_abline}\NormalTok{(}\AttributeTok{intercept =} \DecValTok{0}\NormalTok{, }
              \AttributeTok{slope =} \DecValTok{1}\NormalTok{,}
              \AttributeTok{linetype =} \StringTok{"dotted"}\NormalTok{,}
              \AttributeTok{colour =} \StringTok{"blue"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{annotate}\NormalTok{(}\StringTok{"segment"}\NormalTok{,}
           \AttributeTok{x =}\NormalTok{ xmin\_pred, }
           \AttributeTok{y =} \DecValTok{29}\NormalTok{, }
           \AttributeTok{yend =}\NormalTok{ xmin\_pred,}
           \AttributeTok{colour =} \StringTok{"blue"}\NormalTok{,}
           \AttributeTok{arrow =} \FunctionTok{arrow}\NormalTok{(}\AttributeTok{length =} \FunctionTok{unit}\NormalTok{(}\FloatTok{0.25}\NormalTok{, }\StringTok{"cm"}\NormalTok{))) }\SpecialCharTok{+}
  \FunctionTok{annotate}\NormalTok{(}\StringTok{"segment"}\NormalTok{,}
           \AttributeTok{x =}\NormalTok{ pred\_vals[}\DecValTok{68}\NormalTok{], }
           \AttributeTok{y =} \DecValTok{91}\NormalTok{, }
           \AttributeTok{yend =}\NormalTok{ pred\_vals[}\DecValTok{68}\NormalTok{],}
           \AttributeTok{colour =} \StringTok{"blue"}\NormalTok{,}
           \AttributeTok{arrow =} \FunctionTok{arrow}\NormalTok{(}\AttributeTok{length =} \FunctionTok{unit}\NormalTok{(}\FloatTok{0.25}\NormalTok{, }\StringTok{"cm"}\NormalTok{))) }\SpecialCharTok{+}  
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{data =}\NormalTok{ highlight\_df,}
             \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ y),}
             \AttributeTok{colour =} \StringTok{"blue"}\NormalTok{) }\SpecialCharTok{+}  
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{100}\NormalTok{), }
                     \AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{100}\NormalTok{), }
                     \AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Predicted Vocab scores"}\NormalTok{, }
       \AttributeTok{y =} \StringTok{"Actual Vocab scores"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{12_SimpleLinearRegression_files/figure-pdf/fig-ActualPredictedVocab-1.pdf}}

}

\caption{\label{fig-ActualPredictedVocab}Relationship between actual and
predicted \texttt{Vocab} test scores}

\end{figure}%

In Figure~\ref{fig-ActualPredictedVocab}, the dotted blue line
represents where the points would lie, if \texttt{model1} perfectly
predicted \texttt{Vocab} scores based only on the number of years that
participants spent in formal education. The dots that fall on or very
close to the dotted line stand for L1 participants for whom our model
accurately predicts their \texttt{Vocab} score based solely on the
number of years they spent in formal education. The further the dots are
from the dotted line, the worse the predictions for these speakers. The
distances between each point and the dotted line in
Figure~\ref{fig-ActualPredictedVocab} correspond to the model's
residuals. \textbf{Residuals} are therefore the `left-over' variability
in the data that our model cannot account for.

Residuals can be positive or negative, depending on whether the dots on
the predicted vs.~actual values plot are above or below the dotted line
of perfect fit. However, what matters more are their absolute values.
The larger the residuals, the worse the model fit. Returning to the
question, `How good is our model fit?', we can conclude from
Figure~\ref{fig-ActualPredictedVocab} that it's not great. But we
already knew that from the model's fairly low RÂ² value and, crucially,
we also know that many other factors are likely to account for some of
the remaining variation in vocabulary scores.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-warning-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-warning-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Association does not imply causation.}, titlerule=0mm, leftrule=.75mm]

You may be familiar with the Latin phrase:

\begin{itemize}
\tightlist
\item
  \textbf{\emph{Cum hoc ergo propter hoc}} (`with this, therefore
  because of this')
\end{itemize}

or its English equivalent:

\begin{itemize}
\tightlist
\item
  \textbf{\emph{Correlation does not imply causation}}.
\end{itemize}

They both refer to the human tendency to assume that, if two things are
regularly associated, one probably \emph{causes} the other. However,
this is a logical fallacy. For example, even if we had observed a very
high correlation between the number of years participants were in formal
education and their receptive vocabulary test scores (and had therefore
reported an excellent model fit with very small residuals), this would
by no means imply that, on average, spending more time in formal
education \emph{leads to} greater vocabulary knowledge. A correlation
merely describes an association; it tells us nothing about any causal
relationship.

It is often tempting to interpret the results of statistical tests and
models causally, but this is the result of a well-known human cognitive
bias (see, e.g, Matute et al. 2015; Kaufman \& Kaufman 2018). In the
language sciences and across the social sciences more generally, there
are usually far too many potential factors at play to warrant any causal
interpretation. What we are modelling throughout this chapter and
Chapter~\ref{sec-MLR} are \textbf{statistical associations}. We cannot
draw any conclusions about what \emph{caused} what from these models. In
the context of statistical modelling, we can extend the famous saying
to: \textbf{Association does not imply causation}.

There are three main reasons for this:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  There may be one or more \textbf{confounding variables} that we have
  not accounted for in our model. For instance, we can predict with a
  fair degree of accuracy how much vocabulary an infant understands
  based on their weight. However, the weight of young children is, of
  course, highly correlated with their age and, by extension, the amount
  of language exposure they've had so far in their life. Clearly, it is
  not their weight that is \emph{causing} them to understand more words.
  Unfortunately, it's not always that obvious. Going back to
  \texttt{model1}, it is very likely that some of the younger
  participants in the DÄ…browska (2019) dataset were still in formal
  education at the time of data collection; this includes all those who
  reported being students. In other words, for at least some of the
  participants, the \texttt{EduTotal} variable confounds age and years
  spent in formal education.
\item
  Even if there is a causal effect, it may not be in the
  \textbf{direction} that we assume. For example, the vocabulary
  knowledge of adult learners of a foreign language is probably a fairly
  good predictor of how many foreign language classes these adult
  learners have attended. However, this does not mean that vocabulary
  knowledge \emph{causes} adults to attend classes. If anything, it is
  more likely to be the other way around. Whilst it seems rather obvious
  in this example, in linguistics and education, complex
  interdependencies between predictors are very common. Consider reading
  ability: the more a child reads, the better they become at reading.
  Therefore, we might conclude that spending more time reading leads to
  better reading skills. However, for some children, poor reading skills
  may actually \emph{cause} a lack of motivation and interest in reading
  in the first place. Clearly, determining causal relationships is
  tricky!
\item
  Finally, it is important to consider the possibility that even a
  statistically significant association could be entirely
  \textbf{spurious}. This happens more often than you might think, and
  it is more likely to happen with smaller datasets. Moreover, the more
  we test associations, the more likely we are to find statistically
  significant ones (see Section~\ref{sec-pHacking} on the multiple
  testing problem and Type 1 errors). When associations are very strong,
  we may be tempted to think that they are real, but this may not be the
  case. To raise awareness of this risk,
  \href{https://tylervigen.com/about-spurious-correlations}{Tyler Vigen}
  conducted correlation tests on millions of combinations of randomly
  selected variables and created a website featuring thousands of
  examples of very large and highly significant correlations that are
  all entirely spurious (e.g. Figure~\ref{fig-SpuriousCorrelation}).

  \begin{figure}[H]

  \centering{

  \pandocbounded{\includegraphics[keepaspectratio]{images/Vigen_unhealthy-air-quality-in-johnstown-pennsylvania_correlates-with_masters-degrees-awarded-in-education.png}}

  }

  \caption{\label{fig-SpuriousCorrelation}Find this spurious correlation
  (\href{https://creativecommons.org/licenses/by/4.0/}{CC BY 4.0}) and
  thousands more by Tyler Vigen at
  \url{https://tylervigen.com/spurious-correlations}.}

  \end{figure}%
\end{enumerate}

\end{tcolorbox}

\section{\texorpdfstring{\emph{t}-tests as regression over a binary
variable}{t-tests as regression over a binary variable}}\label{sec-Ttestsregression}

Let us now see how much of the variation across all receptive English
vocabulary test scores in the DÄ…browska (2019) data we can accurately
predict if we only include a single \textbf{categorical binary variable}
predictor in our model: whether the \texttt{Vocab} scores belong to L1
or L2 speakers of English. Hence, in the following model, we keep the
same outcome variable (\texttt{Vocab}), but now we try to predict it
using the \texttt{Group} variable as our single predictor.

We know that, on average, L1 speakers score better on the \texttt{Vocab}
test than L2 speakers. However, as illustrated in
Figure~\ref{fig-IQL1L2}, we also know that there is quite a bit of
variability around the mean scores of the L1 and L2 speakers.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{group\_means }\OtherTok{\textless{}{-}}\NormalTok{ Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{group\_by}\NormalTok{(Group) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{mean\_vocab =} \FunctionTok{mean}\NormalTok{(Vocab))}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ Dabrowska.data, }
       \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Group, }
           \AttributeTok{y =}\NormalTok{ Vocab)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{width =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{stat\_summary}\NormalTok{(}
    \AttributeTok{fun =}\NormalTok{ mean,                     }\CommentTok{\# what to plot}
    \AttributeTok{geom =} \StringTok{"point"}\NormalTok{,                 }\CommentTok{\# as a point}
    \AttributeTok{shape =} \DecValTok{18}\NormalTok{,                     }\CommentTok{\# in a diamond shape}
    \AttributeTok{size =} \DecValTok{4}\NormalTok{,                       }\CommentTok{\# a little larger than the default}
    \AttributeTok{colour =} \StringTok{"blue"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}
    \AttributeTok{data =}\NormalTok{ group\_means,                   }
    \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{as.numeric}\NormalTok{(Group), }
        \AttributeTok{y =}\NormalTok{ mean\_vocab),}
    \AttributeTok{colour =} \StringTok{"blue"}\NormalTok{,}
    \AttributeTok{linewidth =} \DecValTok{1}\NormalTok{,}
    \AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_text}\NormalTok{(}\AttributeTok{data =}\NormalTok{ group\_means,}
    \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{as.numeric}\NormalTok{(Group), }
        \AttributeTok{y =}\NormalTok{ mean\_vocab,}
        \AttributeTok{label =} \FunctionTok{sprintf}\NormalTok{(}\StringTok{"\%.2f"}\NormalTok{, mean\_vocab)), }\CommentTok{\# print the means to two decimal points}
    \AttributeTok{vjust =} \SpecialCharTok{{-}}\FloatTok{1.4}\NormalTok{,}
    \AttributeTok{colour =} \StringTok{"purple"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \ConstantTok{NULL}\NormalTok{,}
    \AttributeTok{y =} \StringTok{"Non{-}verbal IQ (Blocks) test"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{(}\AttributeTok{base\_size =} \DecValTok{14}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{12_SimpleLinearRegression_files/figure-pdf/fig-IQL1L2-1.pdf}}

}

\caption{\label{fig-IQL1L2}Comparison of non-verbal IQ (Blocks) test
scores between L1 and L2 groups}

\end{figure}%

A \emph{t}-test shows that the mean difference between L1 and L2
speakers is statistically significant (\emph{p}~=~\texttt{7.032e-06}):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(Vocab }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Group, }
       \AttributeTok{data =}\NormalTok{ Dabrowska.data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Welch Two Sample t-test

data:  Vocab by Group
t = 4.6768, df = 133.83, p-value = 7.032e-06
alternative hypothesis: true difference in means between group L1 and group L2 is not equal to 0
95 percent confidence interval:
  9.425801 23.240497
sample estimates:
mean in group L1 mean in group L2 
        69.13580         52.80265 
\end{verbatim}

Cohen's \emph{d} is large (\texttt{0.77}) and the 95\% confidence
interval does not go anywhere near zero:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(effectsize)}

\FunctionTok{cohens\_d}\NormalTok{(Vocab }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Group, }
         \AttributeTok{data =}\NormalTok{ Dabrowska.data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Cohen's d |       95% CI
------------------------
0.77      | [0.44, 1.09]

- Estimated using pooled SD.
\end{verbatim}

So we can be confident that the \texttt{Group} variable will be a useful
predictor to predict \texttt{Vocab} in a simple linear regression model.
We can use exactly the same formula as earlier to fit our model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model2 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ Vocab }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Group, }
             \AttributeTok{data =}\NormalTok{ Dabrowska.data)}

\FunctionTok{summary}\NormalTok{(model2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = Vocab ~ Group, data = Dabrowska.data)

Residuals:
    Min      1Q  Median      3Q     Max 
-66.136 -13.580   2.753  17.531  38.308 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   69.136      2.247  30.763  < 2e-16 ***
GroupL2      -16.333      3.440  -4.748 4.64e-06 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 21.32 on 155 degrees of freedom
Multiple R-squared:  0.127, Adjusted R-squared:  0.1213 
F-statistic: 22.54 on 1 and 155 DF,  p-value: 4.644e-06
\end{verbatim}

Let's break down the model summary:

\begin{itemize}
\item
  The \textbf{intercept coefficient estimate} is the predicted
  \texttt{Vocab} score when \texttt{Group} is at its reference level. By
  default, in \texttt{R}, the reference level of a categorical variable
  is the level that comes first alphabetically. Here, it's therefore
  `L1' for the English native speaker group. The model's estimated
  \texttt{Vocab} test score for an L1 participant is therefore
  \texttt{69.136} points. If you check Figure~\ref{fig-IQL1L2}, you will
  see that this value corresponds to the mean L1 \texttt{Vocab} score in
  our data.
\item
  The \textbf{GroupL2 coefficient} \textbf{estimate} is the estimated
  change in \texttt{Vocab} score for an L2 speaker as compared to the
  reference level of an L1 speaker. The estimate is \texttt{-16.333},
  which means that L2 speakers are predicted to score 16.333 points
  \emph{lower} than L1 speakers on this test. If you subtract this
  \texttt{GroupL2} coefficient estimate from the \texttt{Intercept}, you
  will find that it corresponds to the mean L2 \texttt{Vocab} score in
  our data:

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{69.136} \SpecialCharTok{{-}} \FloatTok{16.333}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 52.803
\end{verbatim}
\item
  The \textbf{\emph{p}-value} associated with the coefficient
  \texttt{GroupL2} is extremely small (\texttt{4.64e-06}). It informs us
  that \texttt{Group} is a statistically significant predictor of
  receptive English vocabulary knowledge based on our data.
\item
  The \textbf{adjusted R-squared} value indicates the proportion of
  variation in \texttt{Vocab} scores that the model can account for when
  we know a participant's native-speaker status: 12.13\%
  (\texttt{0.1213}). This value is not particularly impressive. However,
  this is not surprising given that our model attempts to predict
  vocabulary scores based solely on whether someone is an L1 or L2
  speaker of English. With only this information, the model can only
  predict that L1 speakers will achieve the mean L1 score and L2
  speakers the mean L2 score of the sample data.
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{It's (still) all linear regression!}, titlerule=0mm, leftrule=.75mm]

Here, too, it is worth noticing that fitting a simple linear regression
model with a single binary predictor is --- under the hood --- exactly
the same thing as conducting an \textbf{independent} \textbf{two-sample
\emph{t}-test}. Compare the \textbf{\emph{t}-statistic} of the
\emph{t}-test that we computed earlier with that of the
\emph{t}-statistic of the \texttt{GroupL2} coefficient reported in the
summary of our second model. If we ignore the signs of the
\emph{t}-values, we can see that they are almost identical!

Also, notice how the \textbf{\emph{p}-value} computed by the
\texttt{t.test()} function and that reported for our \texttt{GroupL2}
coefficient are almost identical. They both correspond to values that
are extremely close to zero.

The very small differences between these values are due to the default
correction that the \texttt{t.test()} function in \texttt{R} makes for
unequal group variances (see Section~\ref{sec-Variability}). If we
switch off this correction, both the \emph{t}-statistic and the
\emph{p}-value are exactly the same as those reported in the summary of
our second linear model above:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(Vocab }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Group, }
       \AttributeTok{data =}\NormalTok{ Dabrowska.data,}
       \AttributeTok{var.equal =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Two Sample t-test

data:  Vocab by Group
t = 4.7478, df = 155, p-value = 4.644e-06
alternative hypothesis: true difference in means between group L1 and group L2 is not equal to 0
95 percent confidence interval:
  9.537477 23.128821
sample estimates:
mean in group L1 mean in group L2 
        69.13580         52.80265 
\end{verbatim}

\end{tcolorbox}

\section{Regressing over a categorical predictor with more than two
levels}\label{sec-Categoricalpredictor}

The beauty of the statistical modelling approach --- as opposed to the
testing approach --- is that we can include all kinds of predictors in
our models using a single function, \texttt{lm()}, and \texttt{R}'s
formula syntax. So far, we have seen that predictors can be numeric
variables (e.g.~\texttt{EduTotal}) and categorical binary variables
(e.g.~\texttt{Group}). In this section, we see how a categorical
variable with more than two levels can be entered in a linear regression
model.

To demonstrate this, we will now attempt to predict participants'
\texttt{Vocab} scores based on their occupational group
(\texttt{OccupGroup}), which can be one of four broad categories (see
Section~\ref{sec-geoms}):

\begin{quote}
\textbf{C}: Clerical positions

\textbf{I}: Occupationally inactive (i.e.~unemployed, retired, or
homemakers)

\textbf{M}: Manual jobs

\textbf{PS}: Professional-level jobs or studying for a degree
\end{quote}

According to the descriptive statistics visualised in
Figure~\ref{fig-VocabByOccupation}, it looks like participants'
professional occupation group is unlikely to be terribly useful to
predict their vocabulary knowledge. Nonetheless, we can see that ---
perhaps somewhat counter-intuitively --- so-called ``occupationally
inactive'' participants (``I'') tend to score higher than the other
participants.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} 
           \FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ Vocab, }
               \AttributeTok{x =}\NormalTok{ OccupGroup,}
               \AttributeTok{fill =}\NormalTok{ OccupGroup,}
               \AttributeTok{colour =}\NormalTok{ OccupGroup)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.3}\NormalTok{, }
               \AttributeTok{outliers =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# If the outliers are plotted as part of the boxplot, these data points will be duplicated by the geom\_jitter() function.}
  \FunctionTok{geom\_jitter}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.8}\NormalTok{, }
              \AttributeTok{width =} \FloatTok{0.2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_viridis\_d}\NormalTok{(}\AttributeTok{guide =} \StringTok{"none"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_viridis\_d}\NormalTok{(}\AttributeTok{guide =} \StringTok{"none"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{12_SimpleLinearRegression_files/figure-pdf/fig-VocabByOccupation-1.pdf}}

}

\caption{\label{fig-VocabByOccupation}Distribution of vocabulary test
scores across four occupational groups}

\end{figure}%

So, let's compute a simple linear model with \texttt{OccupGroup} as a
predictor of \texttt{Vocab} and examine its summary:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model3 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ Vocab }\SpecialCharTok{\textasciitilde{}}\NormalTok{ OccupGroup, }
             \AttributeTok{data =}\NormalTok{ Dabrowska.data)}

\FunctionTok{summary}\NormalTok{(model3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = Vocab ~ OccupGroup, data = Dabrowska.data)

Residuals:
    Min      1Q  Median      3Q     Max 
-74.406 -13.504   3.372  16.705  34.688 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept)    63.333      3.867  16.379   <2e-16 ***
OccupGroupI    12.393      5.775   2.146   0.0335 *  
OccupGroupM    -9.133      5.160  -1.770   0.0787 .  
OccupGroupPS   -2.261      4.817  -0.469   0.6395    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 21.87 on 153 degrees of freedom
Multiple R-squared:  0.09288,   Adjusted R-squared:  0.07509 
F-statistic: 5.222 on 3 and 153 DF,  p-value: 0.001849
\end{verbatim}

\begin{itemize}
\item
  The estimate of the \textbf{intercept} corresponds to the reference
  level of the \texttt{OccupGroup} variable. Remember that the
  \textbf{reference level} is always the first level. We can check the
  order of the levels in any factor variable using the \texttt{levels()}
  function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{levels}\NormalTok{(Dabrowska.data}\SpecialCharTok{$}\NormalTok{OccupGroup)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "C"  "I"  "M"  "PS"
\end{verbatim}

  By default, the levels are ordered alphabetically. In the
  \texttt{OccupGroup} variable, the first level is ``C'', which
  corresponds to a clerical position. This means that our third model
  predicts that English speakers in a clerical position will score
  \texttt{63.333} points on the \texttt{Vocab} test. Those that belong
  to the ``inactive'' group (``I'') will score \texttt{12.393}
  \emph{more} points than those, i.e.:

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{63.333} \SpecialCharTok{+} \FloatTok{12.393}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 75.726
\end{verbatim}

  Participants who have manual jobs (``M'') are predicted to score
  \texttt{-9.133} points \emph{fewer} than those in clerical positions,
  and so-called ``professionals'' (``PS'') will do slightly worse than
  clerks (\texttt{-2.261} points). Compare these predicted values to
  those observed in the data shown in
  Figure~\ref{fig-VocabByOccupation}.
\item
  The \textbf{adjusted R\textsuperscript{2} value} for our model is
  relatively low (\texttt{0.07509}). This means that our model accounts
  for about 7.5\% of the total variance in \texttt{Vocab} scores across
  the dataset. Given that the R\textsuperscript{2} of \texttt{model1}
  was \texttt{0.1213}, this indicates that \texttt{OccupGroup} is a
  considerably less useful predictor variable to help us predict
  participants' \texttt{Vocab} scores than whether or not they are an L1
  speaker (\texttt{Group}).
\item
  What's more, the model summary reveals that only one of the
  \texttt{model3}'s three coefficient estimates is statistically
  significantly different from 0 at the Î±-level of 0.05:
  \texttt{OccupGroupI} with a \textbf{\emph{p}-value} of
  \texttt{0.0335}. This means that we can reject the null hypothesis
  that there is no difference in \texttt{Vocab} scores between speakers
  of the occupational group ``C'' (the reference level) and those of the
  occupational group ``I''. For the other two occupational groups, we do
  not have enough evidence to reject the null hypothesis of no
  difference compared to the reference level ``C''.
\end{itemize}

We can display the predicted \texttt{Vocab} scores for each occupational
group, together with a 95\% confidence interval around these predicted
values using the \texttt{emmeans()} function from the
\href{https://rvlenth.github.io/emmeans/index.html}{\{emmeans\}} package
(Lenth 2025):

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#install.packages("emmeans")}
\FunctionTok{library}\NormalTok{(emmeans)}

\FunctionTok{emmeans}\NormalTok{(model3, }\SpecialCharTok{\textasciitilde{}}\NormalTok{ OccupGroup)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 OccupGroup emmean   SE  df lower.CL upper.CL
 C            63.3 3.87 153     55.7     71.0
 I            75.7 4.29 153     67.3     84.2
 M            54.2 3.42 153     47.5     60.9
 PS           61.1 2.87 153     55.4     66.7

Confidence level used: 0.95 
\end{verbatim}

The \href{https://pbreheny.github.io/visreg/index.html}{\{visreg\}}
package (Breheny \& Burchett 2017) provides an efficient way of
visualising model predictions and residuals. In
Figure~\ref{fig-PredictedVocabByOccupation} we visualise the model's
predicted values (as blue lines) and the confidence intervals output by
the \texttt{emmeans()} function (as grey bands), together with the
model's residuals (as grey dots).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(visreg)}

\FunctionTok{visreg}\NormalTok{(model3, }\AttributeTok{gg =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Occupational groups"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Predicted Vocab scores"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{12_SimpleLinearRegression_files/figure-pdf/fig-PredictedVocabByOccupation-1.pdf}}

}

\caption{\label{fig-PredictedVocabByOccupation}Distribution of predicted
vocabulary test scores across four occupational groups}

\end{figure}%

When we use the argument ``gg = TRUE'', the \texttt{visreg()} function
outputs a ggplot object that we can then manipulate and customise just
like any other ggplot object (see Chapter~\ref{sec-DataViz}).

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{What about ANOVAs?}, titlerule=0mm, leftrule=.75mm]

You may come across the type of simple linear regression model that we
have just covered, involving a continuous numeric outcome variable and a
categorical predictor with more than two levels, under the guise of a
statistical significance test, namely the \textbf{one-way ANOVA}
(ANalysis Of VAriance).

If you compare the summary of the following one-way ANOVA with that of
the third model (\texttt{model3}), you will spot some similarities.
However, it is necessary to carry out so-called post-hoc tests to get as
much information out of an ANOVA as we can get from the summary of our
simple linear regression model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{anova1 }\OtherTok{\textless{}{-}} \FunctionTok{aov}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ Vocab }\SpecialCharTok{\textasciitilde{}}\NormalTok{ OccupGroup, }
    \AttributeTok{data =}\NormalTok{ Dabrowska.data)}

\FunctionTok{summary}\NormalTok{(anova1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
             Df Sum Sq Mean Sq F value  Pr(>F)   
OccupGroup    3   7495  2498.5   5.222 0.00185 **
Residuals   153  73205   478.5                   
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\end{tcolorbox}

\section{Regression assumptions}\label{sec-AssumptionsLR}

Similar to the statistical tests that we conducted in
Chapter~\ref{sec-Inferential}, linear regression also has a number of
assumptions that should be met if we want to obtain reliable results
that we can trust. It is therefore very important that we check that
these assumptions are met.

\subsection{Assumption 1: Independence}\label{sec-IndependenceSLR}

This is the same assumption as for the statistical tests that we covered
in Chapter~\ref{sec-Inferential} (see Section~\ref{sec-Independence}).
It means that each observation in our dataset should be unrelated to
every other observation. In other words, knowing the value of one data
point shouldn't give us any information about any other data point in
our dataset. This assumption is critical because violations can lead to
unjustifiably low \emph{p}-values and narrower confidence intervals.

Independence violations arise from the way data were collected. For
example, if we test the same person several times, their test results
are likely to be more similar to each other than to the results of other
participants. Similarly, if we collect data over time, today's
measurement might be influenced by yesterday's value. When pupils are
sampled from different schools, pupils within the same school or class
might be more similar to each other than to pupils from other schools
and/or classes.

If our data do not meet the assumption of independence, we need to use a
different analysis method that can account for the fact that our data
points are related to each other. In the language sciences, this is most
commonly achieved using \textbf{mixed-effects models}. In this textbook,
however, we only cover \textbf{fixed-effects} \textbf{models} for which
the assumption of independence must be held.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Further reading ðŸ“š}, titlerule=0mm, leftrule=.75mm]

Here are some great starting points (in alphabetical order) to learn
about mixed-effects models for linguistics research and how to fit them
in \texttt{R}:

\begin{itemize}
\tightlist
\item
  Gries (2021): Chapter 6
\item
  Levshina (2015): Chapter 8
\item
  Winter (2020): Chapters 14-15
\end{itemize}

\end{tcolorbox}

\subsection{Assumption 2: Linearity}\label{sec-LinearitySLR}

The linearity assumption requires that the relationship between the
numeric predictor variables entered into a model and the outcome
variable follows a straight (i.e.~linear) line rather than a curved one.
This assumption is fundamental because linear regression, as the name
suggests, can only ever fit a straight line through the data. If the
true relationship between our predictors is curved, forcing a straight
line through the data will lead to poor predictions and misleading
conclusions.

To check this assumption, it is best to plot the predictor variable
against the outcome variable in the form of a scatter plot (see
Figure~\ref{fig-RTScatterPLot}). When examining such plots, we are
looking for rough straight-line patterns. We are not expecting all the
same data points to fall on the linear regression line (that would be
entirely unrealistic with real data). However, the points should be
scattered around an imaginary straight line without showing any obvious
curved patterns.

In Figure~\ref{fig-Nonlinear}, by contrast, the data points follow a
curved pattern. The regression line is forced to be straight, creating
systematic prediction errors. Such a pattern of errors indicates that
the assumption of linearity is violated.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{12_SimpleLinearRegression_files/figure-pdf/fig-Nonlinear-1.pdf}}

}

\caption{\label{fig-Nonlinear}}

\end{figure}%

\subsection{Assumption 3: Homogeneity of residuals}\label{sec-Residuals}

Linear regression models assume homogeneity --- or constant variance ---
of the residuals (see Section~\ref{sec-Homoscedasticity}). This
assumption is best checked by visually examining the model residuals. In
Figure~\ref{fig-VocabResidual}, we plot the residuals of \texttt{model1}
and check that their variability does not systematically increase or
decrease as the outcome variable increases or decreases.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a data frame containing the model\textquotesingle{}s residuals and predictions:}
\NormalTok{residual\_data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{fitted\_values =} \FunctionTok{fitted}\NormalTok{(model1),}
  \AttributeTok{residuals =} \FunctionTok{residuals}\NormalTok{(model1)}
\NormalTok{)}

\CommentTok{\# Create plot of fitted values vs. residuals:}
\FunctionTok{ggplot}\NormalTok{(residual\_data, }
       \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ fitted\_values, }\AttributeTok{y =}\NormalTok{ residuals)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.6}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{, }\AttributeTok{color =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{linewidth =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Predicted Vocab scores (fitted values)"}\NormalTok{, }
       \AttributeTok{y =} \StringTok{"Model residuals"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{12_SimpleLinearRegression_files/figure-pdf/fig-VocabResidual-1.pdf}}

}

\caption{\label{fig-VocabResidual}Comparison of model residuals with
predicted vocabulary scores}

\end{figure}%

In Figure~\ref{fig-VocabResidual}, we can see that the residuals are not
randomly spread across both sides of the blue line, but rather that they
form a funnel-like shape: the spread of residuals tends to be larger for
low predicted \texttt{Vocab} scores and becomes progressively smaller
for higher predicted scores. This pattern indicates that our model is
more accurate when it comes to predicting high \texttt{Vocab} scores
than lower ones. This constitutes a violation of the assumption of equal
variance or homoscedasticity.

\subsection{Assumption 4: Normality of
residuals}\label{sec-NormalityResiduals}

Ideally, model residuals also ought to be normally distributed, with a
mean of zero. However, this assumption becomes less important as the
sample size increases (Williams, Grajales \& Kurkiewicz 2013: 10).
Again, it is best to check the distribution of residuals visually. As
shown in Figure~\ref{fig-VocabResidualDensity}, the residuals of
\texttt{model1} are fairly normally distributed.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(residual\_data, }
       \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ residuals)) }\SpecialCharTok{+}
  \FunctionTok{geom\_density}\NormalTok{(}\AttributeTok{fill =} \StringTok{"purple"}\NormalTok{,}
               \AttributeTok{alpha =} \FloatTok{0.6}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =} \DecValTok{0}\NormalTok{,}
             \AttributeTok{colour =} \StringTok{"blue"}\NormalTok{,}
             \AttributeTok{linewidth =} \FloatTok{0.8}\NormalTok{,}
             \AttributeTok{linetype =} \StringTok{"dotted"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{12_SimpleLinearRegression_files/figure-pdf/fig-VocabResidualDensity-1.pdf}}

}

\caption{\label{fig-VocabResidualDensity}Distribution of model
residuals}

\end{figure}%

Figure~\ref{fig-VocabResidualDensity} shows that the distribution of the
residuals of \texttt{model1} are slightly left-skewed, but that the
distribution is more or less centered around zero. Indeed, the model's
mean residual is almost exactly zero:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(model1}\SpecialCharTok{$}\NormalTok{residuals)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
-57.7253 -10.7158   0.5255   0.0000  12.4168  36.0334 
\end{verbatim}

When the sample size is small and the model residuals are not normally
distributed, the inferences that we can draw from the model are less
trustworthy. However, with larger sample sizes, regression is relatively
robust to the assumption of normally distributed residuals (Williams,
Grajales \& Kurkiewicz 2013: 3).

\section{Intermediary summary}\label{sec-IntSummary}

In this chapter, we fitted simple linear regression models. These models
demonstrate that we can -- to a greater or lesser degree -- predict (or
\textbf{model}) participants' receptive vocabulary knowledge test scores
on the basis of a single predictor such as the number of years that they
spent in formal education or their occupational group.

We have referred to the variable that we are trying to predict as the
outcome variable. It is worth knowing that it is sometimes also called
the \textbf{dependent variable} because we are attempting to model its
`dependence' on one or more \textbf{independent variables} (which we
have called predictors). In Chapter~\ref{sec-MLR}, we will continue to
use the terminology of \textbf{outcome} and \textbf{predictor}
variables, but you should be aware that some textbooks and researchers
will speak of dependent and independent variables, instead.\footnote{This
  choice of terminology is largely a personal one but, in my experience,
  learners find this terminology less confusing as it is immediately
  obvious what is being predicted (the outcome or dependent variable)
  and what is used to make the prediction (the predictors or the
  independent variables). Another advantage is that the term predictor
  can be used by itself (i.e.~without the word ``variable''), which is
  handy because when we enter a categorical variable into a model, there
  are as many predictors as there are variable levels.}

While our simple linear regression models did allow us to account for
some of the variance in participants' \texttt{Vocab} test scores, like
the statistical tests that we conducted in
Chapter~\ref{sec-Inferential}, they still only allow us to capture one
aspect of receptive vocabulary knowledge at a time. What we really want
to do is to be able to enter several predictor variables into a single
model. For example, it would be interesting to know whether the number
of years spent in formal education remains a significant predictor of
\texttt{Vocab} scores when controlling for participants' occupational
groups. This can be achieved with multiple linear regression, and that's
coming up in the next chapter!

\subsection*{Check your progress ðŸŒŸ}\label{check-your-progress-11}
\addcontentsline{toc}{subsection}{Check your progress ðŸŒŸ}

Well done! You have successfully completed this chapter introducing the
linear regression modelling. You have answered { out of 14 questions}
correctly.

Are you confident that you can\ldots?

\begin{itemize}
\tightlist
\item[$\square$]
  Fit a simple linear model to predict a numeric outcome variable
\item[$\square$]
  Visualise the predictions of a simple linear regression model
\item[$\square$]
  Find out what the intercept of a model corresponds to and interpret
  the model's coefficient estimates
\item[$\square$]
  Interpret a model's adjusted R\textsuperscript{2} coefficient
\item[$\square$]
  Check for the main assumptions of linear regression: independence,
  linearity, homoscedasticity, and normality of residuals
  (Section~\ref{sec-AssumptionsLR})
\end{itemize}

\bookmarksetup{startatroot}

\chapter{\texorpdfstring{Multiple linear reg\texttt{R}ession
modelling}{Multiple linear regRession modelling}}\label{sec-MLR}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-warning-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-warning-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Warning}, titlerule=0mm, leftrule=.75mm]

As with the rest of this textbook (see
\href{https://elenlefoll.github.io/RstatsTextbook/}{Preface}), this
chapter is very much \textbf{work in progress}. Feedback is very
welcome.

\end{tcolorbox}

\subsection*{Chapter overview}\label{chapter-overview-11}
\addcontentsline{toc}{subsection}{Chapter overview}

In this chapter, you will learn how to:

\begin{itemize}
\tightlist
\item
  Fit a linear regression model with multiple predictors
\item
  Center numeric predictor variables when meaningful
\item
  Interpret the coefficient estimates of a multiple linear regression
  model
\item
  Interpret a model's coefficients of determination (multiple
  R\textsuperscript{2} and adjusted R\textsuperscript{2})
\item
  Compute and plot the relative importance of predictors
\item
  Fit interaction effects and interpret their coefficient estimates
\item
  Visualise the predictions of a multiple linear regression model and
  its (partial) residuals
\item
  Report the results of a multiple linear regression model in tabular
  and graphical formats
\item
  Check that the assumptions of multiple linear regression models are
  met.
\end{itemize}

\section{From simple to multiple linear regression
models}\label{from-simple-to-multiple-linear-regression-models}

In Chapter~\ref{sec-SLR}, we used the \texttt{lm()} function to fit
simple linear regression models. We saw that, like the \texttt{t.test()}
and the \texttt{cor.test()} functions, the \texttt{lm()} function takes
a formula as its first argument. Schematically, the formula syntax for a
simple linear regression model takes the form of:

\begin{verbatim}
outcome.variable ~ predictor
\end{verbatim}

In the second half of this chapter, we will continue to try to predict
(i.e.~try to better understand) \texttt{Vocab} scores among L1 and L2
English speakers, but this time, we will do so with \emph{multiple}
predictors. To this end, we will use the \texttt{+} operator to add
predictors to our model formula like this:

\begin{verbatim}
outcome.variable ~ predictor1 + predictor2 + predictor3
\end{verbatim}

A linear regression model can include as many predictors as we like --
or rather, as is meaningful and we have data for! The predictors can be
a mixture of numeric and categorical predictors. Multiple linear
regression modelling is much more powerful than conducting individual
statistical tests (as in Chapter~\ref{sec-Inferential}) or several
simple linear regression models (as in Chapter~\ref{sec-SLR}) because it
enables us to quantify the strength of the association between an
outcome variable and a predictor, while \textbf{controlling for the
other predictors} -- in other words, while holding all the other
predictors constant. Moreover, it also reduces the risk of reporting
false positive results (i.e.~Type 1 error, see
Section~\ref{sec-pHacking}). In this chapter, we will see that we can
learn much more about our data from a single multiple linear regression
model than from a series of individual statistical tests or simple
regression models.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-warning-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-warning-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Prerequisites}, titlerule=0mm, leftrule=.75mm]

As with previous chapters, all the examples, tasks, and quiz questions
from this chapter are based on data from:

\begin{quote}
DÄ…browska, Ewa. 2019. Experience, Aptitude, and Individual Differences
in Linguistic Attainment: A Comparison of Native and Nonnative Speakers.
Language Learning 69(S1). 72--100.
\url{https://doi.org/10.1111/lang.12323}.
\end{quote}

Our starting point for this chapter is the wrangled combined dataset
that we created and saved in Chapter~\ref{sec-DataWrangling}. Follow the
instructions in Section~\ref{sec-filter} to create this \texttt{R}
object.

Alternatively, you can download \texttt{Dabrowska2019.zip} from
\href{https://github.com/elenlefoll/RstatsTextbook/blob/main/Dabrowska2019.zip}{the
textbook's GitHub repository}. To launch the project correctly, unzip
the file and then double-click on the \texttt{Dabrowska2019.Rproj} file.

To begin, load the \texttt{combined\_L1\_L2\_data.rds} file that we
created in Chapter~\ref{sec-DataWrangling}. This file contains the full
data of all the L1 and L2 participants from DÄ…browska (2019). The
categorical variables are stored as factors, and obvious data entry
inconsistencies and typos have been corrected.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(here)}

\NormalTok{Dabrowska.data }\OtherTok{\textless{}{-}} \FunctionTok{readRDS}\NormalTok{(}\AttributeTok{file =} \FunctionTok{here}\NormalTok{(}\StringTok{"data"}\NormalTok{, }\StringTok{"processed"}\NormalTok{, }\StringTok{"combined\_L1\_L2\_data.rds"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Before you get started, check that you have correctly imported the data
by examining the output of \texttt{View(Dabrowska.data)} and
\texttt{str(Dabrowska.data)}. In addition, run the following lines of
code to load the \{tidyverse\} and create ``clean'' versions of both the
L1 and L2 datasets as separate \texttt{R} objects.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}

\NormalTok{L1.data }\OtherTok{\textless{}{-}}\NormalTok{ Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(Group }\SpecialCharTok{==} \StringTok{"L1"}\NormalTok{)}

\NormalTok{L2.data }\OtherTok{\textless{}{-}}\NormalTok{ Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(Group }\SpecialCharTok{==} \StringTok{"L2"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Once you are satisfied that the data have been correctly imported and
that you are familiar with the dataset, you are ready to tap into the
potential of multiple linear regression modelling! ðŸš€

\end{tcolorbox}

\section{Combining multiple predictors}\label{sec-MultipleLM}

In the following, we will attempt to model the variability in the
receptive English vocabulary test scores of L1 and L2 English
participants from DÄ…browska (2019). To this end, we will use multiple
numeric and categorical predictors:

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  participants' native-speaker status (\texttt{Group})
\item
  their \texttt{Age},
\item
  their occupational group (\texttt{OccupGroup}),
\item
  their \texttt{Gender}
\item
  their non-verbal IQ test score (\texttt{Blocks}), and
\item
  the number of years they were in formal education (\texttt{EduTotal}).
\end{enumerate}

We use the \texttt{+} operator to construct the model formula. It
doesn't matter in which order we list the predictors, as the model will
consider them all simultaneously.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model4 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Vocab }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Group }\SpecialCharTok{+}\NormalTok{ Age }\SpecialCharTok{+}\NormalTok{ OccupGroup }\SpecialCharTok{+}\NormalTok{ Gender }\SpecialCharTok{+}\NormalTok{ Blocks }\SpecialCharTok{+}\NormalTok{ EduTotal, }
             \AttributeTok{data =}\NormalTok{ Dabrowska.data)}
\end{Highlighting}
\end{Shaded}

We can then examine the model summary just like we did in
Chapter~\ref{sec-SLR} using the \texttt{summary()} function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(model4)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = Vocab ~ Group + Age + OccupGroup + Gender + Blocks + 
    EduTotal, data = Dabrowska.data)

Residuals:
    Min      1Q  Median      3Q     Max 
-62.825 -10.838   1.831  12.185  38.345 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept)    4.2113    10.7831   0.391 0.696691    
GroupL2      -22.2896     3.4854  -6.395 1.98e-09 ***
Age            0.3718     0.1401   2.654 0.008812 ** 
OccupGroupI    9.9519     5.5999   1.777 0.077600 .  
OccupGroupM   -3.9993     4.5139  -0.886 0.377050    
OccupGroupPS  -1.0629     4.3876  -0.242 0.808919    
GenderM       -3.7177     3.1387  -1.184 0.238131    
Blocks         1.3011     0.3218   4.043 8.44e-05 ***
EduTotal       2.4308     0.6293   3.863 0.000167 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 18.65 on 148 degrees of freedom
Multiple R-squared:  0.3621,    Adjusted R-squared:  0.3276 
F-statistic:  10.5 on 8 and 148 DF,  p-value: 1.327e-11
\end{verbatim}

As with the simple linear regression models, we begin our interpretation
of the model summary with the coefficient estimate for the
\textbf{intercept} (see Section~\ref{sec-Ttestsregression}). The first
question we ask ourselves is:

\begin{itemize}
\tightlist
\item
  In this model, what does the intercept correspond to?
\end{itemize}

Remember that the reference levels of \textbf{categorical predictors}
correspond to the \textbf{first level} of these variables. This is why,
here, the coefficient estimate for the intercept corresponds to a female
English native speaker with a clerical occupation:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{levels}\NormalTok{(Dabrowska.data}\SpecialCharTok{$}\NormalTok{Gender) }\CommentTok{\# \textquotesingle{}Female\textquotesingle{} is the first level.}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "F" "M"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{levels}\NormalTok{(Dabrowska.data}\SpecialCharTok{$}\NormalTok{Group) }\CommentTok{\# \textquotesingle{}L1\textquotesingle{} is the first level.}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "L1" "L2"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{levels}\NormalTok{(Dabrowska.data}\SpecialCharTok{$}\NormalTok{OccupGroup) }\CommentTok{\# \textquotesingle{}C\textquotesingle{} corresponding to a clerical professional occupation is the first level.}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "C"  "I"  "M"  "PS"
\end{verbatim}

The reference level of the \textbf{numeric predictors}, by contrast,
corresponds to the value of~\textbf{zero}. In \texttt{model4},
therefore, the estimated coefficient for the intercept corresponds to
the predicted \texttt{Vocab} score of an adult English native speaker
who belongs to the occupational group ``C'', is female, who is aged 0
(!), scored 0 on the Blocks test, and spent 0 years (!) in formal
education. Needless to say that trying to interpret this value is
utterly meaningless! This is why, in this case, it makes sense to
\textbf{center} our numeric predictor variables before entering them
into our model. Another way to go about this would be to standardise the
predictors using a \emph{z}-transformation (see e.g. Winter 2020:
Section 5.2).

\section{Centering numeric
predictors}\label{centering-numeric-predictors}

Centering involves subtracting a variable's average from each value in
the variable. Typically, we subtract the mean from each value but, given
that we saw that many of the numeric variables in our dataset are not at
all normally distributed (see Section~\ref{sec-DistributionsNumeric}),
here, we will subtract the median instead.

To this end, we use the \texttt{mutate()} function to add three columns
to the \texttt{R} data object \texttt{Dabrowska.data}. These new columns
contain \textbf{transformed} versions of the predictor variables that we
previously entered into our model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dabrowska.data }\OtherTok{\textless{}{-}}\NormalTok{ Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Age\_c =}\NormalTok{ Age }\SpecialCharTok{{-}} \FunctionTok{median}\NormalTok{(Age),}
         \AttributeTok{Blocks\_c =}\NormalTok{ Blocks }\SpecialCharTok{{-}} \FunctionTok{median}\NormalTok{(Blocks),}
         \AttributeTok{EduTotal\_c =}\NormalTok{ EduTotal }\SpecialCharTok{{-}} \FunctionTok{median}\NormalTok{(EduTotal))}
\end{Highlighting}
\end{Shaded}

We have centred the values of the three numeric predictor variables so
that a value of zero in the transformed version corresponds to the
variable's original median value (see Table~\ref{tbl-Untransformed}).

For each pair of variables, the value of~0 in the centered variable
corresponds to the median value of the untransformed variable. As a
result, in the centered variables, each data point is expressed in terms
of how much it is either above the median (positive score) or below the
median (negative score).

\section{Interpreting a model
summary}\label{interpreting-a-model-summary}

We can now fit a new multiple linear regression model that attempts to
predict \texttt{Vocab} scores with the same predictors as
\texttt{model4} above, except that we are now entering the centered
numeric predictor variables instead of the original untransformed ones.
The categorical predictor variables remain the same.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model4\_c }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Vocab }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Group }\SpecialCharTok{+}\NormalTok{ Age\_c }\SpecialCharTok{+}\NormalTok{ OccupGroup }\SpecialCharTok{+}\NormalTok{ Gender }\SpecialCharTok{+}\NormalTok{ Blocks\_c }\SpecialCharTok{+}\NormalTok{ EduTotal\_c, }
             \AttributeTok{data =}\NormalTok{ Dabrowska.data)}

\FunctionTok{summary}\NormalTok{(model4\_c)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = Vocab ~ Group + Age_c + OccupGroup + Gender + Blocks_c + 
    EduTotal_c, data = Dabrowska.data)

Residuals:
    Min      1Q  Median      3Q     Max 
-62.825 -10.838   1.831  12.185  38.345 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept)   70.5852     3.7045  19.054  < 2e-16 ***
GroupL2      -22.2896     3.4854  -6.395 1.98e-09 ***
Age_c          0.3718     0.1401   2.654 0.008812 ** 
OccupGroupI    9.9519     5.5999   1.777 0.077600 .  
OccupGroupM   -3.9993     4.5139  -0.886 0.377050    
OccupGroupPS  -1.0629     4.3876  -0.242 0.808919    
GenderM       -3.7177     3.1387  -1.184 0.238131    
Blocks_c       1.3011     0.3218   4.043 8.44e-05 ***
EduTotal_c     2.4308     0.6293   3.863 0.000167 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 18.65 on 148 degrees of freedom
Multiple R-squared:  0.3621,    Adjusted R-squared:  0.3276 
F-statistic:  10.5 on 8 and 148 DF,  p-value: 1.327e-11
\end{verbatim}

If you compare the summary of \texttt{model4} with that of
\texttt{model4\_c}, you will notice that, whilst the intercept
coefficient estimate has changed, all the other coefficient estimates
have stayed the same.

Let's decipher the summary of \texttt{model4\_c} step-by-step:

\begin{itemize}
\item
  In this model, the estimated coefficient for the \textbf{intercept}
  corresponds to the predicted \texttt{Vocab} score of an English native
  speaker (\texttt{Group}~=~L1) who is aged 31 (the median
  \texttt{Age}), belongs to the occupational group \texttt{C}, is
  female, scored 16 on the Blocks test (the median \texttt{Blocks}
  score), and was in formal education for 14 years (the median number of
  years).
\item
  As with the simple linear regression models that we computed in
  Chapter~\ref{sec-SLR}, the \textbf{coefficient estimates of the
  numeric predictors} (\texttt{Age\_c}, \texttt{Blocks\_c}, and
  \texttt{EduTotal\_c}) correspond to increases or decreases in
  \texttt{Vocab} scores for each additional unit of that predictor
  variable, whilst keeping all other predictors at their reference
  level. Remember that, for numeric predictors, the reference level is
  0, which, given that we have centered them, corresponds to the
  variable's median value. Looking at the coefficient estimate for
  \texttt{Blocks\_c} (\texttt{1.3011}), this means that someone who
  scored one point more than the median score in the Blocks test is
  predicted to have a \texttt{Vocab} test result that is \texttt{1.3011}
  points higher than the intercept, namely:

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{70.5852} \SpecialCharTok{+} \FloatTok{1.3011}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 71.8863
\end{verbatim}
\item
  To calculate the predicted \texttt{Vocab} score of an L1 female
  speaker in a clerical occupation, of median age, who spent the median
  number of years in formal education, but scored an impressive 26
  points on the Blocks test, we multiply the estimated
  \texttt{Blocks\_c} coefficient (\texttt{1.3011}) by 26 minus the
  median Blocks score (16) and add this to the intercept coefficient:

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{70.5852} \SpecialCharTok{+} \FloatTok{1.3011}\SpecialCharTok{*}\NormalTok{(}\DecValTok{26} \SpecialCharTok{{-}} \FunctionTok{median}\NormalTok{(Dabrowska.data}\SpecialCharTok{$}\NormalTok{Blocks))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 83.5962
\end{verbatim}
\item
  You'll be pleased to read that the interpretation of
  \textbf{coefficient estimates of categorical predictors} is less
  involved. Recall that, for categorical variables, the reference level
  is always the variable's first level (see
  Section~\ref{sec-MultipleLM}). If we want to make a prediction for an
  L2 instead of an L1 female speaker, but keep all other predictors at
  the reference level (i.e.~clerical occupation, median age, median
  number of years in formal education, and median Blocks score), all we
  need to do is add the coefficient estimate for \texttt{GroupL2}
  (\texttt{-22.2896}) to the intercept coefficient. Given that this
  coefficient is negative, this addition will result in a predicted
  \texttt{Vocab} score that is lower than the model's reference level:

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{70.5852} \SpecialCharTok{+} \SpecialCharTok{{-}}\FloatTok{22.2896}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 48.2956
\end{verbatim}
\item
  Since very few people are perfectly average, let us now calculate the
  predicted \texttt{Vocab} score of an actual, randomly chosen person:
  the 154\textsuperscript{th} participant in our dataset. As shown below
  using the \{tidyverse\} function \texttt{slice()}, the
  154\textsuperscript{th} participant is a 46-year-old Polish male
  driver who scored 23 points on the Blocks test and attended formal
  education for 10 years.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice}\NormalTok{(}\DecValTok{154}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(Group, Age, NativeLg, Occupation, OccupGroup, Gender, Blocks, EduTotal)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  Group Age NativeLg Occupation OccupGroup Gender Blocks EduTotal
1    L2  46   Polish     Driver          M      M     23       10
\end{verbatim}

  To obtain the model's predicted \texttt{Vocab} score for a 46-year-old
  male L2 speaker with a manual occupation (\texttt{M}) who scored 23
  points on the \texttt{Blocks} test and was in formal education for 10
  years (\texttt{EduTotal}), we combine the coefficient estimates as
  follows:

\phantomsection\label{annotated-cell-604}%
\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{70.5852} \SpecialCharTok{+} \hspace*{\fill}\NormalTok{\circled{1}}
\SpecialCharTok{{-}}\FloatTok{22.2896} \SpecialCharTok{+} \hspace*{\fill}\NormalTok{\circled{2}}
\FloatTok{0.3718} \SpecialCharTok{*}\NormalTok{ (}\DecValTok{46} \SpecialCharTok{{-}} \FunctionTok{median}\NormalTok{(Dabrowska.data}\SpecialCharTok{$}\NormalTok{Age)) }\SpecialCharTok{+} \hspace*{\fill}\NormalTok{\circled{3}}
\SpecialCharTok{{-}}\FloatTok{3.9993} \SpecialCharTok{+} \hspace*{\fill}\NormalTok{\circled{4}}
\SpecialCharTok{{-}}\FloatTok{3.7177} \SpecialCharTok{+} \hspace*{\fill}\NormalTok{\circled{5}}
\FloatTok{1.3011} \SpecialCharTok{*}\NormalTok{ (}\DecValTok{23} \SpecialCharTok{{-}} \FunctionTok{median}\NormalTok{(Dabrowska.data}\SpecialCharTok{$}\NormalTok{Blocks)) }\SpecialCharTok{+} \hspace*{\fill}\NormalTok{\circled{6}}
\FloatTok{2.4308} \SpecialCharTok{*}\NormalTok{ (}\DecValTok{10} \SpecialCharTok{{-}} \FunctionTok{median}\NormalTok{(Dabrowska.data}\SpecialCharTok{$}\NormalTok{EduTotal)) }\hspace*{\fill}\NormalTok{\circled{7}}
\end{Highlighting}
\end{Shaded}

  \begin{description}
  \tightlist
  \item[\circled{1}]
  Intercept coefficient
  \item[\circled{2}]
  L2 speaker
  \item[\circled{3}]
  46 years old
  \item[\circled{4}]
  Manual occupation (OccupGroupM)
  \item[\circled{5}]
  Male
  \item[\circled{6}]
  23 points on Blocks test
  \item[\circled{7}]
  10 years in formal education
  \end{description}

\begin{verbatim}
[1] 45.5401
\end{verbatim}

  \begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-tip-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, titlerule=0mm, leftrule=.75mm]

  You can hover your mouse over the circled numbers to find out how this
  predicted score was calculated. All the coefficient estimates were
  copied from the model summary output by \texttt{summary(model4\_c)}.

  \end{tcolorbox}
\item
  We can check that we did the maths correctly by outputting the model's
  prediction for the 154\textsuperscript{th} observation directly. To
  this end, we apply the \texttt{predict()} function to the model object
  \texttt{model4\_c} and extract the model's predicted score for the
  154\textsuperscript{th} data point:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{predict}\NormalTok{(model4\_c)[}\DecValTok{154}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     154 
45.53989 
\end{verbatim}

  As you can see, we obtain the same predicted \texttt{Vocab} score. The
  minor difference after the decimal point is due to us using
  coefficient estimates rounded-off to four decimal places (as displayed
  in the model's summary output) rather than the exact values to which
  the \texttt{predict()} function has access.
\item
  This is all very well, but how accurate is this model prediction? To
  find out, we can compare this \emph{predicted} score (that our model
  predicts for any male 46-year old with a manual occupation, a Blocks
  score of 23, and 10 years in formal education) to the 46-year-old
  Polish driver's \emph{actual} \texttt{Vocab} score:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice}\NormalTok{(}\DecValTok{154}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(Group, Age, NativeLg, Occupation, OccupGroup, Gender, Blocks, EduTotal, Vocab)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  Group Age NativeLg Occupation OccupGroup Gender Blocks EduTotal    Vocab
1    L2  46   Polish     Driver          M      M     23       10 48.88889
\end{verbatim}
\item
  Our model prediction (\texttt{45.53989}) is close to our Polish
  driver's real \texttt{Vocab} test score (\texttt{48.88889}), but our
  model has slightly underestimated his performance. This
  underestimation results in a positive \textbf{residual}. The residual
  is positive because there are points ``left over'' by the model. Model
  residuals are calculated by subtracting a model's prediction from the
  real, observed value of the outcome variable for any specific data
  point. In our case, we substract our model's predicted \texttt{Vocab}
  score for a male 46-year old with a manual occupation, a Blocks score
  of 23, and 10 years in formal education from the Polish 46-year-old
  driver's actual \texttt{Vocab} score:

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{48.88889} \SpecialCharTok{{-}} \FloatTok{45.53989}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 3.349
\end{verbatim}

  Residuals are also stored in the model object and be accesssed like
  this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model4\_c}\SpecialCharTok{$}\NormalTok{residuals[}\DecValTok{154}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     154 
3.348998 
\end{verbatim}
\item
  We can compare this particular residual (corresponding to the
  154\textsuperscript{th} participant in the dataset) to all model
  residuals (corresponding to all participants) by examining the
  \textbf{summary statistics of all residuals}, which are displayed at
  the top of the model summary output (\texttt{summary(model4\_c)}).
  These descriptive statistics inform us that the residual for the
  154\textsuperscript{th} participant is slightly higher than the
  average (median) residual, but well within the IQR (see
  Section~\ref{sec-IQR}) of model residuals:

\begin{verbatim}
Residuals:
    Min      1Q  Median      3Q     Max 
-62.825 -10.838   1.831  12.185  38.345 
\end{verbatim}

  Looking at the minimum and maximum residuals, we can see that our
  model overestimates at least one person's \texttt{Vocab} score by 63
  points (this is the most negative residual: \texttt{Min}), whilst in
  another case it underestimates it by 38 points (this is the largest
  positive residual: \texttt{Max}).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Run the following code to find out more about the participant whose Vocab score was most dramatically underestimated by our model:}

\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice}\NormalTok{(}\FunctionTok{which.max}\NormalTok{(model4\_c}\SpecialCharTok{$}\NormalTok{residuals)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(Group, NativeLg, Age, Occupation, Gender, Blocks, EduTotal, Vocab)}
\end{Highlighting}
\end{Shaded}
\item
  The \textbf{\emph{p}-values} associated with each coefficient estimate
  in the model summary indicate which predictor variables (or predictor
  variable \emph{levels}, in the case of categorical variables) make a
  statistically significant contribution to the model. You may recall
  that, in Section~\ref{sec-Categoricalpredictor}, we fitted a simple
  linear regression model (\texttt{model3}) which included only
  \texttt{OccupGroup} as a single predictor. In this simple model, the
  coefficient estimate for \texttt{OccupGroupI} made a statistically
  significant contribution to the model at an Î±-level of 0.05
  (\emph{p}~=~0.0335). By contrast, in the present multiple linear
  regression model, the predictor level \texttt{OccupGroupI} does
  \emph{not} make a statistically significant contribution
  (\emph{p}~=~0.077600 which is greater than 0.05). This is because our
  multiple regression model \texttt{model4\_c} includes predictors that
  account for some of the same variance in \texttt{Vocab} scores in the
  data that occupational groups accounted for in our earlier model. This
  leaves less unique variance attributable to occupational group,
  thereby rendering it statistically non-significant.
\item
  From the \textbf{adjusted R-squared} value (\texttt{0.3276}) displayed
  at the bottom of the model summary output, we can see that our
  multiple linear regression model accounts for around 33\% of the total
  variance in \texttt{Vocab} scores found in the DÄ…browska (2019) data.
  This is considerably more than we achieved with any of the simple
  linear models that we fitted in Chapter~\ref{sec-SLR}.
\end{itemize}

\subsection{Interpreting model predictions}\label{sec-ModelPredictions}

Whilst it is important to understand how to interpret the coefficients
of a multiple linear regression model from the model summary, in
practice, it is also always a very good idea to visualise model
predictions. On the one hand, this reduces the risk of making any
obvious interpretation errors and, on the other, it is much easier to
interpret the residuals of a model when they are visualised alongside
model predictions.

To fully visualise the predictions of a multiple linear model, we would
need to be able to plot as many dimensions as there are predictors in
the model. The trouble is that, as humans, we find it difficult to
interpret data visualisations with more than two dimensions. Indeed,
although 3D plots can sometimes be useful (and can easily be generated
in \texttt{R}), we are much better at interpreting two-dimensional
plots. To bypass this inherent human weakness, we will plot the values
predicted by our model on several plots: one for each predictor variable
(see Figure~\ref{fig-VisregVocab}). Run the following command and then
follow the instructions displayed in the Console pane to view the plots
one by one. You may need to resize your Plot pane for the plots to be
displayed. If you have a small screen, you may find the ``ðŸ”Ž Zoom''
option in RStudio's Plots pane useful as it allows you to view the plots
in a separate window.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(visreg)}

\FunctionTok{visreg}\NormalTok{(model4\_c)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{13_MultipleLinearRegression_files/figure-pdf/fig-VisregVocab-1.pdf}}

}

\caption{\label{fig-VisregVocab}\texttt{Vocab} scores as predicted by
\texttt{model4\_c} (blue lines) as a function of various predictor
variables and partial model residuals (grey points)}

\end{figure}%

On plots produced by the \texttt{visreg()} function, as in
Figure~\ref{fig-PredictedVocabByOccupation}, the model's predicted
values are displayed as blue lines. By default, these predictions are
surrounded by 95\% confidence bands, which are visualised as grey areas.
Now that we have several predictors in our model, the points in
\texttt{visreg()} plots represent the model's \textbf{partial
residuals}. Partial residuals are the left-over variance (i.e.~the
residual) relative to the predictor that we are examining after having
subtracted off the contribution of all the other predictors in the
model.

When interpreting the numeric predictors plotted in
Figure~\ref{fig-VisregVocab} above, it is important to remember that we
entered centered numeric predictors in \texttt{model4\_c}. This means
that an \texttt{Age\_c} value of~0 corresponds to the median age in the
dataset: 31 years. This is why Figure~\ref{fig-VisregVocab} features
negative ages: the negative values correspond to participants who are
younger than 31. By contrast, positive scores correspond to participants
who are older than 31 years. This is hardly intuitive so, for the
purposes of visualising and interpreting our model, it is best to
transform these variables back to their original scale (see
Figure~\ref{fig-PredictedVocabAge}). This is achieved by adding the
following \texttt{xtrans} argument within the \texttt{visreg()}
function.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-warning-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-warning-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Warning}, titlerule=0mm, leftrule=.75mm]

Due to a bug that was introduced in the latest version of the \{visreg\}
package, it is currently necessary to install version 2.7 of the
\{visreg\} package for the \texttt{xtrans} argument to work as expected.
You can find out which package version you currently have installed, if
any, using this function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{packageVersion}\NormalTok{(}\StringTok{"visreg"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

If you have version 2.8.0, begin by deleting your current installation:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{remove.packages}\NormalTok{(}\StringTok{"visreg"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Then, install the \{remotes\} package, which allows you to install older
versions of packages (if they are compatible with your \texttt{R}
version). You will likely get a message notifying you that it is
necessary to restart your \texttt{R} session. Click ``yes''.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"remotes"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Finally, load the \{remotes\} library and then install version 2.7.0 of
the \{visreg\} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(remotes)}
\FunctionTok{install\_version}\NormalTok{(}\StringTok{"visreg"}\NormalTok{, }\StringTok{"2.7.0"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(visreg)}

\FunctionTok{visreg}\NormalTok{(model4\_c, }
       \AttributeTok{xvar =} \StringTok{"Age\_c"}\NormalTok{, }
       \AttributeTok{xtrans =} \ControlFlowTok{function}\NormalTok{(x) x }\SpecialCharTok{+} \FunctionTok{median}\NormalTok{(Dabrowska.data}\SpecialCharTok{$}\NormalTok{Age), }
       \AttributeTok{gg =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Age (in years)"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Predicted Vocab scores"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{13_MultipleLinearRegression_files/figure-pdf/fig-PredictedVocabAge-1.pdf}}

}

\caption{\label{fig-PredictedVocabAge}\texttt{Vocab} scores as predicted
by \texttt{model4\_c} for speakers of various ages (blue line) and
partial model residuals represented as points. In the version printed in
the textbook, two data points are highlighted: one representing a very
large positive residual and one representing a very small residual.}

\end{figure}%

In Figure~\ref{fig-PredictedVocabAge}, we focus on one predictor from
our model: \texttt{Age}. We know from our model summary that the
coefficient estimate for age is positive (\texttt{0.3718}), hence the
upward blue line. The grey 95\% confidence band visualises the
uncertainty around the estimated mean predicted \texttt{Vocab} scores.

The points on Figure~\ref{fig-PredictedVocabAge} represent the partial
residuals. This means that there are as many points as there are
participants in the dataset used to fit the model. The further away a
point is from the predicted value (the blue line), the poorer the
prediction is for that particular participant.

Some of the partial residuals are particularly large: the \texttt{Vocab}
score of the 23-year-old Lithuanian bar staff, for instance, is vastly
underestimated. By contrast, the 46-year-old Polish driver's predicted
score is well within the confidence band of the predicted \texttt{Vocab}
score for his age, indicating that our model makes a pretty accurate
prediction for this L2 speaker.

\section{Relative importance of
predictors}\label{relative-importance-of-predictors}

When interpreting a model summary, it is important to remember that the
coefficient estimates of each predictor correspond to a change in
prediction for a one-unit change in that predictor, e.g.~for the
predictor \texttt{Age}, an increase of one year. However, within a
single model, it's quite common for the predictor variables to be
measured in different units. For example, in
\texttt{summary(model4\_c)}, the coefficient estimate for
\texttt{Age\_c} is \texttt{0.3718}, which means that, holding all other
predictors constant, for every year that a participant is older than the
median age, their predicted \texttt{Vocab} score increases by
\texttt{0.3718}. By contrast, the coefficient estimate for the
\texttt{Blocks\_c} predictor (\texttt{1.3011}) corresponds to an
increase in \texttt{Vocab} scores for every additional point that
participants score on the non-verbal IQ Blocks test. Its unit is
therefore test points. For categorical predictor variables, a
single-unit change represents a change from one predictor level to
another, e.g.~from L1 to L2, or from female to male.

Because they are in different units that represent different quantities
or levels, the raw coefficient estimates of a model cannot be compared
to each other. Indeed, depending on the predictor, a one-unit change may
correspond to either a big or a small change. This is where measures of
the relative importance of predictors come in handy. In this chapter, we
will use the \textbf{lmg} metric (that was first proposed by
\textbf{L}indeman, \textbf{M}erenda \& \textbf{G}old in 1980: 119 ff.)
to compare the importance of predictors within a single multiple linear
regression model.

Although not currently widely used in the language sciences (but see,
e.g. DÄ…browska 2019: 14), lmg has a number of advantages. Its
interpretation is fairly intuitive because it is similar to a
coefficient of determination (R\textsuperscript{2}): a value of~0 means
that, in this model, a predictor accounts for 0\% of the variance in the
outcome variable, while a value of~1 would mean that it can be used to
perfectly predict the outcome variable. Calculating lmg values is
computationally involved because the metric includes both direct effects
and is adjusted for all the other predictors of the model. But this need
not worry us because a researcher and statistician, Ulrike GrÃ¶mping, has
developed and published an \texttt{R} package that will do the
computation for us. ðŸ˜Š

Once we have installed and loaded the
\{\href{https://cran.r-universe.dev/relaimpo/doc/manual.html}{relaimpo}\}
package\footnote{Loading the \{relaimpo\} package returns several
  warnings informing us, on the one hand, about packages that
  \{relaimpo\} requires to work and which are therefore also
  automatically loaded (these are called \textbf{dependencies}) and, on
  the other, about objects being \textbf{masked} by different packages,
  e.g.:

\begin{Verbatim}
The following object is masked from â€˜package:dplyrâ€™:

select
\end{Verbatim}

  It is worth paying attention to the latter set of warnings because it
  means that some function names, e.g.~\texttt{select()}, are now shared
  by more than one package in our \texttt{R} environment. This can cause
  code that previously worked fine to suddenly return errors. For
  example, after loading the \{relaimpo\} package, you may find that the
  \texttt{select()} function no longer works as expected because
  \texttt{R} attempts to use the \texttt{select()} function from the
  \{MASS\} package rather than from the tidyverse \{dplyr\} package. To
  avoid this happening, we can manually assign the correct package to
  the function name like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{select }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\NormalTok{select}
\end{Highlighting}
\end{Shaded}

  This ensures that any future mentions of the \texttt{select()}
  function are, by default, interpreted as the function defined in the
  \{dplyr\} package.} (GrÃ¶mping 2006), we can use its
\texttt{calc.relimp()} function to calculate a range of relative
importance metrics for linear models. With the argument ``type'', we
specify that we are interested in the lmg metric:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"relaimpo"}\NormalTok{)}
\FunctionTok{library}\NormalTok{(relaimpo)}
\NormalTok{select }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\NormalTok{select}

\NormalTok{rel.imp.metric }\OtherTok{\textless{}{-}} \FunctionTok{calc.relimp}\NormalTok{(model4\_c, }
                              \AttributeTok{type =} \StringTok{"lmg"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The output of the function is long. We have therefore saved it as a new
\texttt{R} object (\texttt{rel.imp.metric}) in order to retrieve only
the part that we are interested in, namely the lmp values for each
predictor, as a single data frame (\texttt{rel.imp.metric.df}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rel.imp.metric.df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{lmg =}\NormalTok{ rel.imp.metric}\SpecialCharTok{$}\NormalTok{lmg) }\SpecialCharTok{|\textgreater{}} 
    \FunctionTok{rownames\_to\_column}\NormalTok{(}\AttributeTok{var =} \StringTok{"Predictor"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We display this data frame in descending order of lmp values, rounded to
two decimal values:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rel.imp.metric.df }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{lmg =} \FunctionTok{round}\NormalTok{(lmg, }\AttributeTok{digits =} \DecValTok{2}\NormalTok{)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{arrange}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{lmg)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   Predictor  lmg
1      Group 0.15
2 OccupGroup 0.07
3 EduTotal_c 0.05
4      Age_c 0.04
5   Blocks_c 0.04
6     Gender 0.00
\end{verbatim}

These values indicate that, on average, whether or not a participant is
a native or non-native speaker of English (\texttt{Group}) makes the
biggest difference in terms of predicted \texttt{Vocab} scores. Gender,
by contrast, is practically irrelevant in this model.

We can also visualise these values in the form of a bar plot (see
Figure~\ref{fig-ModelLmg}). Note that another nice thing about the lmg
metric is that the lmg values for all the predictors entered in
\texttt{model4\_c} add up to the model's (unadjusted) multiple
R\textsuperscript{2} (\texttt{0.3621}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rel.imp.metric.df }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{fct\_reorder}\NormalTok{(Predictor, lmg),}
                       \AttributeTok{y =}\NormalTok{ lmg)) }\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Predictor in model4\_c"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{coord\_flip}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{13_MultipleLinearRegression_files/figure-pdf/fig-ModelLmg-1.pdf}}

}

\caption{\label{fig-ModelLmg}Relative importance of predictors in
\texttt{model\ 4\_c}}

\end{figure}%

It is also worth noting that the second most important predictor is
\texttt{OccupGroup}. This may come as a surprise given that none of its
levels (or rather none of the differences between the reference level of
\texttt{C} for clerical occupations and the remaining levels) make a
statistically significant contribution to the model. This result
illustrates the value of this type of analysis compared to only looking
at a model's coefficient estimates.

Many studies will compare the coefficient estimates of multiple
regression models using \textbf{standardised coefficients} (see e.g.
Winter 2020: Section 6.2); however, this approach can lead to misleading
conclusions (see e.g. Mizumoto 2023).

\section{Modelling interactions between
predictors}\label{modelling-interactions-between-predictors}

One of the strengths of multiple linear regression is that we can also
model interactions between predictors. This is important because a
predictor's relationship with the outcome variable may depend on another
predictor. Consider \texttt{Age} as a predictor of \texttt{Vocab}
scores. In \texttt{model4\_c}, we saw that this predictor made a
statistically significant contribution to the model. The coefficient was
positive, which means that the model predicts that the older the
participants, the higher their receptive English vocabulary.

However, if you completed {\textbf{Q11.10}}, you might remember that
\texttt{Age} correlates positively with \texttt{Vocab} scores among L1
participants, but does not among L2 participants (see
Figure~\ref{fig-CorVocabAge} below). If anything, the correlation
visualised in the right-hand panel of Figure~\ref{fig-CorVocabAge} is
slightly negative. Moreover, the confidence band is wide enough to draw
a horizontal line through it, which suggests that the data are also
compatible with the null hypothesis of no correlation. As a result, we
can conclude that this slightly negative correlation is not
statistically significant.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dabrowska.data }\SpecialCharTok{|\textgreater{}} 
   \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Age, }
                        \AttributeTok{y =}\NormalTok{ Vocab)) }\SpecialCharTok{+}
     \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
     \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ Group) }\SpecialCharTok{+}
     \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{, }
                 \AttributeTok{se =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{13_MultipleLinearRegression_files/figure-pdf/fig-CorVocabAge-1.pdf}}

}

\caption{\label{fig-CorVocabAge}Observed correlation between
participants' vocabulary scores and their age in both the L1 and L2
groups}

\end{figure}%

Figure~\ref{fig-CorVocabAge} informs us that what we really need is a
model that can account for the fact that \texttt{Age} is probably a
useful predictor of \texttt{Vocab} scores for L1 speakers, but not so
much for L2 speakers. In other words, we'd like to model an
\textbf{interaction} between the predictors \texttt{Age} and
\texttt{Group}.

In \texttt{R}'s formula syntax, interaction terms are denoted with a
colon (\texttt{:}) or an asterisk (\texttt{*}). In the following model,
therefore, we are attempting to predict \texttt{Vocab} scores on the
basis of a person's native-speaker status (\texttt{Group}), their age,
occupational group, gender, Blocks test score, number of years in formal
education, and the interaction between their age and native-speaker
status (\texttt{Age\_c:Group}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model5 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Vocab }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Group }\SpecialCharTok{+}\NormalTok{ Age\_c }\SpecialCharTok{+}\NormalTok{ OccupGroup }\SpecialCharTok{+}\NormalTok{ Gender }\SpecialCharTok{+}\NormalTok{ Blocks\_c }\SpecialCharTok{+}\NormalTok{ EduTotal\_c }\SpecialCharTok{+}\NormalTok{ Age\_c}\SpecialCharTok{:}\NormalTok{Group, }
             \AttributeTok{data =}\NormalTok{ Dabrowska.data)}

\FunctionTok{summary}\NormalTok{(model5)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = Vocab ~ Group + Age_c + OccupGroup + Gender + Blocks_c + 
    EduTotal_c + Age_c:Group, data = Dabrowska.data)

Residuals:
    Min      1Q  Median      3Q     Max 
-66.720 -10.651   1.224  11.939  45.415 

Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept)    69.1314     3.6286  19.052  < 2e-16 ***
GroupL2       -19.9128     3.4699  -5.739 5.26e-08 ***
Age_c           0.5466     0.1471   3.717 0.000286 ***
OccupGroupI     7.8267     5.4824   1.428 0.155525    
OccupGroupM    -4.0769     4.3852  -0.930 0.354058    
OccupGroupPS   -1.7774     4.2686  -0.416 0.677729    
GenderM        -1.6285     3.1213  -0.522 0.602644    
Blocks_c        1.2414     0.3132   3.964 0.000115 ***
EduTotal_c      2.5520     0.6126   4.166 5.27e-05 ***
GroupL2:Age_c  -0.8982     0.2867  -3.133 0.002089 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 18.12 on 147 degrees of freedom
Multiple R-squared:  0.402, Adjusted R-squared:  0.3654 
F-statistic: 10.98 on 9 and 147 DF,  p-value: 5.537e-13
\end{verbatim}

In the model summary above, the interaction term between \texttt{Age}
and \texttt{Group} is the last coefficient listed
(\texttt{GroupL2:Age\_c}). The values confirm our intuition based on our
descriptive visualisation of the data (Figure~\ref{fig-CorVocabAge}):
there is a statistically significant interaction between age and
native-speaker status (\emph{p}~=~0.002089) and the interaction
coefficient for this interaction term (\texttt{Age\_c:Group}) is
negative (\texttt{-0.8982}). This means that, whilst being older is
generally associated with higher \texttt{Vocab} scores for the reference
level of L1 speakers, if a participant is an L2 English speaker, this
trend is reversed.

To understand how this works in practice, let's compare the predicted
\texttt{Vocab} scores of two 35-year-olds: one a native English speaker
and the other a non-native. For the purposes of this illustration, we
will assume that, apart from their native-speaker status, all their
other characteristics correspond to the reference level in our model
(i.e.~they are both female, have a clerical occupation, scored average
on the Blocks test and were in formal education for the median number of
years). For the L1 speaker, we calculate their predicted \texttt{Vocab}
score as before. We take the intercept (\texttt{69.1314}) as our
starting point and then add the \texttt{Age\_c} coefficient
(\texttt{0.5466}) multiplied by the difference between their age and the
median age (which is the reference level of our centered predictor)
(i.e.~35~-~31~=~4). This means that their predicted \texttt{Vocab} score
is:

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{69.1314} \SpecialCharTok{+} \FloatTok{0.5466}\SpecialCharTok{*}\DecValTok{4}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 71.3178
\end{verbatim}

For the L2 speaker, we begin by combining the same coefficient estimates
as above but, this time, we also add the \texttt{GroupL2} coefficient
(\texttt{-19.9128}), and the interaction coefficient
(\texttt{GroupL2:Age\_c}). The interaction coefficient
(\texttt{-0.8982}) has to be multiplied by the difference between the
speaker's age and the median age (which remains 4 years). This L2
speaker's predicted score is therefore:

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{69.1314} \SpecialCharTok{+} \FloatTok{0.5466}\SpecialCharTok{*}\DecValTok{4} \SpecialCharTok{+} \SpecialCharTok{{-}}\FloatTok{19.9128} \SpecialCharTok{+} \SpecialCharTok{{-}}\FloatTok{0.8982}\SpecialCharTok{*}\DecValTok{4}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 47.8122
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-caution-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-caution-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Important}, titlerule=0mm, leftrule=.75mm]

Whenever we have a model with an interaction, we can no longer interpret
the individual coefficient estimates of the predictors that enter into
the interaction by themselves: instead, we must interpret our model's
main effects together with their interaction!

For example, in \texttt{model5}, if we only took account of the model's
main effect for age, we would be misled into thinking that age is
\emph{always} positively associated with \texttt{Vocab} scores. However,
as illustrated in Figure~\ref{fig-CorVocabAge}, we know that this is not
true for L2 speakers --- hence the statistically significant interaction
between \texttt{Age\_c} and \texttt{Group} in \texttt{model5}.

\end{tcolorbox}

The best way to avoid misinterpreting interaction effects is to make
sure you always visualise the predictions of models that involve
interactions. The \texttt{visreg()} function includes a ``by'' argument,
which is ideal for this:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{visreg}\NormalTok{(model5, }
       \AttributeTok{xvar =} \StringTok{"Age\_c"}\NormalTok{, }
       \AttributeTok{by =} \StringTok{"Group"}\NormalTok{,}
       \AttributeTok{xtrans =} \ControlFlowTok{function}\NormalTok{(x) x }\SpecialCharTok{+} \FunctionTok{median}\NormalTok{(Dabrowska.data}\SpecialCharTok{$}\NormalTok{Age), }
       \AttributeTok{gg =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Age (in years)"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Predicted Vocab scores"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{13_MultipleLinearRegression_files/figure-pdf/fig-PredictedVocabAgeInteraction-1.pdf}}

}

\caption{\label{fig-PredictedVocabAgeInteraction}\texttt{Vocab} scores
as predicted by \texttt{model5} for L1 and L2 speakers of various ages
(blue lines) and partial model residuals (grey points)}

\end{figure}%

If we compare Figure~\ref{fig-PredictedVocabAgeInteraction} (in which
the points represent the model's partial residuals) to
Figure~\ref{fig-CorVocabAge} (in which the points represent the actual,
observed values), we can see that the partial residuals are, on average,
smaller than the differences between the observed values and the
regression lines of Figure~\ref{fig-CorVocabAge}. This is because the
regression lines of Figure~\ref{fig-CorVocabAge} correspond to a model
that only includes three coefficients, \texttt{Age\_c}, \texttt{Group}
and their interaction (\texttt{Age\_c:Group}), whereas the partial
residuals in Figure~\ref{fig-PredictedVocabAgeInteraction} correspond to
the left-over variance in \texttt{Vocab} scores once all other
predictors of the \texttt{model5} have been taken into account.

Comparing the model summaries of \texttt{model4\_c} and \texttt{model5},
we can see that adding the interaction term between age and
native-speaker status considerably boosted the amount of variance in
\texttt{Vocab} scores that our model now accounts for: whereas the
adjusted R\textsuperscript{2} of the model without the interaction was
\texttt{0.3276} (33\%), it has now reached \texttt{0.3654} (37\%) thanks
to the added interaction.

\subsection{Interactions between two numeric
predictors}\label{interactions-between-two-numeric-predictors}

We can also model interactions between two numeric predictors. For
instance, we may hypothesise that the positive effect of time spent in
formal education is moderated by age. If this interaction effect were
negative, this would mean that, as people get older, the fact that they
spent longer in formal education becomes less relevant to predict their
current vocabulary knowledge.

Let's add an \texttt{Age\_c:EduTotal\_c} interaction to our model to
find out if our data support this hypothesis.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model6 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Vocab }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Group }\SpecialCharTok{+}\NormalTok{ Age\_c }\SpecialCharTok{+}\NormalTok{ OccupGroup }\SpecialCharTok{+}\NormalTok{ Gender }\SpecialCharTok{+}\NormalTok{ Blocks\_c }\SpecialCharTok{+}\NormalTok{ EduTotal\_c }\SpecialCharTok{+}\NormalTok{ Group}\SpecialCharTok{:}\NormalTok{Age\_c }\SpecialCharTok{+}\NormalTok{ Age\_c}\SpecialCharTok{:}\NormalTok{EduTotal\_c,}
             \AttributeTok{data =}\NormalTok{ Dabrowska.data)}

\FunctionTok{summary}\NormalTok{(model6)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = Vocab ~ Group + Age_c + OccupGroup + Gender + Blocks_c + 
    EduTotal_c + Group:Age_c + Age_c:EduTotal_c, data = Dabrowska.data)

Residuals:
    Min      1Q  Median      3Q     Max 
-68.346 -10.460   0.981  11.830  45.360 

Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)       69.27826    3.65827  18.937  < 2e-16 ***
GroupL2          -20.11637    3.51829  -5.718 5.89e-08 ***
Age_c              0.53820    0.14902   3.612 0.000418 ***
OccupGroupI        7.56857    5.53734   1.367 0.173781    
OccupGroupM       -4.14901    4.40173  -0.943 0.347449    
OccupGroupPS      -1.88806    4.29021  -0.440 0.660525    
GenderM           -1.74885    3.14526  -0.556 0.579044    
Blocks_c           1.24165    0.31409   3.953 0.000120 ***
EduTotal_c         2.66662    0.68007   3.921 0.000135 ***
GroupL2:Age_c     -0.87396    0.29409  -2.972 0.003464 ** 
Age_c:EduTotal_c  -0.01776    0.04520  -0.393 0.694947    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 18.17 on 146 degrees of freedom
Multiple R-squared:  0.4027,    Adjusted R-squared:  0.3618 
F-statistic: 9.842 on 10 and 146 DF,  p-value: 1.78e-12
\end{verbatim}

Looking at the last coefficient estimate in the model summary
(\texttt{Age\_c:EduTotal\_c}), we can see that this second interaction
coefficient is negative -- in line with our hypothesis. However, this
coefficient is also very small (\texttt{-0.01776}). Moreover, its
associated \emph{p}-value (\texttt{0.694947}) warns us that, if there
were no interaction effect (i.e.~under the null hypothesis), we would
have a 69\% probability of observing such a small effect in a dataset
this size simply due to random variation alone.

The model summary also tells us that the amount of variance in
\texttt{Vocab} scores that \texttt{model6} accounts for is 36.18\%
(\texttt{Adjusted\ R-squared:\ \ 0.3618}). This is actually slightly
\emph{less} than \texttt{model5}
(\texttt{Adjusted\ R-squared:\ \ 0.3654}), which did not include this
interaction. In other words, this additional interaction does not help
us to model the association between our predictor variables and
\texttt{Vocab} scores more accurately.

In Figure~\ref{fig-PredictedVocabAgeByEdu}, we visualise this
statistically non-significant interaction to better understand what it
corresponds to. When used to visualise an interaction effect between two
numeric predictors, the \texttt{visreg()} function automatically splits
the second predictor variable (the ``by'' variable) into three
categories corresponding to low, middle, and high values of the
variable. It therefore shows the predicted effect of the numeric
predictor predicted on the \emph{x}-axis in these three different
contexts. If, as in Figure~\ref{fig-PredictedVocabAgeByEdu}, the three
slopes are of the same gradient and the regression lines therefore run
parallel to each other, this indicates that there is no noteworthy
interaction between the two numeric predictors.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{visreg}\NormalTok{(model6,}
       \AttributeTok{xvar =} \StringTok{"Age\_c"}\NormalTok{, }
       \AttributeTok{by =} \StringTok{"EduTotal\_c"}\NormalTok{,}
       \AttributeTok{xtrans =} \ControlFlowTok{function}\NormalTok{(x) x }\SpecialCharTok{+} \FunctionTok{median}\NormalTok{(Dabrowska.data}\SpecialCharTok{$}\NormalTok{Age), }
       \AttributeTok{gg =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Age"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Predicted Vocab scores"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{13_MultipleLinearRegression_files/figure-pdf/fig-PredictedVocabAgeByEdu-1.pdf}}

}

\caption{\label{fig-PredictedVocabAgeByEdu}\texttt{Vocab} scores as
predicted by \texttt{model6} for English speakers of various ages who
were in formal education for a below-average, average, and above-average
length of time (in blue) and partial model residuals (grey points)}

\end{figure}%

Examining Figure~\ref{fig-PredictedVocabAgeByEdu}, we can therefore
conclude that the effect of age on \texttt{Vocab} scores is not
moderated by the number of years that participants spent in formal
education.

The \texttt{visreg()} function provides two ways to visualise
interaction effects between two numeric variables: in
Figure~\ref{fig-PredictedVocabAgeByEdu}, below-average, average, and
above-average number of years in formal education were visualised across
three panels. Figure~\ref{fig-PredictedVocabAgeRL} shows the same
predictions but, this time, the code includes the argument ``overlay =
TRUE'', which results in a single panel with three coloured regression
lines superimposed. This can make it easier to check whether the lines
are parallel. From Figure~\ref{fig-PredictedVocabAgeRL}, it is easier to
see that the three lines are pretty much parallel to each other. This
suggests that the positive effect of \texttt{Age} on \texttt{Vocab}
scores does not change as a function of the number of years that
participants were in formal education.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{visreg}\NormalTok{(model6,}
       \AttributeTok{xvar =} \StringTok{"Age\_c"}\NormalTok{, }
       \AttributeTok{by =} \StringTok{"EduTotal\_c"}\NormalTok{,}
       \AttributeTok{overlay =} \ConstantTok{TRUE}\NormalTok{,}
       \AttributeTok{xtrans =} \ControlFlowTok{function}\NormalTok{(x) x }\SpecialCharTok{+} \FunctionTok{median}\NormalTok{(Dabrowska.data}\SpecialCharTok{$}\NormalTok{Age), }
       \AttributeTok{gg =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Age"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Predicted Vocab scores"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{13_MultipleLinearRegression_files/figure-pdf/fig-PredictedVocabAgeRL-1.pdf}}

}

\caption{\label{fig-PredictedVocabAgeRL}\texttt{Vocab} scores as
predicted by \texttt{model4\_c} for English speakers of various ages who
were in formal education for a below-average (red line), average (green
line), and above-average (blue line) period of time and partial model
residuals (coloured points).}

\end{figure}%

In order to interpret the output of a model featuring a significant
interaction between two numeric predictors, we will now fit a simpler
model predicting the \texttt{Vocab} scores of the L1 participants only
(\texttt{L1.data}) using only two predictor variables: years in formal
education (\texttt{EduTotal}) and Author Recognition Test (\texttt{ART})
scores.

The Author Recognition Test (Acheson, Wells \& MacDonald 2008) is a
measure of print exposure to literature, which is known to be a strong
predictor of vocabulary knowledge (see {Your turn!} section in
Section~\ref{sec-ModelPredictions} above). This is confirmed in the L1
dataset as \texttt{ART} and \texttt{Vocab} scores are strongly
correlated:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{(L1.data}\SpecialCharTok{$}\NormalTok{ART, L1.data}\SpecialCharTok{$}\NormalTok{Vocab)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.6032758
\end{verbatim}

At the same time, we also know that there is a positive, though less
strong, correlation between the number of years that L1 participants
were in formal education and their \texttt{Vocab} test results:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{(L1.data}\SpecialCharTok{$}\NormalTok{EduTotal, L1.data}\SpecialCharTok{$}\NormalTok{Vocab)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.4268695
\end{verbatim}

In the following, we explore the possibility that this positive
association between Author Recognition Test (\texttt{ART}) scores and
\texttt{Vocab} scores may be moderated by the number of years spent in
formal education (\texttt{EduTotal}). If this interaction effect were
negative, this would mean that, if two individuals both have the same
high \texttt{ART} score, but one spent longer in formal education, then
the effect of those extra years of education would not be as strong as
they would be for someone with a lower \texttt{ART} score. To test this
hypothesis, we formulate the following null hypothesis:

\begin{itemize}
\tightlist
\item
  \textbf{H\textsubscript{0}}: The number of years spent in formal
  education does not moderate the association of L1 English speakers'
  \texttt{ART} scores with their \texttt{Vocab} scores.
\end{itemize}

We now fit a model to find out if we have enough evidence to reject this
null hypothesis:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# First, we center the numeric variables in the L1 dataset:}
\NormalTok{L1.data }\OtherTok{\textless{}{-}}\NormalTok{ L1.data }\SpecialCharTok{|\textgreater{}} 
   \FunctionTok{mutate}\NormalTok{(}\AttributeTok{EduTotal\_c =}\NormalTok{ EduTotal }\SpecialCharTok{{-}} \FunctionTok{median}\NormalTok{(EduTotal),}
          \AttributeTok{Blocks\_c =}\NormalTok{ Blocks }\SpecialCharTok{{-}} \FunctionTok{median}\NormalTok{(Blocks),}
          \AttributeTok{Age\_c =}\NormalTok{ Age }\SpecialCharTok{{-}} \FunctionTok{median}\NormalTok{(Age))}

\CommentTok{\# Second, we fit the model with both variables and their two{-}way interaction as predictors:}
\NormalTok{L1.model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Vocab }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ART }\SpecialCharTok{+}\NormalTok{ EduTotal\_c }\SpecialCharTok{+}\NormalTok{ ART}\SpecialCharTok{:}\NormalTok{EduTotal\_c,}
              \AttributeTok{data =}\NormalTok{ L1.data)}

\CommentTok{\# Finally, we inspect the model:}
\FunctionTok{summary}\NormalTok{(L1.model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = Vocab ~ ART + EduTotal_c + ART:EduTotal_c, data = L1.data)

Residuals:
    Min      1Q  Median      3Q     Max 
-54.807  -6.113   0.065  10.927  26.379 

Coefficients:
               Estimate Std. Error t value Pr(>|t|)    
(Intercept)     51.9162     2.7622  18.795  < 2e-16 ***
ART              1.3088     0.1955   6.696 2.09e-09 ***
EduTotal_c       5.2626     1.3272   3.965 0.000151 ***
ART:EduTotal_c  -0.1803     0.0530  -3.402 0.001017 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 15.18 on 86 degrees of freedom
Multiple R-squared:  0.4625,    Adjusted R-squared:  0.4437 
F-statistic: 24.67 on 3 and 86 DF,  p-value: 1.311e-11
\end{verbatim}

The model summary confirms that both predictors individually (i.e.~as
main effects) make statistically significant, positive contributions to
the prediction of \texttt{Vocab} scores. Crucially, the summary also
indicates that the interaction effect (\texttt{ART:EduTotal\_c}) is
statistically significant at Î±~=~0.05 (\emph{p}~=~\texttt{0.001017}).
The negative coefficient estimate of \texttt{-0.1803} means that, for
each additional year of formal education, our model predicts that the
effect of a participant's \texttt{ART} score on \texttt{Vocab} decreases
by 0.1803 points. In simpler terms, the positive effect of reading
literature, as measured by the Author Recognition Test (Acheson, Wells
\& MacDonald 2008), decreases slightly for every year spent in formal
education.

Remember that, whenever we report an interaction, we can no longer
interpret the estimated coefficients of the individual predictors in
isolation. That is because we must also consider the interaction effect.
To understand what this means in practice, let's compare two speakers
from the L1 dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L1.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{18}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(Occupation, OccupGroup, ART, EduTotal, Vocab)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
              Occupation OccupGroup ART EduTotal    Vocab
1 Student/Support Worker         PS  31       13 95.55556
2              Housewife          I  31       17 84.44444
\end{verbatim}

\begin{itemize}
\item
  Participant No.~2 is a student and support worker with an ART score of
  31 points, who has (so far) spent 13 years in formal education.
\item
  Participant No.~18 is a housewife who also scored 31 points on the
  ART, but who was in formal education for a total of 17 years.
\end{itemize}

In general, we can calculate the \texttt{Vocab} scores that our
\texttt{L1.model} predicts by combining the model's coefficient
estimates like this:

\begin{quote}
Intercept +\\
\texttt{ART} coefficient*\texttt{ART} score +\\
\texttt{EduTotal} coefficient*(\texttt{EduTotal} in years - median
\texttt{EduTotal}) +\\
\texttt{ART:EduTotal} coefficient*\texttt{ART} score*(\texttt{EduTotal}
in years - median \texttt{EduTotal})
\end{quote}

Based on the coefficient estimates of the model summary, we can
therefore calculate the predicted \texttt{Vocab} score of the student /
support worker as follows:

\phantomsection\label{annotated-cell-310}%
\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{51.9162} \SpecialCharTok{+} \hspace*{\fill}\NormalTok{\circled{1}}
\FloatTok{1.3088} \SpecialCharTok{*} \DecValTok{31} \SpecialCharTok{+} \hspace*{\fill}\NormalTok{\circled{2}}
\FloatTok{5.2626} \SpecialCharTok{*}\NormalTok{ (}\DecValTok{13} \SpecialCharTok{{-}} \FunctionTok{median}\NormalTok{(L1.data}\SpecialCharTok{$}\NormalTok{EduTotal)) }\SpecialCharTok{+} \hspace*{\fill}\NormalTok{\circled{3}}
\SpecialCharTok{{-}}\FloatTok{0.1803} \SpecialCharTok{*} \DecValTok{31} \SpecialCharTok{*}\NormalTok{ (}\DecValTok{13} \SpecialCharTok{{-}} \FunctionTok{median}\NormalTok{(L1.data}\SpecialCharTok{$}\NormalTok{EduTotal)) }\hspace*{\fill}\NormalTok{\circled{4}}
\end{Highlighting}
\end{Shaded}

\begin{description}
\tightlist
\item[\circled{1}]
Intercept coefficient
\item[\circled{2}]
Main effect of 31 points on the ART test
\item[\circled{3}]
Main effect of 13 years in formal education
\item[\circled{4}]
Interaction effect between 31 points on the ART and having spent 13
years in formal education
\end{description}

\begin{verbatim}
[1] 92.489
\end{verbatim}

And similarly for the housewife with the same ART score:

\phantomsection\label{annotated-cell-311}%
\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{51.9162} \SpecialCharTok{+} \hspace*{\fill}\NormalTok{\circled{1}}
\FloatTok{1.3088} \SpecialCharTok{*} \DecValTok{31} \SpecialCharTok{+} \hspace*{\fill}\NormalTok{\circled{2}}
\FloatTok{5.2626} \SpecialCharTok{*}\NormalTok{ (}\DecValTok{17} \SpecialCharTok{{-}} \FunctionTok{median}\NormalTok{(L1.data}\SpecialCharTok{$}\NormalTok{EduTotal)) }\SpecialCharTok{+} \hspace*{\fill}\NormalTok{\circled{3}}
\SpecialCharTok{{-}}\FloatTok{0.1803} \SpecialCharTok{*} \DecValTok{31} \SpecialCharTok{*}\NormalTok{ (}\DecValTok{17} \SpecialCharTok{{-}} \FunctionTok{median}\NormalTok{(L1.data}\SpecialCharTok{$}\NormalTok{EduTotal)) }\hspace*{\fill}\NormalTok{\circled{4}}
\end{Highlighting}
\end{Shaded}

\begin{description}
\tightlist
\item[\circled{1}]
Intercept coefficient
\item[\circled{2}]
Main effect of 31 points on ART test
\item[\circled{3}]
Main effect of 17 years in formal education
\item[\circled{4}]
Interaction effect between scoring 31 points on the ART and having spent
17 years in formal education
\end{description}

\begin{verbatim}
[1] 91.1822
\end{verbatim}

We can check that we did the maths correctly by checking the model's
prediction for these two individuals. Remember that minor differences
after the decimal point are due to us using rounded coefficient
estimates.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{predict}\NormalTok{(L1.model)[}\DecValTok{2}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
       2 
92.49038 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{predict}\NormalTok{(L1.model)[}\DecValTok{18}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      18 
91.18172 
\end{verbatim}

Notice how these two predicted scores are very similar, even though the
housewife spent longer in formal education than the student / support
worker. This is because, for L1 speakers, greater print exposure and
more education generally lead to a larger vocabulary (as indicated by
the positive main-effect coefficient estimates in \texttt{L1.model}),
but the increase in vocabulary for each additional year of education is
predicted to be smaller for individuals with higher ART scores.

Figure~\ref{fig-VocabEduART} visualises the predictions of
\texttt{L1.model}. How can we interpret these three regression lines?

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{visreg}\NormalTok{(L1.model,}
       \AttributeTok{xvar =} \StringTok{"EduTotal\_c"}\NormalTok{, }
       \AttributeTok{by =} \StringTok{"ART"}\NormalTok{,}
       \AttributeTok{xtrans =} \ControlFlowTok{function}\NormalTok{(x) x }\SpecialCharTok{+} \FunctionTok{median}\NormalTok{(L1.data}\SpecialCharTok{$}\NormalTok{EduTotal), }
       \AttributeTok{gg =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Years in formal education"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Predicted Vocab scores"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{13_MultipleLinearRegression_files/figure-pdf/fig-VocabEduART-1.pdf}}

}

\caption{\label{fig-VocabEduART}\texttt{Vocab} scores as predicted by
\texttt{L1.model} (in blue) and partial model residuals (grey points) as
a function of the number of years speakers were in formal education and
for three ART test scores}

\end{figure}%

For low and mid-level ART scores, the model predicts a positive
correlation between the number of years a participant has spent in
formal education and their predicted \texttt{Vocab} scores. However,
this is not the case for participants who scored high on the ART test
(third panel). The three regression lines representing the model's
predicted scores are clearly not parallel, and it would not be possible
to draw three parallel lines that each stay within their respective 95\%
confidence bands. This confirms that, in this L1 model, the interaction
between ART scores and years in formal education is statistically
significant at Î±~=~0.05. The main effects of these two predictors cannot
be meaningfully interpreted without considering this interaction.

\section{Model selection}\label{model-selection}

Typically, the more predictors we enter in a model, the better the model
fits the data. However, having predictors that contribute very little to
the model and/or whose contributions are not statistically significant
risks lowering the accuracy of the model's predictions on new data. In
this case, we say that the model \textbf{overfits}. A model that
overfits the sample data risks not generalising to other data. If the
aim of our statistical modelling is to infer from our sample to the
general population (see Chapter~\ref{sec-Inferential}), it can sometimes
make sense to try to find an `optimal' model that accounts for as much
of the variance in the outcome variable as possible, relying only on
association effects that we can be fairly confident could not have
occurred due to chance only. This is where model selection comes into
play.

Some people consider model selection to be a bit of an art. This chapter
only aims to introduce the topic and does not cover --- let alone
compare or endorse --- different model selection procedures. Ultimately,
what really matters is that, \emph{if} we intend to apply a model
selection procedure, we decide on the method \emph{before} analysing the
data. The most credible way to settle on a model selection procedure
prior to data analysis is to \textbf{preregister} an analysis plan on an
online platform such as
\href{https://aspredicted.org/}{AsPredicted.org},
\href{https://www.cos.io/initiatives/prereg}{Open Science Framework
(OSF)}, and \href{https://zenodo.org/}{Zenodo} (see Haroz 2022):

\begin{quote}
Preregistration {[}or pre-registration{]} is the practice of posting a
time-stamped, read-only version of your study plan to a public
repository before beginning data collection or analysis. This
establishes a transparent record of your research intentions (Center for
OpenScience 2025).
\end{quote}

Model selection bears the very real risk of (consciously or
unconsciously) ``fishing'' for statistically significant results.
Fishing is a highly Questionable Research Practice (QRP, see
Section~\ref{sec-pHacking}) that consists in trying out different
methods until we arrive at findings that match our theory and/or
hypotheses. For this reason, selection procedures in regression
modelling have been heavily criticised over the years and there are very
good arguments for not engaging in any model selection at all (Thompson
1995; Smith 2018; Harrell 2015: Section 4.3).

\begin{quote}
A fundamental problem with stepwise regression is that some real
explanatory variables that have causal effects on the dependent variable
may happen to not be statistically significant, while nuisance variables
may be coincidentally significant. As a result, the model may fit the
data well in-sample, but do poorly out-of-sample (Smith 2018: 1).
\end{quote}

Nonetheless, in the following section, we will see how we might -
\emph{if} we decide to narrow down predictor variables using model
selection - arrive at an optimal model of \texttt{Vocab} scores among L1
and L2 speakers of English using the \textbf{adjusted
R\textsuperscript{2}} as our model selection decision criterion. Recall
that R\textsuperscript{2} values correspond to the amount of variance in
the outcome variable that a model can predict. The adjusted
R\textsuperscript{2} is particularly useful because it is adjusted for
the number of predictors entered in the model; models with more
predictors are penalised.

\begin{quote}
When using adjusted R\textsuperscript{2} as the decision criterion, we
seek to eliminate or add predictors depending on whether they lead to
the largest improvement in adjusted R\textsuperscript{2} and we stop
when adding or eliminating another predictor does not lead to further
improvement in adjusted R\textsuperscript{2}.

Adjusted R\textsuperscript{2} describes the strength of a model fit, and
it is a useful tool for evaluating which predictors are adding value to
the model, where \emph{adding value} means they are (likely) improving
the accuracy in predicting future outcomes. (Ã‡etinkaya-Rundel \& Hardin
2021:
\href{https://openintro-ims.netlify.app/model-mlr\#sec-model-selection}{Section
8.4})
\end{quote}

There are two common ways to add or remove predictors in a multiple
regression model. These are called backward elimination and forward
selection. They are often called \textbf{stepwise selection} because
they add or remove one variable at a time.

\begin{quote}
\textbf{Backward elimination} starts with the full model -- the model
that includes all potential predictor variables. Predictors are
eliminated one-at-a-time from the model until we cannot improve the
model any further.

\textbf{Forward selection} is the reverse of the backward elimination
technique. Instead of eliminating predictors one-at-a-time, we add
predictors one-at-a-time until we cannot find any predictors that
improve the model any further. (Ã‡etinkaya-Rundel \& Hardin 2021:
\href{https://openintro-ims.netlify.app/model-mlr\#sec-model-selection}{Section
8.4})
\end{quote}

We will use backward elimination as it allows us to start with a
\textbf{full model} that includes all the predictors and any
interactions that we believe are justified on the basis of theory and/or
prior research. At this stage, it is absolutely crucial to think about
which variables and which interactions are genuinely meaningful and
which are not!

With the DÄ…browska (2019) data, several full models can be justified. In
this section, we will attempt to model \texttt{Vocab} scores among L1
participants using four predictors (\texttt{ART}, \texttt{Blocks\_c},
\texttt{Age\_c}, \texttt{EduTotal\_c}, and \texttt{OccupGroup}) and the
following four interactions (\texttt{ART:EduTotal\_c},
\texttt{Blocks\_c:EduTotal\_c}, \texttt{ART:Age\_c}, and
\texttt{Blocks:Age\_c}). We can justify this choice of predictors based
on our current understanding of language learning.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L1.model.full }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Vocab }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ART }\SpecialCharTok{+}\NormalTok{ Blocks\_c }\SpecialCharTok{+}\NormalTok{ Age\_c }\SpecialCharTok{+}\NormalTok{ EduTotal\_c }\SpecialCharTok{+}\NormalTok{ OccupGroup }\SpecialCharTok{+}\NormalTok{ ART}\SpecialCharTok{:}\NormalTok{EduTotal\_c }\SpecialCharTok{+}\NormalTok{ Blocks\_c}\SpecialCharTok{:}\NormalTok{EduTotal\_c }\SpecialCharTok{+}\NormalTok{ ART}\SpecialCharTok{:}\NormalTok{Age\_c }\SpecialCharTok{+}\NormalTok{ Blocks\_c}\SpecialCharTok{:}\NormalTok{Age\_c, }
           \AttributeTok{data =}\NormalTok{ L1.data)}

\FunctionTok{summary}\NormalTok{(L1.model.full)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = Vocab ~ ART + Blocks_c + Age_c + EduTotal_c + OccupGroup + 
    ART:EduTotal_c + Blocks_c:EduTotal_c + ART:Age_c + Blocks_c:Age_c, 
    data = L1.data)

Residuals:
    Min      1Q  Median      3Q     Max 
-42.927  -4.064  -0.374   8.295  28.282 

Coefficients:
                      Estimate Std. Error t value Pr(>|t|)    
(Intercept)         53.1677121  3.7269444  14.266  < 2e-16 ***
ART                  1.1304963  0.2330505   4.851 6.16e-06 ***
Blocks_c             1.4066157  0.3164938   4.444 2.88e-05 ***
Age_c                0.5955643  0.1895092   3.143  0.00237 ** 
EduTotal_c           4.4423564  1.3560678   3.276  0.00157 ** 
OccupGroupI          4.0757889  4.7833634   0.852  0.39678    
OccupGroupM          0.1584647  4.3523775   0.036  0.97105    
OccupGroupPS         0.4096426  4.3947432   0.093  0.92597    
ART:EduTotal_c      -0.1523719  0.0513417  -2.968  0.00398 ** 
Blocks_c:EduTotal_c -0.1428729  0.1311735  -1.089  0.27942    
ART:Age_c           -0.0125418  0.0096286  -1.303  0.19656    
Blocks_c:Age_c      -0.0007549  0.0194954  -0.039  0.96921    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 13.45 on 78 degrees of freedom
Multiple R-squared:  0.6172,    Adjusted R-squared:  0.5632 
F-statistic: 11.43 on 11 and 78 DF,  p-value: 2.492e-12
\end{verbatim}

Our full model has an adjusted R\textsuperscript{2} of \texttt{0.5632},
which means that the combination of these variables and the four
interactions account for about 56\% of the variance in \texttt{Vocab}
scores in the L1 data. However, many of the model coefficients in
\texttt{L1.model.full} are not statistically significant, so we could
try to remove them to see whether this leads to a lower adjusted
R\textsuperscript{2} or not. Strictly speaking, a stepwise selection
procedure would entail removing each interaction one-by-one. To save
space here, we remove all three non-significant interaction terms in a
single backward step:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L1.model.back1 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Vocab }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ART }\SpecialCharTok{+}\NormalTok{ Blocks\_c }\SpecialCharTok{+}\NormalTok{ Age\_c }\SpecialCharTok{+}\NormalTok{ EduTotal\_c }\SpecialCharTok{+}\NormalTok{ OccupGroup }\SpecialCharTok{+}\NormalTok{ ART}\SpecialCharTok{:}\NormalTok{EduTotal\_c, }
           \AttributeTok{data =}\NormalTok{ L1.data)}

\FunctionTok{summary}\NormalTok{(L1.model.back1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = Vocab ~ ART + Blocks_c + Age_c + EduTotal_c + OccupGroup + 
    ART:EduTotal_c, data = L1.data)

Residuals:
    Min      1Q  Median      3Q     Max 
-42.282  -4.835   0.367   8.867  29.110 

Coefficients:
               Estimate Std. Error t value Pr(>|t|)    
(Intercept)    53.61458    3.60809  14.860  < 2e-16 ***
ART             0.97262    0.20158   4.825 6.48e-06 ***
Blocks_c        1.37827    0.31278   4.407 3.19e-05 ***
Age_c           0.42996    0.13122   3.276  0.00155 ** 
EduTotal_c      4.17372    1.30919   3.188  0.00204 ** 
OccupGroupI     4.00028    4.65106   0.860  0.39228    
OccupGroupM     0.74751    4.29748   0.174  0.86235    
OccupGroupPS   -0.22682    4.17410  -0.054  0.95680    
ART:EduTotal_c -0.13709    0.04903  -2.796  0.00646 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 13.38 on 81 degrees of freedom
Multiple R-squared:  0.6067,    Adjusted R-squared:  0.5679 
F-statistic: 15.62 on 8 and 81 DF,  p-value: 1.153e-13
\end{verbatim}

This procedure has actually led to a (very) small increase in our
adjusted R\textsuperscript{2}, which is now \texttt{0.5679}, or 57\%.
Can we simplify our model even further and still account for as much
variance in \texttt{Vocab} scores by dropping categorical predictor
variable \texttt{OccupGroup}?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L1.model.back2 }\OtherTok{\textless{}{-}}  \FunctionTok{lm}\NormalTok{(Vocab }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ART }\SpecialCharTok{+}\NormalTok{ Blocks\_c }\SpecialCharTok{+}\NormalTok{ Age\_c }\SpecialCharTok{+}\NormalTok{ EduTotal\_c }\SpecialCharTok{+}\NormalTok{ ART}\SpecialCharTok{:}\NormalTok{EduTotal\_c, }
           \AttributeTok{data =}\NormalTok{ L1.data)}

\FunctionTok{summary}\NormalTok{(L1.model.back2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = Vocab ~ ART + Blocks_c + Age_c + EduTotal_c + ART:EduTotal_c, 
    data = L1.data)

Residuals:
    Min      1Q  Median      3Q     Max 
-43.379  -5.410   0.824   8.369  32.573 

Coefficients:
               Estimate Std. Error t value Pr(>|t|)    
(Intercept)     54.2022     2.4512  22.113  < 2e-16 ***
ART              0.9970     0.1922   5.187 1.46e-06 ***
Blocks_c         1.3251     0.3030   4.373 3.49e-05 ***
Age_c            0.4843     0.1092   4.434 2.78e-05 ***
EduTotal_c       4.3116     1.2812   3.365  0.00115 ** 
ART:EduTotal_c  -0.1472     0.0471  -3.125  0.00244 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 13.21 on 84 degrees of freedom
Multiple R-squared:  0.6025,    Adjusted R-squared:  0.5788 
F-statistic: 25.46 on 5 and 84 DF,  p-value: 1.521e-15
\end{verbatim}

This new model has a slightly higher adjusted R\textsuperscript{2}
(\texttt{0.5788}), so it looks like this was another sensible
simplification of our model. All of the remaining coefficient estimates
in \texttt{L1.model.back2} make statistically significant contributions
to the model. If we try to remove one, we can expect that the amount of
variance that our model can account for will drop. For example, we can
try to remove \texttt{Age} as a predictor from our model to see what
happens:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L1.model.back3 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Vocab }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ART }\SpecialCharTok{+}\NormalTok{ Blocks\_c }\SpecialCharTok{+}\NormalTok{ EduTotal\_c }\SpecialCharTok{+}\NormalTok{ ART}\SpecialCharTok{:}\NormalTok{EduTotal\_c, }
           \AttributeTok{data =}\NormalTok{ L1.data)}

\FunctionTok{summary}\NormalTok{(L1.model.back3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = Vocab ~ ART + Blocks_c + EduTotal_c + ART:EduTotal_c, 
    data = L1.data)

Residuals:
    Min      1Q  Median      3Q     Max 
-48.816  -7.486  -0.458   9.970  26.546 

Coefficients:
               Estimate Std. Error t value Pr(>|t|)    
(Intercept)    52.08245    2.65487  19.618  < 2e-16 ***
ART             1.38099    0.18951   7.287  1.5e-10 ***
Blocks_c        0.90754    0.31806   2.853  0.00543 ** 
EduTotal_c      3.59093    1.40344   2.559  0.01228 *  
ART:EduTotal_c -0.15028    0.05201  -2.890  0.00489 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 14.59 on 85 degrees of freedom
Multiple R-squared:  0.5095,    Adjusted R-squared:  0.4864 
F-statistic: 22.07 on 4 and 85 DF,  p-value: 1.615e-12
\end{verbatim}

Indeed, \texttt{L1.model.back3} accounts for 49\% of the variance
(adjusted R\textsuperscript{2} = \texttt{0.4864}), which is considerably
less than \texttt{L1.model.back2}. We will therefore report and
interpret \texttt{L1.model.back2}.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Reporting a multiple regression model}, titlerule=0mm, leftrule=.75mm]

There are many ways to report the \textbf{numerical} results of a
statistical model. Researchers typically include a table reporting the
model's coefficient estimates (sometimes referred to as Î², ``beta''),
together with a measure of variability around these estimates
(e.g.~standard error or confidence intervals), as well as their
associated \emph{p}-values. In addition, it is important to report the
accuracy of the model. Different so-called \textbf{goodness-of-fit
measures} are used for this; one of the most common being the adjusted
coefficient of determination, R\textsuperscript{2}. The output of the
\texttt{summary()} function includes all of these statistics and is
therefore suitable for a research report.

Alternatively, the
\href{https://strengejacke.github.io/sjPlot/}{\{sjPlot\}} library
(LÃ¼decke 2020) includes a handy function that produces nicely formatted
tables to report all kinds of models, including multiple linear
regression models. When you run the \texttt{tab\_model()} function in
RStudio, the table will be displayed in the Viewer pane. By default, it
includes the model's coefficient estimates and 95\% confidence intervals
around these estimates, as well as \emph{p}-values formatted in bold if
they are below 0.05.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"sjPlot"}\NormalTok{)}
\FunctionTok{library}\NormalTok{(sjPlot)}

\FunctionTok{tab\_model}\NormalTok{(L1.model.back2)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}cccc@{}}
\toprule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
~ & \multicolumn{3}{c@{}}{%
Vocab} \\
Predictors & Estimates & CI & p \\
(Intercept) & 54.20 & 49.33~--~59.08 & \textbf{\textless0.001} \\
ART & 1.00 & 0.61~--~1.38 & \textbf{\textless0.001} \\
Blocks c & 1.33 & 0.72~--~1.93 & \textbf{\textless0.001} \\
Age c & 0.48 & 0.27~--~0.70 & \textbf{\textless0.001} \\
EduTotal c & 4.31 & 1.76~--~6.86 & \textbf{0.001} \\
ART Ã— EduTotal c & -0.15 & -0.24~--~-0.05 & \textbf{0.002} \\
Observations & \multicolumn{3}{l@{}}{%
90} \\
R\textsuperscript{2} / R\textsuperscript{2} adjusted &
\multicolumn{3}{l@{}}{%
0.603 / 0.579} \\
\end{longtable}

Check the documentation of the \texttt{tab\_model()} function and the
\href{https://strengejacke.github.io/sjPlot/}{sjPlot website} to learn
about its many useful formatting options:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{?tab\_model}
\end{Highlighting}
\end{Shaded}

It is also recommended to \textbf{visualise} the model's predictions and
its (partial) residuals. Again, there are many ways to achieve this in
\texttt{R}, but we will stick to using the \{visreg\} library. Run the
following command and follow the instructions displayed in the Console
to view all the plots in RStudio. You may need to resize your Plot pane
or use the Zoom button to properly view the plots.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{visreg}\NormalTok{(L1.model.back2)}
\end{Highlighting}
\end{Shaded}

Running the \texttt{visreg()} function on this multiple regression model
outputs several warning messages in the Console. These messages are
important and should not be ignored:

\begin{verbatim}
Note that you are attempting to plot a 'main effect' in a model that contains an interaction. This is potentially misleading; you may wish to consider using the 'by' argument.
\end{verbatim}

The message warns us that we should not attempt to interpret
\texttt{ART} and \texttt{EduTotal\_c} as \textbf{main effects} because
our model includes an \textbf{interaction} that involves these two
variables. Indeed, the fourth plot (see
Figure~\ref{fig-PredictedVocabEduL1}) suggests that the more years an L1
speaker spends in formal education, the greater their \texttt{Vocab}
score; however, because our model includes an interaction effect, the
authors of the \{visreg\} package are warning us that the strength or
even the direction of this effect could change depending on individuals'
\texttt{ART} score.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{visreg}\NormalTok{(}\AttributeTok{fit =}\NormalTok{ L1.model.back2, }
       \AttributeTok{xvar =} \StringTok{"EduTotal\_c"}\NormalTok{,}
       \AttributeTok{xtrans =} \ControlFlowTok{function}\NormalTok{(x) x }\SpecialCharTok{+} \FunctionTok{median}\NormalTok{(L1.data}\SpecialCharTok{$}\NormalTok{EduTotal), }
       \AttributeTok{gg =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Years in formal education"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Predicted Vocab scores"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()    }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Conditions used in construction of plot
ART: 10.5
Blocks_c: 0
Age_c: 0
\end{verbatim}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{13_MultipleLinearRegression_files/figure-pdf/fig-PredictedVocabEduL1-1.pdf}}

}

\caption{\label{fig-PredictedVocabEduL1}Predicted vocabulary scores for
L1 speakers as a function of the number of years that they were in
formal education (in blue) and partial residuals (grey points).}

\end{figure}%

As suggested by the warning message, we can (and should!) visualise
interactions like this one using the ``by'' argument.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{visreg}\NormalTok{(}\AttributeTok{fit =}\NormalTok{ L1.model.back2, }
       \AttributeTok{xvar =} \StringTok{"EduTotal\_c"}\NormalTok{, }
       \AttributeTok{xtrans =} \ControlFlowTok{function}\NormalTok{(x) x }\SpecialCharTok{+} \FunctionTok{median}\NormalTok{(L1.data}\SpecialCharTok{$}\NormalTok{EduTotal), }
       \AttributeTok{by =} \StringTok{"ART"}\NormalTok{,}
       \AttributeTok{gg =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Years in formal education"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Predicted Vocab scores"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()  }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{13_MultipleLinearRegression_files/figure-pdf/fig-PredictedVocabEduLit-1.pdf}}

}

\caption{\label{fig-PredictedVocabEduLit}Predicted vocabulary scores for
L1 speakers as a function of the number of years that they were in
formal education and across three different ART scores (in blue) and
partial residuals (grey points).}

\end{figure}%

Figure~\ref{fig-PredictedVocabEduLit} shows the predicted effect of the
number of years in formal education on \texttt{Vocab} scores for
participants who scored 3, 10, and 31 points on the \texttt{ART}. The
regression line shows a positive relation between \texttt{Vocab} scores
and years in formal education for below-average and average \texttt{ART}
scores, but the regression line is almost flat for above-average
\texttt{ART} scores. This indicates that, for individuals who score very
high on the \texttt{ART}, years in education is no longer a useful
predictor of their \texttt{Vocab} scores.

Finally, we should also report the outcome of our model assumption
checks. This is covered in the following section.

\end{tcolorbox}

\section{Checking model assumptions}\label{sec-ModelAssumptions}

As we saw in Section~\ref{sec-AssumptionsLR}, it is crucial that we
check that our models meet the assumptions of linear regression models
before we interpret them because, if they don't, our models may be
unreliable. In some cases, modelling issues may already be visible from
the model summary. Warning signs include very large model residuals,
coefficient estimates reported as \texttt{NA}, and warning messages
stating that the model is ``singular'' or that it has ``failed to
converge''.

These problems can occur for a number of reasons, but most often because
the model is too complex given the data available: the sample size may
be too small or the data too sparse for certain combinations of
predictors (e.g.~if you try to enter gender and native language as
predictors in a model, but for some languages only female native
speakers are represented in the dataset). There are often ways around
these issues, but they are beyond the scope of this introductory
textbook (see recommended readings below and
\href{https://elenlefoll.github.io/RstatsTextbook/A_FurtherResources.html}{next-step
resources}).

The model assumptions for multiple linear regression models are the same
as for simple linear regression models (see
Section~\ref{sec-AssumptionsLR}):

\begin{itemize}
\tightlist
\item
  Independence of the data points (see Section~\ref{sec-Independence}
  and Section~\ref{sec-IndependenceSLR})
\item
  Linear relationships between the predictors (see
  Section~\ref{sec-Linearity} and Section~\ref{sec-LinearitySLR})
\item
  Homogeneity of the model residuals (see
  Section~\ref{sec-Homoscedasticity} and Section~\ref{sec-Residuals})
\item
  Normality of the model residuals (see Section~\ref{sec-Normality})
\item
  No overly influential outliers (see Section~\ref{sec-Linearity})
\end{itemize}

There is only one additional assumption that is specific to models that
include multiple predictors:

\begin{itemize}
\tightlist
\item
  No multicollinearity
\end{itemize}

In the following, we use the \texttt{check\_model()} function from the
\{\href{https://easystats.github.io/performance}{performance}\} package
(LÃ¼decke, Ben-Shachar, et al. 2021) to check these assumptions
graphically. The \texttt{check\_model()} function also requires the
installation of the
\{\href{https://aloy.github.io/qqplotr/index.html}{qqplotr}\} (Almeida,
Loy \& Hofmann 2018) and
\{\href{https://easystats.github.io/see/}{see}\} (LÃ¼decke, Patil, et al.
2021) packages to work.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"performance"}\NormalTok{, }\StringTok{"qqplotr"}\NormalTok{, }\StringTok{"see"}\NormalTok{))}
\FunctionTok{library}\NormalTok{(performance)}
\end{Highlighting}
\end{Shaded}

Running the function on the saved model object should generate a large
figure comprising six plots\footnote{If your version of RStudio does not
  display the six-panelled figure but instead only a white canvas, this
  is probably because your Plots pane is too small. In this case, you
  will need to make it as large as possible and then try running the
  function again. If that doesn't work either, don't worry as we will
  save and examine individual plots from now on.} (see
Figure~\ref{fig-6plots}). In the following, we will learn to interpret
them one-by-one.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{check\_model}\NormalTok{(L1.model.back2)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{13_MultipleLinearRegression_files/figure-pdf/fig-6plots-1.pdf}}

}

\caption{\label{fig-6plots}Six plots output by the
\texttt{check\_model()} function for assessing key assumptions of linear
regression models}

\end{figure}%

By default, \texttt{check\_model()} outputs a single figure that allows
us to check the most important model assumptions. This large figure is
useful for reporting purposes, but it is rather unwieldy for
interpretation. Changing the ``panel'' argument of the
\texttt{check\_model()} function to \texttt{FALSE} returns a list of
\{ggplot\} objects that we can save to our local environment as
\texttt{diagnostic.plots}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diagnostic.plots }\OtherTok{\textless{}{-}} \FunctionTok{plot}\NormalTok{(}\FunctionTok{check\_model}\NormalTok{(L1.model.back2, }\AttributeTok{panel =} \ConstantTok{FALSE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Then we can use the double square bracket operator to display a single
diagnostic plot from this saved list:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diagnostic.plots[[}\DecValTok{1}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{13_MultipleLinearRegression_files/figure-pdf/fig-PosteriorPredictiveCheck-1.pdf}}

}

\caption{\label{fig-PosteriorPredictiveCheck}Comparing the distribution
of observed vocabulary scores (green) with data simulated based on model
predictions (blue)}

\end{figure}%

This first plot (Figure~\ref{fig-PosteriorPredictiveCheck}) is another
way to compare the model's predicted values (represented here as blue
distributions) with the real-life (i.e observed) outcome variable
(represented here in green). We can see that the center of the
distribution of observed \texttt{Vocab} scores is slightly shifted to
the right compared to most distributions of model-predicted data. That
said, the simulated distributions are close to the real distribution and
largely follow a similar shape. If they didn't, this would suggest that
a linear regression model may not be suitable for our data.

The second plot (Figure~\ref{fig-Linearity}) is designed to check the
assumption of \textbf{linearity}. If the predictors are linearly
related, the green reference line should be flat and horizontal. Our
reference line is slightly curved, but it remains possible to draw a
horizontal line through the grey band, hence we can conclude that the
assumption of linearity is not severely violated.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diagnostic.plots[[}\DecValTok{2}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{13_MultipleLinearRegression_files/figure-pdf/fig-Linearity-1.pdf}}

}

\caption{\label{fig-Linearity}The relationship between model predictions
(fitted values) and model residuals}

\end{figure}%

Note that Figure~\ref{fig-Linearity} is actually the same kind of plot
as Figure~\ref{fig-VocabResidual} that we generated to check the
assumption of equal (or constant) variance,
i.e.~\textbf{homoscedasticity}. Fitted values is another term for
predicted values. Thus, in Figure~\ref{fig-Linearity}, the \emph{x}-axis
represents the \texttt{Vocab} scores predicted by the model. When the
assumption of homoscedasticity is met, the model residuals are randomly
distributed above and below 0, i.e.~they do not notably increase or
decrease as predicted values increase.

The \{performance\} package proposes a different kind of diagnostic plot
to check the assumption of \textbf{homoscedasticity} (see
Figure~\ref{fig-Homoscedasticity}) with the square-root of the absolute
standardised residuals on the \emph{y}-axis. A roughly flat and
horizontal green reference line indicates homoscedasticity. Again,
although the reference line in Figure~\ref{fig-Homoscedasticity} is by
no means perfectly flat, a flat line can be drawn within the grey band,
suggesting that the assumption is met.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diagnostic.plots[[}\DecValTok{3}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{13_MultipleLinearRegression_files/figure-pdf/fig-Homoscedasticity-1.pdf}}

}

\caption{\label{fig-Homoscedasticity}The relationship between the model
predictions (fitted values) and the square root of absolute standardised
residuals}

\end{figure}%

The fourth diagnostic plot (Figure~\ref{fig-Influential}) helps to
detect \textbf{outliers} in the data that may have a particularly strong
influence on our model. It is based on Cook's distance (see Levshina
2015: Section 7.2.4; Sonderegger 2023: Section 5.7.3). Any points that
fall outside the dashed green lines fall outside Cook's distance and are
considered \textbf{influential observations}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diagnostic.plots[[}\DecValTok{4}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{13_MultipleLinearRegression_files/figure-pdf/fig-Influential-1.pdf}}

}

\caption{\label{fig-Influential}The relationship between leverage as
estimated using Cook's distance and standardised residuals}

\end{figure}%

In Figure~\ref{fig-Influential}, no data point falls outside of Cook's
distance so we are not concerned about influential observations
violating the assumptions of our model. Still, it is interesting to
briefly explore the most influential observations, which are labelled by
their index number in the dataset. In Figure~\ref{fig-Influential}, one
of these is participant number~21:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L1.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice}\NormalTok{(}\DecValTok{21}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(Age, Gender, Occupation, Blocks, EduTotal, ART, Vocab)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  Age Gender      Occupation Blocks EduTotal ART    Vocab
1  41      F Senior Lecturer      9       21  43 93.33333
\end{verbatim}

As you can see, this female senior lecturer is rather unusual: she
performed below average in the non-verbal IQ test (\texttt{Blocks}), yet
achieved the second-highest \texttt{Vocab} score. She also performed far
above average on the author recognition test (\texttt{ART}) and reported
the longest period in formal education among the L1 participants
(\texttt{EduTotal}).

In some cases, it may be justified to remove overly influential
outliers; however, this should typically only be done when we are fairly
certain that the outliers were caused by a technical error, such as a
measuring instrument not functioning properly, or a human error, such as
a participant misunderstanding the direction of a response scale. All
other outliers may be theoretically interesting: if our aim is to
generalise our model to the full population, we must be prepared to
include some unusual observations that also belong to that population.
In the case of L1 English speakers, this includes people who spent more
than 20 years in formal education and are seemingly much more into
languages than the kind of abstract puzzles typically found in
non-verbal IQ tests!

The fifth diagnostic plot output by the \texttt{check\_model()} function
(Figure~\ref{fig-Collinearity}) serves to check the assumption of a no
\textbf{multicollinearity}. Multicollinearity, or \textbf{high
collinearity}, refers to a strong linear dependence between predictors
such that they do not contribute unique or independent information to
the model. Multicollinearity should not be confused with a strong
correlation between two individual predictors as measured using the
\texttt{cor.test()} function (see Section~\ref{sec-Correlations}). What
matters here is the association between one or more predictor variables,
conditional on the other variables in the model. This is considerably
more complex to calculate, but luckily, there are several functions that
allow us to do just that in \texttt{R}.

The \{performance\} package relies on the Variance Inflation Factor
(VIF) to quantify collinearity. Typically, VIF scores above 10 are
considered to indicate a problematic degree of collinearity between some
predictors. Figure~\ref{fig-Collinearity} indicates that our model does
not suffer from multicollinearity.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diagnostic.plots[[}\DecValTok{5}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{13_MultipleLinearRegression_files/figure-pdf/fig-Collinearity-1.pdf}}

}

\caption{\label{fig-Collinearity}VIF factor and 95\% confidence interval
for each predictor in the model}

\end{figure}%

Finally, the sixth plot output by the \texttt{check\_model()} function
serves to check the assumption of the normality of the residuals. In
Section~\ref{sec-NormalityResiduals}, we achieved this by visualising
the distribution of residuals as a density plot (see
Figure~\ref{fig-VocabResidualDensity}). The diagnostic plot presented in
Figure~\ref{fig-Normality}, by contrast, is a so-called Q-Q plot
(quantile-quantile plot). These plots are designed to compare the shapes
of distributions. If the residuals follow a perfect normal distribution,
the points should all fall along the straight reference line (in green).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diagnostic.plots[[}\DecValTok{6}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{13_MultipleLinearRegression_files/figure-pdf/fig-Normality-1.pdf}}

}

\caption{\label{fig-Normality}Q-Q plot}

\end{figure}%

In Figure~\ref{fig-Normality}, we see that most residuals follow a
normal distribution, except some observations at the tails of the
distribution. This is fairly typical and, if the other model assumptions
are not violated, minor deviations from normality like this are unlikely
to be a problem, especially with larger sample sizes.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, titlerule=0mm, leftrule=.75mm]

The \texttt{check\_model()} vignette provides more detailed information
on these diagnostic plots, how to interpret them, and what to do if the
assumptions are not met:
\url{https://easystats.github.io/performance/articles/check_model.html}.

\end{tcolorbox}

\section{Tapping into the potential of statistical
modelling}\label{tapping-into-the-potential-of-statistical-modelling}

This chapter aimed to provide first insights into the potential of
statistical modelling with multiple predictors. It has demonstrated the
importance of considering interactions between predictors and of
visualising both the observed sample data, as well as simulated values
based on the predictions of a statistical model.

You may have noted that \emph{p}-values were \emph{not} at the heart of
this chapter. Instead, we focused on model accuracy (as measured by the
adjusted R\textsuperscript{2}), coefficient estimates and their relative
importance (as measured by the lmg metric), predicted values, and
(partial) model residuals. This is because there is an unfortunate
tendency among some students and researchers to be misled into thinking
that, if a result turns out to be statistically significant, it must be
true. As statistician Andrew Gelman puts it, some confuse statistics for
``a form of modern alchemy, transforming the uncertainty and variation
of the laboratory and field measurements into clean scientific
conclusions that can be taken as truth'' (Gelman 2018: 43).

I hope that working through Chapter~\ref{sec-Inferential} to
\ref{sec-MLR} has shown you that a big part of learning to work with
quantitative data in the language sciences is really about ``embracing
variation and accepting uncertainty'' (Gelman 2019). If we intend to
report inferential statistics based on sample data, it is important that
we decide on significance level thresholds, model selection criteria,
and other such parameters \emph{before} conducting our data analysis (on
the benefits of preregistrating protocols and methods in the language
sciences, see e.g. Mertzen, Lago \& Vasishth 2021; Roettger 2021). If we
want to test hypotheses, these must be well-defined hypotheses and we
must strive to quantify (and ideally also visualise!)
\textbf{uncertainty}. Ultimately, it is worth remembering that ``in
almost all practical data analysis situations -- we can only draw
uncertain conclusions from data, regardless of whether we manage to
obtain statistical significance or not'' (Vasishth \& Gelman 2021:
1311). Crucially, we must be extremely careful when extrapolating our
results beyond the range of our observed data.

As explained in Chapter~\ref{sec-Inferential}, statistical inference
based on NHST and \emph{p}-values comes from a statistical framework
called \textbf{frequentism}, which happens to (currently) be the most
widely used framework in the language sciences. Taking a frequentist
approach means that the statistical properties of the hypothesis tests
that we conduct are considered under hypothetical replications of our
study with new sample data. This is because, in the frequentist
framework, we estimate the long-run probability of observing certain
effects, were we to repeat the study many times. This is one of the
reasons why \textbf{replication} (see Section~\ref{sec-Reproducibility})
is key to advancing our knowledge of linguistics and language teaching
and learning. Whenever we have small sample sizes, small effect sizes
(i.e.~small coefficient estimates in statistical models), large
measurement error, and/or lots of variability among the target
population -- as is often the case in the language sciences -- we simply
cannot get \textbf{reliable} results from a single sample.

Does this mean that we should give up with quantitative data analysis
and statistics all together? No, of course not. On the contrary, this
uncertainty makes research in the language sciences all the more
interesting and worth pursuing! It also means that there is much more to
be learnt in terms of methods. In this chapter, you have learnt about
\textbf{frequentist fixed-effects linear regression models}. These
models are incredibly useful and can be used in many contexts, but they
make a number of important assumptions that do not always hold:

\begin{itemize}
\item
  Perhaps the most obvious, yet one that we have not discussed so far,
  is that the \textbf{outcome variable} of a linear regression model
  must be \textbf{quantitative}, like the \texttt{Vocab} variable in
  \texttt{Dabrowska.data}, which ranges from -13.33 to 95.56 (see
  Section~\ref{sec-Variables}). Other types of statistical models can be
  used to model other types of outcome variables, including:

  \begin{itemize}
  \item
    \textbf{Binomial (or binary) logistic regression models} allow us to
    predict \textbf{binary} outcomes (e.g.~whether or not a verb is
    negated) (to find out more, see e.g. Levshina 2015: Chapter~12;
    Sonderegger 2023: Chapter~6; Winter 2020: Chapter~12).
  \item
    \textbf{Multinomial logistic regression models} can be used to model
    \textbf{categorical} outcome variables with more than two levels
    (e.g.~which modal verb is used in certain constructions) (to find
    out more, see e.g. Levshina 2015: Chapter~13).
  \item
    \textbf{Poisson regression models} are used to model \textbf{count}
    variables, i.e.~discrete numeric variables such as the frequency of
    fillers (such as \emph{uh} and \emph{oh}) in certain contexts (to
    find out more, see e.g. Winter 2020: Chapter~13).
  \end{itemize}
\item
  The observations (i.e.~the data points) used to fit a
  \textbf{fixed-effect model} must be independent of each other. In the
  language sciences, such a situation is actually quite rare (see
  Section~\ref{sec-IndependenceSLR}). To model interdependencies between
  observations, we can fit \textbf{mixed-effects models} (also called
  multilevel or hierarchical models) (to learn more, see e.g. Gries
  2021: Chapter~6; Sonderegger 2023: Chapters~8-10; Winter 2020:
  Chapters~14-15).
\item
  Linear regression models assume \textbf{linear} relationships between
  the predictors. There are different ways to circumvent this problem.
  In some cases, predictors can be \textbf{transformed} to meet this
  assumption (see Winter 2020: Chapter~5). In others, it may be wiser to
  model non-linear associations with other kinds of models such as
  Generalised Additive Mixed Models Wieling (2018).
\item
  Finally, we need not stick to the \textbf{frequentist} school of
  statistics. In fact, quantitative linguists are increasingly turning
  to \textbf{Bayesian} statistics and finding that Bayesian models help
  them work with the particularities of linguistic data (to learn more
  about Bayesian statistics, see e.g. Levshina 2022; Nicenboim, Schad \&
  Vasishth 2026).
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Recommended further reading ðŸ“š}, titlerule=0mm, leftrule=.75mm]

In this textbook, we have only just scratched the surface of statistical
modelling. We can do much, much more with these kinds of models, and
there are lots of additional things to take into account. The good news
is that there are lots of excellent resources to help you continue your
statistical modelling journey. Here are some good places to get started
(in alphabetical order):

\begin{itemize}
\tightlist
\item
  Gries, Stefan Thomas. 2021. \emph{Statistics for linguistics with R: A
  practical introduction} (De Gruyter Mouton Textbook).
  3\textsuperscript{rd} revised edition. De Gruyter Mouton.
\item
  Levshina, Natalia. 2015. \emph{How to do linguistics with R: Data
  exploration and statistical analysis}. John Benjamins.
\item
  Nicenboim, Bruno, Daniel Schad \& Shravan Vasishth. 2026. Introduction
  to Bayesian Data Analysis for cognitive science (Chapman \& Hall/CRC
  Statistics in the Social and Behavioral Sciences Series). CRC Press.
  Open Access version: \url{https://bruno.nicenboim.me/bayescogsci/}.
\item
  Sonderegger, Morgan. 2023. \emph{Regression modeling for linguistic
  data}. Cambridge, Massachusetts: The MIT Press. Open Access version:
  \url{https://osf.io/pnumg/}.
\item
  Winter, Bodo. 2020. Statistics for Linguists: An Introduction Using R.
  Routledge.
\end{itemize}

Although they go further than the present textbook, you will also find
that these resources begin by explaining many of the things already
covered in this chapter and previous chapters, and that's actually a
good thing. There's no harm in revising these complex topics from a
different perspective, with different examples, \texttt{R} packages, and
coding styles.

\end{tcolorbox}

\subsection*{Check your progress ðŸŒŸ}\label{check-your-progress-12}
\addcontentsline{toc}{subsection}{Check your progress ðŸŒŸ}

Congratulations: you have successfully completed the most difficult
chapter of this textbook! You have answered { out of 16 questions}
correctly.

Are you confident that you can\ldots?

\begin{itemize}
\tightlist
\item[$\square$]
  Fit a linear regression model in \texttt{R} with multiple numeric and
  categorical predictors and interpret the intercept and predictor
  coefficients
\item[$\square$]
  Center numeric predictors to make the intercept more meaningful and
  improve model interpretability
\item[$\square$]
  Assess the importance of predictors using metrics like lmg
\item[$\square$]
  Visualise and interpret model predictions (with confidence bands) and
  partial residuals using the \{visreg\} library
\item[$\square$]
  Model and interpret interactions between predictors in multiple linear
  regression models, and visualise these interactions to see how one
  predictor moderates the effect of another on the outcome variable
\end{itemize}

\bookmarksetup{startatroot}

\chapter{\texorpdfstring{Rep\texttt{R}oducible research and academic
w\texttt{R}iting in
Quarto}{RepRoducible research and academic wRiting in Quarto}}\label{sec-LiteRateProgramming}

\subsection*{Chapter overview}\label{chapter-overview-12}
\addcontentsline{toc}{subsection}{Chapter overview}

So far, we have seen how we can export the outputs of our analyses
conducted in \texttt{R} in the form of tables
(Section~\ref{sec-ExportingRObjects}) and graphics
(Section~\ref{sec-ExploringPlots}). However, in most situations, we want
to communicate our research in a way that allows us to combine both text
and analysis outputs. This is where \textbf{literate programming} comes
into play!

In this chapter, you will learn:

\begin{itemize}
\tightlist
\item
  about the concept of literate programming
\item
  why reproducibility matters
\item
  how to make your research more reproducible
\item
  how to use Quarto to write research reports, theses, and academic
  papers
\item
  how to export and share your research in different formats including
  HTML, PDF, LibreOffice Writer, and Microsoft Word.
\end{itemize}

\section{Literate programming}\label{sec-LitProgramming}

The basic idea of literate programming is that we combine text, code,
and code outputs (i.e.~tables, statistics, and plots) within a single
document that can be exported into different formats for sharing and
publishing.

Literate programming can be implemented in different authoring formats.
Up until very recently, the most common format for \texttt{R} projects
was \href{https://rmarkdown.rstudio.com/}{R Markdown}. For
\texttt{Python} projects, \href{https://jupyter.org/}{Jupyter Notebooks}
remains the standard to date. In this chapter, we will focus on Quarto,
a relatively new
\href{https://en.wikipedia.org/wiki/Free_and_open-source}{open-source}
scientific and technical authoring and publishing system that has the
advantage of supporting many different programming languages. This means
that code in \texttt{R}, \texttt{Python}, \texttt{Julia}, and other
languages can be combined into one document, making project management
and collaboration much easier. Quarto also allows us to readily export
(or \textbf{render}) our documents to multiple formats such as HTML,
PDF, and Word (see Figure~\ref{fig-QuartoFormats} and
Section~\ref{sec-PublishingFormats}).

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/AHorst_Quarto.png}}

}

\caption{\label{fig-QuartoFormats}A schematic representation showing
Quarto can understand multi-language input and produce multi-format
outputs (Artwork \href{https://creativecommons.org/licenses/by/4.0/}{CC
BY 4.0}
\href{https://allisonhorst.com/cetinkaya-rundel-lowndes-quarto-keynote}{Allison
Horst} from the
\href{https://mine.quarto.pub/hello-quarto/\#/hello-quarto-title}{``Hello,
Quarto'' keynote} by Julia Lowndes and Mine Ã‡etinkaya-Rundel, first
presented at the RStudio Conference 2022).}

\end{figure}%

Literate programming is particularly useful for \textbf{academic
research} and \textbf{data science}. Did you know that this entire
textbook was written in Quarto? I chose this format because it allows
for the seamless combination of explanations with nicely formatted
\texttt{R} code and code outputs (i.e.~all of the textbook's tables,
data visualisations, quiz questions, etc.). It also automatically
generates consistent section and figure numbers, cross-references,
bibliographic references, and much more. By the end of this chapter,
you'll be ready to start writing your own term paper, dissertation,
thesis, journal article, blog, or presentation slides in Quarto.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, titlerule=0mm, leftrule=.75mm]

Quarto documents are designed to:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Help you \textbf{collaborate} with other researchers (including your
  future self!) who are interested in both reproducing your results and
  understanding how you reached them (i.e.~the code).
\item
  Provide you with a convenient \textbf{environment} in which to do
  research - a kind of ``modern-day lab notebook where you can capture
  not only what you did, but also what you were thinking'' (Wickham,
  Ã‡etinkaya-Rundel \& Grolemund 2023).
\item
  \textbf{Communicate} your analyses to others, including those who are
  not familiar with any programming language.
\end{enumerate}

\end{tcolorbox}

\section{Reproducible research}\label{sec-Reproducibility}

Not only is using Quarto (or any other literate programming format, see
Section~\ref{sec-LitProgramming}) very convenient, it also helps us make
our research more reproducible. Unfortunately, the terms
\textbf{reproducible}, \textbf{replicable} and \textbf{repeatable} are
often confused and, not helping matters, some definitions in the
literature contradict each other. In this textbook, we will adopt the
terminology of \href{https://book.the-turing-way.org/}{The Turing Way}.
We thus define \textbf{reproducibility} as the ability of an independent
researcher or team to obtain the same results as in a study using the
same data and methods as the original study (see
Figure~\ref{fig-reproducibility-terminology}).

This is in contrast to \textbf{replicability}, where the same methods,
but different data are used; and \textbf{robustness}, where the same
data, but different methods are used. Finally, if a finding can be
reliably observed across different datasets with different methods, then
we can say that the finding is \textbf{generalisable}.

\begin{figure}

\centering{

\includegraphics[width=4.625in,height=\textheight,keepaspectratio]{images/TuringWay_reproducible-matrix.jpg}

}

\caption{\label{fig-reproducibility-terminology}Defining
\emph{reproducibility} and related terms
(\href{https://creativecommons.org/licenses/by/4.0/}{CC BY 4.0}
\href{https://book.the-turing-way.org/reproducible-research/overview/overview-definitions}{The
Turing Way Community} )}

\end{figure}%

Given this definition, reproducibility might seem like a low bar to
pass. You might be thinking: shouldn't it be obvious that we'll get the
same results if we repeat a study using exactly the same data and
method? Well, yes, it should be. But it very often isn't! For a start,
to be able to even attempt to reproduce the results of a study, the
underlying data must be available. Linguists that share their primary
data as Ewa DÄ…browska did as part of her 2019 publication (DÄ…browska
2019) remain the exception rather than the norm (see Bochynska et al.
2023)\footnote{Although there is ground for optimism here as more and
  more linguists and language education scholars are beginning to make
  their data open in repositories such as
  \href{https://www.iris-database.org/}{IRIS},
  \href{https://dataverse.no/dataverse/trolling}{TROLLing}, and the
  \href{https://osf.io/}{Open Science Framework (OSF)} (see
  Section~\ref{sec-Sharing}).}. Second, the data must be available in an
accessible format and must be published together with enough
documentation to be understandable to an independent researcher. Third,
the author(s) of the original study need to have very diligently
documented all their data wrangling and analyses steps. The best way to
do this is undoubtedly to use code that does not require closed-source
software (e.g.~a researcher without a license for SPSS or Stata will not
be able to run SPSS or Stata scripts, see Section~\ref{sec-OpenSource}).
This open code must be shared in an accessible format, too. Fourth,
independent researchers need to be able to run these scripts. To this
end, it is important that they know exactly which tools were used. Thus,
if the analyses were conducted in \texttt{R}, they need to know which
\texttt{R} version and which packages and package versions were used
(Section~\ref{sec-Packages}). They also need to know in which order the
scripts were run and, finally, the scripts must run on their own
computers without any errors. So now, reproducibility doesn't sound
quite so easy, right? Luckily, if we apply the principles of literate
programming in \textbf{Quarto}, we can go a long way towards ensuring
that our research is reproducible.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Going furtherï¸ ðŸš€}, titlerule=0mm, leftrule=.75mm]

To find out more about best practices for reproducible research, check
out \href{https://book.the-turing-way.org/}{The Turing Way}'s excellent
\href{https://book.the-turing-way.org/reproducible-research/reproducible-research}{Guide
for Reproducible Research}.

\end{tcolorbox}

\section{Getting started with Quarto}\label{sec-StartQuarto}

We will be writing Quarto documents from the \emph{RStudio}
IDE\footnote{Many other IDEs (Integrated Developer Environments, see
  Section~\ref{sec-IDE}) support Quarto, including
  \href{https://code.visualstudio.com/download}{VS Code},
  \href{https://jupyter.org/}{JupyterLab}, and
  \href{https://neovim.io/}{Neovim}. Feel free to pick the IDE that you
  are most comfortable with!}, which conveniently ships with a version
of Quarto, meaning that no additional installation\footnote{If you need
  to install Quarto, go to \url{https://quarto.org/docs/get-started/}
  and download the latest Quarto version that is compatible with your
  operating system. Once the download is completed (which may take
  several minutes), double-click on the installer file that you
  downloaded and click your way through the installation process.} is
required for you to use Quarto on your computer.

To get started with Quarto:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  In \emph{RStudio}, create a new Project by selecting
  \emph{File~\textgreater~New Project\ldots{}} in the main menu, or by
  clicking on the ``new project'' button.

  \begin{enumerate}
  \def\labelenumii{\roman{enumii}.}
  \tightlist
  \item
    You can choose the first option to create a new folder for your
    project if you've not yet got one,
  \item
    or the second option to select an existing project directory.
  \end{enumerate}
\item
  Then, create a new Quarto document by navigating to
  \emph{File~\textgreater~New File~\textgreater~Quarto
  Document\ldots{}}, or clicking on the ``new document'' button and
  selecting ``\emph{Quarto Document\ldots''}. A dialogue menu will
  appear (Figure~\ref{fig-QuartoNew}). Leave everything as is and simply
  click on''Create'' at the bottom.
\end{enumerate}

\begin{figure}

\centering{

\includegraphics[width=4.17708in,height=\textheight,keepaspectratio]{images/Quarto_new.png}

}

\caption{\label{fig-QuartoNew}Creating a new Quarto document}

\end{figure}%

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \emph{RStudio} has now opened a new, untitled Quarto file
  (\texttt{.qmd}). Change the title of the document (which is not the
  same as its filename!) and add the following three lines to the
  \textbf{document header} by copying and pasting the following lines at
  the top of the document. Quarto document headers\footnote{Note that,
    in YAML syntax, character strings such as the document title and
    your name must be enclosed in quotation marks. By contrast, the date
    is not enclosed in quotation marks because it is a dynamic variable
    that will be adjusted to your computer's system date so that, every
    time you render the document, the date will be updated.} are written
  in \textbf{YAML} which, I kid you not, stands for \emph{Yet Another
  Markup Language}! ðŸ˜…
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\PreprocessorTok{{-}{-}{-}}
\FunctionTok{title}\KeywordTok{:}\AttributeTok{ }\StringTok{"Learning Quarto"}
\FunctionTok{subtitle}\KeywordTok{:}\AttributeTok{ }\StringTok{"by reproducing the descriptive statistics of DÄ…browska\textquotesingle{}s (2019) study"}
\FunctionTok{author}\KeywordTok{:}\AttributeTok{ }\StringTok{"Write your name here"}
\FunctionTok{date}\KeywordTok{:}\AttributeTok{ last{-}modified}
\PreprocessorTok{{-}{-}{-}}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\item
  Click on the ``save'' button in the menu bar or navigate to
  \emph{File~\textgreater~Save} to save your \texttt{.qmd} file. You
  will be prompted to give it a name. This could be
  \texttt{LearningQuarto.qmd} (see Section~\ref{sec-FileNaming} for tips
  on how to name files).
\item
  To check your Quarto installation, render your document by either
  selecting \emph{File~\textgreater~Render Document} in the main menu,
  or clicking on ``Render'' button in the Quarto menu bar (see
  Figure~\ref{fig-QuartoRender}). Your \texttt{.qmd} file will
  automatically be rendered to HTML (Quarto's default rendering format).
\item
  Navigate to the folder where you saved your \texttt{.qmd} file to find
  the rendered HTML file. You can use a Finder (on macOS) or File
  Explorer window (on Windows) or go to the ``Files'' pane in
  \emph{RStudio} to do this. The rendered version of your file will have
  the same filename as your Quarto document, but with the file extension
  \texttt{.html} (e.g.~\texttt{LearningQuarto.html}). If you open on the
  file, it will appear in your default web browser (e.g.~Firefox,
  Chrome, Safari). You should see that the HTML document features the
  title of your document, your name as the author, and today's date (see
  Figure~\ref{fig-QuartoHTML}).
\end{enumerate}

\begin{figure}

\begin{minipage}{0.50\linewidth}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/LearningQuarto_qmd.png}}

}

\caption{\label{fig-QuartoRender}The \texttt{.qmd} file as opened in
RStudio}

\end{figure}%

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/LearningQuarto_html.png}}

}

\caption{\label{fig-QuartoHTML}The \texttt{.html} file as opened in a
web browser}

\end{figure}%

\end{minipage}%

\end{figure}%

For now, the document is empty. In the next sections, you will learn how
to add text, code, and code outputs to your Quarto document.

\subsection{\texorpdfstring{\emph{RStudio}'s visual
editor}{RStudio's visual editor}}\label{rstudios-visual-editor}

You may have noticed that \emph{RStudio} proposes two different modes in
which Quarto documents can be edited: \textbf{Source} and
\textbf{Visual} (see Figure~\ref{fig-QuartoModes}).

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/Quarto_Source-Visual.png}}

}

\caption{\label{fig-QuartoModes}Source and Visual mode in RStudio}

\end{figure}%

The Visual mode offers a
\href{https://en.wikipedia.org/wiki/WYSIWYM}{WYSIWYM} (What You See Is
What You Mean) authoring experience. This means that, in Visual model,
you will immediately see the effect of your formatting on screen. For
example, to format a word in italics, you can click on the corresponding
button in the toolbar (see Figure~\ref{fig-QuartoModes}) or use the
keyboard shortcut (âŒ˜/Ctrl + I) --- just like you would in
text-processing software --- and this will immediately display the text
in italics.

\section{Markdown text}\label{sec-Markdown}

Writing and formatting text in \emph{RStudio}'s Visual editor is very
similar to writing in a word-processing software such as LibreOffice
Writer or Microsoft Word. In the background, however, \emph{RStudio}
automatically converts your formatted text to \textbf{Markdown} in the
underlying source code of your \texttt{.qmd} file. Markdown is a
\textbf{plain-text format}. For example, in Markdown, words in italics
are enclosed in asterisks like this: \texttt{*italics*}.
\textbf{?@tbl-markdown} displays the Markdown syntax for other
formatting options commonly used in academic writing.

The best way to get the hang of Markdown is simply to try things out.
You will also find a handy cheatsheet under \emph{Help \textgreater{}
Markdown Quick Reference}. Remember that you can always go back to the
\textbf{Visual} mode to format your text if that's easier for you. When
it comes to debugging any Quarto syntax errors, however, it's usually
easier to catch these in plain text, so you'll typically want to switch
to the \textbf{Source} mode for that.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, titlerule=0mm, leftrule=.75mm]

Markdown is gaining in popularity and is now widely supported across
many platforms, from text editors to content management systems,
ensuring that your formatting remains consistent and portable. Whether
you're writing documentation, creating blog posts, or taking notes,
Markdown's simplicity and versatility make it a valuable skill to have
beyond academic writing and Quarto.

You can learn more about Markdown here:
\url{https://www.markdownguide.org/basic-syntax/}.

\end{tcolorbox}

\newpage

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, titlerule=0mm, leftrule=.75mm]

To make writing in Quarto more convenient and less error-prone, you can
switch on a \textbf{spell-checker} within \emph{RStudio}. To do so, go
to \emph{Tools~\textgreater~Global Options\ldots~\textgreater~Spelling}.

\end{tcolorbox}

\section{Code chunks}\label{sec-Chunks}

To run code inside a Quarto document, we need to insert a code chunk.
There are three ways to do so:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Using the keyboard shortcut âŒ˜/Ctrl + Option + i
\item
  Clicking on the green ``Insert chunk'' button icon in the editor
  toolbar
\item
  Manually typing the chunk delimiters
  \texttt{\textasciigrave{}\textasciigrave{}\textasciigrave{}\{r\}} and
  \texttt{\textasciigrave{}\textasciigrave{}\textasciigrave{}}
\end{enumerate}

It is definitely worth learning the keyboard shortcut as it will save
you a lot of time in the long run!

In the code chunk below, \texttt{\{r\}} tells Quarto that this chunk is
written in the programming language \texttt{R}. If we wanted to embed a
chunk of Python code, we must begin it with
\texttt{\textasciigrave{}\textasciigrave{}\textasciigrave{}\{python\}}
instead.

Using one of the three aforementioned options, insert the following
\texttt{R} code chunk in your document.

\begin{Shaded}
\begin{Highlighting}[]
\InformationTok{\textasciigrave{}\textasciigrave{}\textasciigrave{}\{r\}}
\FunctionTok{library}\NormalTok{(here)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\InformationTok{\textasciigrave{}\textasciigrave{}\textasciigrave{}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# https://github.com/quarto{-}dev/quarto{-}cli/issues/4492\#issuecomment{-}1548655951}
\NormalTok{emoji }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) \{}
  \ControlFlowTok{if}\NormalTok{ (knitr}\SpecialCharTok{::}\FunctionTok{is\_latex\_output}\NormalTok{()) \{}
\NormalTok{ stringr}\SpecialCharTok{::}\FunctionTok{str\_c}\NormalTok{(}\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{emoji\{"}\NormalTok{, }\FunctionTok{str\_replace\_all}\NormalTok{(x, }\StringTok{"\_"}\NormalTok{, }\StringTok{"{-}"}\NormalTok{), }\StringTok{"\}"}\NormalTok{)}
\NormalTok{  \} }\ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{ (knitr}\SpecialCharTok{::}\FunctionTok{is\_html\_output}\NormalTok{()) \{}
\NormalTok{ stringr}\SpecialCharTok{::}\FunctionTok{str\_c}\NormalTok{(}\StringTok{":"}\NormalTok{, x, }\StringTok{":"}\NormalTok{)}
\NormalTok{  \} }\ControlFlowTok{else}\NormalTok{ x}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

As you are working on your code within a Quarto document, you can either
run:

\begin{itemize}
\tightlist
\item
  each individual line of code using the keyboard shortcut âŒ˜/Ctrl~âŽ or
\item
  the entire code chunk either by clicking the
  ``Run''\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/notebook-run-chunk.png}}icon
  or with shortcut â‡§~âŒ˜/Ctrl~âŽ.
\end{itemize}

\emph{RStudio} will execute the code and display the results either
within your document (below each chunk) or in the Console, depending on
your \emph{RStudio} settings.\footnote{You can change this behaviour in
  your \emph{RStudio} preferences under \emph{Tools \textgreater{}
  Global Options \textgreater{}} R Markdown by selecting or unselecting
  the option: ``Show output inline for all R Markdown documents''.}

Chunk output can be customised with \textbf{chunk options}. There are
many options to choose from but, the most important options control
whether your code block should be executed when you render your Quarto
document and what results are inserted in the rendered version:

\begin{itemize}
\item
  \texttt{eval:\ false} prevents code from being evaluated. Given that
  the code is not run, no code outputs will be generated.
\item
  \texttt{include:\ false} runs the code, but does not show the code or
  its outputs in the rendered document. This option is useful for code
  chunks that are not informative to the readers of your document.
\item
  \texttt{echo:\ false} prevents the code from appearing in the rendered
  document, but displays the code outputs. This option is useful when
  you want to present the results of your analyses to people who are not
  interested in the underlying code.
\item
  \texttt{message:\ false} or \texttt{warning:\ false} prevents messages
  or warnings from appearing in the rendered document.
\end{itemize}

It is also possible to \textbf{label} code chunks using the
\texttt{label} option (see Figure~\ref{fig-QuartoEvalFalse}). This can
help to navigate long Quarto documents and to quickly identify which
code chunk is causing errors during rendering. Chunk labels should be
short but meaningful. They should not contain spaces or any other
special characters except hyphens (\texttt{-}).

In \emph{RStudio} the simplest way to set a chunk option is by clicking
the gear icon on the chunk you want to modify. This way, you can both
choose a label and set up your chunk. If you prefer to write code chunk
options manually, these are placed at the top of the corresponding chunk
following \texttt{\#\textbar{}}, as in the chunk below. As you can see
in Figure~\ref{fig-EvalFalseRendered}, the \texttt{eval:\ false} chunk
option means that the rendered document includes the mathematical
operation \texttt{13\ *\ 13}, but not the result because the chunk was
not executed during the rendering process.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/Quarto_eval-false.png}}

}

\caption{\label{fig-QuartoEvalFalse}Quarto document in Source mode in
\emph{RStudio}}

\end{figure}%

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/Quarto_rendered_eval-false.png}}

}

\caption{\label{fig-EvalFalseRendered}Rendered HTML document opened in a
browser}

\end{figure}%

\end{minipage}%

\end{figure}%

\section{Inline code}\label{sec-Inline}

So far, we have seen how we can insert and format text in Quarto and how
we can add code chunks with various options. But, to make the most of
literate programming, we want to combine the two.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-warning-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-warning-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Prerequisites}, titlerule=0mm, leftrule=.75mm]

This chapter assumes that you are familiar with the following research
article (which was first introduced in Section~\ref{sec-AccessingData}):

\begin{quote}
DÄ…browska, Ewa. 2019. Experience, Aptitude, and Individual Differences
in Linguistic Attainment: A Comparison of Native and Nonnative Speakers.
Language Learning 69(S1). 72-100.
\url{https://doi.org/10.1111/lang.12323}.
\end{quote}

Our starting point for this chapter are the author's \textbf{original
datasets}, which are linked in the article's Appendix S4.

\begin{quote}
\emph{Appendix S4: Datasets}

DÄ…browska, E. (2018). L1 data {[}Data set{]}. Retrieved from
\href{https://www.iris-database.org/details/9pplf-S7kw3}{https://www.iris-database.org/iris/app/home/detail?id=york:935513}

DÄ…browska, E. (2018). L2 data {[}Data set{]}. Retrieved from
\href{https://www.iris-database.org/details/L8w1U-ZDgnH}{https://www.iris-database.org/iris/app/home/detail?id=york:935514}
\end{quote}

You will only be able to reproduce the analyses and answer the quiz
questions from this chapter if you have successfully imported these two
two datasets. To do so, follow the instructions from
Section~\ref{sec-RProject} to Section~\ref{sec-ImportingDataCSV} and
complete {\textbf{Q6.8}}---{\textbf{Q6.12}}.

Alternatively, you can download \texttt{Dabrowska2019.zip} from
\href{https://github.com/elenlefoll/RstatsTextbook/raw/69d1e31be7394f2b612825f031ebffeb75886390/Dabrowska2019.zip}{the
textbook's GitHub repository}, which contains both datasets. To launch
the project correctly, first unzip the file and then double-click on the
\texttt{Dabrowska2019.Rproj} file.

\end{tcolorbox}

Insert the following \texttt{R} chunk to load the DÄ…browska (2019) data
so that it may be used in your Quarto document. As this
\texttt{import-data} chunk requires the \texttt{here()} function, make
sure that it comes \emph{after} the \texttt{setup} chunk because, when
the document is rendered, code chunks will be executed in the order that
they appear. If the \{here\} library is not loaded \emph{before} the
data are imported, the rendering process will be aborted and an error
message will be displayed in the Console.

To begin, we will reproduce the following basic descriptive statistics
about the two datasets:

\begin{quote}
Ninety native speakers (42 male and 48 female) and 67 nonnative speakers
of English (21 male and 46 female) were recruited through personal
contacts, church and social clubs, and advertisements in local
newspapers. (DÄ…browska 2019: 5)
\end{quote}

As you may recall from Chapter~\ref{sec-VaRiablesAndFunctions}, the
number of native and non-native participants corresponds to the number
of rows in the corresponding dataset:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{nrow}\NormalTok{(L1.data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 90
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{nrow}\NormalTok{(L2.data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 67
\end{verbatim}

In Quarto, we can use \textbf{inline code} to dynamically insert these
numbers in our paragraph. Inline code in \texttt{R} begins with
\texttt{\textasciigrave{}\{r\}} and ends with a single backtick
\texttt{\textasciigrave{}}. It is best to use the Source mode to insert
inline code. Using the Source mode, add the following section to your
Quarto document and render it to HTML.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{\#\# Descriptive statistics about the participants}

Â \InformationTok{\textasciigrave{}\{r\} nrow(L1.data)\textasciigrave{}}\NormalTok{ native speakers and }\InformationTok{\textasciigrave{}\{r\} nrow(L2.data)\textasciigrave{}}\NormalTok{ nonnative speakers of English were recruited through personal contacts, church and social clubs, and advertisements in local newspapers.}
\end{Highlighting}
\end{Shaded}

The rendered version should read like this (if you are obtaining
different numbers, this either means that you have tempered with the
original data files or that they have been corrupted)\footnote{Using
  Microsoft Excel to open these \texttt{.csv} files can corrupt the
  files and can happen even if you did not use Excel yourself (e.g.~on
  some Windows computers, this is sometimes done automatically as part
  of the download process). To find out more, see
  Section~\ref{sec-ExcelWarning}.}:

\begin{quote}
\subsection*{Descriptive statistics about the
participants}\label{descriptive-statistics-about-the-participants}
\addcontentsline{toc}{subsection}{Descriptive statistics about the
participants}

90 native speakers and 67 nonnative speakers of English were recruited
through personal contacts, church and social clubs, and advertisements
in local newspapers.
\end{quote}

Inline code should only be used for very simple code, ideally with no
more than one function, as in
\texttt{\textasciigrave{}\{r\}\ nrow(L1.data)\textasciigrave{}}. To
insert the output of more complex operations, it is best to write the
code and save its output(s) to the local environment in a \textbf{hidden
code chunk} (using the option \texttt{\#\textbar{}\ include:\ false},
see Section~\ref{sec-Chunks}).

The saved objects (\texttt{L1.males} and \texttt{L1.females}) each
contain one number. They can therefore be directly called within the
text as inline code:

\begin{Shaded}
\begin{Highlighting}[]
Â \InformationTok{\textasciigrave{}\{r\} nrow(L1.data)\textasciigrave{}}\NormalTok{ native speakers (}\InformationTok{\textasciigrave{}\{r\} L1.males\textasciigrave{}}\NormalTok{ male and }\InformationTok{\textasciigrave{}\{r\} L1.females\textasciigrave{}}\NormalTok{ female) and }\InformationTok{\textasciigrave{}\{r\} nrow(L2.data)\textasciigrave{}}\NormalTok{ nonnative speakers of English were recruited through personal contacts, church and social clubs, and advertisements in local newspapers.}
\end{Highlighting}
\end{Shaded}

When rendered, the paragraph will read:

\begin{quote}
90 native speakers (42 male and 48 female) and 67 nonnative speakers of
English were recruited through personal contacts, church and social
clubs, and advertisements in local newspapers.
\end{quote}

If we want to start our paragraph with 90 written in as a word rather
than in digits, we can use the \texttt{numbers\_to\_words\ function()}
function from the \{xfun\} package. First, you'll need to install the
\{xfun\} package and then add a line to your \texttt{setup} chunk to
load it.\footnote{To make your Quarto document even more reproducible,
  you can replace your \texttt{setup} chunk with the following function
  that will automatically check if a package needs to be installed
  before it is loaded:

\begin{Shaded}
\begin{Highlighting}[]
\InformationTok{\textasciigrave{}\textasciigrave{}\textasciigrave{}\{r\}}
\CommentTok{\#| label: improved{-}setup}
\CommentTok{\# List of packages necessary in this Quarto document:}
\NormalTok{packages }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"here"}\NormalTok{, }\StringTok{"tidyverse"}\NormalTok{, }\StringTok{"xfun"}\NormalTok{)}

\CommentTok{\# Function to install the packages that are not yet installed:}
\NormalTok{installed\_packages }\OtherTok{\textless{}{-}}\NormalTok{ packages }\SpecialCharTok{\%in\%}               \FunctionTok{rownames}\NormalTok{(}\FunctionTok{installed.packages}\NormalTok{()) }
\ControlFlowTok{if}\NormalTok{ (}\FunctionTok{any}\NormalTok{(installed\_packages }\SpecialCharTok{==} \ConstantTok{FALSE}\NormalTok{)) \{ }\FunctionTok{install.packages}\NormalTok{(packages[}\SpecialCharTok{!}\NormalTok{installed\_packages], }\AttributeTok{repos =} \StringTok{"https://packagemanager.rstudio.com/all/latest"}\NormalTok{) \}}

\CommentTok{\# Function to load the packages without printing any messages:}
\FunctionTok{invisible}\NormalTok{(}\FunctionTok{lapply}\NormalTok{(packages, library, }\AttributeTok{character.only =} \ConstantTok{TRUE}\NormalTok{))}
\InformationTok{\textasciigrave{}\textasciigrave{}\textasciigrave{}}
\end{Highlighting}
\end{Shaded}

  To ensure that the correct package version is installed, consider
  using \{renv\} or \{rix\} for your project (see
  Section~\ref{sec-Packages}).}

\begin{Shaded}
\begin{Highlighting}[]
\InformationTok{\textasciigrave{}\textasciigrave{}\textasciigrave{}\{r\}}
\CommentTok{\#install.packages("xfun")}
\FunctionTok{library}\NormalTok{(xfun)}
\InformationTok{\textasciigrave{}\textasciigrave{}\textasciigrave{}}
\end{Highlighting}
\end{Shaded}

First, you can test that it works by running this code:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{numbers\_to\_words}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(L1.data))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "ninety"
\end{verbatim}

To start our paragraph with a capital letter, we'll need to set the
function's \texttt{cap} argument to \texttt{TRUE}.

\begin{Shaded}
\begin{Highlighting}[]
\InformationTok{\textasciigrave{}Â \{\{r\}\} numbers\_to\_words(nrow(L1.data), cap = TRUE)\textasciigrave{}}\NormalTok{ native speakers (}\InformationTok{\textasciigrave{}\{r\} L1.males\textasciigrave{}}\NormalTok{ male and }\InformationTok{\textasciigrave{}\{r\} L1.females\textasciigrave{}}\NormalTok{ female) and }\InformationTok{\textasciigrave{}\{r\} nrow(L2.data)\textasciigrave{}}\NormalTok{ nonnative speakers of English...}
\end{Highlighting}
\end{Shaded}

Next, we want to reproduce the following descriptive statistics about
the L1 participants:

\begin{quote}
The L1 participants were all born and raised in the United Kingdom and
were selected to ensure a range of ages, occupations, and educational
backgrounds. The age range was from 17 to 65 years (\emph{M} = 38,
\emph{SD} = 16). (DÄ…browska 2019: 5)
\end{quote}

We can use the base \texttt{R} functions \texttt{min()}, \texttt{max()},
\texttt{mean()}, and \texttt{sd()} to compute these values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{The L1 participants were all born and raised in the United Kingdom and were selected to ensure a range of ages, occupations, and educational backgrounds. The age range was from }\InformationTok{\textasciigrave{}\{r\} min(L1.data$Age)\textasciigrave{}}\NormalTok{ to }\InformationTok{\textasciigrave{}\{r\} max(L1.data$Age)\textasciigrave{}}\NormalTok{ years (*M* = \textasciigrave{}\{r\} mean(L1.data$Age)\textasciigrave{}, *SD* = }\InformationTok{\textasciigrave{}\{r\} sd(L1.data$Age)\textasciigrave{}}\NormalTok{).}
\end{Highlighting}
\end{Shaded}

The rendered document will read:

\begin{quote}
The L1 participants were all born and raised in the United Kingdom and
were selected to ensure a range of ages, occupations, and educational
backgrounds. The age range was from 17 to 65 years (\emph{M} =
37.5444444, \emph{SD} = 16.148998).
\end{quote}

Whilst these values are correct, in practice, we want to round them off
to the nearest integer. To this end, we can wrap the \texttt{round()}
function around the \texttt{mean()} and \texttt{max()} function (see
Section~\ref{sec-Nesting}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{The L1 participants were all born and raised in the United Kingdom and were selected to ensure a range of ages, occupations, and educational backgrounds. The age range was from }\InformationTok{\textasciigrave{}\{r\} min(L1.data$Age)\textasciigrave{}}\NormalTok{ to }\InformationTok{\textasciigrave{}\{r\} max(L1.data$Age)\textasciigrave{}}\NormalTok{ years (*M* = \textasciigrave{}\{r\} round(mean(L1.data$Age))\textasciigrave{}, *SD* = }\InformationTok{\textasciigrave{}\{r\} round(sd(L1.data$Age))\textasciigrave{}}\NormalTok{).}
\end{Highlighting}
\end{Shaded}

The rendered document will read:

\begin{quote}
The L1 participants were all born and raised in the United Kingdom and
were selected to ensure a range of ages, occupations, and educational
backgrounds. The age range was from 17 to 65 years (\emph{M} = 38,
\emph{SD} = 16).
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{More complex inline computations}, titlerule=0mm, leftrule=.75mm]

For more complex computations, it is much better to compute the values
in a dedicated code chunk. This also allows you to add \textbf{code
annotation} which is important to ensure that other researchers (and
your future self!) understand the reasoning behind the code.

For example, the following annotated code chunk can be used to reproduce
the descriptive statistics concerning L1 participants' professional
occupations and foreign language skills.

\begin{Shaded}
\begin{Highlighting}[]
\InformationTok{\textasciigrave{}\textasciigrave{}\textasciigrave{}\{r\}}
\CommentTok{\#| label: L1{-}jobs}

\CommentTok{\# Counting manual job participants using a tidyverse solution:}
\NormalTok{L1.manualjobs }\OtherTok{\textless{}{-}}\NormalTok{ L1.data }\SpecialCharTok{|\textgreater{}} \CommentTok{\# Select L1 dataset}
  \FunctionTok{count}\NormalTok{(OccupGroup) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# Tally each level of OccupGroup}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{proportion =}\NormalTok{ n}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(n)) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# Calculate proportion}
  \FunctionTok{filter}\NormalTok{(OccupGroup }\SpecialCharTok{==} \StringTok{"M"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# Select only the manual occupations}
  \FunctionTok{pull}\NormalTok{(proportion) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# Select just the proportion value}
  \FunctionTok{round}\NormalTok{(}\DecValTok{2}\NormalTok{)}

\CommentTok{\# Alternative: Counting manual job participants using a base R solution:}
\NormalTok{L1.manualjobs }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(}\FunctionTok{proportions}\NormalTok{(}\FunctionTok{table}\NormalTok{(L1.data}\SpecialCharTok{$}\NormalTok{OccupGroup))[}\StringTok{"M"}\NormalTok{], }\AttributeTok{digits =} \DecValTok{2}\NormalTok{)}

\CommentTok{\# Counting clerical job participants}
\NormalTok{L1.clerical }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(}\FunctionTok{proportions}\NormalTok{(}\FunctionTok{table}\NormalTok{(L1.data}\SpecialCharTok{$}\NormalTok{OccupGroup))[}\StringTok{"C"}\NormalTok{], }\AttributeTok{digits =} \DecValTok{2}\NormalTok{)}

\CommentTok{\# Counting professional job participants}
\NormalTok{L1.pro.num }\OtherTok{\textless{}{-}}\NormalTok{ L1.data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(OccupGroup }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}PS\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}PS \textquotesingle{}}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{count}\NormalTok{()}

\NormalTok{L1.pro }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{((L1.pro.num}\SpecialCharTok{/} \FunctionTok{nrow}\NormalTok{(L1.data)), }\AttributeTok{digits =} \DecValTok{2}\NormalTok{)}

\CommentTok{\# Counting professionally inactive participants}
\NormalTok{L1.inactive }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(}\FunctionTok{proportions}\NormalTok{(}\FunctionTok{table}\NormalTok{(L1.data}\SpecialCharTok{$}\NormalTok{OccupGroup))[}\StringTok{"I"}\NormalTok{], }\AttributeTok{digits =} \DecValTok{2}\NormalTok{)}

\CommentTok{\# Counting participants who speak at least one language other than English}
\NormalTok{L1.otherlgs }\OtherTok{\textless{}{-}}\NormalTok{ L1.data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(OtherLgs }\SpecialCharTok{!=} \StringTok{"None"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{count}\NormalTok{()}
\InformationTok{\textasciigrave{}\textasciigrave{}\textasciigrave{}}
\end{Highlighting}
\end{Shaded}

The values saved to the local environment as \texttt{R} objects can then
be inserted inline within the Markdown text as follows:

\begin{Shaded}
\begin{Highlighting}[]
\InformationTok{\textasciigrave{}Â \{\{r\}\} numbers\_to\_words((L1.manualjobs*100), cap = TRUE)\textasciigrave{}}\NormalTok{ percent of the participants held manual jobs, }\InformationTok{\textasciigrave{}\{r\} L1.clerical*100\textasciigrave{}}\NormalTok{\% held clerical positions, and }\InformationTok{\textasciigrave{}\{r\} L1.pro*100\textasciigrave{}}\NormalTok{\% had professional{-}level jobs or were studying for a degree; the remaining }\InformationTok{\textasciigrave{}\{r\} L1.inactive*100\textasciigrave{}}\NormalTok{\% were occupationally inactive (i.e. unemployed, retired, or homemakers). In terms of education, participantsâ€™ backgrounds ranged from no formal qualifications to Ph.D., with corresponding differences in the number of years spent in full{-}time education (from }\InformationTok{\textasciigrave{}\{r\} min(L1.data$EduYrs)\textasciigrave{}}\NormalTok{ to }\InformationTok{\textasciigrave{}\{r\} max(L1.data$EduYrs)\textasciigrave{}}\NormalTok{; *M* = \textasciigrave{}\{r\} round(mean(L1.data$EduYrs))\textasciigrave{}, *SD* = }\InformationTok{\textasciigrave{}\{r\} round(sd(L1.data$EduYrs))\textasciigrave{}}\NormalTok{). }\InformationTok{\textasciigrave{}\{r\} L1.otherlgs\textasciigrave{}}\NormalTok{ participants reported a working knowledge of another language; the rest described themselves as monolinguals.}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\section{Tables}\label{sec-QuartoTables}

The easiest way to manually construct a table in a Quarto document in
\emph{RStudio} is to switch to Visual mode and click on
\emph{Insert~\textgreater~Table}. You can choose how many rows and
columns you need and then fill in your table in the Visual editor.

\begin{longtable}[]{@{}lll@{}}
\caption{Terminology used in this chapter}\tabularnewline
\toprule\noalign{}
& Same data & Different data \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
& Same data & Different data \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Same analysis method} & Reproducible & Replicable \\
\textbf{Different analysis method} & Robust & Generalisable \\
\end{longtable}

~

When you switch to the Source mode, you will see that, in
\textbf{Markdown} (see Section~\ref{sec-Markdown}), your table has been
converted to a \textbf{pipe table}. Pipe tables allow for column
alignment and captions.

\begin{Shaded}
\begin{Highlighting}[]
\PreprocessorTok{|}                               \PreprocessorTok{|}\NormalTok{ Same data    }\PreprocessorTok{|}\NormalTok{ Different data }\PreprocessorTok{|}
\PreprocessorTok{|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\PreprocessorTok{|}\NormalTok{ **Same analysis method**      }\PreprocessorTok{|}\NormalTok{ Reproducible }\PreprocessorTok{|}\NormalTok{ Replicable     }\PreprocessorTok{|}
\PreprocessorTok{|}\NormalTok{ **Different analysis method** }\PreprocessorTok{|}\NormalTok{ Robust       }\PreprocessorTok{|}\NormalTok{ Generalisable  }\PreprocessorTok{|}

\NormalTok{: Terminology used in this chapter}
\end{Highlighting}
\end{Shaded}

Most of the time, however, you will want to display tabular results
based on data that you have imported, wrangled, and/or analysed in
\texttt{R}. If the output of a code chunk within your Quarto document is
a table, it will be displayed in your rendered document by default
(unless you specify a chunk option to hide its output, see
Section~\ref{sec-Chunks}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L1.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{count}\NormalTok{(OtherLgs,}
        \AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  OtherLgs  n
1     None 84
2   German  3
3   French  2
4  Spanish  1
\end{verbatim}

However, this output is not particularly nicely formatted. There are
several \texttt{R} packages designed to create tables that are
``presentation-ready''. One of these is the \textbf{\{gt\}} package.
Beyond its main function \texttt{gt()}, it offers many more functions to
further style tables such as \texttt{cols\_label()} to change the column
headers. You will need to install this package before you can use it
(see Section~\ref{sec-Packages}).

\begin{Shaded}
\begin{Highlighting}[]
\InformationTok{\textasciigrave{}\textasciigrave{}\textasciigrave{}\{r\}}
\CommentTok{\#| label: tbl{-}L1{-}languages}
\CommentTok{\#| tbl{-}cap: "Example of a \{gt\} table"}
\CommentTok{\#| tbl{-}cap{-}location: top}
\CommentTok{\#| tbl{-}colwidths: [80,20]}

\CommentTok{\#install.packages("gt")}
\FunctionTok{library}\NormalTok{(gt)}

\NormalTok{L1.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{count}\NormalTok{(OtherLgs, }
        \AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{gt}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{cols\_label}\NormalTok{(}
    \AttributeTok{OtherLgs =} \StringTok{"Additional language"}\NormalTok{, }
    \AttributeTok{n =} \StringTok{"N"}\NormalTok{)}
\InformationTok{\textasciigrave{}\textasciigrave{}\textasciigrave{}}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tbl-L1-languages}Example of a \{gt\} table}

\centering{

\fontsize{12.0pt}{14.0pt}\selectfont
\begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}lr}
\toprule
Additional language & N \\ 
\midrule\addlinespace[2.5pt]
None & 84 \\ 
German & 3 \\ 
French & 2 \\ 
Spanish & 1 \\ 
\bottomrule
\end{tabular*}

}

\end{table}%

In addition, Quarto also has a range of \textbf{chunk options} to
customise the display of tables, including \texttt{tbl-cap} for the
addition of a table caption and \texttt{tbl-cap-location} to determine
where the caption is placed. Note that, in the above chunk, the table's
\texttt{label} chunk option begins with \texttt{tbl-}. This allows for
in-text cross-referencing to the table with the insertion of
\texttt{@tbl-L1-languages} within the text of the Quarto document, which
will automatically be rendered as the following linked and numbered
cross-reference: Table~\ref{tbl-L1-languages}.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Going further}, titlerule=0mm, leftrule=.75mm]

The Quarto guide provides further information about formatting tables:
\url{https://quarto.org/docs/authoring/tables.html}.

\end{tcolorbox}

\section{Figures}\label{sec-QuartoFigures}

In Quarto documents, figures can either be inserted from image files
(e.g.~\texttt{.png} or \texttt{.jpeg} files, see
Section~\ref{sec-FileExtensions}) or from the output of a code chunk
(e.g.~a plot, see Chapter~\ref{sec-DataViz}).

\subsection{Images}\label{sec-QuartoImages}

To embed an image from an external file, you can use the ``Insert'' menu
in \emph{RStudio}'s Visual editor and select ``Figure / Image'' (see
Figure~\ref{fig-InsertingImage}). This will open up a menu where you can
select the image that you want to insert, as well as add
\textbf{alt-text} (see Section~\ref{sec-StatsLabs}) and a
\textbf{caption}. The easiest way to adjust the size of an embedded
image is to click on the image and then adjust the size of the image
with the blue circle in the bottom-right corner of the image (see
Figure~\ref{fig-InsertingImage}).

\begin{figure}

\centering{

\includegraphics[width=5in,height=\textheight,keepaspectratio]{images/Quarto_Insert_image.png}

}

\caption{\label{fig-InsertingImage}Adjusting the size of an image in
Quarto}

\end{figure}%

Below is the source code for Figure~\ref{fig-RealisticPipeline} in
Markdown. The code includes the \textbf{relative path} to the image file
(see Section~\ref{sec-FoldersPaths}) relative to the \textbf{project
directory} (see Section~\ref{sec-RProject}). In the example below, the
image file \texttt{BERD\_pipeline-real.jpg} is located in a subfolder
called \texttt{images}. If you want to try this out yourself, you will
need to create this subfolder within your own project directory and save
Figure~\ref{fig-RealisticPipeline} to this subfolder.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{!}\CommentTok{[}\OtherTok{A more realistic research pipeline [CC{-}BY 4.0](https://creativecommons.org/licenses/by/4.0/) @seiboldBERDCourseMake2023}\CommentTok{](images/BERD\_pipeline{-}real.jpg)}\NormalTok{\{\#fig{-}RealisticPipeline fig{-}alt="Cartoon drawing of a complex set of pipes with various entry points for \textbackslash{}"data\textbackslash{}" and a single output: a research paper with text, a table, and a plot. Sections of the pipe are coloured according to the processes that they correspond to. These include data cleaning, overview, figures, modelling, and text." width="480"\}}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\centering{

\includegraphics[width=5in,height=\textheight,keepaspectratio]{images/BERD_pipeline-real.jpg}

}

\caption{\label{fig-RealisticPipeline}A more realistic research pipeline
(\href{https://creativecommons.org/licenses/by/4.0/}{CC BY 4.0} Seibold
\& MÃ¼ller)}

\end{figure}%

This example embedded image includes a caption (that, itself, includes a
link), an alt-text (see Section~\ref{sec-StatsLabs}), and a custom width
in pixel. Note that, in the source code, special characters such as
quotation marks need to be escaped using a backslash
\texttt{\textbackslash{}}. Tags beginning with \texttt{\#fig-} can be
used to cross-reference images by replacing the \texttt{\#} with
\texttt{@}. Hence, in this chapter, \texttt{@fig-RealisticPipeline} in
the Quarto source code is rendered as
Figure~\ref{fig-RealisticPipeline}.

Figures can be arranged in many ways. The example below uses the
\texttt{:::} \textbf{div syntax} to display two images side-by-side.
This syntax also allows for subcaptions as shown in
Figure~\ref{fig-Pipelines}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{::: \{\#fig{-}Pipelines layout{-}ncol="2"\}}
\AlertTok{![An idealised research pipeline](images/BERD\_pipeline{-}simple.jpg)}\NormalTok{\{\#fig{-}IdealisedPipeline\}}

\AlertTok{![A more realistic research pipeline](images/BERD\_pipeline{-}real.jpg)}\NormalTok{\{\#fig{-}RealisticPipeline2\}}

\NormalTok{Research workflows as pipelines (}\CommentTok{[}\OtherTok{CC BY 4.0}\CommentTok{](https://creativecommons.org/licenses/by/4.0/)}\NormalTok{ @seiboldBERDCourseMake2023)}
\NormalTok{:::}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.50\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/BERD_pipeline-simple.jpg}}

}

\subcaption{\label{fig-IdealisedPipeline}An idealised research pipeline}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/BERD_pipeline-real.jpg}}

}

\subcaption{\label{fig-RealisticPipeline2}A more realistic research
pipeline}

\end{minipage}%

\caption{\label{fig-Pipelines}Research workflows as pipelines
(\href{https://creativecommons.org/licenses/by/4.0/}{CC BY 4.0} Seibold
\& MÃ¼ller)}

\end{figure}%

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Going further}, titlerule=0mm, leftrule=.75mm]

To find out more about inserting and arranging figures, check out the
detailed Quarto guide:
\url{https://quarto.org/docs/authoring/figures.html}.

\end{tcolorbox}

\subsection{Plots}\label{sec-QuartoPlots}

If your Quarto document includes code chunks that generate plots, they
will automatically be integrated in your rendered document. Plots will
either appear immediately after the corresponding code chunk or where
the code chunk would be, if you chose to hide the code chunk that
generated the plot with the \texttt{echo:\ false} option.

As with computed tables (see Section~\ref{sec-QuartoTables}), various
code chunk options can be added to customise the look of computed
figures in rendered documents. Compare the code chunk options below and
the generated output in Figure~\ref{fig-scatterplot}.

\begin{Shaded}
\begin{Highlighting}[]
\InformationTok{\textasciigrave{}\textasciigrave{}\textasciigrave{}\{r\}}
\CommentTok{\#| label: fig{-}scatterplot}
\CommentTok{\#| fig{-}cap: "L2 participants\textquotesingle{} lexical proficiency in English and their professional occupational group"}
\CommentTok{\#| fig{-}height: 5}
\CommentTok{\#| fig{-}asp: 0.618}
\CommentTok{\#| message: false}

\NormalTok{L2.data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ VocabR, }
                       \AttributeTok{y =}\NormalTok{ CollocR)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{colour =}\NormalTok{ OccupGroup),}
             \AttributeTok{size =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_colour\_viridis\_d}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Vocabulary test scores"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Collocation test scores"}\NormalTok{,}
       \AttributeTok{colour =} \StringTok{"Occupational}\SpecialCharTok{\textbackslash{}n}\StringTok{groups"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\InformationTok{\textasciigrave{}\textasciigrave{}\textasciigrave{}}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{14_LiterateProgramming_files/figure-pdf/fig-scatterplot-1.pdf}}

}

\caption{\label{fig-scatterplot}L2 participants' lexical proficiency in
English and their professional occupational group}

\end{figure}%

According to the authors of ``R for Data Science'', figure sizing and
scaling is ``an art and science and getting things right can require an
iterative trial-and-error approach'' (Wickham, Ã‡etinkaya-Rundel \&
Grolemund 2023). This is because there are five main options that
control figure sizing: \texttt{fig-width}, \texttt{fig-height},
\texttt{fig-asp}, \texttt{out-width} and \texttt{out-height}. The first
three control the size of the figure created by \texttt{R}, whereas the
latter two control the size at which it is inserted in the rendered
document.

If you are sharing your research analyses and results in HTML format,
you can also embed \textbf{interactive plots} (see
Section~\ref{sec-InteractivePlots}) in your Quarto documents. In HTML
format, it is therefore possible to hover over
Figure~\ref{fig-scatterplot-plotly} to explore the data interactively.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/IntPlotCh14a.png}}

}

\caption{\label{fig-scatterplot-plotly}Screenshot of the interactive
plot of L2 participants' lexical proficiency in English}

\end{figure}%

\section{References}\label{sec-References}

An important aspect of academic writing is the inclusion of in-text
bibliographic references (\textbf{citations}) and a well-formatted list
of references (also referred to as a \textbf{bibliography}).
\emph{RStudio}'s Visual editor makes inserting bibliographic references
very convenient. To insert a reference, click on ``Insert'' and then
select ``Citation'' or use the keyboard shortcut âŒ˜/Ctrl â‡§ F8. This opens
up a menu (see Figure~\ref{fig-QuartoCitation}) giving you the option to
search for the source that you'd like to cite on your own computer
(e.g.~in your own Zotero database, if you use Zotero), via the
\href{https://www.crossref.org/}{Crossref} database, or directly using a
\href{https://forrt.org/glossary/english/doi/}{DOI}.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/Quarto_citation.png}}

}

\caption{\label{fig-QuartoCitation}Search menu for bibliographic
reference}

\end{figure}%

Alternatively, if you start typing \texttt{@} in the Visual editor, a
quick reference menu will appear. Either way, any references that you
add will be displayed as \texttt{@} followed by a \textbf{reference
identifier}. For example, in the source code of this Quarto document,
every reference to DÄ…browska (2019) is indicated as
\texttt{@DabrowskaExperienceAptitudeIndividual2019}.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, titlerule=0mm, leftrule=.75mm]

For more information on how to format your in-text citations, see the
\href{https://quarto.org/docs/authoring/footnotes-and-citations.html\#sec-citations}{Quarto
guide}.

\end{tcolorbox}

When you insert your first reference in a Quarto document,
\emph{RStudio} will automatically create a \texttt{references.bib} file
in your project folder. All references are automatically added to this
new \href{https://en.wikipedia.org/wiki/BibLaTeX}{BibLaTeX} file. As
shown below, \texttt{.bib} files contain entries that begin with
\texttt{@} followed by the type of reference (\texttt{article},
\texttt{book}, \texttt{manual}, \texttt{url}, etc.) and the reference
identifier (e.g.~\texttt{DabrowskaExperienceAptitudeIndividual2019},
\texttt{wickhamDataScienceImport2023}). The rest of the entries contains
structured information about each reference including its title, date of
publication, and DOI or ISBN.

\begin{codelisting}

\caption{\texttt{references.bib}}

\begin{Shaded}
\begin{Highlighting}[]
\VariableTok{@article}\NormalTok{\{}
  \OtherTok{DabrowskaExperienceAptitudeIndividual2019}\NormalTok{,}
  \DataTypeTok{title}\NormalTok{=\{Experience, Aptitude, and Individual Differences in Linguistic Attainment: A Comparison of Native and Nonnative Speakers\},}
  \DataTypeTok{volume}\NormalTok{=\{69\},}
  \DataTypeTok{ISSN}\NormalTok{=\{1467{-}9922\},}
  \DataTypeTok{url}\NormalTok{=\{https://onlinelibrary.wiley.com/doi/abs/10.1111/lang.12323\},}
  \DataTypeTok{DOI}\NormalTok{=\{10.1111/lang.12323\},}
  \DataTypeTok{number}\NormalTok{=\{S1\},}
  \DataTypeTok{journal}\NormalTok{=\{Language Learning\},}
  \DataTypeTok{author}\NormalTok{=\{DÄ…browska, Ewa\},}
  \DataTypeTok{year}\NormalTok{=\{2019\},}
  \DataTypeTok{pages}\NormalTok{=\{72â€“100\}}
\NormalTok{\}}

\VariableTok{@book}\NormalTok{\{}
  \OtherTok{wickhamDataScienceImport2023}\NormalTok{,}
  \DataTypeTok{place}\NormalTok{=\{Beijing, Boston, Farnham, Sebastopol, Tokyo\},}
  \DataTypeTok{edition}\NormalTok{=\{2\},}
  \DataTypeTok{title}\NormalTok{=\{R for Data Science: Import, tidy, transform, visualize, and model data\},}
  \DataTypeTok{ISBN}\NormalTok{=\{978{-}1{-}4920{-}9740{-}2\},}
  \DataTypeTok{url}\NormalTok{=\{https://r4ds.hadley.nz/\},}
  \DataTypeTok{publisher}\NormalTok{=\{Oâ€™Reilly\},}
  \DataTypeTok{author}\NormalTok{=\{Wickham, Hadley and Ã‡etinkaya{-}Rundel, Mine and Grolemund, Garrett\},}
  \DataTypeTok{year}\NormalTok{=\{2023\} }
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\end{codelisting}

In order to connect this \texttt{bibliography.bib} file with our Quarto
document, we need to add a \texttt{bibliography} key to our \textbf{YAML
header}. Provided that our \texttt{references.bib} file is located in
the same folder as our Quarto document (which is what \emph{RStudio}
does by default), we can simply add the following line to our document
header:

\begin{Shaded}
\begin{Highlighting}[]
\PreprocessorTok{{-}{-}{-} }
\FunctionTok{title}\KeywordTok{:}\AttributeTok{ }\StringTok{"Learning Quarto"}
\FunctionTok{subtitle}\KeywordTok{:}\AttributeTok{ }\StringTok{"by reproducing the descriptive statistics of DÄ…browska\textquotesingle{}s (2019) study"}
\FunctionTok{author}\KeywordTok{:}\AttributeTok{ }\StringTok{"Elen Le Foll"}
\FunctionTok{date}\KeywordTok{:}\AttributeTok{ last{-}modified}
\FunctionTok{bibliography}\KeywordTok{:}\AttributeTok{ references.bib}
\PreprocessorTok{{-}{-}{-}}
\end{Highlighting}
\end{Shaded}

With this modified YAML header, when the document is rendered, a
bibliography will automatically be added to the end of the document.
This means that, if you have citations in your document, it is a good
idea to include a header section \texttt{\#\ References} at the end of
the document.

\begin{quote}
\subsection*{References}\label{references}
\addcontentsline{toc}{subsection}{References}

DÄ…browska, Ewa. 2019. ``Experience, Aptitude, and Individual Differences
in Linguistic Attainment: A Comparison of Native and Nonnative
Speakers.'' \emph{Language Learning} 69 (S1): 72-100.
\url{https://doi.org/10.1111/lang.12323}.

Wickham, Hadley, Mine Ã‡etinkaya-Rundel, and Garrett Grolemund. 2023.
\emph{R for Data Science: Import, Tidy, Transform, Visualize, and Model
Data}. 2nd ed.~O'Reilly. \url{https://r4ds.hadley.nz/}.
\end{quote}

By default, Quarto will use the
\href{https://chicagomanualofstyle.org/}{Chicago Manual of Style}
author-date citation format (as above). However, we can point to a
different \textbf{citation stylesheet} in the form of a \texttt{.csl}
(Citation Style Language) file in the YAML header. This allows us to
determine exactly how our bibliography and in-text citations should be
formatted. Many institutions, publishers, and journals have their own
(sometimes annoyingly specific!) requirements. Luckily, the open-source
research community has put together a large repository of citation
stylesheets for you to choose from: \url{https://www.zotero.org/styles}.
You can download any of these stylesheets (as a \texttt{.csl} file),
place the file in your project folder, and then link it to your Quarto
document by adding a \texttt{cls} key to your header.

\begin{Shaded}
\begin{Highlighting}[]
\PreprocessorTok{{-}{-}{-}}
\FunctionTok{title}\KeywordTok{:}\AttributeTok{ }\StringTok{"Learning Quarto"}
\FunctionTok{subtitle}\KeywordTok{:}\AttributeTok{ }\StringTok{"by reproducing the descriptive statistics of DÄ…browska\textquotesingle{}s (2019) study"}
\FunctionTok{author}\KeywordTok{:}\AttributeTok{ }\StringTok{"Elen Le Foll"}
\FunctionTok{date}\KeywordTok{:}\AttributeTok{ last{-}modified}
\FunctionTok{bibliography}\KeywordTok{:}\AttributeTok{ references.bib}
\FunctionTok{csl}\KeywordTok{:}\AttributeTok{ international{-}journal{-}of{-}learner{-}corpus{-}research.csl}
\PreprocessorTok{{-}{-}{-}}
\end{Highlighting}
\end{Shaded}

For example, if you wanted to submit your paper to the
\href{https://benjamins.com/catalog/ijlcr}{International Journal of
Learner Corpus Research}, you can download the
\href{https://www.zotero.org/styles/international-journal-of-learner-corpus-research}{corresponding
CLS stylesheet} from the \href{https://www.zotero.org/styles}{Zotero
styles database}, save it in your project folder, and link to it in your
YAML header as above. When rendered, your document's bibliography will
then read:

\begin{quote}
\subsection*{References}\label{references-1}
\addcontentsline{toc}{subsection}{References}

DÄ…browska, E. (2019). Experience, Aptitude, and Individual Differences
in Linguistic Attainment: A Comparison of Native and Nonnative Speakers.
\emph{Language Learning}, \emph{69}(S1), 72-100.
\url{https://doi.org/10.1111/lang.12323}.

Wickham, H., Ã‡etinkaya-Rundel, M., \& Grolemund, G. (2023). \emph{R for
data science: Import, tidy, transform, visualize, and model data} (2nd
ed.). O'Reilly. Retrieved from \url{https://r4ds.hadley.nz/}.
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Literature management}, titlerule=0mm, leftrule=.75mm]

Managing the large number of references that we need to consult, read,
and cite when doing research can be a real challenge. The good news is
that \textbf{reference management software} are there to help you
overcome this challenge! Whether you are working on a term paper, a
Master's dissertation, PhD thesis, or post-doctoral project, it is
\emph{always} worth investing the time to learn to use a reference
manager!

\href{https://www.zotero.org/}{Zotero} is a free and open-source
bibliographic reference manager that will help you organise all your
sources and generate beautifully formatted bibliographies for all your
projects. It offers various
\href{https://www.zotero.org/download/}{browser extensions} that enable
you to quickly add references to your library directly from your web
browser.

What's more, Zotero can be integrated in RStudio, making it very easy to
include BibTeX-formatted references in your Quarto documents. Find out
more in the
\href{https://rstudio.github.io/visual-markdown-editing/citations.html}{\emph{RStudio}
documentation}.

\end{tcolorbox}

\section{Computing environment}\label{sec-Packages}

In addition to referencing academic papers, it is also very important
that we reference which \textbf{\texttt{R} version} we used for our
analyses and which \textbf{packages} and package versions. This serves
two purposes:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Independent researchers (and our future selves!) know exactly what
  they need to be able to \textbf{reproduce} our analyses (see
  Section~\ref{sec-Reproducibility}).
\item
  We give \textbf{credit} to the kind people who spent time and effort
  developing and sharing the \texttt{R} packages that we used for our
  analyses (see Section~\ref{sec-OpenSource}).
\end{enumerate}

The easiest way to ``give credit where credit is due'' to \texttt{R}
package developers is to use the
\href{https://pakillo.github.io/grateful/}{\{grateful\}} package. Its
\texttt{cite\_packages()} function will scan your project for all the
\texttt{R} packages that are used and generate a BibTeX file called
\texttt{grateful-refs.bib} that contains the package references.

You will first need to add a reference to the BibTeX file generated by
\{grateful\} in your YAML header. This means that your Quarto document
will now have two bibliography files, which is fine as long as you use
the following YAML syntax to reference them both.

\begin{Shaded}
\begin{Highlighting}[]
\PreprocessorTok{{-}{-}{-}}
\FunctionTok{bibliography}\KeywordTok{:}\AttributeTok{ }
\AttributeTok{  }\KeywordTok{{-}}\AttributeTok{ references.bib}
\AttributeTok{  }\KeywordTok{{-}}\AttributeTok{ grateful{-}refs.bib}
\PreprocessorTok{{-}{-}{-}}
\end{Highlighting}
\end{Shaded}

Having first installed the package, load the \{grateful\} library and
call the \texttt{cite\_packages(output\ =\ "paragraph")} function. This
will generate a paragraph that mentions all the packages used in the
document and add their references to the bibliography (either at the
bottom of your rendered Quarto document or, in the case of this
textbook, in the corresponding chapter, see
Section~\ref{sec-References}).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#install.packages("grateful")}
\FunctionTok{library}\NormalTok{(grateful)}

\FunctionTok{cite\_packages}\NormalTok{(}\AttributeTok{output =} \StringTok{"paragraph"}\NormalTok{, }
              \AttributeTok{out.dir =} \FunctionTok{tempdir}\NormalTok{(), }
              \AttributeTok{omit =} \ConstantTok{NULL}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We used R v. 4.5.2 (\textbf{base?}) and the following R packages: car v.
3.1.3 (\textbf{car?}), checkdown v. 0.0.13 (\textbf{checkdown?}),
colorBlindness v. 0.1.9 (\textbf{colorBlindness?}), datasauRus v. 0.1.9
(\textbf{datasauRus?}), effectsize v. 1.0.1 (\textbf{effectsize?}),
emmeans v. 2.0.1 (\textbf{emmeans?}), ggmosaic v. 0.4.0
(\textbf{ggmosaic2023a?}; \textbf{ggmosaic2023b?}), ggrepel v. 0.9.6
(\textbf{ggrepel?}), ggwordcloud v. 0.6.2 (\textbf{ggwordcloud?}),
grateful v. 0.3.0 (\textbf{grateful?}), gt v. 1.2.0 (\textbf{gt?}), here
v. 1.0.2 (\textbf{here?}), janeaustenr v. 1.0.0 (\textbf{janeaustenr?}),
kableExtra v. 1.4.0 (\textbf{kableExtra?}), knitcitations v. 1.0.12
(\textbf{knitcitations?}), knitr v. 1.51 (\textbf{knitr2014?};
\textbf{knitr2015?}; \textbf{knitr2025?}), paletteer v. 1.6.0
(\textbf{paletteer?}), patchwork v. 1.3.2 (\textbf{patchwork?}),
performance v. 0.15.2 (\textbf{performance?}), plotly v. 4.11.0
(\textbf{plotly?}), qqplotr v. 0.0.7 (\textbf{qqplotr?}), RefManageR v.
1.4.0 (\textbf{RefManageR2014?}; \textbf{RefManageR2017?}), relaimpo v.
2.2.7 (\textbf{relaimpo?}), report v. 0.6.2 (\textbf{report?}),
rmarkdown v. 2.30 (\textbf{rmarkdown2018?}; \textbf{rmarkdown2020?};
\textbf{rmarkdown2025?}), scales v. 1.4.0 (\textbf{scales?}), sjPlot v.
2.9.0 (\textbf{sjPlot?}), skimr v. 2.2.1 (\textbf{skimr?}), tidyverse v.
2.0.0 (\textbf{tidyverse?}), tools v. 4.5.2 (\textbf{tools?}), truncnorm
v. 1.0.9 (\textbf{truncnorm?}), viridis v. 0.6.5 (\textbf{viridis?}),
visreg v. 2.8.0 (\textbf{visreg?}), xfun v. 0.55 (\textbf{xfun?}).

Alternatively, \texttt{cite\_packages()} can generate a table with all
the package names, versions, and references. Table~\ref{tbl-packages}
lists all of the packages used in the making of this textbook. To
display functioning links and references, the table is rendered using
the \texttt{kable()} function from the \{knitr\} package.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#install.packages("knitr")}
\NormalTok{pkgs }\OtherTok{\textless{}{-}} \FunctionTok{cite\_packages}\NormalTok{(}\AttributeTok{output =} \StringTok{"table"}\NormalTok{, }
                      \AttributeTok{out.dir =} \FunctionTok{tempdir}\NormalTok{(), }
                      \AttributeTok{omit =} \ConstantTok{NULL}\NormalTok{)}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(pkgs)}
\end{Highlighting}
\end{Shaded}


\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2143}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1143}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.6714}}@{}}

\caption{\label{tbl-packages}}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Package
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Version
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Citation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
base & 4.5.2 &
(\textbf{base?}) \\
car & 3.1.3 &
(\textbf{car?}) \\
checkdown & 0.0.13 &
(\textbf{checkdown?}) \\
colorBlindness & 0.1.9 &
(\textbf{colorBlindness?}) \\
datasauRus & 0.1.9 &
(\textbf{datasauRus?}) \\
effectsize & 1.0.1 &
(\textbf{effectsize?}) \\
emmeans & 2.0.1 &
(\textbf{emmeans?}) \\
ggmosaic & 0.4.0 &
(\textbf{ggmosaic2023a?});
(\textbf{ggmosaic2023b?}) \\
ggrepel & 0.9.6 &
(\textbf{ggrepel?}) \\
ggwordcloud & 0.6.2 &
(\textbf{ggwordcloud?}) \\
gt & 1.2.0 &
(\textbf{gt?}) \\
here & 1.0.2 &
(\textbf{here?}) \\
janeaustenr & 1.0.0 &
(\textbf{janeaustenr?}) \\
kableExtra & 1.4.0 &
(\textbf{kableExtra?}) \\
knitcitations & 1.0.12 &
(\textbf{knitcitations?}) \\
knitr & 1.51 &
(\textbf{knitr2014?});
(\textbf{knitr2015?});
(\textbf{knitr2025?}) \\
paletteer & 1.6.0 &
(\textbf{paletteer?}) \\
patchwork & 1.3.2 &
(\textbf{patchwork?}) \\
performance & 0.15.2 &
(\textbf{performance?}) \\
plotly & 4.11.0 &
(\textbf{plotly?}) \\
qqplotr & 0.0.7 &
(\textbf{qqplotr?}) \\
RefManageR & 1.4.0 &
(\textbf{RefManageR2014?});
(\textbf{RefManageR2017?}) \\
relaimpo & 2.2.7 &
(\textbf{relaimpo?}) \\
report & 0.6.2 &
(\textbf{report?}) \\
rmarkdown & 2.30 &
(\textbf{rmarkdown2018?});
(\textbf{rmarkdown2020?});
(\textbf{rmarkdown2025?}) \\
scales & 1.4.0 &
(\textbf{scales?}) \\
sjPlot & 2.9.0 &
(\textbf{sjPlot?}) \\
skimr & 2.2.1 &
(\textbf{skimr?}) \\
tidyverse & 2.0.0 &
(\textbf{tidyverse?}) \\
tools & 4.5.2 &
(\textbf{tools?}) \\
truncnorm & 1.0.9 &
(\textbf{truncnorm?}) \\
viridis & 0.6.5 &
(\textbf{viridis?}) \\
visreg & 2.8.0 &
(\textbf{visreg?}) \\
xfun & 0.55 &
(\textbf{xfun?}) \\

\end{longtable}

Tracking the versions of the packages that your code relies on is
important if you want your analysis code to be \textbf{reproducible} in
the long-run (i.e.~so that you or a colleague can run it next month or
next year). However, manually installing these packages with these exact
versions is hardly feasible. To simplify the process of re-creating the
same \textbf{project environment}, consider using \{renv\}.

The \href{https://rstudio.github.io/renv/index.html}{\{renv\}} library
keeps track of the package versions that your project depends on, and
ensures that those exact versions are installed whenever and wherever
your project is opened. \{renv\} provides each project with its own
isolated package library, ensuring that you can update packages in new
projects without risking breaking older projects. To create
project-specific environments that additionally include system
dependencies, you will need to check out the
\href{https://docs.ropensci.org/rix/}{\{rix\}} package. Both of these
packages aim to make \texttt{R} projects more isolated, portable and
therefore reproducible.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Going further ðŸ“š}, titlerule=0mm, leftrule=.75mm]

Accessible introductions to stabilising your computing environment can
be found at
\url{https://berd-nfdi.github.io/BERD-reproducible-research-course/3-3-stabilize.html}
and
\url{https://the-turing-way.netlify.app/reproducible-research/renv.html}.

\end{tcolorbox}

\section{Sharing HTML documents}\label{sec-EmbedResources}

You may have noticed that, in addition to creating an \texttt{.html}
file, rendering your Quarto document has also generated a folder
containing any necessary data, images, stylesheets or other files
required to display the HTML version of your document. This is because
Quarto keeps external resources separate from the main HTML file by
default. While this is advantageous for large documents and complex
projects, it does mean that your HTML document can only be viewed if
both the \texttt{.html} file and its associated folder are shared.

If you want to share a \textbf{single, self-contained} \texttt{.html}
file with someone else or upload it somewhere, you will need to
\textbf{embed} all the necessary files directly inside your HTML file.
This is achieved by adding the following option at the end of your
document's YAML header:

\begin{Shaded}
\begin{Highlighting}[]
\PreprocessorTok{{-}{-}{-}}
\FunctionTok{format}\KeywordTok{:}
\AttributeTok{  }\FunctionTok{html}\KeywordTok{:}
\AttributeTok{    }\FunctionTok{embed{-}resources}\KeywordTok{:}\AttributeTok{ }\CharTok{true}
\PreprocessorTok{{-}{-}{-}}
\end{Highlighting}
\end{Shaded}

With this setting, Quarto will package all the necessary resources
inside the HTML file, resulting in a self-contained document that is
easy to share as it can be viewed in any web browser (e.g.~Firefox,
Google Chrome, Safari).

If you intend to share a longer Quarto document, it may be a good idea
to number the headings and sub-headings (\texttt{number-sections}) and
to include a table of content (\texttt{toc}). You can do this by adding
the following two lines to the \texttt{format} section of your YAML
header:

\begin{Shaded}
\begin{Highlighting}[]
\PreprocessorTok{{-}{-}{-}}
\FunctionTok{format}\KeywordTok{:}
\AttributeTok{  }\FunctionTok{html}\KeywordTok{:}
\AttributeTok{    }\FunctionTok{embed{-}resources}\KeywordTok{:}\AttributeTok{ }\CharTok{true}
\AttributeTok{    }\FunctionTok{number{-}sections}\KeywordTok{:}\AttributeTok{ }\CharTok{true}
\AttributeTok{    }\FunctionTok{toc}\KeywordTok{:}\AttributeTok{ }\CharTok{true}
\PreprocessorTok{{-}{-}{-}}
\end{Highlighting}
\end{Shaded}

\section{Other publishing formats}\label{sec-PublishingFormats}

So far, we have only tried rendering our Quarto document to
\textbf{HTML}, which is the default publishing format for Quarto
documents. HTML has many advantages and is great for publishing online,
but the beauty of Quarto is that you can share and publish your research
in many other formats, too.

\begin{figure}

\centering{

\includegraphics[width=4.86458in,height=\textheight,keepaspectratio]{images/AHorst_many-qmd-to-output.png}

}

\caption{\label{fig-Penguin}Artwork
(\href{https://creativecommons.org/licenses/by/4.0/}{CC BY 4.0} Allison
Horst from the \href{https://mine.quarto.pub/hello-quarto/}{``Hello,
Quarto'' keynote} by Julia Lowndes and Mine Ã‡etinkaya-Rundel, presented
at RStudio Conference 2022.}

\end{figure}%

\subsection{Word, LibreOffice \& co.}\label{word-libreoffice-co.}

Your supervisor or colleague may request a \textbf{Microsoft Word}
version of your Quarto document and, thankfully, this is no problem. You
can change the rendering format to a \texttt{.docx} file by amending the
format option in your \textbf{YAML header}:

\begin{Shaded}
\begin{Highlighting}[]
\PreprocessorTok{{-}{-}{-}}
\FunctionTok{format}\KeywordTok{:}\AttributeTok{ docx}
\PreprocessorTok{{-}{-}{-}}
\end{Highlighting}
\end{Shaded}

With this format option, rendering your Quarto document will generate a
\texttt{.docx} file that includes your text, any code that you wanted to
show in your document, and all of the code outputs that you wanted to
share, such as your statistics, graphs, and tables.

Some of the formatting options available for HTML also work in the
\texttt{.docx} format:

\begin{Shaded}
\begin{Highlighting}[]
\PreprocessorTok{{-}{-}{-}}
\FunctionTok{format}\KeywordTok{:}
\AttributeTok{  }\FunctionTok{docx}\KeywordTok{:}
\AttributeTok{    }\FunctionTok{embed{-}resources}\KeywordTok{:}\AttributeTok{ }\CharTok{true}
\AttributeTok{    }\FunctionTok{number{-}sections}\KeywordTok{:}\AttributeTok{ }\CharTok{true}
\AttributeTok{    }\FunctionTok{toc}\KeywordTok{:}\AttributeTok{ }\CharTok{true}
\PreprocessorTok{{-}{-}{-}}
\end{Highlighting}
\end{Shaded}

Note, however, that any options that are not available in the rendering
format specified are ignored without warning or error messages.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-warning-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-warning-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Not rendering code chunks in specific formats}, titlerule=0mm, leftrule=.75mm]

Dynamic code outputs, such as the \textbf{interactive} \{plotly\} graph
displayed in Figure~\ref{fig-scatterplot-plotly}, cannot be meaningfully
rendered to \textbf{static formats}, such as Microsoft Word or PDF.
Attempting to do so can cause rendering errors such as:

\begin{verbatim}
Error: Functions that produce HTML output found in document targeting docx output.
Please change the output type of this document to HTML.
\end{verbatim}

To fix this, add the following options to any code chunk that generates
content that only works in HTML:

\begin{Shaded}
\begin{Highlighting}[]
\InformationTok{\textasciigrave{}\textasciigrave{}\textasciigrave{}\{r\}}
\CommentTok{\#| eval: !expr \textquotesingle{}knitr::is\_html\_output()\textquotesingle{}}

\FunctionTok{ggplotly}\NormalTok{(L2.scatter2)}
\InformationTok{\textasciigrave{}\textasciigrave{}\textasciigrave{}}
\end{Highlighting}
\end{Shaded}

These options ensure that the code chunk is ignored when the document is
rendered to any format other than HTML.

\end{tcolorbox}

When you open the \texttt{.docx} version of your Quarto document in
Microsoft Word, you may get a number of warnings (e.g.
Figure~\ref{fig-MSWordWarnings}). You can safely click ``Yes'' or
``Close'' to get rid of these warnings and open up your Word file. If
you cannot open a rendered document in Microsoft Word, I recommend
rendering to \texttt{.odt} instead (see below).

\begin{figure}

\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics[width=2.46875in,height=\textheight,keepaspectratio]{images/Word_popup.png}

}

\subcaption{\label{fig-WordRecovery}}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics[width=2.46875in,height=\textheight,keepaspectratio]{images/Word_repairs.png}

}

\subcaption{\label{fig-WordRepairs}}

\end{minipage}%

\caption{\label{fig-MSWordWarnings}Examples of popup menus that may
appear when opening the \texttt{.docx} version of a Quarto document.}

\end{figure}%

To share your work with \textbf{LibreOffice}, \textbf{OnlyOffice}, and
\textbf{OpenOffice} users, use the \texttt{.odt} rendering option. This
will generate an \textbf{OpenDocument} --- an open standard file format
that can be opened in any text-processing software, including Microsoft
Word.

\begin{Shaded}
\begin{Highlighting}[]
\PreprocessorTok{{-}{-}{-}}
\FunctionTok{format}\KeywordTok{:}\AttributeTok{ odt}
\PreprocessorTok{{-}{-}{-}}
\end{Highlighting}
\end{Shaded}

By default, the quality of the images and graphs in rendered
\texttt{.docx} and \texttt{.odt} files is low. This is to keep the file
size reasonable. You can improve the quality of the rendered images by
specifying the \textbf{image definition} in the YAML option. To do so,
replace the format line that you added above with the following lines.
Make sure that you indent each line correctly as shown below; otherwise,
you will get an error when you try to render your document.

\begin{Shaded}
\begin{Highlighting}[]
\PreprocessorTok{{-}{-}{-}}
\FunctionTok{format}\KeywordTok{:}\AttributeTok{ }
\AttributeTok{  }\FunctionTok{odt}\KeywordTok{:}
\AttributeTok{    }\FunctionTok{fig{-}dpi}\KeywordTok{:}\AttributeTok{ }\DecValTok{300}
\PreprocessorTok{{-}{-}{-}}
\end{Highlighting}
\end{Shaded}

\subsection{PDF}\label{pdf}

It is also possible to render Quarto documents to PDF; however, this
requires you to have \href{https://en.wikipedia.org/wiki/LaTeX}{LaTeX}
installed on your computer. Alternatively, you can use
\href{https://quarto.org/docs/output-formats/typst.html}{Typst} --- a
new open-source markup-based typesetting system designed to be as
powerful as LaTeX but easier to use.

If you don't already have your favourite LaTex distribution, Quarto
developers recommend that you use the
\href{https://yihui.org/tinytex/}{TinyTeX} distribution to render
\texttt{.qmd} files to PDF. To \textbf{install} (or update) TinyTeX, go
to the Terminal pane in \emph{RStudio} and run the following command:

\begin{codelisting}

\caption{\texttt{Terminal}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{quarto install tinytex}
\end{Highlighting}
\end{Shaded}

\end{codelisting}

This is likely to take a few minutes but you will only need to do it
once. Afterwards, you can add the following line to your Quarto YAML
header and you're ready to render to PDF!

\begin{Shaded}
\begin{Highlighting}[]
\PreprocessorTok{{-}{-}{-}}
\FunctionTok{format}\KeywordTok{:}\AttributeTok{ pdf}
\PreprocessorTok{{-}{-}{-}}
\end{Highlighting}
\end{Shaded}

HTML being the default format, some options available for HTML are not
-- at least by default -- available in other publishing formats. Many of
the basic options, however, work across different formats. The YAML
header options below can be used to include a table of content with
numbered sections at the start of the PDF version of your document. It
also includes two options that are specific to the PDF format and which
are particularly useful for academic writing: the first will print a
list of figures (\texttt{lof}) and the second a list of tables
(\texttt{lot}).

\begin{Shaded}
\begin{Highlighting}[]
\PreprocessorTok{{-}{-}{-}}
\FunctionTok{format}\KeywordTok{:}\AttributeTok{  }
\AttributeTok{  }\FunctionTok{pdf}\KeywordTok{:}
\AttributeTok{    }\FunctionTok{number{-}sections}\KeywordTok{:}\AttributeTok{ }\CharTok{true}
\AttributeTok{    }\FunctionTok{toc}\KeywordTok{:}\AttributeTok{ }\CharTok{true}
\AttributeTok{    }\FunctionTok{lof}\KeywordTok{:}\AttributeTok{ }\CharTok{true}
\AttributeTok{    }\FunctionTok{lot}\KeywordTok{:}\AttributeTok{ }\CharTok{true}
\PreprocessorTok{{-}{-}{-}    }
\end{Highlighting}
\end{Shaded}

\subsection{Slides}\label{slides}

In research, it's quite common that you will be working on a project
that will be submitted as a paper or thesis (e.g.~in PDF format)
\emph{and} that you'll also want to \textbf{present} in class, to your
research group, or at a conference. Conveniently, we can turn any Quarto
document into \textbf{presentation slides}. There are currently three
presentation formats to choose from:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.4329}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.5671}}@{}}
\toprule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\href{https://quarto.org/docs/presentations/revealjs/}{Revealjs} & An
open-source HTML presentation framework. \textbar{}
\texttt{format:\ revealjs} \textbar{} \\
\href{https://quarto.org/docs/presentations/powerpoint.html}{Power-Point}
& Microsoft Office's presentation editing software. \textbar{}
\texttt{format:\ pptx} \textbar{} \\
\href{https://quarto.org/docs/presentations/beamer.html}{Beamer} & A
LaTeX class for producing presentations and slides in PDF format.
\textbar{} \texttt{format:\ beamer} \textbar{} \\
\end{longtable}

I recommend using \textbf{Revealjs}. The best way to get a sense of what
is possible is to explore the
\href{https://quarto.org/docs/presentations/revealjs/demo/}{demo}
presentation from the
\href{https://quarto.org/docs/presentations/revealjs/}{Quarto Guide}.

\section{Conclusion}\label{conclusion-1}

This chapter only just scratched the surface of what's possible in
Quarto. The Quarto documentation is very detailed and well worth
exploring to find out what else you can do in Quarto:
\url{https://quarto.org/docs/guide/}. From books to blogs and
interactive dashboards, the world's your oyster! ðŸš€

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, rightrule=.15mm, bottomrule=.15mm, breakable, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, coltitle=black, opacityback=0, colback=white, bottomtitle=1mm, left=2mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Going further with Quarto}, titlerule=0mm, leftrule=.75mm]

\emoji{right-arrow} For those of you who want to dive a little deeper, I
heartily recommend the final chapter of ``An Introduction to
Quantitative Text Analysis for Linguistics: Reproducible Research Using
R'' by Jerid Francom:
\url{https://qtalr.com/book/part_5/11_contribute.html}.

\emoji{right-arrow} Quarto has many functionalities that are
particularly attractive to those of us involved in higher education
teaching and academic research. Watch
\href{https://www.youtube.com/embed/EbAAmrB0luA?si=AwJ3szmUKECHfwuS}{Quarto
for Academics} (20 minutes) by Mine Ã‡etinkaya-Rundel to find out more.

\emoji{right-arrow} Thinking of writing an entire M.A./PhD thesis or
book in Quarto? Cameron Patrick wrote his PhD thesis in Quarto and has
helpfully put together some great tips so that his ``pain and suffering
can help reduce yours''. Well worth reading if you're thinking of
rendering a complex (set of) Quarto document(s) to PDF:
\url{https://cameronpatrick.com/post/2023/07/quarto-thesis-formatting/}.

\emoji{right-arrow} Last but not least, the latest edition of ``R for
Data Science'' also has a great chapter on communicating the results of
data science projects using Quarto:
\url{https://r4ds.hadley.nz/communicate}.

\end{tcolorbox}

\subsection*{\texorpdfstring{Check your progress
\emoji{glowing-star}}{Check your progress }}\label{check-your-progress-13}
\addcontentsline{toc}{subsection}{Check your progress
\emoji{glowing-star}}

Well done! You have successfully completed this chapter on literate
programming using Quarto. You have answered { out of 15 questions}
correctly.

Are you confident that you can\ldots?

\begin{itemize}
\tightlist
\item[$\square$]
  Explain the concepts of literate programming and reproducible research
  to a friend or colleague (Section~\ref{sec-LitProgramming})
\item[$\square$]
  Write and format text (bold, first-level heading, italics, etc.) in a
  Quarto document (Section~\ref{sec-Markdown})
\item[$\square$]
  Insert a code chunk in a Quarto document and use inline codes
  (Section~\ref{sec-Chunks})
\item[$\square$]
  Insert code chunks in a Quarto document and make use of inline code
  (Section~\ref{sec-Inline})
\item[$\square$]
  Insert tables in a Quarto document (Section~\ref{sec-QuartoTables})
\item[$\square$]
  Embed images and plots in a Quarto document
  (Section~\ref{sec-QuartoFigures})
\item[$\square$]
  Render (i.e.~export) your \texttt{.qmd} document to HTML, Microsoft
  Word, and PDF (Section~\ref{sec-PublishingFormats})
\end{itemize}

\bookmarksetup{startatroot}

\chapter{\texorpdfstring{Resea\texttt{R}ching with
AI?}{ReseaRching with AI?}}\label{sec-AI}

To be added soon\ldots{}

\bookmarksetup{startatroot}

\chapter*{References}\label{sec-References}
\addcontentsline{toc}{chapter}{References}

\markboth{References}{References}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-acheson2008}
Acheson, Daniel J., Justine B. Wells \& Maryellen C. MacDonald. 2008.
New and updated tests of print exposure and reading abilities in college
students. \emph{Behavior Research Methods} 40(1). 278--289.
\url{https://doi.org/10.3758/brm.40.1.278}.

\bibitem[\citeproctext]{ref-alhazmiVisualInterpretationStandard2020}
Alhazmi, Fahd. 2020. A visual interpretation of the standard deviation.
\emph{Medium}.
\url{https://towardsdatascience.com/a-visual-interpretation-of-the-standard-deviation-30f4676c291c}.

\bibitem[\citeproctext]{ref-almeidaGgplot2CompatibleQuantilequantile2018}
Almeida, Alexandre, Adam Loy \& Heike Hofmann. 2018. ggplot2 compatible
quantile-quantile plots in {R}. \emph{The R Journal} 10(2). 248--261.
\url{https://doi.org/10.32614/RJ-2018-051}.

\bibitem[\citeproctext]{ref-alvinashcraftMaximumPathLength2022}
alvinashcraft, alexbuckgit, ArcticLampyrid \& bearmannl. 2022. Maximum
path length limitation. \emph{Learn Microsoft}.
\url{https://learn.microsoft.com/en-us/windows/win32/fileio/maximum-file-path-limitation}.

\bibitem[\citeproctext]{ref-BaayenAnalyzinglinguisticdata2008}
Baayen, R. Harald. 2008. \emph{Analyzing linguistic data: A practical
introduction to statistics using {R}}. Cambridge University Press.

\bibitem[\citeproctext]{ref-barrettWhyShouldUse2018}
Barrett, Malcolm. 2018. Why should {I} use the here package when {I}'m
already using projects?
\url{https://malco.io/articles/2018-11-05-why-should-i-use-the-here-package-when-i-m-already-using-projects}.

\bibitem[\citeproctext]{ref-ben-shachar2020}
Ben-Shachar, Mattan, Daniel LÃ¼decke \& Dominique Makowski. 2020.
Effectsize: Estimation of effect size indices and standardized
parameters. \emph{Journal of Open Source Software} 5(56). 2815.
\url{https://doi.org/10.21105/joss.02815}.

\bibitem[\citeproctext]{ref-berez-kroekerOpenHandbookLinguistic2022}
Berez-Kroeker, Andrea L., Bradley McDonnell, Eve Koller \& Lauren B.
Collister. 2022. \emph{The {Open} {Handbook} of {Linguistic} {Data}
{Management}}. MIT Press.
\url{https://doi.org/10.7551/mitpress/12200.001.0001}.

\bibitem[\citeproctext]{ref-bochynskaReproducibleResearchPractices2023a}
Bochynska, Agata, Liam Keeble, Caitlin Halfacre, Joseph V. Casillas,
Irys-AmÃ©lie Champagne, Kaidi Chen, Melanie RÃ¶thlisberger, Erin M.
Buchanan \& Timo B. Roettger. 2023. Reproducible research practices and
transparency across linguistics. \emph{Glossa Psycholinguistics} 2(1).
\url{https://doi.org/10.5070/G6011239}.

\bibitem[\citeproctext]{ref-brehenyVisualizationRegressionModels2017}
Breheny, Patrick \& Woodrow Burchett. 2017. Visualization of Regression
Models Using visreg. \emph{The R Journal} 9(2). 56.
\url{https://doi.org/10.32614/RJ-2017-046}.

\bibitem[\citeproctext]{ref-bryanLetGitStarted}
Bryan, Jennifer. 2018. \emph{Let's {Git} started: Happy {Git} and
{GitHub} for the useR}. Open Education Resource.
\url{https://happygitwithr.com/}.

\bibitem[\citeproctext]{ref-bryanProjectorientedWorkflow2017}
Bryan, Jenny. 2017. Project-oriented workflow. \emph{Tidyverse.org}.
\url{https://www.tidyverse.org/blog/2017/12/workflow-vs-script/}.

\bibitem[\citeproctext]{ref-busterudVerbPlacementL32023}
Busterud, Guro, Anne Dahl, Dave Kush \& Kjersti Faldet Listhaug. 2023.
Verb placement in L3 french and L3 german: The role of language-internal
factors in determining cross-linguistic influence from prior languages.
\emph{Linguistic Approaches to Bilingualism}. John 13(5). 693--716.
\url{https://doi.org/10.1075/lab.22058.bus}.

\bibitem[\citeproctext]{ref-centerforopenscienceChoosingRightPreregistration2025}
Center for OpenScience. 2025. Choosing the Right Preregistration
Template: A Guide for Researchers.
\url{https://www.cos.io/blog/choosing-preregistration-template-guide-for-researchers}.

\bibitem[\citeproctext]{ref-cetinkaya-rundelIntroductionModernStatistics2021}
Ã‡etinkaya-Rundel, Mine \& Johanna Hardin. 2021. \emph{Introduction to
modern statistics}. Second. Leanpub.
\url{https://openintro-ims.netlify.app/}.

\bibitem[\citeproctext]{ref-clevelandGraphicalPerceptionVisual1987}
Cleveland, William S. \& Robert McGill. 1987. Graphical perception: The
visual decoding of quantitative information on graphical displays of
data. \emph{Journal of the Royal Statistical Society: Series A
(General)} 150(3). 192--210. \url{https://doi.org/10.2307/2981473}.

\bibitem[\citeproctext]{ref-cohenStatisticalPowerAnalysis1988}
Cohen, Jacob. 1988. \emph{Statistical power analysis for the behavioral
sciences}. 2. ed., reprint. New York, NY: Psychology Press.

\bibitem[\citeproctext]{ref-teiconsortiumTEIP5Guidelines2025}
Consortium, TEI. 2025. TEI P5: Guidelines for electronic text encoding
and interchange. Zenodo. \url{https://doi.org/10.5281/zenodo.17161156}.

\bibitem[\citeproctext]{ref-DabrowskaExperienceAptitudeIndividual2019}
DÄ…browska, Ewa. 2019. Experience, aptitude, and individual differences
in linguistic attainment: A comparison of native and nonnative speakers.
\emph{Language Learning} 69(S1). 72--100.
\url{https://doi.org/10.1111/lang.12323}.

\bibitem[\citeproctext]{ref-dauberNonProgrammersGuideSocial2024}
Dauber, Daniel. 2024. \emph{R for non-programmers: A guide for social
scientists}. Open Education Resource.
\url{https://bookdown.org/daniel_dauber_io/r4np_book/}.

\bibitem[\citeproctext]{ref-douglasIntroduction2024}
Douglas, Alex, Deon Roos, Francesca Mancini \& David Lusseau. 2024.
\emph{An introduction to {R}}. \url{https://intro2r.com/}.

\bibitem[\citeproctext]{ref-fewPiesDessert}
Few, Stephen. Save the pies for dessert. \emph{August 2007}.
\url{http://www.perceptualedge.com/articles/08-21-07.pdf}.

\bibitem[\citeproctext]{ref-FieldDiscoveringstatisticsusing2012}
Field, Andy P., Jeremy Miles \& ZoÃ« Field. 2012. \emph{Discovering
statistics using r}. Sage.

\bibitem[\citeproctext]{ref-garnierSjmgarnierViridisCRAN2023}
Garnier, Simon, Noam Ross, BoB Rudis, Antoine Filipovic-Pierucci, Tal
Galili, Timelyportfolio, Alan O'Callaghan, et al. 2023.
\emph{Sjmgarnier/viridis: CRAN release v0.6.3}. Zenodo.
\url{https://doi.org/10.5281/ZENODO.4679423}.

\bibitem[\citeproctext]{ref-gelmanEthicsStatisticalPractice2018}
Gelman, Andrew. 2018. Ethics in statistical practice and communication:
Five recommendations. \emph{Significance} 15(5). 40--43.
\url{https://doi.org/10.1111/j.1740-9713.2018.01193.x}.

\bibitem[\citeproctext]{ref-gelmanEmbracingVariationAccepting2019}
Gelman, Andrew. 2019. Embracing variation and accepting uncertainty:
Implications for science and metascience.
\url{https://www.youtube.com/watch?v=VQCcMP4A5Ks}.

\bibitem[\citeproctext]{ref-godfreyBrailleRImprovedAccess2025}
Godfrey, A. Jonathan R., Debra Warren, Deepayan Sarkar, Gabriel Becker,
James Thompson, Paul Murrell, Timothy Bilton \& Volker Sorge. 2025.
\emph{BrailleR: Improved access for blind users}.
\url{https://github.com/ajrgodfrey/BrailleR}.

\bibitem[\citeproctext]{ref-goodScopeLinguisticData2022}
Good, Jeff. 2022. The scope of linguistic data. In Andrea L.
Berez-Kroeker, Bradley McDonnell, Eve Koller \& Lauren B. Collister
(eds.), \emph{The open handbook of linguistic data management}, 27--47.
MIT Press. \url{https://doi.org/10.7551/mitpress/12200.001.0001}.

\bibitem[\citeproctext]{ref-griesStatisticsLinguisticsPractical2021}
Gries, Stefan Thomas. 2021. \emph{Statistics for linguistics with {R}: A
practical introduction} (De Gruyter Mouton Textbook). 3rd revised
edition. de Gruyter Mouton.

\bibitem[\citeproctext]{ref-gruxf6mping2006}
GrÃ¶mping, Ulrike. 2006. Relative Importance for Linear Regression in{R}:
The Package relaimpo. \emph{Journal of Statistical Software} 17(1).
\url{https://doi.org/10.18637/jss.v017.i01}.

\bibitem[\citeproctext]{ref-harozComparisonPreregistrationPlatforms2022}
Haroz, Steve. 2022. Comparison of preregistration platforms.
\url{https://doi.org/10.31222/osf.io/zry2u}.

\bibitem[\citeproctext]{ref-harrellRegressionModelingStrategies2015}
Harrell, Frank E. 2015. \emph{Regression modeling strategies: With
applications to linear models, logistic and ordinal regression, and
survival analysis} (Springer Series in Statistics). Springer
International Publishing.
\url{https://doi.org/10.1007/978-3-319-19425-7}.

\bibitem[\citeproctext]{ref-horstOpenscapesTidyData2020}
Horst, Allison \& Julie Lowndes. 2020. Openscapes - {Tidy} data for
efficiency, reproducibility, and collaboration.
\url{https://openscapes.org/blog/2020-10-12-tidy-data/}.

\bibitem[\citeproctext]{ref-hvitfeldtPaletteerComprehensiveCollection2021}
Hvitfeldt, Emil. 2021. \emph{Paletteer: Comprehensive collection of
color palettes}. \url{https://github.com/EmilHvitfeldt/paletteer}.

\bibitem[\citeproctext]{ref-IRIS2011}
iris-database.org. 2011. IRIS. \url{https://iris-database.org/}.

\bibitem[\citeproctext]{ref-kaufmanIllusionCausalityCognitive2018}
Kaufman, Allison B. \& James C. Kaufman (eds.). 2018. The illusion of
causality: A cognitive bias underlying pseudoscience. In
\emph{Pseudoscience}. The MIT Press.
\url{https://doi.org/10.7551/mitpress/10747.003.0007}.

\bibitem[\citeproctext]{ref-kungDevelopingDataManagement2022}
Kung, Susan Smythe. 2022. Developing a data management plan. In Andrea
L. Berez-Kroeker, Bradley McDonnell, Eve Koller \& Lauren B. Collister
(eds.), \emph{The open handbook of linguistic data management},
101--115. MIT Press.
\url{https://doi.org/10.7551/mitpress/12200.001.0001}.

\bibitem[\citeproctext]{ref-lakensImprovingYourStatistical2022}
Lakens, DaniÃ«l. 2022. \emph{Improving your statistical inferences}.
Zenodo. \url{https://doi.org/10.5281/ZENODO.6409077}.

\bibitem[\citeproctext]{ref-lausbergCodingGesturalBehavior2009}
Lausberg, Hedda \& Han Sloetjes. 2009. Coding gestural behavior with the
NEUROGES-ELAN system. \emph{Behavior Research Methods} 41(3). 841--849.
\url{https://doi.org/10.3758/BRM.41.3.841}.

\bibitem[\citeproctext]{ref-LeFollTextbookEnglishCorpusBased2022}
Le Foll, Elen. 2022. \emph{{Textbook} {English}: A corpus-based analysis
of the language of EFL textbooks used in secondary schools in {France},
{Germany} and {Spain}}. OsnabrÃ¼ck University PhD thesis.
\url{https://doi.org/10.48693/278}.

\bibitem[\citeproctext]{ref-lenthEmmeansEstimatedMarginal2025a}
Lenth, Russell V. 2025. \emph{Emmeans: Estimated marginal means, aka
least-squares means}. \url{https://rvlenth.github.io/emmeans/}.

\bibitem[\citeproctext]{ref-LevshinaHowlinguisticsData2015}
Levshina, Natalia. 2015. \emph{How to do linguistics with {R}: Data
exploration and statistical analysis}. John Benjamins.

\bibitem[\citeproctext]{ref-levshinaComparingBayesianFrequentist2022}
Levshina, Natalia. 2022. Comparing Bayesian and Frequentist Models of
Language Variation: The Case of Help + (to-)Infinitive. In Ole SchÃ¼tzler
\& Julia SchlÃ¼ter (eds.), 224--258. 1st edn. Cambridge University Press.
\url{https://doi.org/10.1017/9781108589314.009}.

\bibitem[\citeproctext]{ref-lindemanIntroductionBivariateMultivariate1980}
Lindeman, Richard Harold, Peter Francis Merenda \& Ruth Z. Gold. 1980.
\emph{Introduction to bivariate and multivariate analysis}. Scott,
Foresman.

\bibitem[\citeproctext]{ref-luxfcdecke2020}
LÃ¼decke, Daniel. 2020. \emph{sjPlot: Data visualization for statistics
in social science}. \url{https://CRAN.R-project.org/package=sjPlot}.

\bibitem[\citeproctext]{ref-ludeckePerformancePackageAssessment2021}
LÃ¼decke, Daniel, Mattan S. Ben-Shachar, Indrajeet Patil, Philip Waggoner
\& Dominique Makowski. 2021. {performance}: An {R} package for
assessment, comparison and testing of statistical models. \emph{Journal
of Open Source Software} 6(60). 3139.
\url{https://doi.org/10.21105/joss.03139}.

\bibitem[\citeproctext]{ref-ludeckeSeePackageVisualizing2021}
LÃ¼decke, Daniel, Indrajeet Patil, Mattan S. Ben-Shachar, Brenton M.
Wiernik, Philip Waggoner \& Dominique Makowski. 2021. See: An {R}
package for visualizing statistical models. \emph{Journal of Open Source
Software} 6(64). 3393. \url{https://doi.org/10.21105/joss.03393}.

\bibitem[\citeproctext]{ref-matejkaSameStatsDifferent2017}
Matejka, Justin \& George Fitzmaurice. 2017. Same stats, different
graphs: Generating datasets with varied appearance and identical
statistics through simulated annealing. In, 12901294. New York, NY, USA:
Association for Computing Machinery.
\url{https://doi.org/10.1145/3025453.3025912}.

\bibitem[\citeproctext]{ref-matuteIllusionsCausalityHow2015}
Matute, Helena, Fernando Blanco, Ion Yarritu, Marcos DÃ­az-Lago, Miguel
A. Vadillo \& Itxaso Barberia. 2015. Illusions of causality: How they
bias our everyday thinking and how they could be reduced.
\emph{Frontiers in Psychology}. Frontiers 6.
\url{https://doi.org/10.3389/fpsyg.2015.00888}.

\bibitem[\citeproctext]{ref-mertzenBenefitsPreregistrationHypothesisdriven2021}
Mertzen, Daniela, Sol Lago \& Shravan Vasishth. 2021. The benefits of
preregistration for hypothesis-driven bilingualism research.
\emph{Bilingualism: Language and Cognition} 24(5). 807--812.
\url{https://doi.org/10.1017/S1366728921000031}.

\bibitem[\citeproctext]{ref-mizumotoCalculatingRelativeImportance2023}
Mizumoto, Atsushi. 2023. Calculating the relative importance of multiple
regression predictor variables using dominance analysis and random
forests. \emph{Language Learning} 73(1). 161--196.
\url{https://doi.org/10.1111/lang.12518}.

\bibitem[\citeproctext]{ref-MizumotoLinguaFrancaAdvantages2016}
Mizumoto, Atsushi \& Luke Plonsky. 2016. R as a lingua franca:
Advantages of using r for quantitative research in applied linguistics.
\emph{Applied Linguistics} 37(2). 284--291.
\url{https://doi.org/10.1093/applin/amv025}.

\bibitem[\citeproctext]{ref-nicenboimIntroductionBayesianData2026}
Nicenboim, Bruno, Daniel Schad \& Shravan Vasishth. 2026.
\emph{Introduction to Bayesian Data Analysis for cognitive science}
(Chapman \& Hall/CRC Statistics in the social and behavioral sciences
series). Boca Raton London New York: CRC Press, Taylor \& Francis Group.
\url{https://doi.org/10.1201/9780429342646}.

\bibitem[\citeproctext]{ref-nimonStatisticalAssumptionsSubstantive2012}
Nimon, Kim F. 2012. Statistical assumptions of substantive analyses
across the general linear model: A mini-review. \emph{Frontiers in
Psychology} 3. \url{https://doi.org/10.3389/fpsyg.2012.00322}.

\bibitem[\citeproctext]{ref-ouColorBlindnessSafeColor2021}
Ou, Jianhong. 2021. \emph{colorBlindness: Safe color set for color
blindness}. \url{https://CRAN.R-project.org/package=colorBlindness}.

\bibitem[\citeproctext]{ref-paquotCoreMetadataSchema2024}
Paquot, Magali, Alexander KÃ¶nig, Egon W. Stemle \& Jennifer-Carmen Frey.
2024. The core metadata schema for learner corpora (LC-meta):
Collaborative efforts to advance data discoverability, metadata quality
and study comparability in L2 research. \emph{International Journal of
Learner Corpus Research} 10(2). 280--300.
\url{https://doi.org/10.1075/ijlcr.24010.paq}.

\bibitem[\citeproctext]{ref-parsonsCommunitysourcedGlossaryOpen2022}
Parsons, Sam, FlÃ¡vio Azevedo, Mahmoud M. Elsherif, Samuel Guay, Owen N.
Shahim, Gisela H. Govaart, Emma Norris, et al. 2022. A community-sourced
glossary of {open} {scholarship} terms. \emph{Nature Human Behaviour}.
Nature 6(3). 312--318. \url{https://doi.org/10.1038/s41562-021-01269-4}.

\bibitem[\citeproctext]{ref-plonskyHowBigBig2014}
Plonsky, Luke \& Frederick L. Oswald. 2014. How big is {``big''}?
Interpreting effect sizes in L2 research. \emph{Language Learning}
64(4). 878--912. \url{https://doi.org/10.1111/lang.12079}.

\bibitem[\citeproctext]{ref-pratRelatingNaturalLanguage2020}
Prat, Chantel S., Tara M. Madhyastha, Malayka J. Mottarella \& Chu-Hsuan
Kuo. 2020. Relating natural language aptitude to individual differences
in learning programming languages. \emph{Scientific Reports}. Nature
10(1). 3817. \url{https://doi.org/10.1038/s41598-020-60661-8}.

\bibitem[\citeproctext]{ref-rcoreteamLanguageEnvironmentStatistical2024}
R Core Team. 2024. \emph{R: A language and environment for statistical
computing}. R Foundation for Statistical Computing.
\url{https://www.R-project.org/}.

\bibitem[\citeproctext]{ref-roettgerPreregistrationExperimentalLinguistics2021}
Roettger, Timo B. 2021. Preregistration in experimental linguistics:
Applications, challenges, and limitations. \emph{Linguistics}. De 59(5).
1227--1249. \url{https://doi.org/10.1515/ling-2019-0048}.

\bibitem[\citeproctext]{ref-schimkeFirstLanguageInfluence2018}
Schimke, Sarah, Israel de la Fuente, Barbara Hemforth \& Saveria
Colonna. 2018. First language influence on second language offline and
online ambiguous pronoun resolution. \emph{Language Learning} 68(3).
744--779. \url{https://doi.org/10.1111/lang.12293}.

\bibitem[\citeproctext]{ref-schweinbergerDataManagementVersion2022}
Schweinberger, Martin. 2022. Data management, version control, and
reproducibility. \url{https://ladal.edu.au/repro.html}.

\bibitem[\citeproctext]{ref-seiboldBERDCourseMake2023}
Seibold, Heidi \& Rabea MÃ¼ller. BERD course: Make your research
reproducible. \url{https://doi.org/10.17605/OSF.IO/RUPT7}.

\bibitem[\citeproctext]{ref-silgeJaneaustenrJaneAusten2022}
Silge, Julia. 2022. \emph{Janeaustenr: Jane {Austen}'s complete novels}.
\url{https://CRAN.R-project.org/package=janeaustenr}.

\bibitem[\citeproctext]{ref-smithStepAwayStepwise2018}
Smith, Gary. 2018. Step away from stepwise. \emph{Journal of Big Data}.
SpringerOpen 5(1). 1--12.
\url{https://doi.org/10.1186/s40537-018-0143-6}.

\bibitem[\citeproctext]{ref-sondereggerRegressionModelingLinguistic2023}
Sonderegger, Morgan. 2023. \emph{Regression modeling for linguistic
data}. The MIT Press.

\bibitem[\citeproctext]{ref-soskuthyGeneralisedAdditiveMixed2017}
SÃ³skuthy, MÃ¡rton. Generalised additive mixed models for dynamic analysis
in linguistics: A practical introduction.
\url{https://doi.org/10.48550/arXiv.1703.05339}.

\bibitem[\citeproctext]{ref-universityofsouthcarolinaAlternativeText}
South Carolina, University of. Alternative text. \emph{Digital
Accessibility}.
\url{https://sc.edu/about/offices_and_divisions/digital-accessibility/toolbox/best_practices/alternative_text/}.

\bibitem[\citeproctext]{ref-TabachnickUsingMultivariateStatistics2014}
Tabachnick, Barbara G. \& Linda S. Fidell. 2014. \emph{Using
multivariate statistics} (Always Learning). Pearson new international
edition, sixth edition. Pearson.

\bibitem[\citeproctext]{ref-theturingwaycommunityTuringWayHandbook2022}
The Turing Way Community. 2022. The {Turing} {Way}: A handbook for
reproducible, ethical and collaborative research (1.0.2). Zenodo.
\url{https://doi.org/10.5281/zenodo.3233853}.

\bibitem[\citeproctext]{ref-thompsonStepwiseRegressionStepwise1995}
Thompson, Bruce. 1995. Stepwise regression and stepwise discriminant
analysis need not apply here: A guidelines editorial. \emph{Educational
and Psychological Measurement}. SAGE 55(4). 525--534.
\url{https://doi.org/10.1177/0013164495055004001}.

\bibitem[\citeproctext]{ref-trippelMetadataResearchData2025}
Trippel, Thorsten. 2025. Metadata for research data. In Piotr BaÅ„ski,
Ulrich Heid \& Laura Herzberg (eds.), \emph{Harmonizing language data:
Standards for linguistic resources}, 251--279. De Gruyter.
\url{https://www.degruyterbrill.com/document/doi/10.1515/9783112208212-011/html}.

\bibitem[\citeproctext]{ref-vasishthHowEmbraceVariation2021}
Vasishth, Shravan \& Andrew Gelman. 2021. How to embrace variation and
accept uncertainty in linguistic and psycholinguistic data analysis.
\emph{Linguistics} 59(5). 1311--1342.
\url{https://doi.org/10.1515/ling-2019-0051}.

\bibitem[\citeproctext]{ref-w3cwebaccessibilityinitiativewaiImages2022}
(WAI), W3C Web Accessibility Initiative. 2022. Images. \emph{Strategies,
standards, resources to make the Web accessible to people with
disabilities}. \url{https://www.w3.org/WAI/tutorials/images/}.

\bibitem[\citeproctext]{ref-Wickhamggplot2Elegantgraphics2016}
Wickham, Hadley. 2016. \emph{ggplot2: Elegant graphics for data
analysis}. New York: Springer. \url{https://ggplot2.tidyverse.org}.

\bibitem[\citeproctext]{ref-wickhamDataScienceImport2023}
Wickham, Hadley, Mine Ã‡etinkaya-Rundel \& Garrett Grolemund. 2023.
\emph{R for data science: Import, tidy, transform, visualize, and model
data}. 2nd edition. O'Reilly. \url{https://r4ds.hadley.nz/}.

\bibitem[\citeproctext]{ref-TidyMessyData}
Wickham, Hadley, Davis Vaughan \& Maximilian Girlich. Tidy messy data.
\url{https://tidyr.tidyverse.org/}.

\bibitem[\citeproctext]{ref-wielingAnalyzingDynamicPhonetic2018}
Wieling, Martijn. 2018. Analyzing dynamic phonetic data using
generalized additive mixed modeling: A tutorial focusing on articulatory
differences between L1 and L2 speakers of english. \emph{Journal of
Phonetics} 70. 86--116.
\url{https://doi.org/10.1016/j.wocn.2018.03.002}.

\bibitem[\citeproctext]{ref-wilkinsonGrammarGraphics2005}
Wilkinson, Leland. 2005. \emph{The {Grammar} of {Graphics}} (Statistics
and Computing). New York: Springer.
\url{https://doi.org/10.1007/0-387-28695-0}.

\bibitem[\citeproctext]{ref-williamsAssumptionsMultipleRegression2013}
Williams, Matt N., Carlos Alberto GÃ³mez Grajales \& Dason Kurkiewicz.
2013. Assumptions of multiple regression: Correcting two misconceptions.
\emph{Practical Assessment, Research, and Evaluation} 18(11).

\bibitem[\citeproctext]{ref-windhouwerComponentMetadataInfrastructure2022}
Windhouwer, Menzo \& Twan Goosen. 2022. Component metadata
infrastructure. In Darja FiÅ¡er \& Andreas Witt (eds.), \emph{CLARIN: The
infrastructure for language resources}, 191--222. De Gruyter.
\url{https://doi.org/10.1515/9783110767377-008}.

\bibitem[\citeproctext]{ref-winterStatisticsLinguistsIntroduction2020}
Winter, Bodo. 2020. \emph{Statistics for linguists: An introduction
using {R}}. Routledge. \url{https://doi.org/10.4324/9781315165547}.

\bibitem[\citeproctext]{ref-withersMetadataManagementArbil2012}
Withers, Peter. 2012. Metadata management with arbil. In V. D. Arranz,
B. Broeder, M. Gaiffe, M. Gavrilidou \& M. Monachini (eds.),
\emph{Proceedings of the workshop describing LRs with metadata: Towards
flexibility and interoperability in the documentation of LR}, 72--75.
European Language Resources Association (ELRA).
\url{http://www.lrec-conf.org/proceedings/lrec2012/workshops/11.LREC2012\%20Metadata\%20Proceedings.pdf\#page=79}.

\bibitem[\citeproctext]{ref-yeTransferEffectsComputational2022}
Ye, Jiachu, Xiaoyan Lai \& Gary Ka-Wai Wong. 2022. The transfer effects
of computational thinking: A systematic review with meta-analysis and
qualitative synthesis. \emph{Journal of Computer Assisted Learning}
38(6). 1620--1638. \url{https://doi.org/10.1111/jcal.12723}.

\end{CSLReferences}




\end{document}
